[{"type": "text", "text": "Adaptive Sampling for Efficient Softmax Approximation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tavor Z. Baharav\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ryan Kang\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eric and Wendy Schmidt Center Broad Institute Cambridge, MA, 02142   \nbaharav@broadinstitute.org ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science Stanford University Stanford, CA 94305 txryank@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Colin Sullivan\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eric Luxenberg   \nGridmatic   \nCupertino, CA 95014   \neric@gridmatic.com   \nMo Tiwari   \nDepartment of Computer Science   \nStanford University   \nStanford, CA, 94305   \nmotiwari@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "AI Division Software Engineering Institute Pittsburgh, PA 15213 csullivan@sei.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "David Tse ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mert Pilanci ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Electrical Engineering Stanford University Stanford, CA 94305 dntse@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Electrical Engineering Stanford University Stanford, CA 94305 pilanci@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The softmax function is ubiquitous in machine learning and optimization applications. Computing the full softmax evaluation of a matrix-vector product can be computationally expensive in high-dimensional settings. In many applications, however, it is sufficient to calculate only the top few outputs of the softmax function. In this work, we present an algorithm, dubbed AdaptiveSoftmax, that adaptively computes the top $k$ softmax values more efficiently than the full softmax computation, with probabilistic guarantees. We demonstrate the sample efficiency improvements afforded by AdaptiveSoftmax on real and synthetic data to corroborate our theoretical results. AdaptiveSoftmax yields $>10\\mathbf{x}$ gain over full softmax computation on most datasets, yielding up to $30\\mathrm{x}$ improvement for Mistral7B evaluated on the Wikitext dataset. The adaptive method we propose for estimating the partition function (the softmax denominator) is of independent interest and can be used in other applications such as kernel density estimation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The softmax function appears in many different fields and applications. It is often used in multiclass classification problems, as the final operation in a neural network to obtain a probability distribution over classes, in reinforcement learning to obtain a probability distribution over possible actions, and in statistical mechanics to derive various thermodynamic quantities. ", "page_idx": 0}, {"type": "text", "text": "In machine learning applications, the softmax function often appears as the final operation in classification models and in attention layers. Crucially, the softmax function takes a vector of weights as input and returns a probability distribution defined by those weights. Formally, the softmax function for a given temperature parameter $\\beta\\in\\mathbb R$ is defined as: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sigma_{\\beta}(\\mu)_{i}=\\frac{e^{\\beta\\mu_{i}}}{\\sum_{j}e^{\\beta\\mu_{j}}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mu\\in\\mathbb{R}^{n}$ is the input vector of weights, also referred to as the logits. Usually, the logits are the result of a matrix-vector product (e.g., in a fully connected layer where the softmax is used as the nonlinear activation function). The output of the softmax function (Equation 1) is a probability distribution that is a \u201csoft\u201d version of the max operator that is differentiable with respect to the logits. The softmax function can thus be used in gradient-based algorithms as a proxy for the non-differentiable max function. ", "page_idx": 1}, {"type": "text", "text": "Intuitively, the temperature parameter $\\beta$ controls the peakiness of the softmax output. A larger $\\beta$ corresponds to a peakier distribution and a \u201charder\u201d max. The choice of $\\beta=1$ corresponds to the canonical softmax function $\\sigma$ , and the choice of $\\beta=\\infty$ corresponds to the \u201chard\u201d argmax. The denominator of Equation (1), $\\sum_{j}e^{\\beta\\mu_{j}}$ , is called the partition function and is denoted by $Z_{\\beta}$ . ", "page_idx": 1}, {"type": "text", "text": "The softmax function is critical in many popular, recent machine learning applications like large language models (LLMs). However, it can present a computational bottleneck in high-dimensional applications. During the training of neural networks, for example, each training example $x$ requires the computation of the softmax function $\\sigma_{\\beta}(x)$ , the partition function $Z_{\\beta}(x)$ , and their gradients. During inference of these models, the number of possible labels for next-token prediction corresponds to the vocabulary size, which can be in the hundreds of thousands for common languages such as English. As such, there has been significant recent interest in accelerating the computation of the softmax function and its derivatives [37, 14, 15]. ", "page_idx": 1}, {"type": "text", "text": "Key Observations: In many applications, we are only interested in identifying the top few outputs of the softmax function; in these settings, it is unnecessary to compute the smaller entries. This suggests that some of the computation of the full softmax function may be unnecessary and motivates our study. First, we observe that when the input vector to the softmax function is the result of a matrix-vector product, we can approximate the intermediary computation instead of exactly computing it. This, in turn, allows us to approximate the output of the softmax function and converts the problem of computing the softmax function from a computational one to a statistical one. We also note that the softmax output is heavily influenced by the largest input elements which suggests that we can allocate computation adaptively to larger input elements to estimate them with greater certainty. This procedure is inspired by recent work in multi-armed bandits that converts computational problems to statistical ones [4]. ", "page_idx": 1}, {"type": "text", "text": "Outline: We begin this study with a summary of related work in Section 2. In Section 3, we formalize the reduction of computing the softmax function to a statistical estimation problem. In Section 4, we propose the AdaptiveSoftmax algorithm based on this reduction. In the same section, we provide probably approximately correct (PAC) guarantees for AdaptiveSoftmax and prove that it is more efficient than brute force computation. Crucially, AdaptiveSoftmax allocates greater computational resources towards important output values. In Section 5, we demonstrate the empirical advantages of our algorithm in several real-world applications, including in a multiclass classification setting and in large language models. In Section 6, we conclude with a discussion of further applications, potential limitations, and directions for future work. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent work has identified the computational complexity of the softmax function as a significant bottleneck in recent machine learning applications [37]. Well before the attention revolution, [36] proposed methods to accelerate softmax computation via a hierarchical model. In their work, a binary tree representing possible outputs as leaves is used and, at each step, the model must predict which path to traverse to a leaf. For a balanced tree with $n$ leaves, the computational complexity of the softmax function is reduced to $O(\\log n)$ from $O(n)$ , at the expense of $O(n)$ internal classifiers and providing only an approximate output dictated by the quality of the clustering. Google\u2019s word2vec models used Huffman trees instead of vanilla binary trees in a similar tree-based approach [37]. Other approaches include target sampling, noise contrastive estimation and self normalization (summarized [14]), but all of these methods reduce the complexity in terms of the vocabulary size $n$ , rather than by the dimension $d$ . Additionally, our proposed algorithm provides direct PAC guarantees on the original softmax output, instead of approximating the softmax in a sequence of steps without provable accuracy guarantees. Independently, some works have developed fast methods to approximate the softmax gradients during training by using importance sampling over the classes, improving scaling with respect to $n$ [11, 10] leading to a sampled softmax. This is in contrast with AdaptiveSoftmax , which utilizes importance sampling in an orthogonal direction to subsample the features efficiently, enabling gains in $d$ at both train and test time. These sampled softmax methods were later specialized to kernel-based sampling, resulting in provably bounded bias [12, 38]. However, these and other optimized methods [23] typically require prior knowledge about the desired output label frequencies, leaving them susceptible to phenomena like distribution shift between training and inference data, where the frequency distribution changes at the time the model is evaluated. Unlike these approaches, AdaptiveSoftmax does not require auxiliary knowledge, is adaptive on a per instance basis, and provides provable guarantees for the true softmax computation directly rather than a proxy. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our algorithm is inspired by adaptive sampling techniques from the multi-armed bandit literature. Randomized algorithms based on multi-armed bandit algorithms have seen a surge of recent work, due to their ability to provide instance-adaptive guarantees for a variety of problems. This idea was first formalized in the specific case of Monte Carlo Tree Search [26] and later studied in the context of hyper-parameter tuning [31]. Recent work has formalized this approach into the framework of Bandit-Based Monte Carlo Optimization [4], where the computational task is reduced to one of statistical estimation that is solved efficiently with adaptivity. Applications of this framework include finding the medoid of a dataset [5, 7, 41], $k$ -nearest neighbor graph construction [30, 34], Monte Carlo permutation-based multiple testing [46], and an adaptive singular value decomposition (SVD) [24]. Most relevant is the recent work of [8], where the authors provide a general framework for adaptive sampling to approximate function evaluation at unknown but estimable points. This work provides general guarantees, but requires a bound on the Lipschitz factor of the function\u2019s gradients as input and has potentially poor performance on specific function classes due to its generality. ", "page_idx": 2}, {"type": "text", "text": "A sub-problem in our softmax approximation is identifying the index of the largest component; this is equivalent to the Maximum Inner Product Search (MIPS) problem on the preceding matrix-vector product. MIPS is a common problem that has inspired many directions of research [22, 32]. Many of these algorithms focus on specific use cases and place restrictive assumptions on the data (e.g., that all elements of the matrix-vector product are positive), require preprocessing, are not adaptive to the underlying data distribution, or lack PAC guarantees. One large family of MIPS algorithms are based on locality-sensitive hashing (LSH) [19, 2]. In addition to significant preprocessing overhead and practical implementation issues, a shortcoming of these LSH-based approaches is that the maximum dot product is often small compared to the vector norms in high dimensions, which necessitates many hashes and significant storage space (often orders of magnitude more than the data itself). Promising LSH-based algorithms have recently been applied to the problem of softmax computation [1, 15]. These methods focus on intensive preprocessing and work primarily by attaining gains in terms of $n$ . In contrast, AdaptiveSoftmax subsamples matrix-vector products and obtains gains with respect to $d$ . Furthermore, AdaptiveSoftmax provides an instance-adaptive algorithm with no required preprocessing and still has PAC guarantees. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we focus on the problem of identifying the $k$ largest entries of the softmax of an input vector that is the result of a computationally expensive matrix-vector multiplication. Specifically, we analyze the setting where the input vector $\\mu$ is the result of a matrix vector product $A x$ , as is common in the final linear layer of neural networks (such scenarios frequently arise in other machine learning problems as well [4]). Our objective is to design an algorithm that can with probability at least $1-\\delta$ estimate the top $k$ values to multiplicative accuracy $\\varepsilon$ , where $\\varepsilon$ and $\\delta$ are given input parameters. For clarity of exposition, we focus on the case of $k=1$ , i.e., identifying and estimating the largest component. All our theoretical results, however, easily extend to the setting $k>1$ (discussed in Section 4.2). ", "page_idx": 2}, {"type": "text", "text": "Notation: We use $[n]$ to denote the set $\\{1,2,\\ldots,n\\}$ and $\\Vert\\cdot\\Vert$ to denote the vector $\\ell_{2}$ norm, unless otherwise specified. We use $\\Vert\\cdot\\Vert_{\\Psi_{2}}$ to denote the Orlicz norm (i.e., the sub-Gaussianity parameter or variance proxy) of a random variable; this is discussed in greater detail in Appendix A.2 [44]. For matrix $A$ and vector $x$ , we denote the resulting product as $\\mu=A x$ . Assuming for notational simplicity that the arms are in sorted order $\\mu_{1}\\,>\\,\\mu_{2}\\,\\geq\\,.\\,.\\,\\mu_{n}$ , we define the gaps between the entries of $\\mu$ as $\\Delta_{i}\\,=\\,\\mu_{1}\\,-\\,\\mu_{i}$ . We use the convention from the best-arm identification literature that $\\Delta_{1}=\\Delta_{2}$ and assume that $\\Delta_{2}>0$ (this assumption is easily relaxed). Furthermore, we define $\\alpha_{i}=e^{\\beta\\mu_{i}}$ and $\\gamma_{i}=e^{\\beta\\mu_{i}/2}$ , which are proportional to the optimal first (respectively, second) order sampling frequencies; these are discussed further in Section B.2. Finally, we define $\\pmb{p}$ as the softmax output, and $i^{*}$ as its largest entry (assumed to be unique), i.e., ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma_{\\beta}(A x)=p,\\quad i^{*}=\\underset{i\\in[n]}{\\mathrm{argmax}}\\,p_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our goal is to design an algorithm which efficiently outputs the best index $i^{*}$ and an estimate its value where, with probability at least $1-\\delta$ , the best index is correct and the estimated value is within a factor of $\\epsilon$ multiplicative accuracy. Mathematically, defining the algorithm\u2019s outputs as $\\widehat{i^{*}}\\in[n]$ and $\\hat{p}_{i^{*}}\\,\\in\\,[0,1]$ , we define the success events $E_{\\mathrm{id}},E_{\\mathrm{est}}$ where the algorithm identifies th e largest entry, and where it estimates its value to within multiplicative accuracy $\\epsilon$ . We define the algorithm as providing $(\\varepsilon,\\delta)$ -PAC guarantees if these events happen simultaneously with probability at least $1-\\delta$ , with respect to the randomness of the algorithm. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\mathrm{id}}=\\left\\{\\widehat{i^{*}}=i^{*}\\right\\}}\\\\ &{E_{\\mathrm{est}}=\\left\\{(1-\\varepsilon)p_{i^{*}}\\leq\\widehat{p}_{i^{*}}\\leq(1+\\varepsilon)p_{i^{*}}\\right\\}.}\\\\ &{\\mathbb{P}\\left(E_{\\mathrm{id}}\\cap E_{\\mathrm{est}}\\right)\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our objective them becomes to design an algorithm that satisfies Equation (4) and minimizes the requisite sample complexity. ", "page_idx": 3}, {"type": "text", "text": "4 AdaptiveSoftmax Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now introduce the AdaptiveSoftmaxAlgorithm, which approximates the output of the softmax in Algorithm 1. First, AdaptiveSoftmax approximates the softmax normalization constant $Z_{\\beta}$ to a multiplicative accuracy of $\\epsilon/4$ via NormalizationEstimation (Algorithm 2). Next, AdaptiveSoftmax identifies the best arm (or top $k$ arms, depending on the setting) using a standard multi-armed bandit algorithm, BestArmId (Algorithm 3). In our setting, \u201carms\u201d correspond to different rows of $A$ and pulling arm $i$ corresponds to computing a coordinate-wise scalar product $A_{i,j}x_{j}$ for some coordinate $j$ (we provide a more formal overview of the best-arm identification problem and the associated algorithm in Appendix A). Finally, AdaptiveSoftmax estimates the value of the identified best arm (or top $k$ arms) to a multiplicative accuracy of $\\epsilon/4$ by sampling each arm a sufficient number of times via EstimateArm (Algorithm 10). ", "page_idx": 3}, {"type": "text", "text": "We prove $(\\varepsilon,\\delta)$ -PAC guarantees for AdaptiveSoftmax by union-bounding over the error probabilities in each step of Algorithm 1. Our results will show that, with probability at least $1-\\delta$ , AdaptiveSoftmax is able to identify the largest output of the softmax function and estimate its value to multiplicative accuracy \u03f5. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Adaptive Softmax ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: Input: Matrix $A$ , vector $x$ , temperature $\\beta$ , error $\\epsilon$ , failure probability $\\delta$ , variance proxy $\\sigma^{2}$   \n2: Output: $\\hat{p}_{i^{*}}$ and $\\widehat{i}^{*}$ , highest softmax probability and its index   \n3: # Estimate denom inator of softmax   \n4: $\\hat{Z}\\gets:$ NormalizationEstimation $.(A,x,\\beta,\\epsilon/4,\\delta/3,\\sigma^{2})$   \n5: # Compute index of best arm.   \n6: $\\widehat{i^{*}}\\gets\\mathsf{B e s t A r m I d}\\left(A,x,\\delta/3,\\sigma^{2}\\right)$   \n7: # Estimate value of best arm   \n8: $\\hat{\\mu}_{i^{*}}\\gets$ EstimateAr $\\mathfrak{n}(A_{i^{*}},x,\\epsilon/4,\\delta/3)$   \n9: $\\hat{p}_{i^{*}}=\\exp(\\beta\\hat{\\mu}_{i^{*}})/\\hat{Z}$   \n10: return $\\hat{p}_{i^{*}},\\hat{i^{*}}$ ", "page_idx": 3}, {"type": "text", "text": "These inputs are typical in the multi-armed bandit setting, but the variance proxy $\\sigma^{2}$ merits additional discussion. In order for our random-sampling-based approach to succeed, a bound on the rate of ", "page_idx": 3}, {"type": "text", "text": "1: Input: Matrix $A$ , vector $x$ , temperature $\\beta$ , target error \u03f5, failure probability $\\delta$ , variance proxy $\\sigma^{2}$   \n2: Output: $\\hat{Z_{\\beta}}$ , estimate of the partition function ", "page_idx": 4}, {"type": "text", "text": "3: Compute $\\hat{\\mu}_{i}$ using $T_{0}=17\\beta^{2}\\sigma^{2}\\log(6n/\\delta)$ coordinate samples for each arm ", "page_idx": 4}, {"type": "text", "text": "$\\begin{array}{r}{C_{i}\\gets\\sqrt{\\frac{2\\sigma^{2}\\log\\left(\\frac{6n}{\\delta}\\right)}{T_{0}}}}\\end{array}$   \n5: \u03b1\u02c6i \u2190e\u03b2(\u00b5\u02c6i\u2212Ci)   \n6: \u03b3\u02c6 \u2190e\u03b2(\u00b5\u02c6i\u2212Ci)/2   \n7: Sample each arm $n_{i}=\\operatorname*{min}(\\tilde{n}_{i},d)$ times to recompute the estimates $\\hat{\\mu}_{i}$ ", "page_idx": 4}, {"type": "text", "text": ", where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{n}_{i}=\\beta^{2}\\sigma^{2}\\operatorname*{max}\\left(17\\log\\left(\\frac{6n}{\\delta}\\right),\\frac{16\\sqrt{2}\\log\\left(\\frac{6n}{\\delta}\\right)\\left(\\sum_{j}\\widehat{\\gamma}_{j}\\right)\\widehat{\\gamma}_{i}}{\\epsilon\\sum_{j}\\widehat{\\alpha}_{j}},16\\log\\left(\\frac{12}{\\delta}\\right)\\epsilon^{-2}\\frac{\\widehat{\\alpha}_{i}}{\\sum_{j}\\widehat{\\alpha}_{j}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "8: return $\\widehat{Z_{\\beta}}=\\sum_{i}e^{\\beta\\hat{\\mu}_{i}}$ ", "page_idx": 4}, {"type": "text", "text": "concentration of the estimators $\\hat{\\mu_{i}}$ is required; the quantity $\\sigma^{2}$ governs the concentration rate, as we discuss in Appendix A. In practice, such a bound holds very generally, for example as long as $A$ and $x$ have bounded entries. For algorithmic simplicity we utilize the following assumption. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. We assume that we are given a variance proxy bound $\\sigma^{2}$ for the sub-Gaussian parameters of the constructed estimators: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sigma^{2}\\geq\\|A_{i J}x_{J}\\|_{\\Psi_{2}}\\,\\forall\\,i,\\quad\\mathrm{for}\\;J\\sim\\mathrm{Unif}([n]).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We provide theoretical guarantees for AdaptiveSoftmax under Assumption 1. Recall that we defined our optimal first and second order sampling frequencies $\\alpha_{i}=e^{\\beta\\mu_{i}}$ and $\\gamma_{i}=e^{\\beta\\mu_{i}/2}$ (see Appendix B.2). We first show in Proposition 1 that our softmax normalization estimation algorithm (Algorithm 2) obtains the desired guarantees. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. For input $\\varepsilon\\in(0,1/2)$ , $\\delta\\in(0,1)$ , and $\\sigma$ satisfying Assumption $^{\\,l}$ , Algorithm 2 will, with probability at least $1-\\delta$ , estimate $\\begin{array}{r}{Z_{\\beta}=\\sum_{j}e^{\\beta\\mu_{j}}}\\end{array}$ to a multiplicative accuracy of \u03f5. On this success event, Algorithm 2 requires at most ", "page_idx": 4}, {"type": "equation", "text": "$$\nC\\beta^{2}\\sigma^{2}\\left(n\\log\\left(\\frac{n}{\\delta}\\right)+\\log\\left(\\frac{n}{\\delta}\\right)\\left(\\sum_{j}\\gamma_{j}\\right)^{2}\\left(\\epsilon\\sum_{j}\\alpha_{j}\\right)^{-1}+\\log\\left(\\frac{1}{\\delta}\\right)\\varepsilon^{-2}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "samples for some absolute constant $C$ , where non-asymptotic bounds with numerical constants are provided in Appendix $B$ . ", "page_idx": 4}, {"type": "text", "text": "With the sample complexity of Algorithm 2 bounded, the complexity of best arm identification and the cost of estimating the best arm to a target accuracy are readily available from the multi-armed bandit literature. This enables us to state an overall result for AdaptiveSoftmax (Algorithm 1) in the following Theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. For input $\\varepsilon\\in(0,1/2)$ , $\\delta\\in(0,1).$ , and $\\sigma$ satisfying Assumption 1, Algorithm 1 identifies the largest component in $\\sigma_{\\beta}(A x)$ and estimates its value to a multiplicative accuracy of $\\epsilon$ with probability at least $1-\\delta$ , as in (4). On this success event, the algorithm uses $T$ samples where ", "page_idx": 4}, {"type": "equation", "text": "$$\nT\\leq C\\sigma^{2}\\left(\\beta^{2}n\\log\\left(\\frac{n}{\\delta}\\right)+\\sum_{i=1}^{n}\\frac{\\log\\left(\\frac{n\\log d}{\\delta}\\right)}{\\Delta_{i}^{2}}+\\frac{\\beta^{2}\\log\\left(\\frac{n}{\\delta}\\right)\\left(\\sum_{j}\\gamma_{j}\\right)^{2}}{\\epsilon\\sum_{j}\\alpha_{j}}+\\frac{\\beta^{2}\\log(1/\\delta)}{\\varepsilon^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some absolute constant $C$ . Tighter bounds with non-asymptotic numerical constants are provided in Appendix $B$ . ", "page_idx": 4}, {"type": "text", "text": "The proofs of these two results are detailed in Appendix B; we provide some intuition and brief sketches of the proofs here. ", "page_idx": 4}, {"type": "text", "text": "For Proposition 1, we first show that we can estimate the quantities $\\{\\alpha_{i}\\},\\{\\gamma_{i}\\}$ , to constant multiplicative error with high probability. This allows us to construct a sampling scheme based off of the asymptotically optimal sampling frequencies, and guarantee that each arm is sampled at least half of what this asymptotically optimal frequency requires. Then, sampling each arm $i$ enough so that the first order Taylor expansion of $e^{\\beta\\hat{\\mu}_{i}}$ is sufficiently accurate, we can sample further according to these determined frequencies to guarantee PAC estimation. This is an improved and specialized modification of [8] that exploits the structure of the softmax function to remove the assumption of Lipschitz gradients and yield improved sample complexity (this is discussed further in Appendix B.3). ", "page_idx": 5}, {"type": "text", "text": "Next, we utilize a classical best-arm identification algorithm to identify the best arm with high probability, leveraging standard results in Bandit-Based Monte Carlo Optimization [6]. Finally, we sample the identified best arm enough times to estimate its value to multiplicative accuracy $\\epsilon/4$ with high probability. By union bounding over these error probabilities, we achieve the desired PAC guarantees. ", "page_idx": 5}, {"type": "text", "text": "4.1 Interpreting Theoretical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now simplify and further interpret the sample complexity results in Theorem 1. First, note that the $\\varepsilon^{-2}$ dependence exhibited by NormalizationEstimation (Algorithm 2) is optimal: it is inherent even in estimating the mean of the best arm to accuracy $\\varepsilon$ . The cost stemming from the second order error, which scales as $\\varepsilon^{-1}$ , is bounded between $\\beta^{2}\\sigma^{2}\\,\\mathrm{\\bar{log}}(n/\\delta)\\varepsilon^{-1}$ and $n\\beta^{2}\\overline{{\\sigma}}^{2}\\log(n/\\delta)\\varepsilon^{-1}$ , where in the case where one arm is much better than the rest this will match the first term. Concretely, we analyze the setting where the minimum gap is $\\Delta$ , i.e. $\\mu_{1}-\\mu_{i}=\\Delta_{i}\\geq\\Delta$ for all $i$ . ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Under the conditions of Theorem $^{\\,I}$ , when the minimum gap is at least $\\Delta$ , Algorithm 1 identifies and provides $(\\varepsilon,\\delta)$ -PAC estimation (Equation (4)) of the largest softmax entry, using ", "page_idx": 5}, {"type": "equation", "text": "$$\nT\\leq C\\left(\\beta^{2}\\sigma^{2}\\log\\left(\\frac{n}{\\delta}\\right)\\left(n+\\frac{\\varepsilon^{-1}n^{2}}{n+e^{\\beta\\Delta}}\\right)+\\beta^{2}\\sigma^{2}\\varepsilon^{-2}\\log(1/\\delta)+n\\sigma^{2}\\log\\left(\\frac{n\\log d}{\\delta}\\right)\\Delta^{-2}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "samples for some universal constant $C$ . In the case where the gap is large $\\textstyle\\Delta\\geq{\\frac{2}{\\beta}}\\log n,$ ), $\\beta$ is not too small, and $d<e^{e^{n}}$ (see Equation (47) for precise statement), this can be simplified to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T\\leq C\\beta^{2}\\sigma^{2}\\left(\\log\\left(\\frac{n}{\\delta}\\right)\\left(n+\\varepsilon^{-1}\\right)+\\varepsilon^{-2}\\log(1/\\delta)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where all sample complexities are for the $1-\\delta$ success event. ", "page_idx": 5}, {"type": "text", "text": "The proof of this upper bound is in Appendix B.1. Note that this directly implies that when the gap is large (i.e. there is a clear largest output element) and $\\varepsilon$ is constant, the sample complexity is nearly linear in $n$ and is upper bounded by $C\\beta^{2}\\sigma^{2}n\\log(n/\\delta)$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Implementation details and extensions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "There are many techniques that we can use to extend and improve AdaptiveSoftmax in practice.   \nWe discuss changes from the written algorithm in detail in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Randomized Hadamard Transformation: The variance-proxy bound $\\sigma^{2}$ of the arms plays a large factor in the AdaptiveSoftmax algorithm\u2019s sample complexity. The underlying sub-Gaussianity parameter of these estimators can be improved using techniques from randomized numerical linear algebra, such as the randomized Hadamard transform [42]. If a small number of entries dramatically increase the variance of the estimator, then the randomized Hadamard transform will make the coordinates more uniform. We provide theoretical guarantees for this approach in Appendix A.3.1. ", "page_idx": 5}, {"type": "text", "text": "Top- $k$ Identification: Extending our algorithmic results from best arm identification (top 1) to identifying multiple components $(\\mathrm{top}\\ k)$ follows directly from existing multi-armed bandit algorithms. Numerous algorithms have been developed for this setting [21], and variants for computational settings have been developed and studied in [6]. For simplicity and clarity, we focused on the top 1 identification in this paper, but the top $k$ extension readily follows. Furthermore, in numerical experiments we observe estimating the normalization constant $Z_{\\beta}$ dominates the sample complexity, and the increase in cost from identifying the top $k$ arms and estimating their values to multiplicative accuracy $\\varepsilon/4$ is minimal. ", "page_idx": 5}, {"type": "text", "text": "Relaxing Assumption of Known sub-Gaussian Parameter $\\sigma^{2}$ : Assumptions regarding known arm concentration parameters are common in multi-armed bandit works and simplify theoretical exposition. These results can naturally be extended in several directions. One simple extension is to the setting where we have a separate sub-Gaussianity parameter $\\sigma_{i}^{2}$ for each arm, i.e., heterogeneous variances. A more practical extension is to the setting where we do not have a bound on the subGaussianity parameter for each arm but know that the arm returns are bounded. In this setting, a common multi-armed bandit approach is to utilize the empirical variance [35]. These approaches are discussed further in [8]. ", "page_idx": 6}, {"type": "text", "text": "Improved Estimators $\\hat{\\mu}$ : Na\u00efvely, the AdaptiveSoftmax algorithm samples coordinates uniformly at random with replacement from the set of coordinates $\\{1,\\ldots,d\\}$ to estimate each $\\textstyle\\sum_{j}A_{i j}x_{j}$ . This procedure can be improved in several ways. For example, we may utilize importance sampling and sample each coordinate with probability $z_{j}\\propto|x_{j}|$ . Furthermore, we can sample coordinates without replacement; this is known to yield tighter confidence intervals than sampling with replacement [9]. We can combine these techniques and compute the effective variance as in [18]. Sampling without replacement can be achieved in a computationally efficient manner via Gumbel sampling [27]. We discuss these details further in Appendix A.3; these details may be of independent interest. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate the empirical advantages of AdaptiveSoftmax over the brute-force softmax computation in terms of sample complexity. All of our results are reproducible via a 1-line script, publicly available on GitHub at github.com/ThrunGroup/adaptiveSoftmax. ", "page_idx": 6}, {"type": "text", "text": "5.1 Complexity on Synthetic Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Crucially, the AdaptiveSoftmax algorithm scales sublinearly in $d$ . More precisely, Corollary 1 implies that, for fixed $\\varepsilon$ and $\\delta$ , the sample complexity of the AdaptiveSoftmax algorithm scales as $O(n\\log n)$ . We now empirically validate this behavior. ", "page_idx": 6}, {"type": "text", "text": "We first run the AdaptiveSoftmax algorithm on two synthetic datasets. In each dataset, we generate $A$ and $x$ with $n=100$ and vary $d$ . ", "page_idx": 6}, {"type": "text", "text": "In the first synthetic dataset, we set $x$ to be a $d$ -dimensional vector of all 1s. We draw each element of $A\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,1)$ and add the vector of all 1s to the first row of A, thereby planting a signal. In expectation, the first row of $A$ will have inner product $d$ with $x$ whereas all other rows will have inner product 0 with $x$ . Furthermore, all arms have expected variance $\\sigma_{i}^{2}$ that scales with $d$ . ", "page_idx": 6}, {"type": "image", "img_path": "XsNA2b8GPz/tmp/c9e06192c93ea67d1bd8f883f8ebc133dbbca3639fe7d33814c65b01546da732.jpg", "img_caption": ["(a) Scaling Baseline: All-Ones Query "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "XsNA2b8GPz/tmp/50e05488bc2f6e93daaa31e3a344788a9b246fcf5498550d90664b35eb2b37ec.jpg", "img_caption": ["(b) Scaling Baseline: Sign Query "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: Sample complexity of the AdaptiveSoftmax algorithm and the brute-force softmax computation on two different synthetic datasets as a function of $d$ . Error bars are obtained from 100 random trials. The sample complexity of the AdaptiveSoftmax algorithm scales with respect to $d$ for (a) but does not for (b), as expected. The average gains for $\\delta=\\bar{10\\%}$ and $\\varepsilon=30\\%$ are $3.953\\times$ for (a) and $29.039\\times$ for (b), increasing with increasing dimension. Confidence intervals are 1std. ", "page_idx": 6}, {"type": "text", "text": "In the second synthetic dataset, we draw each element of $A\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,1)$ and set $x$ to be $|A_{1,:}|$ , the entrywise absolute value of the first row of $A$ . Here, arms have expected variance $\\sigma_{i}^{2}=\\Theta(1)$ . ", "page_idx": 6}, {"type": "text", "text": "Figures 1(a) and 1(b) demonstrates the scaling of the AdaptiveSoftmax Algorithm on each of the two datasets. On the first synthetic dataset, the AdaptiveSoftmax algorithm scales with $d$ because the variance proxies $\\sigma_{i}^{2}$ do. On the second synthetic dataset, the AdaptiveSoftmax algorithm does not exhibit significant scaling with $d$ . On both datasets, the AdaptiveSoftmax algorithm significantly outperforms the na\u00efve brute-force computation of the softmax function. ", "page_idx": 7}, {"type": "text", "text": "5.2 Multinomial Logistic Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Multinomial logistic regression (MNL) is a form of multiclass classification in which the final operation performed by the classifier is of the form: ", "page_idx": 7}, {"type": "equation", "text": "$$\nP(y=c)=\\frac{e^{\\beta(w_{c}^{\\top}h(x))}}{\\sum_{c^{\\prime}=1}^{C}e^{\\beta(w_{c^{\\prime}}^{\\top}h(x))}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "i.e., the probabilities that datapoint $x$ belongs to each class $c$ is given by the softmax applied to the vector $W h(x)$ , where $W$ is the matrix containing rows $w_{1},\\dots,w_{c}$ and $h(x)$ is a latent representation of $x$ (i.e., the forward pass of some neural network on $x$ ). ", "page_idx": 7}, {"type": "text", "text": "The multinomial logistic regression is naturally amenable to accelerated softmax computation in Equation (5). In many real-world settings, both the number of classes $C$ and the dimension of the latent representation $h(x)$ (and therefore the dimensionality of each $w_{c}$ ) can be very large, motivating the usage of AdaptiveSoftmax to identify and estimate the probability of the most likely class. However, the application of AdaptiveSoftmax extends far past vanilla MNL. For instance, the final layer of any neural network classifier utilizing softmax can also be viewed as an MNL problem. We now provide several such practical settings for which we demonstrate the benefits of applying the AdaptiveSoftmax algorithm. ", "page_idx": 7}, {"type": "text", "text": "5.3 AdaptiveSoftmax Performance on Real Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now demonstrate the performance of the AdaptiveSoftmax algorithm on several real-world datasets. For each setting, we provide the sample complexity gain relative to the sample complexity of the brute-force, na\u00efve softmax computation sample complexity nd. We also provide the success rate of our algorithm in each setting, i.e., the proportion of times the AdaptiveSoftmax algorithm correctly identifies the maximum likelihood output (i.e. $\\hat{i}^{\\star}=i^{\\star}$ ) and estimates its probability $p_{i}{\\star}$ within a multiplicative error of $\\varepsilon=30\\%$ . ", "page_idx": 7}, {"type": "text", "text": "5.3.1 Application to CNNs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider the application of AdaptiveSoftmax to CNN classifiers on two distinct image classification datasets: ", "page_idx": 7}, {"type": "text", "text": "1. The MNIST dataset, containing black and white images of handwritten digits as input and ten output classes representing all ten possible digits.   \n2. The EuroSAT dataset, containing RGB satellite imagery as input and ten output classes, representing possible land types (e.g., river, residential, etc) ", "page_idx": 7}, {"type": "text", "text": "On both of these datasets and for distinct architectures, we show that AdaptiveSoftmax provides a drastic improvement in sample efficiency. ", "page_idx": 7}, {"type": "text", "text": "MNIST For the MNIST dataset, we train a shallow CNN from scratch with two convolutional blocks (Conv2d, ReLu, MaxPool, BatchNorm). This model achieves over $99\\%$ accuracy on the test set. The matrix $A$ is obtained by extracting the weight matrix of the model\u2019s final linear layer. The vector $x$ is extracted as the output of the final hidden layer (the layer before the final linear layer) constructed by passing the MNIST image through the trained model and flattening the result. The dimensionality of $x$ is adjusted by changing the number of output channels of the convolution blocks. The sample complexity of our algorithm is measured by running the algorithm on 1000 different images in test set with same matrix $A$ . The empirical error rate $\\delta$ is calculated as the fraction of experiments where the adaptive algorithm fails to identify the same class, or fails to estimate the probability to accuracy $\\epsilon$ , as assigned by exact computation. ", "page_idx": 7}, {"type": "text", "text": "EuroSAT We also utilize a larger pre-trained CNN classifier fine-tuned on the EuroSAT dataset, to show that AdaptiveSoftmax works with larger more sophisticated CNNs. Specifically, we freeze all convolution blocks of VGG-19 (pretrained on ImageNet) and changed the final output dimension to 10 classes for EuroSAT without freezing the weights. The resulting model achieves $92\\%$ accuracy on the test set. As before, the matrix $A$ can be extracted from the weights of the final linear layer and the vector $x$ represents the final hidden layer activations. The empirical error rate $\\delta$ is calculated in the same manner as for MNIST. ", "page_idx": 8}, {"type": "table", "img_path": "XsNA2b8GPz/tmp/5cbc859fd64b080c2cdcd35564bbda5f94339bd4c918da5e00f0cfae8c25fe8a.jpg", "table_caption": ["Table 1: Performance improvement and success rate afforded by AdaptiveSoftmax for multinomial logistic regression on two different real-world datasets. We used a total of $q=800$ test queries to measure success rate. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3.2 Application to LLMs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We also apply the AdaptiveSoftmax algorithm to LLMs using HuggingFace\u2019s AutoModelForCausalLM module for the task-generation task [45]. The matrix $A$ is the lm-head layer for each model, and the queries $x$ are the final hidden states of the model that is extracted by running a forward pass of the model on the given dataset with a window moving at a certain stride. The context window and stride is modified to generate a desired number of queries. ", "page_idx": 8}, {"type": "table", "img_path": "XsNA2b8GPz/tmp/afbd697c89c6d59ff7bd071aaa3fc3742c11d50f80de64c88ede939aed31c6d9.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance improvement and success rate afforded by AdaptiveSoftmax for LLM inference (improvement for final softmax layer). Experiment details in Section 5.3.2. We used $q=1000$ unseen test queries to measure $\\delta$ -accuracy. "], "page_idx": 8}, {"type": "text", "text": "Our matrix $A$ is the extracted lm-head from HuggingFace\u2019s AutoModelForCausalLM for the four models: GPT-2 $(n\\,=\\,50257,d\\,=\\,768)$ , Llama3-7B $\\mathit{\\Omega}_{\\mathit{\\Omega}}n\\mathrm{\\Omega}=\\mathrm{\\Omega}128256$ , $d\\,=\\,40961$ ), Mistral7B $\\left[n\\,=\\right.$ 32000, $d=4096$ ), and Gemma7B $(n=256000,d=3072)$ ). Our task is task-generation, and we generate our queries $x$ by using two datasets (Wikitext and Penn Treebank) with a sliding window of certain stride. Stride and context window is set to get $q=1000$ number of queries. Constants and confidence intervals given by theory are empirically quite loose, so we tuned algorithm parameters (constant coefficients for stage length and confidence interval width) on initial training data, described in Appendix C. An aggressive tuning strategy was undertaken in order to demonstrate the potential gains in sample complexity provided by AdaptiveSoftmax . Specifically, constant multiples were applied to variance estimate within Algorithm 3 and Algorithm 2. Due to the limited sample set, this approach occasionally overoptimized the constants on training data, yielding lower success rates than targeted. However, from the results, it is clear that this target parameter still provides users sufficient control over the tradeoff between true success rate and sample complexity. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion, Limitations, and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we proposed a theoretically novel and practically efficient algorithm for approximating the softmax function. We provided theoretical guarantees on the accuracy of our approximation and demonstrated that, with fewer samples than exact computation, we can approximate the softmax function to the desired accuracy. We further demonstrated the viability of our proposed algorithm in two real-world settings, multinomial logistic regression and LLM inference. ", "page_idx": 8}, {"type": "text", "text": "A potential limitation of our proposed algorithm is that it is most beneficial when the inner dimension of the matrix vector product is high dimensional; its beneftis over exact computation are more modest when the inner dimension is small. In particular, the exact computation of the matrix-vector product preceding a softmax operation is usually performed efficiently using BLAS (Basic Linear Algebra Subroutines, which are highly optimized). Adaptivity at its core is inherently sequential, whereas BLAS operations take advantage of batch computation. In this work we proposed minimally adaptive algorithms, with only a logarithmic number of rounds of adaptivity, but there are important directions of future work to realize these theoretical gains in practice. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations: Theoretical sample complexity bounds are useful for understanding the fundamental properties of an algorithm, but in practice, wall-clock time is often the metric of interest. Many of the steps in our algorithm can be batched and made BLAS efficient, yielding comparable wall clock times to brute force computation. However, in general adaptivity is the opposite of batching, as can be seen when we modify our algorithm to adapt to arm specific variances. In this case, we must sample each arm individually, as the number of samples required for each arm is different. This is a trade-off between adaptivity and wall-clock time, and in practice, the choice of which to prioritize depends on the specific application (energy efficiency, computational resources, etc.). There are also possible theoretical analyses, where we can e.g. create batches of arms with similar empirical variance and sample all arms within a batch together, leading to a trade-off between adaptivity and batched computational efficiency. Additionally, in large language models, the final softmax layer is often not a computationally significant step, so while such a method may greatly accelerate multinomial logistic regression, more work may be required to have this accelerate LLMs. ", "page_idx": 9}, {"type": "text", "text": "Given the ubiquity of the softmax function in today\u2019s machine learning workflows, we hope that our algorithm will help pave the way for an optimized adaptive softmax that can accelerate a wide class of machine learning models. An interesting direction of future work is trying to combine this multi-armed bandit approach with LSH [15] to obtain (for the attention case) subquadratic complexity in $n$ , and sublinear complexity in $d$ . The adaptive method we propose for estimating the normalizing constant of the softmax function is of independent interest, and holds potential for applications in kernel density estimation and other machine learning tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Mert Pilanci was supported in part by the National Science Foundation (NSF) under Grant DMS2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. Tavor Baharav was supported by funding from the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Josh Alman and Zhao Song. \u201cFast attention requires bounded entries\u201d. In: Advances in Neural Information Processing Systems 36 (2024).   \n[2] Alexandr Andoni et al. \u201cPractical and optimal LSH for angular distance\u201d. In: Advances in neural information processing systems 28 (2015).   \n[3] Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. \u201cExploration\u2013exploitation tradeoff using variance estimates in multi-armed bandits\u201d. In: Theoretical Computer Science 410.19 (2009), pp. 1876\u20131902.   \n[4] Vivek Bagaria et al. \u201cBandit-Based Monte Carlo Optimization for Nearest Neighbors\u201d. In: IEEE Journal on Selected Areas in Information Theory (2021).   \n[5] Vivek Bagaria et al. \u201cMedoids in almost-linear time via multi-armed bandits\u201d. In: International Conference on Artificial Intelligence and Statistics (2018), pp. 500\u2013509.   \n[6] Tavor Baharav and Tze Leung Lai. \u201cAdaptive Data Depth via Multi-Armed Bandits\u201d. In: Journal of Machine Learning Research 24.155 (2023), pp. 1\u201329.   \n[7] Tavor Baharav and David Tse. \u201cUltra fast medoid identification via correlated sequential halving\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[8] Tavor Baharav et al. \u201cApproximate Function Evaluation via Multi-Armed Bandits\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2022, pp. 108\u2013135.   \n[9] R\u00e9mi Bardenet and Odalric-Ambrym Maillard. \u201cConcentration inequalities for sampling without replacement\u201d. In: Bernoulli 21.3 (2015), pp. 1361\u20131385.   \n[10] Yoshua Bengio and Jean-S\u00e9bastien Sen\u00e9cal. \u201cAdaptive importance sampling to accelerate training of a neural probabilistic language model\u201d. In: IEEE Transactions on Neural Networks 19.4 (2008), pp. 713\u2013722.   \n[11] Yoshua Bengio and Jean-S\u00e9bastien Sen\u00e9cal. \u201cQuick training of probabilistic neural nets by importance sampling\u201d. In: International Workshop on Artificial Intelligence and Statistics. PMLR. 2003, pp. 17\u201324.   \n[12] Guy Blanc and Steffen Rendle. \u201cAdaptive sampled softmax with kernel based sampling\u201d. In: International conference on machine learning. PMLR. 2018, pp. 590\u2013599.   \n[13] S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. \u201cRegret analysis of stochastic and nonstochastic multi-armed bandit problems\u201d. In: Foundations and Trends\u00ae in Machine Learning 5.1 (2012), pp. 1\u2013122.   \n[14] Wenlin Chen, David Grangier, and Michael Auli. \u201cStrategies for Training Large Vocabulary Neural Language Models\u201d. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016, pp. 1975\u20131985.   \n[15] Insu Han et al. \u201cHyperAttention: Long-context Attention in Near-Linear Time\u201d. In: arXiv preprint arXiv:2310.05869 (2023).   \n[16] Eshcar Hillel et al. \u201cDistributed exploration in multi-armed bandits\u201d. In: Advances in Neural Information Processing Systems 26 (2013).   \n[17] Ari Holtzman et al. \u201cThe curious case of neural text degeneration\u201d. In: arXiv preprint arXiv:1904.09751 (2019).   \n[18] Daniel G Horvitz and Donovan J Thompson. \u201cA generalization of sampling without replacement from a finite universe\u201d. In: Journal of the American statistical Association 47.260 (1952), pp. 663\u2013685.   \n[19] Piotr Indyk and Rajeev Motwani. \u201cApproximate nearest neighbors: towards removing the curse of dimensionality\u201d. In: Proceedings of the thirtieth annual ACM symposium on Theory of computing. 1998, pp. 604\u2013613.   \n[20] Andrei Ivanov et al. \u201cData movement is all you need: A case study on optimizing transformers\u201d. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711\u2013732.   \n[21] Kevin Jamieson and Robert Nowak. \u201cBest-arm identification algorithms for multi-armed bandits in the fixed confidence setting\u201d. In: Annual Conference on Information Sciences and Systems. 2014, pp. 1\u20136.   \n[22] Herv\u00e9 J\u00e9gou et al. \u201cSearching in one billion vectors: re-rank with source coding\u201d. In: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2011, pp. 861\u2013864.   \n[23] Armand Joulin et al. \u201cEfficient softmax approximation for GPUs\u201d. In: International conference on machine learning. PMLR. 2017, pp. 1302\u20131310.   \n[24] Govinda Kamath, Tavor Baharav, and Ilan Shomorony. \u201cAdaptive learning of rank-one models for efficient pairwise sequence alignment\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 7513\u20137525.   \n[25] Zohar Karnin, Tomer Koren, and Oren Somekh. \u201cAlmost optimal exploration in multi-armed bandits\u201d. In: International Conference on Machine Learning. 2013, pp. 1238\u20131246.   \n[26] Levente Kocsis and Csaba Szepesv\u00e1ri. \u201cBandit based monte-carlo planning\u201d. In: European conference on machine learning. Springer. 2006, pp. 282\u2013293.   \n[27] Wouter Kool, Herke Van Hoof, and Max Welling. \u201cStochastic beams and where to find them: The gumbel-top- $\\cdot\\mathbf{k}$ trick for sampling sequences without replacement\u201d. In: International Conference on Machine Learning. PMLR. 2019, pp. 3499\u20133508.   \n[28] Tze Leung Lai and Herbert Robbins. \u201cAsymptotically efficient adaptive allocation rules\u201d. In: Advances in applied mathematics 6.1 (1985), pp. 4\u201322.   \n[29] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[30] Daniel LeJeune, Reinhard Heckel, and Richard Baraniuk. \u201cAdaptive estimation for approximate $k$ -nearest-neighbor computations\u201d. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 3099\u20133107.   \n[31] Lisha Li et al. \u201cHyperband: A novel bandit-based approach to hyperparameter optimization\u201d. In: The Journal of Machine Learning Research 18.1 (2017), pp. 6765\u20136816.   \n[32] Stephan S Lorenzen and Ninh Pham. \u201cRevisiting wedge sampling for budgeted maximum inner product search\u201d. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer. 2020, pp. 439\u2013455.   \n[33] Pinyan Lu, Chao Tao, and Xiaojin Zhang. \u201cVariance-dependent best arm identification\u201d. In: Uncertainty in Artificial Intelligence. PMLR. 2021, pp. 1120\u20131129.   \n[34] Blake Mason, Ardhendu Tripathy, and Robert Nowak. \u201cNearest neighbor search under uncertainty\u201d. In: Uncertainty in Artificial Intelligence. PMLR. 2021, pp. 1777\u20131786.   \n[35] Andreas Maurer and Massimiliano Pontil. \u201cEmpirical bernstein bounds and sample variance penalization\u201d. In: arXiv preprint arXiv:0907.3740 (2009).   \n[36] Frederic Morin and Yoshua Bengio. \u201cHierarchical probabilistic neural network language model\u201d. In: International workshop on artificial intelligence and statistics. PMLR. 2005, pp. 246\u2013252.   \n[37] Kezban Dilek Onal et al. \u201cNeural information retrieval: At the end of the early years\u201d. In: Information Retrieval Journal 21 (2018), pp. 111\u2013182.   \n[38] Ankit Singh Rawat et al. \u201cSampled softmax with random fourier features\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[39] Max Simchowitz, Kevin Jamieson, and Benjamin Recht. \u201cThe simulator: Understanding adaptive sampling in the moderate-confidence regime\u201d. In: Conference on Learning Theory. PMLR. 2017, pp. 1794\u20131834.   \n[40] Aleksandrs Slivkins et al. \u201cIntroduction to multi-armed bandits\u201d. In: Foundations and Trends\u00ae in Machine Learning 12.1-2 (2019), pp. 1\u2013286.   \n[41] Mo Tiwari et al. \u201cBanditPAM: Almost linear time k-medoids clustering via multi-armed bandits\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 10211\u201310222.   \n[42] Joel A Tropp. \u201cImproved analysis of the subsampled randomized Hadamard transform\u201d. In: Advances in Adaptive Data Analysis 3 (2011), pp. 115\u2013126.   \n[43] Tim Vieira. Gumbel-max trick and weighted reservoir sampling. 2014. URL: http : / / timvieira . github . io / blog / post / 2014 / 08 / 01 / gumbel - max - trick - and - weighted-reservoir-sampling/.   \n[44] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Vol. 48. Cambridge university press, 2019.   \n[45] Thomas Wolf et al. \u201cHuggingface\u2019s transformers: State-of-the-art natural language processing\u201d. In: arXiv preprint arXiv:1910.03771 (2019).   \n[46] Martin Zhang, James Zou, and David Tse. \u201cAdaptive monte carlo multiple testing via multiarmed bandits\u201d. In: International Conference on Machine Learning. Proceedings of Machine Learning Research. 2019, pp. 7512\u20137522. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Bandit preliminaries ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To make this work accessible to a broad audience, we provide a self contained introduction to the multi-armed bandit setting. ", "page_idx": 12}, {"type": "text", "text": "A.1 Best arm identification ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We consider a stochastic multi-armed bandit problem [13, 40, 29] with $n$ arms (distributions), where each arm $i$ has an unknown mean reward $\\mu_{i}$ . At each time step $t$ , the algorithm selects an arm $I_{t}\\in[n]$ and receives a reward $X_{I_{t},t}$ drawn from the distribution of arm $I_{t}$ . Early work in the multi-armed bandit literature focused on the regret minimization setting, where the goal is to maximize the cumulative reward (sum of arm pulls observed so far), motivated by applications such as online advertising and gambling [28]. Recent work has seen increased interest in the best-arm identification setting, where the goal is to identify the arm with the highest mean reward with high probability, motivated by applications such as clinical trials. Significant research has been devoted to obtaining optimal logarithmic factors, but for the sake of clarity we highlight here a simpler and empirically well performing algorithm, multi-round $\\epsilon$ -arm from [25]. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 3 BestArmId (modification of best-arm identification from [25]) Input: $n$ arms, error probability $\\delta$ , variance proxy $\\sigma^{2}$ Output: Best arm $i^{\\star}$ with probability at least $1-\\delta$ $\\begin{array}{l}{S_{0}\\stackrel{\\circ}{\\leftarrow}[n]}\\\\ {r\\leftarrow0}\\\\ {t_{0}\\leftarrow0}\\end{array}$ while $|S_{r}|>1$ do $r\\gets r+1$ \u03f5r \u21902\u2212r $\\dot{t_{r}}\\gets\\lceil8\\sigma^{2}\\epsilon_{r}^{-2}\\log(4n r^{2}/\\delta)\\rceil$ for all arms $i\\in S_{r-1}$ do Pull arm $\\textit{i t}_{r}-t_{r-1}$ times and observe rewards $X_{i,t_{r-1}+1},\\ldots,X_{i,t_{r}}$ $\\begin{array}{r l}&{\\hat{\\mu}_{i,r}\\leftarrow\\frac{1}{t_{r}}\\sum_{s=1}^{t_{r}}X_{i,s}}\\\\ &{C_{i,r}\\leftarrow\\sqrt{2\\sigma^{2}\\log(4n r^{2}/\\delta)/t_{r}}}\\end{array}$ $\\triangleright$ Update mean estimates $\\triangleright$ Compute confidence interval width end for Set $S_{r}\\gets\\{i\\in S_{r-1}:\\hat{\\mu}_{i,r}+C_{i,r}\\geq\\operatorname*{max}_{j\\in S_{r-1}}\\hat{\\mu}_{j,r}-C_{j,r}\\}$ \u25b7Filter \u201cbad\u201d arms end while return $i^{*}$ , the only element in $S_{r}$ $\\triangleright$ We assume $i^{*}$ is unique (this assumption is easily relaxed) ", "page_idx": 12}, {"type": "text", "text": "The algorithm proceeds in rounds, maintaining a set of arms $S_{r}$ that are still in contention for being the best arm. At each round $r$ the algorithm pulls each surviving arm such that we can construct a high probability confidence interval of width $\\epsilon_{r}/2$ around the mean of each arm. Then, any arms whose empirical mean plus confidence interval is less than the maximum empirical mean minus confidence interval are eliminated. If this is the case, then that arms mean is with high probability less than the maximum mean, and so it is eliminated from contention. This preserves the best arm with high probability, and the algorithm terminates when only one arm remains (this best arm). ", "page_idx": 12}, {"type": "text", "text": "A.2 Sub-gaussian random variables ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Following the exposition of [44], for a strictly increasing convex function $\\psi:\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ where $\\psi(0)=0$ , the $\\psi$ -Orlicz norm of a random variable $X$ is defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|X\\|_{\\psi}\\triangleq\\operatorname*{inf}\\left\\{t>0|\\mathbb{E}\\left[\\psi\\left(t^{-1}|X||\\right)\\right]\\leq1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "$\\|X\\|_{\\psi}$ is infinite if the expectation $\\mathbb{E}\\left[\\psi\\left(t^{-1}|X||\\right)\\right]$ does not exist for any finite $t$ . The sub-Gaussian parameter of a random variable is defined as the $\\psi_{2}$ -Orlicz norm, where $\\psi_{2}(u)=e^{u^{2}}-1$ . ", "page_idx": 12}, {"type": "text", "text": "Standard results (Hoeffding\u2019s lemma) provide that for a random variable $X$ such that $a\\leq X\\leq b$ almost surely, that $\\begin{array}{r}{\\|X\\|_{\\psi_{2}}\\leq\\frac{(b-a)^{2}}{4}}\\end{array}$ . ", "page_idx": 12}, {"type": "image", "img_path": "XsNA2b8GPz/tmp/6f7b24dca296a00d3d56aed04e6e600628d1ed157f677bd32c1a12152797043f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 2: Distribution of arm pulls for the best, middle, and worst arms for a random query. The Gaussian fti plotted for each arm is computed using the empirical variance of the given arm, and can be seen to closely match the empirical distribution, indicating that the arm pull distributions are well approximated by a Gaussian. This merits the assumption of sub-Gaussianity. Arm pulls represent importance weighted samples based on the magnitude of the query vector, so for an arm $i$ and sample $j$ , the value is $A_{i,j}*\\operatorname{sgn}(x_{j})*d$ . $\\sigma$ is computed across all samples for an arm, and windows are truncated at the lowest and highest ends of the $3\\sigma$ ranges across arms for viewing clarity. ", "page_idx": 13}, {"type": "text", "text": "Hoeffding\u2019s inequality provides a useful concentration bound, where for $X$ with $\\|X\\|_{\\psi_{2}}\\leq\\sigma^{2}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|X-\\mathbb{E}[X]|\\ge t\\right)\\le\\exp\\left(-\\frac{t^{2}}{2\\sigma^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2.1 Sub-gaussianity in practice ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The assumption of sub-Gaussianity is the only assumption that we make in this paper. It is one of the weakest assumptions possible (does not assume that the arms are Bernoulli or Gaussian), and is a common assumption in the multi-armed bandit and adaptive computation literature [4]. Unfortunately, without such an assumption, no nontrivial results are possible; consider the case where we do not have preprocessing access to $A$ , the vector $x$ is all 1s, and $A$ is all 1s except for a randomly selected entry which has value 2. In this case, any algorithm for PAC computation of softmax $(A x)$ with $\\delta=1-1/n$ (even just identification of the largest entry of $A x$ ) requires $\\Omega(n d)$ samples. More practically though, these vectors are the result of a machine learning pipeline, and not of adversarial construction. As shown by our simulations, this worst case scenario never occurs in practice, and arm pulls are generally well approximated by a Gaussian (see Figure 2). Additionally, note that for any fixed problem instance, all arm pulls are bounded, and are thus sub-Gaussian. ", "page_idx": 13}, {"type": "text", "text": "A.3 Improved estimators ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To provide theoretical guarantees in multi-armed bandit problems, stringent assumptions are often required, e.g. Assumption 1. This is so that we can provide high probability guarantees on the concentration of the estimator constructed as the empirical mean of the observed samples. Often, these assumptions are phrased as either that random variables corresponding to arm pulls are bounded in [0, 1] a.s., or that they are $\\sigma^{2}$ sub-Gaussian, with a known bound on $\\sigma^{2}$ . Such analyses have been generalized to bounded random variables with a known bound, where the algorithm is able to adapt to the unknown variance [33, 35]. In this work, as is often done to make multi-armed bandit algorithms more performant [4], we instead directly use the empirical variance of an estimator as its sub-Gaussian parameter. In the Gaussian case, a random variable\u2019s sub-Gaussian parameter $\\sigma^{2}$ matches its variance: as our arm pulls are constructed as the sum of many $(d)$ terms, which can be thought of as weakly dependent, our arm pulls can be thought of as Gaussian random variables with variance $\\sigma^{2}$ . Since in practice, we do not have a good bound on the magnitude of these arm pulls, we directly use Hoeffding\u2019s concentration inequalities [44] with the empirical variance $\\hat{\\sigma}^{2}$ , as opposed to an empirical Bernstein type concentration inequality [35]. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A.3.1 Randomized Hadamard Transform ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We discuss applying a randomized Hadamard transform to reduce the sub-Gaussian parameter of our estimators. Define the rotation matrix $\\begin{array}{r}{R=\\frac{1}{\\sqrt{d}}D H}\\end{array}$ , where $D$ is a diagonal matrix with diagonal entries equiprobably $\\pm1$ . $H$ is a Hadamard matrix $\\mathit{d}$ must be a power of 2, 0 padded if necessary). Then, we have that applying the transform $R$ to $A$ and $R^{\\top}$ to $x$ yields arms with better sub-Gaussian parameters. Concretely, define $Z=A R$ and $y=R^{\\top}x$ . Analyzing $y$ first, we have that no entry of $y$ is too large, as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\|y\\|_{\\infty}\\ge t)\\le\\displaystyle\\sum_{i=1}^{n}\\mathbb{P}\\left(\\left|R_{i}^{\\top}x\\right|\\ge t\\right)}\\\\ &{\\qquad\\qquad=n\\mathbb{P}\\left(\\displaystyle\\sum_{j=1}^{d}\\xi_{j}d^{-1/2}x_{j}\\ge t\\right)}\\\\ &{\\qquad\\qquad\\le2n\\exp\\left(-\\displaystyle\\frac{2t^{2}}{4\\sum_{j=1}^{d}\\frac{1}{d}x_{j}^{2}}\\right)}\\\\ &{\\qquad\\qquad=2n\\exp\\left(-\\displaystyle\\frac{t^{2}}{2\\frac{1}{d}\\|x\\|_{2}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first inequality is a union bound over the $n$ points and plugs in for $y\\,=\\,R^{\\top}x$ . The second equality uses the fact that the Hadamard matrix multiplied by the random diagonal $\\pm1$ matrix $D$ makes $R_{i}$ i.i.d. $\\pm1$ , and we use $\\xi_{j}$ to denote these Rademacher i.i.d. $\\pm1$ random variables. Next we use Hoeffding\u2019s inequality. Finally, we simplify. ", "page_idx": 14}, {"type": "text", "text": "Thus, with probability at least $\\begin{array}{r}{1-\\delta,\\|y\\|_{\\infty}<\\|x\\|_{2}\\sqrt{\\frac{2\\log(2n/\\delta)}{d}}.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "A similar argument can be made for $Z\\,=\\,A R$ , showing that each entry in the $i$ -th row is upper bounded by $\\|A_{i}\\|_{2}\\sqrt{\\frac{2\\log(2n d/\\delta)}{d}}$ , holding simultaneously for all $i,j$ . ", "page_idx": 14}, {"type": "text", "text": "Since $A x=Z y$ , we can use bandits to approximate $Z y$ instead of $A x$ . With the above analysis, we have a bound on the maximum entry of $Z_{i J}y_{J}$ , giving us a bound on the sub-Gaussian parameter of each arm with probability $\\geq1-\\delta$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i j}|Z_{i j}y_{j}|=\\operatorname*{max}_{i}{\\|A_{i}\\|_{2}\\|x\\|_{2}}{\\frac{2\\log(4n d/\\delta)}{d}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Whereas before: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i j}|A_{i j}x_{j}|=\\operatorname*{max}_{i}\\|A_{i}\\|_{\\infty}\\|x\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In practice, we did not observe that this transformation yielded much improved variance, as opposed to simply importance sampling. Thus, we do not utilize this in our main algorithm. ", "page_idx": 14}, {"type": "text", "text": "A.3.2 Importance sampling ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Instead of naively sampling each coordinate uniformly, we can construct improved estimators using importance sampling. Concretely, consider the unbiased estimator $Z$ where $\\begin{array}{r}{\\mathbb{P}(Z=\\frac{1}{p_{j}}A_{j}x_{j})=p_{j}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "for some probability distribution $\\{p_{j}\\}$ where $p_{j}\\,>\\,0$ for all $j$ and $\\textstyle\\sum_{j}p_{j}=1$ . Now, we are left with the design choice of how to construct $\\{p_{j}\\}$ ; naively, this is $1/d$ . Unpacking the variance of our estimator, we see: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Var}(Z)=\\sum_{j}{\\frac{1}{p_{j}}}\\left(A_{j}x_{j}\\right)^{2}-\\mu^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider the case where $A_{j}\\stackrel{\\mathrm{i.i.d.}}{\\sim}Q$ for some distribution $Q$ with $\\mathbb{E}[Q^{2}]=\\lambda^{2}$ (an empirically not unreasonable assumption), where $\\{x_{j}\\}$ are fixed constants. We assume $x$ is known, but $A$ isn\u2019t. Then, we simplify the expected variance with respect to this randomness in $A$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{Var}(Z)\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{j}\\frac{1}{p_{j}}\\left(A_{j}x_{j}\\right)^{2}\\right]-\\mu^{2}}\\\\ &{\\qquad\\qquad=\\lambda^{2}\\displaystyle\\sum_{j}\\frac{x_{j}^{2}}{p_{j}}-\\mu^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Simplifying this for $Z_{\\mathrm{naive}}$ where $\\begin{array}{r}{p_{j}=\\frac{1}{d}}\\end{array}$ , and for $Z_{\\mathrm{opt}}$ where $p_{j}\\propto\\left|x_{j}\\right|$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathrm{Var}(Z_{\\mathrm{naive}})]=d\\|x\\|_{2}^{2}-\\mu^{2}}\\\\ {\\mathbb{E}[\\mathrm{Var}(Z_{\\mathrm{opt}})]=\\|x\\|_{1}^{2}-\\mu^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the case where we are using the same matrix $A$ over many vectors $x$ (as is often case e.g. in LLM inference, or multinomial logistic regression), we can make our leverage scores a function of precomputed quantities based off of $A$ , not just $x$ . In this case, it makes sense to consider forms of leverage sampling; in this work we consider taking $p_{j}\\propto|x_{j}||\\sum_{i}|A_{i j}|$ . ", "page_idx": 15}, {"type": "text", "text": "A.3.3 Sampling without replacement using Gumbel trick ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Weighted sampling with replacement becomes wasteful in larger sample size regimes, for which the same high-weight elements are sampled repeatedly. It therefore becomes desirable to remove sampled elements from consideration after they are sampled, re-weighting the remaining elements accordingly. We could naively repeat this iterative process of sampling, removing, and re-weighting our elements until we ended up with a sample of the desired size, say $k$ . However, this process is sequential and quite slow. Fortunately, as noted in [43], sampling with replacement according to a set of weights is equivalent to perturbing the logits $\\lambda_{1,...n}$ of our desired sample weights with draws from the i.i.d standard Gumbel distribution and taking the elements with the top- $\\cdot k$ perturbed logits as our sample, as detailed in Algorithm 6 and 7. This process is easily batched and is much faster as a result. ", "page_idx": 15}, {"type": "text", "text": "Further, taking the $(k+1)$ -th perturbed logit as an empirical threshold $\\tau$ , the inclusion of an element $j$ in our sample is solely dependent on whether or not its perturbed logit value exceeded this threshold. This derivation is detailed in [27], and gives us the following expression (Gumbel CDF) for the inclusion probability of element $j$ in a set $S$ of size $k$ drawn without replacement according to weights $w$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\pi_{j}=\\mathbb{P}\\left(j\\in S\\right)}\\\\ {\\hat{\\pi}_{j}=1-\\exp(-\\exp(\\lambda_{j}-\\tau))}\\\\ {\\mathbb{E}\\left[\\frac{\\displaystyle\\mathbb{1}_{\\{j\\in S\\}}}{\\hat{\\pi}_{j}}\\right]=\\frac{1}{\\pi_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "These empirical estimates of the marginal probabilities of selection for each column allow us to generate a sequence of estimators for each arms\u2019 mean, with improved variance discussed in the next section. ", "page_idx": 15}, {"type": "text", "text": "A.3.4 Variance estimation for Gumbel Samples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Gumbel sampling trick used in A.3.3 with the fixed empirical threshold also provides us a different lens on our sampling process. Namely, we can compute these closed form inclusion probabilities $\\pi$ , ", "page_idx": 15}, {"type": "text", "text": "and by setting this empirical threshold, we may treat the inclusion of separate elements as independent. Given these observations (sampled $S$ ), an unbiased estimator for the variance of $\\hat{\\mu}_{i}$ , constructed as the importance sampling weighted mean of the observations, can be computed from [18] as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\hat{\\mu}_{i}=\\frac{1}{|S|}\\sum_{j\\in S}\\frac{1}{\\hat{\\pi}_{j}}A_{i j}x_{j}}}\\\\ {{\\displaystyle\\mathbb{E}\\left[\\hat{\\mu}_{i}\\right]=\\mu_{i}}}\\\\ {{\\mathrm{Var}(\\hat{\\mu}_{i};\\tau)=\\sum_{j\\in S}(A_{i,j}x_{j})^{2}\\left(\\frac{1-\\pi_{j}}{\\pi_{j}^{2}}\\right)+\\sum_{j\\neq k}^{n}(A_{i,j}A_{i,k}x_{j}x_{k})\\left(\\frac{\\pi_{j,k}-\\pi_{j}\\pi_{k}}{\\pi_{j,k}\\pi_{j}\\pi_{k}}\\right)}}\\\\ {{\\displaystyle\\widehat{\\mathrm{Var}}(\\hat{\\mu}_{i})=\\sum_{j\\in S}(A_{i,j}x_{j})^{2}\\left(\\frac{1-\\hat{\\pi}_{j}}{\\hat{\\pi}_{j}^{2}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following the analysis of [18] and assuming that our threshold $\\tau$ is fixed, we may conclude that the estimate of the variance of $\\hat{\\mu}_{i}$ , the quantity $\\operatorname{Var}(\\hat{\\mu}_{i};\\tau)$ , is an unbiased estimate of the true variance $\\operatorname{Var}(\\hat{\\mu}_{i})$ . Further, in all datasets we analyzed, the entries of $A$ were generally symmetric, zero-mean, and not correlated with the corresponding entries of $x$ , as seen in Figure 3(a). Since $\\tau$ and the values of $\\pi$ are selected solely based on the values of $x$ , these two further assumptions make the second order term $\\begin{array}{r}{\\left(A_{i,j}A_{i,k}x_{j}x_{k}\\right)\\left(\\frac{\\pi_{j,k}-\\pi_{j}\\pi_{k}}{\\pi_{j,k}\\pi_{j}\\pi_{k}}\\right)}\\end{array}$ zero in expectation. ", "page_idx": 16}, {"type": "text", "text": "Thus, for simplicity, we discard this latter summation and treat $\\widehat{\\mathrm{Var}}(\\hat{\\mu}_{i})$ as an unbiased estimate of the variance throughout our implementation, which can be computed in linear instead of quadratic time (updated in constant vs linear time). In practice, as observed in Figure 4, this variance estimator $\\widehat{\\mathrm{Var}}(\\hat{\\mu}_{i})$ (solid green line) provides a far better estimate of the Gumbel sampler\u2019s true variance than other methods. ", "page_idx": 16}, {"type": "image", "img_path": "XsNA2b8GPz/tmp/756d03b1e30aa5477351c23a1864ef871d34579ef58605fd17089b088a16f534.jpg", "img_caption": ["(a) Sampled entries of $A$ and $_x$ "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "XsNA2b8GPz/tmp/80b09ad0a798d49676976566bb949d93d0ba1ccf69cde7f4e6dc2902df94530e.jpg", "img_caption": ["(b) Sampled values of the second order term "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 3: (a) Sampled entries of $A$ and the corresponding entries of $x$ for Mistral on the Wikitext dataset. The values of $A$ are symmetrical about 0 and not correlated with $x$ . (b) Sampled values of the second order term $\\begin{array}{r}{\\left(A_{i,j}A_{i,k}x_{j}x_{k}\\right)\\left(\\frac{\\pi_{j,k}-\\pi_{j}\\pi_{k}}{\\pi_{j,k}\\pi_{j}\\pi_{k}}\\right)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We begin by proving a standard best arm identification result, for a slightly modified version of the round-based algorithm from [16]. ", "page_idx": 16}, {"type": "image", "img_path": "XsNA2b8GPz/tmp/64d7f1cb19d6fef9551f95dddd2883ab31be62c49d8560685ed4cfaa71d34f63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: Variance estimates vs. empirical mean squared error. This demonstrates the dramatic improvement afforded by improved estimators and tight confidence intervals. [imp] indicates importance sampling, [wr] with replacement, [wor] without replacement, [fpc-sparse] includes the finite population correction factor. ", "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Best-arm identification). With probability at least $1-\\delta$ , Algorithm BestArmId identifies the top softmax value correctly with probability using a number of observations at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\frac{32\\sigma^{2}\\ln\\left(\\frac{4n}{\\delta}\\log_{2}^{2}\\left(4/\\Delta_{i}\\right)\\right)}{\\Delta_{i}^{2}},d\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Following the proof from [16], since an arm\u2019s mean is exactly computed after $d$ pulls, we have that the best arm will be identified with probability at least $1-\\delta$ requiring for arm $i$ a number of pulls at most ", "page_idx": 17}, {"type": "equation", "text": "$$\nn_{i}\\le\\operatorname*{min}\\left(\\frac{32\\sigma^{2}\\ln\\left(\\frac{4n}{\\delta}\\log_{2}^{2}\\left(4/\\Delta_{i}\\right)\\right)}{\\Delta_{i}^{2}},d\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing over arms yields the desired result, noting that the best arm is pulled at most as many times as it takes to eliminate every other arm (i.e. the second best arm, $\\Delta_{1}=\\Delta_{2}$ ). ", "page_idx": 17}, {"type": "text", "text": "Note that if $\\begin{array}{r}{\\Delta_{i}\\leq\\frac{C}{\\sqrt{d}}}\\end{array}$ , then the second term $(d)$ will be selected, for a sufficiently small absolute constant $C$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\nn_{i}\\leq\\frac{C\\sigma^{2}\\log\\left(\\frac{n}{\\delta}\\log(d)\\right)}{\\Delta_{i}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, the total sample complexity on this success event is at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T=\\displaystyle\\sum_{i=1}^{n}n_{i}}\\\\ {\\quad\\leq\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\frac{32\\sigma^{2}\\ln\\left(\\frac{4n}{\\delta}\\log_{2}^{2}\\left(4/\\Delta_{i}\\right)\\right)}{\\Delta_{i}^{2}},d\\right)}\\\\ {\\quad\\leq C\\displaystyle\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\frac{\\sigma^{2}\\log\\left(\\frac{n}{\\delta}\\log(d)\\right)}{\\Delta_{i}^{2}},d\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We additionally require a lemma for estimating the mean of the best-arm in a PAC sense. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Exponential best arm estimation). Sampling an arm using ", "page_idx": 17}, {"type": "equation", "text": "$$\nT=\\frac{32\\sigma^{2}\\beta^{2}\\log(2/\\delta)}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "samples guarantees that $e^{\\beta\\hat{\\mu}_{k}}$ estimates $e^{\\beta\\operatorname*{max}_{i}\\mu_{i}}$ to multiplicative accuracy \u03f5, with probability at least $1-\\delta$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We estimate the mean of arm $k$ after $T$ draws using the plug-in estimator $\\hat{\\mu}_{k}$ . For simplicity, assume $\\beta=1$ , where in the end we scale the sample complexity by $\\beta^{2}$ . Sub-Gaussian concentration provides that with probability at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{\\mu}_{k}-\\mu_{k}|\\leq\\sqrt{2\\sigma^{2}\\log(2/\\delta)/T}=\\epsilon/4}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which in turn implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log(1-\\epsilon)\\leq-\\epsilon/4\\leq\\hat{\\mu}_{k}-\\mu_{k}\\leq\\epsilon/4\\leq\\log(1+\\epsilon)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $0<\\epsilon<1$ . Then, exponentiating both sides yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n1-\\epsilon\\leq e^{\\hat{\\mu}_{k}-\\mu_{k}}\\leq1+\\epsilon\\iff(1-\\epsilon)e^{\\mu_{k}}\\leq e^{\\hat{\\mu}_{k}}\\leq(1+\\epsilon)e^{\\mu_{k}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Scaling the number of samples by $\\beta^{2}$ yields the desired result. ", "page_idx": 18}, {"type": "text", "text": "With these two steps in place, we are now ready to tackle the novel technical challenge of this work; estimating the normalization constant of the softmax. We denote the softmax normalization constant as $\\begin{array}{r}{f(\\pmb{\\mu})\\stackrel{\\smile}{=}\\sigma_{\\beta}(\\pmb{\\mu})=\\sum_{i}e^{\\beta\\mu_{i}}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition 2 (Softmax normalization estimation: restatement of Proposition 1). Under Assumption $^{\\,l}$ , Algorithm 2 will, with probability at least $1-\\delta$ , estimate $\\begin{array}{r}{f_{\\beta}(\\pmb{\\mu})^{\\dagger}=\\sum_{j}e^{\\beta\\mu_{j}}}\\end{array}$ to a multiplicative accuracy of \u03f5, using a number of samples at most ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=2n T_{0}+T_{1}}\\\\ &{\\quad=34\\beta^{2}\\sigma^{2}\\log(6n/\\delta)n+\\frac{91\\sigma^{2}\\beta^{2}\\log\\left(6n/\\delta\\right)\\,\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}}\\\\ &{\\quad+\\,\\frac{16\\beta^{2}\\sigma^{2}\\log\\left(12/\\delta\\right)}{\\epsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. To prove Proposition 1, we want to show that with high probability, we can upper and lower bound our plugin estimator as ", "page_idx": 18}, {"type": "equation", "text": "$$\n(1-\\epsilon)f(\\mu)\\leq f(\\hat{\\mu})\\leq(1+\\varepsilon)f(\\mu),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "giving us our desired multiplicative error bound. We construct several success events that collectively guarantee our bound holds, and that occur with high probability. ", "page_idx": 18}, {"type": "text", "text": "\u2022 $E_{1}$ is the event where our estimated optimal sampling frequencies are not too far from the unknown optimal frequencies, i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\alpha}_{i}\\geq\\alpha_{i}/2,\\qquad\\beta C_{i}<1,\\quad i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On this event, we will sample arms sufficiently in the second round. ", "page_idx": 18}, {"type": "text", "text": "\u2022 $E_{2}$ is the event where all estimators $\\hat{\\mu}_{i}$ are within their 2-sided confidence intervals in stage 2, i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\hat{\\mu}_{i}-\\mu_{i}|\\leq\\sqrt{2\\sigma^{2}\\log(12n/\\delta)/n_{i}},\\quad i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On this event, we can bound the error in the exponentiated estimator. ", "page_idx": 18}, {"type": "text", "text": "\u2022 $E_{3}$ is the event where the first and second order errors are small, i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n-{\\frac{\\epsilon}{2}}f(\\mu)\\leq\\sum_{i}e^{\\beta\\mu_{i}}\\beta(\\mu_{i}-{\\hat{\\mu}}_{i})\\leq{\\frac{\\epsilon}{2}}f(\\mu)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i}e^{\\beta\\mu_{i}}\\beta^{2}(\\mu_{i}-\\hat{\\mu}_{i})^{2}\\leq\\frac{\\epsilon}{2}f(\\mu).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "These two terms arise from bounding $f(\\hat{\\mu})$ . ", "page_idx": 18}, {"type": "text", "text": "We now show that if $E_{1}$ and $E_{2}$ and $E_{3}$ all occur, then our desired bound holds. ", "page_idx": 19}, {"type": "text", "text": "Lemma 3 shows that when $E_{2}$ holds, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{\\mu})\\leq\\displaystyle\\sum_{i}e^{\\beta\\mu_{i}}\\left(1+\\beta(\\mu_{i}-\\hat{\\mu}_{i})+\\beta^{2}(\\mu_{i}-\\hat{\\mu}_{i})^{2}\\right)}\\\\ &{\\qquad=f(\\mu)+\\underbrace{\\sum_{i}e^{\\beta\\mu_{i}}\\beta(\\mu_{i}-\\hat{\\mu}_{i})}_{(1)}+\\underbrace{\\sum_{i}e^{\\beta\\mu_{i}}\\beta^{2}(\\mu_{i}-\\hat{\\mu}_{i})^{2}}_{(2)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that if $E_{3}$ holds as well, the expression is upper bounded by $(1+\\epsilon)f(\\mu)$ , since each of (1) and (2) is bounded above by $(\\epsilon/2)f(\\mu)$ . ", "page_idx": 19}, {"type": "text", "text": "All that remains is to show the lower bound also holds. We use the global inequality $1+x\\leq e^{x}$ to lower bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{f(\\hat{\\mu})}&{=}&{\\displaystyle\\sum_{i}e^{\\beta\\hat{\\mu}_{i}}}\\\\ &{=}&{\\displaystyle\\sum_{i}e^{\\beta\\mu_{i}+\\beta(\\hat{\\mu}_{i}-\\mu_{i})}}\\\\ &{\\geq}&{f(\\mu)+\\displaystyle\\sum_{i}e^{\\beta\\mu_{i}}\\beta(\\hat{\\mu}_{i}-\\mu_{i})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Again, under $E_{2}$ , (1) is lower bounded by $-\\epsilon/2f(\\mu)$ , which implies our desired lower bound $(1^{-}\\!-\\epsilon)f(\\mu)\\le f(\\hat{\\mu})$ . Thus, all three events holding guarantees our desired bound holds. ", "page_idx": 19}, {"type": "text", "text": "Now that we know our bound holds on the joint success event, all that remains is to show it holds with sufficiently high probability. To do so, we invoke our lemmas which characterize the probability of each event. ", "page_idx": 19}, {"type": "text", "text": "The probability of $E_{1},E_{2}$ , and $E_{3}$ all holding is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}(E_{1}E_{2}E_{3})=\\mathbb{P}(E_{1})\\mathbb{P}(E_{2}|E_{1})P(E_{3}|E_{2}E_{1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 3 says that if each arm is sampled $T_{0}$ times, $P(E_{1})\\ge1-\\delta/3.$ . Lemma 4 says $P(E_{2}|E_{1})\\ge$ $1-\\delta/3$ . Lemma 5 and Lemma 6 together show $P(E_{3}|E_{2}E_{1})\\ge1-\\delta/3$ . Thus, taken together we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{P}(E_{1}E_{2}E_{3})}&{=}&{\\mathbb{P}(E_{1})\\mathbb{P}(E_{2}|E_{1})P(E_{3}|E_{2}E_{1})}\\\\ &{\\geq}&{(1-\\delta/3)^{3}}\\\\ &{\\geq}&{1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as desired, and so we are done. ", "page_idx": 19}, {"type": "text", "text": "The lemmas and their proofs follow. ", "page_idx": 19}, {"type": "text", "text": "Lemma 3. For $\\begin{array}{r}{n_{i}=T_{0}=17\\beta^{2}\\sigma^{2}\\log(\\frac{6n}{\\delta}).}\\end{array}$ for all $i$ , we have that the event $E_{1}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\alpha}_{i}\\geq\\alpha_{i}/2,\\quad\\beta C_{i}<1,\\quad\\quad\\quad i=1,\\ldots,n\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "occurs with probability at least $1-\\delta/3$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. By a standard Chernoff bound, with $\\sigma^{2}$ a bound on the sub-Gaussian parameter of all arms, we have that with probability at least $1-\\delta/3$ that for all $i=1,\\hdots,n$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\alpha}_{i}=\\frac{e^{\\beta(\\hat{\\mu}_{i}-C_{i})}}{\\sum_{j}e^{\\beta(\\hat{\\mu}_{j}-C_{j})}}\\geq\\frac{e^{\\beta(\\mu_{i}-2C_{i})}}{\\sum_{j}e^{\\beta\\mu_{j}}}\\geq\\frac{1}{2}\\alpha_{i},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where \u03b1i = $\\begin{array}{r}{\\alpha_{i}=\\frac{e^{\\beta\\mu_{i}}}{\\sum_{j}e^{\\beta\\mu_{j}}}}\\end{array}$ e\u03b2\u00b5j , and Ci = $\\begin{array}{r}{C_{i}=\\sqrt{\\frac{2\\sigma^{2}\\log\\left(6/n\\delta\\right)}{T_{0}}}}\\end{array}$ is the Chernoff confidence interval width constructed such that $C_{i}<\\log(2)/2\\beta$ and so $\\beta C_{i}<1$ . To simplify constants, we use that $8/\\ln^{2}(2)<17$ .\u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Sampling as $n_{i}\\geq T_{0}$ guarantees that conditioned on $E_{1}$ , $E_{2}$ occurs with probability at least $1-\\delta/3$ and that on $E_{2}$ , $\\begin{array}{r}{f(\\hat{\\mu})\\leq f(\\mu)+\\sum_{i}\\beta(\\mu_{i}+\\hat{\\mu}_{i})+\\sum_{i}\\beta^{2}(\\mu_{i}-\\hat{\\mu}_{i})^{2}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Suppose we sample each arm $n_{i}$ times. Note that by a Chernoff bound on each arm, ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\hat{\\mu}_{i}-\\mu_{i}|\\leq\\sqrt{2\\sigma^{2}\\log(6n/\\delta)/n_{i}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds on each arm independently with probability at least $1-\\delta/3n$ , so all arms are within the two-sided bound with probability at least $1-\\delta/3$ . ", "page_idx": 20}, {"type": "text", "text": "We upper bound the plugin estimator $f(\\hat{\\mu})$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{f(\\hat{\\mu})}&{=}&{\\displaystyle\\sum_{i}e^{\\beta\\hat{\\mu}_{i}}}\\\\ &{=}&{\\displaystyle\\sum_{i}e^{\\beta\\mu_{i}+\\beta(\\mu_{i}-\\hat{\\mu}_{i})}}\\\\ &{\\le}&{\\displaystyle\\sum_{i}e^{\\beta\\mu_{i}}\\left(1+\\beta(\\mu_{i}-\\hat{\\mu}_{i})+\\beta^{2}(\\mu_{i}-\\hat{\\mu}_{i})^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in (28) we use the upper bound $e^{x}\\leq1+x+x^{2}$ for $x\\le1.79$ on the event $E_{2}$ , since on $E_{2}$ , $(\\mu_{i}-\\hat{\\mu}_{i})\\leq1/\\beta$ . This is because $n_{i}\\geq T_{0}$ , and $T_{0}$ samples already guarantees this. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma 5 (First-order error concentration). On the event $E_{2}\\cap E_{1}$ , the first order error ", "page_idx": 20}, {"type": "equation", "text": "$$\nG=\\sum_{i}e^{\\beta\\mu_{i}}\\beta(\\mu_{i}-\\hat{\\mu}_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|G|\\geq\\frac{\\epsilon}{2}\\sum_{i}e^{\\beta\\mu_{i}}\\right)\\leq\\delta/3.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. First, defining $E_{f}$ as the failure event $\\begin{array}{r}{|G|\\geq\\frac{\\epsilon}{2}\\sum_{i}e^{\\beta\\mu_{i}}}\\end{array}$ , note that ", "page_idx": 20}, {"type": "equation", "text": "$$\nP(E_{f})=P(E_{f}|E_{2})P(E_{2})+P(E_{f}|\\overline{{E_{2}}})P(\\overline{{E_{2}}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "implies ", "page_idx": 20}, {"type": "equation", "text": "$$\nP(E_{f}|E_{2})=\\frac{P(E_{f})-P(E_{f}|\\overline{{E_{2}}})P(\\overline{{E_{2}}})}{P(E_{2})}\\leq2P(E_{f}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use that since $\\delta<1$ , $P(E_{2})\\geq1/2$ . Thus, it suffices to show that $P(E_{f})\\leq\\delta/6$ ", "page_idx": 20}, {"type": "text", "text": "$\\mathrm{G}$ is a sum of independent sub-Gaussian random variables, each scaled by a constant. Thus, we have the two-sided tail bound that with probability at least $1-\\delta/6$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\sum_{i}e^{\\beta\\mu_{i}}\\beta(\\mu_{i}-\\hat{\\mu}_{i})\\right|\\leq\\sqrt{2B^{2}\\log(12/\\delta)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with the sum having sub-Gaussian parameter ", "page_idx": 20}, {"type": "equation", "text": "$$\nB^{2}=\\sum_{i}e^{2\\beta\\mu_{i}}\\beta^{2}\\sigma^{2}/n_{i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging in our value of $B^{2}$ we find ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i}e^{\\beta\\mu_{\\beta}}\\beta\\big(\\mu_{i}-\\hat{\\mu}_{i}\\big)\\Bigg|}}\\\\ &{\\leq}&{\\sqrt{2\\log(12/\\delta)\\sum_{i}e^{2\\beta\\mu_{i}\\beta^{2}\\sigma^{2}/n_{i}}}}\\\\ &{\\leq}&{\\beta\\sigma\\sqrt{2\\log(12/\\delta)}\\left(\\sum_{i}e^{2\\beta\\mu_{i}}\\frac{1}{\\sigma^{2}\\Sigma_{\\gamma}e^{\\beta\\mu_{i}}}\\right)^{1/2}}\\\\ &{\\leq}&{\\beta\\sigma\\sqrt{2\\log(12/\\delta)}\\left(\\left(2\\sum_{j}e^{\\beta\\mu_{i}}\\right)\\sum_{i}e^{\\beta\\mu_{i}}\\frac{1}{T}\\right)^{1/2}}\\\\ &{=}&{2\\beta\\sigma\\sqrt{\\log(12/\\delta)}f(\\mu/T^{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in (30) we use that on $\\mathbb{E}_{1},\\hat{\\alpha}_{i}\\geq\\frac{1}{2}\\alpha_{i}$ , and so $n_{i}\\geq\\alpha_{i}T/2$ . Thus ", "page_idx": 21}, {"type": "equation", "text": "$$\nT\\geq16\\beta^{2}\\sigma^{2}\\epsilon^{-2}\\log(12/\\delta)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is sufficient to yield the desired multiplicative error of $\\varepsilon/2$ with probability at least $1-\\delta/3$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 6. If arm i is pulled at least 4 2\u03c32\u03b22 log( 6n/\u03b4)\u03b3i jn=1 \u03b3j times, then on the success events $E_{1},E_{2}$ where the confidence intervals hold, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{n}\\beta^{2}e^{\\beta\\mu_{j}}\\left(\\mu_{j}-\\hat{\\mu}_{j}\\right)^{2}\\leq\\frac{\\varepsilon}{2}\\sum_{j}e^{\\beta\\mu_{j}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and Algorithm 2 will require a number of arm pulls at most ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{91\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Bounding the second order error, we utilize the fact that we have sampled proportional to $\\hat{\\gamma}_{i}$ . On the event $E_{2}$ (where $\\begin{array}{r}{n_{i}\\geq\\frac{\\gamma_{i}}{\\sqrt{2}}T,}\\end{array}$ the second order error can be bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{n}\\beta^{2}e^{\\beta\\mu_{i}}\\ (\\mu_{i}-\\hat{\\mu}_{i})^{2}}}\\\\ &{\\leq\\sum_{i=1}^{n}2\\sigma^{2}\\beta^{2}e^{\\beta\\mu_{i}}\\log(6n/\\delta)/n_{i}}\\\\ &{\\leq2\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\sum_{i=1}^{n}\\frac{e^{\\beta\\mu_{i}}}{\\hat{\\gamma}_{i}\\hat{T}}}\\\\ &{\\leq\\frac{2\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)}{T}\\sum_{i=1}^{n}\\frac{e^{\\beta\\mu_{i}}}{\\gamma_{i}}}\\\\ &{=\\frac{2\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)}{T}\\left(\\sum_{i=1}^{n}e^{\\beta\\mu_{i}/2}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We want this second order error to be at most $\\underline{{{\\epsilon}}}\\sum_{i}e^{\\beta\\mu_{i}}$ , and so require $T$ to satisfy the inequality below ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\ge\\frac{4\\sqrt{2}\\sigma^{2}\\beta^{2}\\log\\left(6n/\\delta\\right)\\,\\left(\\sum_{i=1}^{n}e^{\\beta\\mu_{i}/2}\\right)^{2}}{\\epsilon\\sum_{i}e^{\\beta\\mu_{i}}}}\\\\ &{\\quad=\\frac{4\\sqrt{2}\\sigma^{2}\\beta^{2}\\log\\left(6n/\\delta\\right)\\,\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Algorithmically this is not a valid $T$ to use, since it depends on the unknown $\\mu_{i}$ . However, since $\\hat{\\alpha}_{i},\\hat{\\gamma}_{i}$ are close to their true values on the good event $E_{1}$ , we can use these estimates. Thus, we take as our budget $T$ for this second order error: ", "page_idx": 22}, {"type": "equation", "text": "$$\nT=\\frac{16\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\left(\\sum_{i=1}^{n}\\hat{\\gamma}_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\hat{\\alpha}_{i}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which on the event $E_{1}$ is larger than (38). This is a random quantity, so to analyze the requisite sample complexity, we use the fact than on $E_{1}$ , we can bound the quantity in (39) as ", "page_idx": 22}, {"type": "equation", "text": "$$\nT\\leq\\frac{64\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note $64{\\sqrt{2}}<91$ so we use this simple constant in the statement of the lemma. ", "page_idx": 22}, {"type": "text", "text": "This can be directly compared to the case where we only sample according to $\\alpha_{i}$ , not $\\gamma_{i}$ , which would yield a sample complexity of ", "page_idx": 22}, {"type": "equation", "text": "$$\nT\\geq\\frac{8n\\sigma^{2}\\log(6n/\\delta)\\beta^{2}}{\\epsilon}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "samples. Note that since $\\begin{array}{r}{\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}\\leq n\\sum_{i}\\alpha_{i}}\\end{array}$ by Cauchy-Schwarz, this is always an improvement (up to absolute constants) up to a factor of $n$ . ", "page_idx": 22}, {"type": "text", "text": "Now we can provide a proof of the main theorem. ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 1. We utilize the adaptive approximation subroutine with error probability $\\delta^{\\prime}=$ $\\delta/3$ and error $\\varepsilon^{\\prime}=\\varepsilon/4$ from Lemma 6. With $1-\\delta^{\\prime}$ probability, it requires a number of samples at most ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\leq34\\beta^{2}\\sigma^{2}\\log(18n/\\delta)n}\\\\ &{\\quad+\\,\\frac{363\\sigma^{2}\\beta^{2}\\log\\left(18n/\\delta\\right)\\,\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}}\\\\ &{\\quad+\\,\\frac{256\\beta^{2}\\sigma^{2}\\log\\left(36/\\delta\\right)}{\\varepsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Best arm identification is called with error probability $\\delta^{\\prime}=\\delta/3$ . From Lemma 1 this requires sample complexity ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\frac{32\\sigma^{2}\\ln\\left(\\frac{12n}{\\delta}\\log_{2}^{2}\\left(4/\\Delta_{i}\\right)\\right)}{\\Delta_{i}^{2}},d\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then estimate the best arms mean using Lemma 2 to accuracy $\\varepsilon^{\\prime}=\\varepsilon/4$ with error probability $\\delta^{\\prime}=\\delta/3$ . This requires a number of samples at most ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{512\\sigma^{2}\\beta^{2}\\log(6/\\delta)}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By a union bound, all these algorithms succeed with probability at least $1-\\delta$ . Analyzing the multiplicative error, we have that $[1/(1+\\varepsilon^{\\prime}),1/(1-\\varepsilon^{\\prime})]\\in[1-2\\varepsilon,1+2\\varepsilon]$ for $0<\\varepsilon\\le1/2$ , and that $(1\\pm\\varepsilon_{1})(1\\pm\\varepsilon_{2})\\in(1\\pm(\\varepsilon_{1}\\dot{+}\\varepsilon_{2}+\\varepsilon_{1}\\varepsilon_{2})$ . Thus, on these success events, the numerator is approximated to accuracy $\\varepsilon/4$ , and the denominator to accuracy $\\varepsilon/4$ . The denominator error converts to a multiplicative error of $\\varepsilon/2$ in the numerator, which combines to yield an error of $3\\varepsilon/4\\!+\\!\\varepsilon^{2}/8<\\varepsilon$ , as $\\varepsilon<.5$ . This allows us to simplify the total sample complexity as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\le\\frac{512\\sigma^{2}\\beta^{2}\\log(6/\\delta)}{\\epsilon^{2}}+\\frac{256\\beta^{2}\\sigma^{2}\\log(36/\\delta)}{\\varepsilon^{2}}}\\\\ &{\\phantom{\\frac{512\\sigma^{2}\\beta^{2}\\log(6/\\delta)}{\\varepsilon^{2}}}\\le\\frac{768\\sigma^{2}\\beta^{2}\\log(36/\\delta)}{\\varepsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\leq34\\beta^{2}\\sigma^{2}\\log(18n/\\delta)n}\\\\ &{\\qquad+\\displaystyle\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\frac{32\\sigma^{2}\\beta^{2}\\ln\\left(\\frac{12n}{\\delta}\\log_{2}^{2}(4/\\Delta_{i})\\right)}{\\Delta_{i}^{2}},d\\right)}\\\\ &{\\qquad+\\frac{363\\sigma^{2}\\beta^{2}\\log(18n/\\delta)\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}}\\\\ &{\\qquad+768\\sigma^{2}\\beta^{2}\\log(36/\\delta)\\epsilon^{-2}}\\\\ &{\\qquad\\leq C\\left(\\beta^{2}\\sigma^{2}\\left[n\\log\\left(\\frac{n}{\\delta}\\right)+\\displaystyle\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\frac{\\log\\left(\\frac{n\\log d}{\\delta}\\right)}{\\Delta_{i}^{2}},d\\right)+\\frac{\\log\\left(\\frac{n}{\\delta}\\right)\\left(\\displaystyle\\sum_{j=1}^{n}\\gamma_{j}\\right)^{2}}{\\epsilon\\sum_{j}\\alpha_{j}}+\\frac{\\log(1/\\delta)}{\\varepsilon^{2}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Algorithmically, in the second stage, arm $i$ needs to be pulled a number of times ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{i}=17\\beta^{2}\\sigma^{2}\\log(6n/\\delta)}\\\\ &{\\quad\\quad+\\frac{16\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\,(\\sum_{i=1}^{n}\\hat{\\gamma}_{i})^{2}}{\\epsilon\\sum_{i}\\hat{\\alpha}_{i}}\\frac{\\hat{\\gamma}_{i}}{\\sum_{j}\\hat{\\gamma}_{j}}}\\\\ &{\\quad\\quad+16\\beta^{2}\\sigma^{2}\\log(12/\\delta)\\epsilon^{-2}\\frac{\\hat{\\alpha}_{i}}{\\sum_{j}\\hat{\\alpha}_{j}}}\\\\ &{\\quad\\quad\\le17\\beta^{2}\\sigma^{2}\\log(6n/\\delta)}\\\\ &{\\quad\\quad\\quad+\\frac{64\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\,\\left(\\sum_{j}\\gamma_{j}\\right)\\gamma_{i}}{\\epsilon\\sum_{j}\\alpha_{j}}}\\\\ &{\\quad\\quad\\quad+\\frac{16\\beta^{2}\\sigma^{2}\\log(12/\\delta)\\hat{\\alpha}_{i}}{\\epsilon^{2}\\sum_{j}\\alpha_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we upper bound on the success event $E_{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Combining this with the initial $T_{0}$ pulls, the pulls from best arm identification, and the pulls from estimating the value of the best arm, we have that the overall sample complexity for arm $i$ is upper bounded as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{n}_{i}=34\\beta^{2}\\sigma^{2}\\log(6n/\\delta)}\\\\ &{\\phantom{\\frac{1}{\\Delta}}+\\frac{64\\sqrt{2}\\sigma^{2}\\beta^{2}\\log(6n/\\delta)\\left(\\sum_{j}\\gamma_{j}\\right)\\gamma_{i}}{\\epsilon\\sum_{j}\\alpha_{j}}}\\\\ &{\\phantom{\\frac{1}{\\Delta}}+\\frac{16\\beta^{2}\\sigma^{2}\\log(12/\\delta)\\alpha_{i}}{\\epsilon^{2}\\sum_{j}\\alpha_{j}}}\\\\ &{\\phantom{\\frac{1}{\\Delta}}+\\frac{32\\sigma^{2}\\ln\\left(\\frac{12n}{\\delta}\\log_{2}^{2}(4/\\Delta_{i})\\right)}{\\Delta_{i}^{2}}}\\\\ &{\\phantom{\\frac{1}{\\Delta}}+\\frac{512\\sigma^{2}\\beta^{2}\\log(6/\\delta)}{\\epsilon^{2}}\\mathbb{1}\\{i=1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "And so, the total algorithmic sample complexity on the success event is ", "page_idx": 23}, {"type": "equation", "text": "$$\nT\\leq\\sum_{i}\\operatorname*{min}\\left(\\tilde{n}_{i},d\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "B.1 Interpreting the results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We work to provide a simplified (looser) bound on the sample complexity when the minimum gap is $\\Delta$ . The worst case sample complexity in this case is when the best arm has mean $\\mu_{1}$ , and all the rest have mean $\\mu_{1}-\\Delta$ . ", "page_idx": 24}, {"type": "text", "text": "This allows us to simplify the overall sample complexity as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Gamma\\leq C\\left(\\sigma^{2}\\left[\\beta^{2}n\\log\\left(\\frac{n}{\\delta}\\right)+\\sum_{i=1}^{n}\\frac{\\log\\left(\\frac{n\\log d}{\\delta}\\right)}{\\Delta_{i}^{2}}+\\beta^{2}\\frac{\\log\\left(\\frac{n}{\\delta}\\right)\\left(\\displaystyle\\sum_{j=1}^{n}\\gamma_{j}\\right)^{2}}{\\epsilon\\sum_{j}\\alpha_{j}}+\\beta^{2}\\frac{\\log\\left(1/\\delta\\right)}{\\varepsilon^{2}}\\right]\\right)~~~~~(45)}\\\\ &{\\displaystyle\\leq C\\left(\\beta^{2}\\sigma^{2}\\log\\left(\\frac{n}{\\delta}\\right)\\left(n+\\varepsilon^{-1}\\left(\\frac{1+n^{2}e^{-\\beta\\Delta}}{1+n e^{-\\beta\\Delta}}\\right)\\right)+\\beta^{2}\\sigma^{2}\\varepsilon^{-2}\\log(1/\\delta)+n\\sigma^{2}\\log\\left(\\frac{n\\log d}{\\delta}\\right)\\Delta^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use the fact that for $n>2$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\left(\\displaystyle\\sum_{j=1}^{n}\\gamma_{j}\\right)^{2}}{\\sum_{j}\\alpha_{j}}=\\frac{\\left(1+(n-1)e^{-\\beta\\Delta/2}\\right)^{2}}{1+(n-1)e^{-\\beta\\Delta}}}}\\\\ &{}&{\\leq2\\frac{1+(n-1)^{2}e^{-\\beta\\Delta}}{1+(n-1)e^{-\\beta\\Delta}}}\\\\ &{}&{\\leq C\\left(\\frac{1+n^{2}e^{-\\beta\\Delta}}{1+n e^{-\\beta\\Delta}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Evaluating our sample complexity in (45), when $\\begin{array}{r}{\\Delta>\\frac{2}{\\beta}\\log n}\\end{array}$ , this term is bounded by a constant, and the sample complexity can be more simply bounded as. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\leq C\\left(\\beta^{2}\\sigma^{2}\\log\\left(\\frac{n}{\\delta}\\right)\\left(n+\\varepsilon^{-1}\\right)+\\beta^{2}\\sigma^{2}\\varepsilon^{-2}\\log(1/\\delta)+n\\sigma^{2}\\frac{\\log\\Big(\\frac{n\\log d}{\\delta}\\Big)}{\\log^{2}n}\\right)}\\\\ &{\\quad\\leq C\\beta^{2}\\sigma^{2}\\left(\\log\\left(\\frac{n}{\\delta}\\right)\\left(n+\\varepsilon^{-1}\\right)+\\varepsilon^{-2}\\log(1/\\delta)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the last line we require the condition that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\log\\left(\\frac{n\\log d}{\\delta}\\right)}{\\log^{2}n}\\leq\\beta^{2}\\log\\left(\\frac{n}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "i.e. $d$ is not doubly exponential in $n$ and $\\beta$ is not too small. ", "page_idx": 24}, {"type": "text", "text": "Assumption 2 (large gap, moderate $\\beta$ , moderate $d$ ). We assume that (47) holds, and that $\\begin{array}{r}{\\Delta>{\\frac{2}{\\beta}}\\log n}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Under the conditions of Assumption 2, the sample complexity in Theorem 1 can be simplified as in Corollary 1. ", "page_idx": 24}, {"type": "text", "text": "B.2 Asymptotic optimality of sampling frequencies ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Following the approach of [8], we can show the asymptotic optimality of our sampling frequencies (sampling proportional to $\\alpha_{i}$ for minimizing the first order error, and $\\gamma_{i}$ for minimizing our bound on the second order error). ", "page_idx": 24}, {"type": "text", "text": "B.2.1 First order frequencies $\\alpha_{i}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Considering a plug-in estimator $\\hat{\\mu}$ ; analyzing the first order taylor expansion of its error, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(\\hat{\\mu})-f(\\mu)=\\nabla f(\\mu)^{\\top}(\\hat{\\mu}-\\mu)+O\\left(\\|\\hat{\\mu}-\\mu\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus in the high accuracy regime $\\left(\\varepsilon\\right)\\left.\\left.0\\right)$ ), we can consider only the first order term. Assuming Gaussian noise in our arm pulls (an identical result holds for sub-Gaussian noise), the first order ", "page_idx": 24}, {"type": "text", "text": "error can be bounded as (assuming we use $T$ pulls, and sample arm $i$ , $p_{i}T$ times for a probability distribution $p$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla f(\\mu)^{\\top}({\\hat{\\mu}}-\\mu)\\sim{\\mathcal{N}}\\left(0,\\sum_{i=1}^{n}{\\frac{\\beta^{2}e^{2\\beta\\mu_{i}}}{T p_{i}}}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Optimizing over probability distributions $p$ , using Sion\u2019s minimax theorem and strong duality as in [8] gives us that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\alpha^{\\star}=\\underset{p}{\\operatorname{argmin}}\\sum_{i=1}^{n}\\beta^{2}\\frac{e^{2\\beta\\mu_{i}}}{T p_{i}}}}\\\\ {{\\displaystyle=\\underset{p}{\\operatorname{argmin}}\\operatorname*{max}_{\\lambda}\\sum_{i=1}^{n}\\frac{e^{2\\beta\\mu_{i}}}{T p_{i}}+\\lambda\\left(\\sum_{i}p_{i}-1\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "utilizing strong duality, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial p_{i}}=-e^{2\\beta\\mu_{i}}p_{i}^{-2}+\\lambda=0\\quad\\implies\\quad\\alpha^{\\star}\\propto e^{\\beta\\mu}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that in the limit as $\\varepsilon\\rightarrow0$ , multiplicative and additive error objectives are equivalent. ", "page_idx": 25}, {"type": "text", "text": "B.2.2 Second order sampling frequencies $\\gamma_{i}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "With the first order error term in hand, the second order error term is left to be analyzed: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\beta^{2}e^{\\beta\\mu_{i}}\\left(\\mu_{i}-\\hat{\\mu}_{i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We minimize a bound on this given by our confidence intervals $\\begin{array}{r}{|\\mu_{i}-\\hat{\\mu}_{i}|\\leq\\frac{c}{\\sqrt{n_{i}}}}\\end{array}$ , where $c$ is some constant. We identify what sampling distribution $p$ minimizes this second order bound, defining $n_{i}=p_{i}T$ . ", "page_idx": 25}, {"type": "text", "text": "By a similar argument as for the first order analysis: ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\gamma^{\\star}=\\underset{p}{\\operatorname{argmin}}\\displaystyle\\sum_{i=1}^{n}\\beta^{2}e^{\\beta\\mu_{i}}{\\frac{1}{T p_{i}}}}\\\\ &{\\qquad=\\underset{p}{\\operatorname{argmin}}\\operatorname*{max}_{\\lambda}\\sum_{i=1}^{n}e^{\\beta\\mu_{i}}{\\frac{1}{p_{i}}}+\\lambda\\left(\\sum_{i}p_{i}-1\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "utilizing strong duality, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{\\partial}{\\partial p_{i}}}=-e^{\\beta\\mu_{i}}p_{i}^{-2}+\\lambda=0\\quad\\implies\\quad\\gamma\\propto e^{\\beta\\mu/2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Gains by sampling according to $\\gamma_{i}$ Sampling according to $\\gamma$ gives a second order error bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\sum_{i}e^{\\beta\\mu_{i}/2}\\right)^{2}=\\|\\gamma\\|_{1}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as opposed to sampling according to $_{\\alpha}$ which gives a second order error bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\nn\\sum_{i}e^{\\beta\\mu_{i}}=n\\|\\gamma\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By standard norm inequalities, sampling according to $\\gamma$ is always at least as good, and up to a factor of $n$ improvement in the case where one entry in the softmax is much larger than the rest; exactly the case of interest. ", "page_idx": 25}, {"type": "text", "text": "B.3 Comparison with [8] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In [8], the problem of estimating a real valued function $f$ to additive accuracy $\\varepsilon$ with probability at least $1-\\delta$ is studied, under the assumption that the function has $L$ -Lipschitz gradients. For the case of softmax estimation, the gradients are not Lipschitz due to the unbounded nature of the exponential. However, if we evaluate the norm of the gradient at a point $\\pmb{\\mu}$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla f(\\pmb{\\mu})\\|^{2}=\\underset{c\\rightarrow0}{\\operatorname*{lim}}\\,\\underset{\\|\\pmb{u}\\|\\leq c}{\\operatorname*{max}}\\,c^{-2}\\,\\|\\nabla f(\\pmb{\\mu}+\\pmb{u})-\\nabla f(\\pmb{\\mu})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\underset{c\\rightarrow0}{\\operatorname*{lim}}\\,\\underset{\\|\\pmb{u}\\|\\leq c}{\\operatorname*{max}}\\,c^{-2}\\displaystyle\\sum_{i}\\left(\\beta e^{\\beta(\\mu_{i}+\\b{u}_{i})}-\\beta e^{\\beta\\mu_{i}}\\right)^{2}}\\\\ &{\\quad\\quad\\quad=\\underset{c\\rightarrow0}{\\operatorname*{lim}}\\,\\underset{\\|\\pmb{u}\\|\\leq c}{\\operatorname*{max}}\\,c^{-2}\\beta^{2}\\displaystyle\\sum_{i}e^{2\\beta\\mu_{i}}\\left(e^{\\beta u_{i}}-1\\right)^{2}}\\\\ &{\\quad\\quad\\quad=\\beta^{4}\\underset{\\pmb{i}}{\\operatorname*{max}}\\,e^{2\\beta\\mu_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Theorem 1 of [8] states that the number of samples required to achieve $\\varepsilon$ additive error with probability at least $1-\\delta$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\nT=O\\left(\\frac{\\|\\nabla f(\\pmb{\\mu}\\|_{1}^{2}\\log(1/\\delta)}{\\varepsilon^{2}}+\\frac{n^{2}L\\log(n/\\delta)}{\\varepsilon}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the noise variance $\\sigma^{2}$ is assumed to be 1. Since the error in our setting is multiplicative, we are interested in $\\begin{array}{r}{\\varepsilon^{\\prime}=\\epsilon f(\\mu)=\\varepsilon\\sum_{i}e^{\\beta\\mu_{i}}}\\end{array}$ . Additionally, $\\begin{array}{r}{\\|\\nabla f(\\pmb{\\mu})\\|_{1}^{2}=\\beta^{2}\\left(\\sum_{i}e^{\\beta\\mu_{i}}\\right)^{2}}\\end{array}$ . Thus, the number of samples required is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=O\\left(\\frac{\\left\\Vert\\nabla f(\\mu)\\right\\Vert_{1}^{2}\\log(1/\\delta)}{\\varepsilon^{2}}+\\frac{n^{2}L\\log(n/\\delta)}{\\varepsilon}\\right)}\\\\ &{\\quad=O\\left(\\frac{\\beta^{2}\\left(\\sum_{i}e^{\\beta\\mu_{i}}\\right)^{2}\\log(1/\\delta)}{\\varepsilon^{2}\\left(\\sum_{i}e^{\\beta\\mu_{i}}\\right)^{2}}+\\frac{n^{2}\\beta^{4}\\operatorname*{max}_{i}e^{2\\beta\\mu_{i}}\\log(n/\\delta)}{\\varepsilon\\sum_{i}e^{\\beta\\mu_{i}}}\\right)}\\\\ &{\\quad=O\\left(\\frac{\\beta^{2}\\log(1/\\delta)}{\\varepsilon^{2}}+\\frac{n^{2}\\beta^{4}\\operatorname*{max}_{i}e^{2\\beta\\mu_{i}}\\log(n/\\delta)}{\\varepsilon\\sum_{i}e^{\\beta\\mu_{i}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is to be compared with the sample complexity of the proposed algorithm in this paper, for the specific setting of softmax normalization estimation, which is ", "page_idx": 26}, {"type": "equation", "text": "$$\nT\\leq C\\beta^{2}\\left(n\\log\\left({\\frac{n}{\\delta}}\\right)+\\left(\\log\\left({\\frac{n}{\\delta}}\\right)\\left(\\sum_{j=1}^{n}\\gamma_{j}\\right)^{2}\\right)\\left(\\epsilon\\sum_{j}\\alpha_{j}\\right)^{-1}+{\\frac{\\log\\left({\\frac{1}{\\delta}}\\right)}{\\varepsilon^{2}}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "taking $\\sigma^{2}=1$ to compare results. ", "page_idx": 26}, {"type": "text", "text": "The constant term independent of $\\varepsilon$ is to linearize the exponential. The $\\varepsilon^{-2}$ term matches between the two settings, as asymptotically the optimal strategy is indeed to sample according to the first derivative. The term scaling with $\\bar{\\varepsilon}^{-1}$ improves dramatically on that of prior work. Note that in the case of $f(\\pmb{\\mu})=\\|\\pmb{\\mu}\\|_{2}^{2}$ , the second order term (scaling with $\\varepsilon^{-1}$ ) can be improved to $O(n^{3/2}L\\varepsilon^{-1})$ , as the mean of the second order error can be removed. Thus, we can see the massive improvement afforded by our more refined algorithm, tailored for the specific structure of the softmax function. ", "page_idx": 26}, {"type": "text", "text": "B.4 Extension to heterogeneous arm variances ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Adapting bandit algorithms to settings with heterogeneous variances has been done in both the standard regret [3] and best arm identification [33] settings. ", "page_idx": 26}, {"type": "text", "text": "For best arm identification, sacrificing log factors for the sake of clarity, empirical-Bernstein-based confidence intervals [35] can be constructed where we iteratively pull each arm once, try and ", "page_idx": 26}, {"type": "text", "text": "eliminate, and progress. Union bounding over the $^{n d}$ possible pulls naively upper bounds this, yielding a complexity of ", "page_idx": 27}, {"type": "equation", "text": "$$\nO\\left(\\sum_{i=1}^{n}\\operatorname*{min}\\left(\\left(\\frac{\\sigma_{i}^{2}}{\\Delta_{i}^{2}}+\\frac{1}{\\Delta_{i}^{2}}\\right)\\log\\left(\\frac{\\delta}{n d}\\right),d\\right)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This assumes that all arms are bounded in [0, 1] with variance $\\sigma_{i}^{2}$ . ", "page_idx": 27}, {"type": "text", "text": "We additionally require a lemma for estimating the mean of the best-arm in a PAC sense. Lemma 7 (Exponential best arm estimation). Sampling arm $i$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nT=\\frac{32\\sigma_{i}^{2}\\beta^{2}\\log(2/\\delta)}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "samples guarantees that $e^{\\beta\\hat{\\mu}_{i}}$ estimates $e^{\\beta\\mu_{i}}$ to multiplicative accuracy \u03f5, with probability at least $1-\\delta$ . ", "page_idx": 27}, {"type": "text", "text": "This trivially follows from the proof of Lemma 2. ", "page_idx": 27}, {"type": "text", "text": "For the softmax normalization estimation, we know from [8] that the optimal first order sampling frequencies are to sample arm $i$ a number of times proportional to ", "page_idx": 27}, {"type": "equation", "text": "$$\nn_{i}\\propto\\sigma_{i}e^{\\beta\\mu_{i}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, sampling like this yields an additive first order that scales as $\\sum_{i}\\sigma_{i}e^{\\beta\\mu_{i}}$ , which cannot be easily related to $\\sum_{i}e^{\\beta\\mu_{i}}$ , as we would need to get multiplicative error bounds. Thus, we instead use suboptimal target first order sampling frequencies, scaling with $\\sigma_{i}^{2}$ , to avoid this analysis issue ", "page_idx": 27}, {"type": "equation", "text": "$$\nn_{i}\\propto\\sigma_{i}^{2}e^{\\beta\\mu_{i}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Scaling the number of pulls for each arm by $\\sigma_{i}^{2}$ yields: ", "page_idx": 27}, {"type": "text", "text": "Proposition 3 (Softmax normalization estimation: heterogeneous variances variant of Proposition 1). Under Assumption $^{\\,l}$ , Algorithm 2 will, with probability at least $1-\\delta$ , estimate $\\begin{array}{r}{f_{\\beta}(\\pmb{\\mu})=\\dot{\\sum}_{j}\\,e^{\\beta\\mu_{j}}}\\end{array}$ to a multiplicative accuracy of \u03f5, using a number of samples for arm $i$ at most ", "page_idx": 27}, {"type": "equation", "text": "$$\nn_{i}=34\\beta^{2}\\sigma_{i}^{2}\\log(6n/\\delta)n+\\frac{91\\sigma_{i}^{2}\\beta^{2}\\log(6n/\\delta)\\left(\\sum_{i=1}^{n}\\gamma_{i}\\right)^{2}}{\\epsilon\\sum_{i}\\alpha_{i}}+\\frac{16\\beta^{2}\\sigma_{i}^{2}\\log(12/\\delta)}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The proof of this proposition follows similarly to Proposition 2, as sampling proportional to $\\sigma_{i}^{2}$ cancels the differing variances, essentially reducing the problem to the homogeneous setting (suboptimally). ", "page_idx": 27}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this appendix, we present the implementation details of our algorithm. In Algorithm 4, we provide pseudocode with greater detail about our implementation of the Adaptive Softmax algorithm. We note that Algorithm 4 contains some implementation differences from the original Algorithm 1 presented in Section 4. None of these changes materially affect the output of the algorithm; nonetheless, we provide a discussion of them here to enable reproducibility of our experimental results. Our results are also reproducible via a 1-line script in our code submission. In the following subsections, we describe each of these implementation details. ", "page_idx": 27}, {"type": "text", "text": "As an important note, we consider all variables global unless stated otherwise. This is to say, calling \u201cpull arms\u201d updates the state of arm mean estimates $\\hat{\\mu}$ , their variance estimates $\\hat{\\sigma}$ , the number of pulls per arm $\\{n_{i}\\}$ , inclusion probabilities $\\pi$ , etc. We define the estimators based on Gumbel sampling according to importance weights as the set of arms $\\boldsymbol{\\mathcal{A}}$ . This idea of treating an arm simply as a sequence of estimators with confidence intervals was pioneered for the computational setting in [4], and saw further usage in [24]. ", "page_idx": 27}, {"type": "text", "text": "C.1 Reusing Arm Pulls ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In our theoretical analysis in Appendix B, each phase of Algorithm 1 is handled independently. This allows us to union bound the error probabilities of each phase of the algorithm. In our implementation, ", "page_idx": 27}, {"type": "text", "text": "Algorithm 4 Adaptive Softmax (implementation details) ", "page_idx": 28}, {"type": "text", "text": "1: Input: Matrix $A$ , vector $x$ , temperature $\\beta$ , error $\\epsilon$ , failure probability $\\delta$ 2: Output: With probability at least $1-\\delta$ , the argmax coordinate $i^{*}$ and an estimate $\\hat{p}_{i^{*}}$ of its probability such that $(1-\\epsilon)p_{i^{*}}\\le\\hat{p}_{i^{*}}\\le(1+\\epsilon)p_{i^{*}}$ .   \n3: $w\\leftarrow$ GetImportanceWeights $(A,x)$ \u25b7Algorithm 5 4: P, $c\\gets$ GumbelPermutation $(w)$   \n5: Construct set of arms $\\boldsymbol{\\mathcal{A}}$ from $A,x,P,c$   \n6: \u03c32 = \u03b21 $\\triangleright$ Initial variance to pull arms to 7: PullToVariance $({\\mathcal{A}},\\sigma^{2})$ $\\triangleright$ Algorithm 8   \n8: $i^{*}\\gets\\mathtt{B e s t A r m I d}(\\mathcal{A},\\delta/2,\\hat{\\sigma}^{2})$ $\\triangleright$ Algorithm 3 9: $\\hat{\\mu}_{i^{*}}\\gets\\langle A_{i\\cdot},x\\rangle$ \u25b7Exact computation of $\\hat{\\mu}_{i^{*}}=\\mu_{i^{*}}$   \n10: $\\hat{Z}\\gets:$ NormalizationEstimation $\\iota(\\mathcal{A},\\varepsilon/2,\\delta/2,\\hat{\\sigma})$ $\\triangleright$ Algorithm 2   \n11: Compute estimated probability as $\\hat{p}_{i^{*}}=e^{\\beta\\hat{\\mu}_{i^{*}}}/\\hat{Z}$   \n12: return $i^{*},\\hat{p}_{i^{*}}$ ", "page_idx": 28}, {"type": "text", "text": "Algorithm 5 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1: Input: Matrix $A\\in\\mathbb{R}^{n\\times d}$ , vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$   \n2: Output: $w\\in\\mathbb{R}^{d}$ , vector of importance weights   \n3: for all $j=1,\\ldots,d$ do   \n4: Compute $\\begin{array}{r}{w_{j}=|x_{j}|\\sum_{i=1}^{n}|A_{i,j}|}\\end{array}$ \u25b7 $\\ell_{1}$ norm of ith column of $A$   \n5: end for   \n6: return $w$ $\\triangleright$ Element-wise multiplication of $\\ell_{1}$ norms of columns of $A$ , and $|x|$ ", "page_idx": 28}, {"type": "text", "text": "Algorithm 6 GumbelPermutation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1: Input: Importance weights $w$   \n2: Output: Permutation $P$ , cached outputs $c$ for inclusion probability calculation   \n3: Draw each $\\xi_{i}\\overset{\\mathrm{i.i.d}}{\\sim}\\mathrm{Gumbel}(0,1)$   \n4: $L\\gets\\log w$ $\\triangleright$ Log importance weights   \n5: $L^{\\prime}\\gets L+\\xi$ \u25b7Log importance weights perturbed by i.i.d. Gumbel noise   \n6: h \u2190sorted $(L^{\\prime})$ $\\triangleright$ Compute thresholds in decreasing order   \n7: P \u2190ordering(L\u2032) \u25b7Compute sorting order of thresholds (argsort)   \n8: $c\\gets(L,h)$ \u25b7Store log importance weight and sorted thresholds for later use   \n9: return $P,c$ ", "page_idx": 28}, {"type": "text", "text": "Algorithm 7 InclusionProbabilities ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1: Input: Sample size $k$ , cached outputs $c=(L,h)$ from GumbelPermutation   \n2: Output: Inclusion probabilities vector $\\pi$   \n3: $L,h\\gets c$   \n4: $H=-\\infty$ \u25b7Initialize cutoff threshold   \n5: if $k<d$ then   \n6: $H\\leftarrow h_{k}$ $\\triangleright$ Set cutoff threshold to $k$ th largest perturbed log importance weight   \n7: end if   \n8: $\\pi=1-\\exp(-\\exp(L-H))$ \u25b7Inclusion probabilities; derived from Gumbel CDF   \n9: return $\\pi$ ", "page_idx": 28}, {"type": "text", "text": "Algorithm 8 PullToVariance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1: Input: Set of arms (sequence of estimators) $\\boldsymbol{\\mathcal{A}}$ , variance $\\sigma^{2}$   \n2: Set $\\zeta=.1$ , multiplicative pull increase factor   \n3: while there exists an arm $i$ with $\\hat{\\sigma}_{i}^{2}>\\sigma^{2}$ do   \n4: $A^{\\prime}\\leftarrow\\{i\\in\\mathcal{A}:\\hat{\\sigma}_{i}^{2}>\\sigma^{2}\\}$ , $n_{i}$ is corresponding number of pulls   \n5: PullArms $(A^{\\prime},(1+\\zeta)n_{i})$ )   \n6: end while   \n1: Input: Set of arms (sequence of estimators) $\\boldsymbol{\\mathcal{A}}$ , target number of pulls per arm $\\{N_{i}\\}$   \n2: for all arms $i$ do   \n3: if $n_{i}\\geq N_{i}$ then   \n4: continue \u25b7Do not pull arm $i$ if it has already been pulled $N_{i}$ times   \n5: end if   \n6: Compute $k_{i}=N_{i}-n_{i}$   \n7: $\\pi^{\\prime}\\gets$ InclusionProbabilities $(k_{i},c)$   \n8: Sample $k_{i}$ coordinates without replacement according to weights $\\pi^{\\prime}$   \n9: Update mean estimate \u00b5\u02c6i \u2190 nNii \u00b5\u02c6i + kNii ski=1X\u03c0i\u2032,s   \n10: Update variance estimate $\\begin{array}{r}{\\hat{\\sigma}_{i}^{2}\\gets\\frac{n_{i}^{2}}{N_{i}^{2}}\\hat{\\sigma}_{i}^{2}+\\frac{1}{N_{i}^{2}}\\sum_{s=1}^{k_{i}}\\left(A_{i,(s)}x_{(s)}\\right)^{2}\\frac{1-\\pi_{(s)}^{\\prime}}{\\pi_{(s)}^{\\prime2}}}\\end{array}$   \n11: end for ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Algorithm 10 EstimateArm ", "text_level": 1, "page_idx": 29}, {"type": "table", "img_path": "XsNA2b8GPz/tmp/bc2ab9827a743d33465a3f79e83bdd8e418c5436bfc5b0ca3f376f452f9c1347.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "however, we re-use arm pulls across different parts of Algorithm 1. Intuitively, once an arm return has been observed (corresponding to a scalar multiplication of an element of $A$ with an element of $x^{\\dagger},$ ), it can be used to warm-start estimates of $\\hat{\\mu}$ and $\\textstyle{\\hat{\\sigma}}^{2}$ in later stages of the algorithm. In practice, we observe that the re-use of arm pulls does not affect the correctness of our algorithm and yields significant sample complexity improvements compared to cold-starting each stage of the algorithm independently. ", "page_idx": 29}, {"type": "text", "text": "C.2 Exact Computation of Best Arm ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Line 9 of Algorithm 4, we compute the mean of our estimated best arm $i^{*}$ exactly, and set the \u201cestimate\u201d $\\hat{\\mu}_{i}~=~\\mu_{i}$ . This allows us to reduce the approximation fidelity required by NormalizationEstimation (Algorithm 2) to $\\frac\\epsilon2$ instead of $\\frac{\\epsilon}{4}$ and saves a constant factor in sample complexity. Furthermore, this computation of $\\mu_{i}$ is efficiently computed as a vector-vector dot product. ", "page_idx": 29}, {"type": "text", "text": "In practice, we found that Algorithm 1 usually required over $d$ samples for the best arm. As such, performing the computation $\\bar{\\mu_{i}}=\\left\\langle A_{i\\cdot},x\\right\\rangle$ (reusing coordinate-wise samples from previous stages of the algorithm) did not significantly increase sample complexity. ", "page_idx": 29}, {"type": "text", "text": "C.3 Initial Pulls $(T_{0})$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In NormalizationEstimation (Algorithm 2), the initial number of arm pulls $T_{0}$ depends on $\\sigma^{2}$ . However, as discussed in Appendix A.3, this variance proxy is often unknown a priori. As such, we set T0 =1d0 . We observe that this choice of $T_{0}$ works well in experiments, across multiple datasets. ", "page_idx": 29}, {"type": "text", "text": "C.4 Tuning ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We note that despite the changes made above, there still exists some looseness in practice. To remedy this, we scale our variance estimates by a constant factor to reduce the amount of pulls needed to reach target variances, and improving the gains of our algorithm as a result. We generate these constant factors for each dataset/model by tuning on a separate \u201ctraining\u201d group of queries. Tuning is performed, generally, via bisection to discover the minimal factor which still satisfies our provided failure probability parameter $\\delta$ . This bisection is performed in geometric space and terminates when the $\\mathrm{log_{10}}$ difference between the low and high end of our interval is within $\\bar{10}^{-2}$ . The range of factors we consider is $[10^{-6},1]$ . We first tune the constant factor independently for the variance estimates used in bandits such that the algorithm successfully identifies the best arm with a rate of at least $1-\\delta$ on our training set. Next, we tune the constant factor for the variance estimate used in log norm estimation such that the entire SFTM algorithm succeeds with a rate of at least $1-\\delta$ on our training set. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "C.5 Wall-clock improvement ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The focus of this paper is to develop the first provably adaptive softmax algorithm with PAC guarantees, highlighting its dramatically improved sample complexity across a wide variety of models and tasks. The eventual goal of this method is to enable wall-clock improvements in hardware implementations. These next steps of converting our provably and empirically performant method into a hardware optimized wall-clock efficient algorithm is an exciting direction of future work, which we detail below. In most modern-day transformer architectures, memory I/O serves as the primary bottleneck [20]. AdaptiveSoftmax already presents an opportunity to significantly scale down the number of entries of the matrix that must be loaded at inference time, and, in the future - if memory remains the bottleneck - improve model bandwidth by a similar factor. This objective appears in reach, since we have designed the components of AdaptiveSoftmax to be amenable to tiling and parallelization. Most notably, our implementation of AdaptiveSoftmax uses the same column to generate an importance-weighted sample for each active arm. The reasons for this implementation decision are two-fold. First, it takes advantage of the locality of entries in the same column to load samples faster, and, second, it removes intra-column correlation, which can yield theoretically improved performance [7]. Adjacent column samples can also be combined by simply summing their respective importance weights - admitting a simple tiling of our matrix that could easily be sized particularly to fti individual tiles into SRAM on a GPU along with a copy of the vector and the current mean/variance estimates for each arm. Then, we can dynamically load these tiles into SRAM based on the arm estimates as we do currently. The successive elimination bandits algorithm utilized by AdaptiveSoftmax is also, by choice, quite easily parallelizable. We may also store two copies of our matrix \u2014 one with wider tiles and one with taller tiles \u2014 to take advantage of this structure at all stages of the AdaptiveSoftmax algorithm: both when a larger number of samples is necessary for fewer arms, in later stages of adaptive estimation, and when a smaller number of samples is necessary for many arms, in earlier stages of the adaptive estimation. This said, we observe in our experiments that the bulk of compute is invested in our early samples of many arms. Just using basic parallelization to speed up this step could result in the desired speed improvements. ", "page_idx": 30}, {"type": "text", "text": "C.6 Robustness to parameters ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The user-desired parameters can take a wide range. We kept $\\epsilon=30\\%$ constant across all simulations because we observed varying $\\epsilon$ did not result in significant changes to the performance of adaSoftmax. Further, we suspect that for most users, $\\delta$ will fall in the range we considered: $90-99\\%$ . However, to assuage any concerns and verify our assertion that adaSoftmax is not sensitive to choice of $\\epsilon$ or $\\delta$ , we include here adaSoftmax with a much wider range of parameters on the MNIST dataset in Table 3. ", "page_idx": 30}, {"type": "table", "img_path": "XsNA2b8GPz/tmp/3ab4c2b6b0c3ebc5320955bfc986e48001d1036bcd8b68e31d30e78446c6755e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 3: Success rate $(\\%)$ and FLOP gains ${\\bf\\Psi}({\\bf x})$ for adaSoftmax with varied $\\delta$ and $\\varepsilon$ on the MNIST dataset, showing the improved performance across a wide range of parameters, and that raising $\\varepsilon$ past 0.1 causes minimal difference in performance. ", "page_idx": 30}, {"type": "text", "text": "D Additional extensions and comments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "D.1 Effect of temperature ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Temperature is treated as a fixed constant (fixed parameter for the problem at hand, not tunable by the algorithm). This is because tuning the temperature fundamentally changes the problem. With higher temperatures, the only arms that matter are the best and second best arms, and so adaptivity is extremely helpful. At low temperatures, the output will be essentially the uniform distribution, and the computation is trivial and adaptivity unhelpful. With respect to other parameters, the error probability and FLOP gains of adaSoftmax are insensitive to changes in $\\varepsilon$ and vary most with the choice of $\\delta$ . We demonstrate this trend on the MNIST dataset in Table 3. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "D.2 Application to Nucleus Sampling ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The analysis posed in this paper focuses on identifying the largest entry in the softmax output and estimating its associated probability. As discussed above, this naturally extends to identifying the $k$ largest elements in the output vector by replacing the bandit best arm identification algorithm with any top- $k$ identification algorithm [39]. However, in LLM inference, the goal is often to draw a sample from the softmax output distribution via nucleus sampling [17]. Nucleus sampling avoids specifying $k$ directly; instead, it provides cumulative probability $p$ and requires the identification of the top $k$ elements such that $k$ is the smallest value such that the sum of the probabilities of the top $k$ elements is greater than $p$ . The next token is then sampled from the renormalization probability distribution on these $k$ elements. Our adaptive sampling algorithm naturally applies to the nucleus sampling setting. AdaptiveSoftmax can maintain a predicted set of arms $S$ such that the sum of the arm probabilities $\\hat{p}$ is greater than $p$ based on pessimistic arm mean estimates. Then, we iteratively sample: a) arms in $S$ , sampling both the arm with the lowest mean minus LCB (in an attempt to verify the boundary), as well as the arm with the widest confidence interval (in order to better estimate $\\hat{p}_{.}$ ), and b) sampling the top arm in $[n]\\setminus S$ , to see if it belongs in $S$ . For simplicity and concreteness, in this work we focus on identifying and estimating the probability of the top-1 element, but this is an exciting direction of future work. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the theoretical and practical contributions of the paper, highlighting its theoretical guarantees, sample complexity reduction, and potential for wall-clock improvements. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss assumptions on known sub-Gaussian parameter bounds in Assumption 1 and computational difficulties in realizing sample complexity gains as wall-clock gains. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All formal claims are stated with their requisite assumptions in the main text, with proof sketches. Detailed theoretical proofs are provided or cited in the appendix. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a one-line reproducibility script to reproduce the results in the paper. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our codebase publicly available on github, and is reproducible via a 1-line reproducibility script: https://github.com/ThrunGroup/adaptiveSoftmax. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is described in the paper ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Error bars are provided for all plots. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: This is described in the paper. ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] Justification: The research in this paper conforms with the stated Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss broader impacts in the final section of the paper. There are minimal societal effects, and potential energy and environmental savings from more efficient computation. ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: The algorithms proposed in this paper do not have high risk for misuse. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: All resources are publicly available. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide a publicly available github https://github.com/ThrunGroup/ adaptiveSoftmax that is well documented and has a 1-line reproducibility script. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not use or perform any research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not have any human subjects, and does not require IRB approvals. ", "page_idx": 33}]