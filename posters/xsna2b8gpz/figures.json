[{"figure_path": "XsNA2b8GPz/figures/figures_6_1.jpg", "caption": "Figure 1: Sample complexity of the Adaptive Softmax algorithm and the brute-force softmax computation on two different synthetic datasets as a function of d. Error bars are obtained from 100 random trials. The sample complexity of the AdaptiveSoftmax algorithm scales with respect to d for (a) but does not for (b), as expected. The average gains for \u03b4 = 10% and \u03b5 = 30% are 3.953\u00d7 for (a) and 29.039\u00d7 for (b), increasing with increasing dimension. Confidence intervals are 1std.", "description": "This figure shows the sample complexity of the Adaptive Softmax algorithm and the naive (brute-force) softmax computation on two synthetic datasets, as a function of the dimension (d).  The figure demonstrates that Adaptive Softmax has sublinear scaling with d in certain scenarios, significantly outperforming the naive approach.  Error bars represent the standard deviation over 100 random trials.  The gains from using Adaptive Softmax are shown to increase as the dimension increases.", "section": "Complexity on Synthetic Data"}, {"figure_path": "XsNA2b8GPz/figures/figures_6_2.jpg", "caption": "Figure 1: Sample complexity of the Adaptive Softmax algorithm and the brute-force softmax computation on two different synthetic datasets as a function of d. Error bars are obtained from 100 random trials. The sample complexity of the AdaptiveSoftmax algorithm scales with respect to d for (a) but does not for (b), as expected. The average gains for \u03b4 = 10% and \u03b5 = 30% are 3.953\u00d7 for (a) and 29.039\u00d7 for (b), increasing with increasing dimension. Confidence intervals are 1std.", "description": "This figure shows the sample complexity of AdaptiveSoftmax and the naive softmax computation on two synthetic datasets.  Dataset (a) shows that AdaptiveSoftmax scales sublinearly with the dimension (d) of the input, while dataset (b) shows that the sample complexity of AdaptiveSoftmax is largely independent of d. This demonstrates the sublinear scaling of AdaptiveSoftmax in the dimension of the input and its significantly improved efficiency compared to the naive approach.", "section": "Complexity on Synthetic Data"}, {"figure_path": "XsNA2b8GPz/figures/figures_13_1.jpg", "caption": "Figure 1: Sample complexity of the Adaptive Softmax algorithm and the brute-force softmax computation on two different synthetic datasets as a function of d. Error bars are obtained from 100 random trials. The sample complexity of the AdaptiveSoftmax algorithm scales with respect to d for (a) but does not for (b), as expected. The average gains for \u03b4 = 10% and \u03b5 = 30% are 3.953\u00d7 for (a) and 29.039\u00d7 for (b), increasing with increasing dimension. Confidence intervals are 1std.", "description": "This figure demonstrates the scaling behavior of the Adaptive Softmax algorithm and a standard softmax computation on two synthetic datasets.  The x-axis represents the dimension (d) of the input vector, and the y-axis represents the sample complexity.  Two subfigures (a) and (b) show results for different synthetic data generation methods.  In (a), the Adaptive Softmax algorithm's sample complexity scales with d, as expected, while in (b), it does not. Error bars indicate the standard deviation across 100 random trials.  The figure highlights the significant gains in sample efficiency achieved by the Adaptive Softmax algorithm, especially in (b), where the gain is larger than 29x.", "section": "Complexity on Synthetic Data"}, {"figure_path": "XsNA2b8GPz/figures/figures_16_1.jpg", "caption": "Figure 3: (a) Sampled entries of A and the corresponding entries of x for Mistral on the Wikitext dataset. The values of A are symmetrical about 0 and not correlated with x. (b) Sampled values of the second order term (Ai,j; Ai,kXjXk)(Tj,k\u2212\u03c0j\u03c0k)/(\u03c0j\u03c0k)", "description": "This figure shows the distribution of sampled entries of matrix A and vector x for the Mistral language model on the Wikitext dataset.  Panel (a) is a 2D histogram showing a symmetrical distribution centered around zero, indicating no correlation between A and x values. Panel (b) displays a histogram of the second-order term, which is relevant for variance calculations in the AdaptiveSoftmax algorithm.", "section": "A.3 Improved estimators"}, {"figure_path": "XsNA2b8GPz/figures/figures_16_2.jpg", "caption": "Figure 3: (a) Sampled entries of A and the corresponding entries of x for Mistral on the Wikitext dataset. The values of A are symmetrical about 0 and not correlated with x. (b) Sampled values of the second order term (Ai,j; Ai,kXjXk) (Tj,k - TjTk)/(Tj\u03c0k)", "description": "This figure visualizes the properties of the sampled entries of matrix A and vector x in the context of the Mistral model applied to the Wikitext dataset.  Panel (a) shows a 2D histogram illustrating the relationship between the sampled values from A and x, revealing a symmetrical distribution around zero and demonstrating a lack of correlation between the two sets of values. Panel (b) presents the distribution of the calculated second-order term, (Ai,j; Ai,kXjXk)(Tj,k - TjTk)/(Tj\u03c0k). The distribution of this second-order term is shown to be highly concentrated around zero. These observations support the assumptions made in the theoretical analysis of the paper regarding the statistical properties of the data.", "section": "A.3 Improved estimators"}, {"figure_path": "XsNA2b8GPz/figures/figures_17_1.jpg", "caption": "Figure 4: Variance estimates vs. empirical mean squared error. This demonstrates the dramatic improvement afforded by improved estimators and tight confidence intervals. [imp] indicates importance sampling, [wr] with replacement, [wor] without replacement, [fpc-sparse] includes the finite population correction factor.", "description": "This figure compares different variance estimation methods for the AdaptiveSoftmax algorithm.  It shows the mean squared error (MSE) against the sample size.  The methods compared include: variance estimation with replacement (wr), variance estimation with importance sampling (imp) with and without replacement, variance estimation with importance sampling and finite population correction (fpc-sparse), and a new variance estimation method proposed in the paper. The results demonstrate that the new method significantly reduces the MSE compared to the existing methods, especially with larger sample sizes.", "section": "B Proofs"}]