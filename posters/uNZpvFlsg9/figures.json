[{"figure_path": "uNZpvFlsg9/figures/figures_1_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \"agree\" it.", "description": "This figure illustrates the PiCO framework, which uses peer review among LLMs to evaluate their capabilities without human intervention.  LLMs answer unlabeled questions and then evaluate each other's responses. Each LLM is assigned a learnable capability parameter that influences its final ranking. The goal is to create a consistent ranking based on the assumption that higher-level LLMs provide more accurate evaluations and achieve higher scores. The entropy reduction aspect aims to minimize inconsistencies in the evaluations.", "section": "1 Introduction"}, {"figure_path": "uNZpvFlsg9/figures/figures_2_1.jpg", "caption": "Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN, CIN, and LIS, respectively", "description": "This figure illustrates three metrics used to evaluate how well a learned ranking of LLMs aligns with human preferences.  These metrics quantify the difference between a learned ranking and a ground truth ranking.  The figure shows an example of each metric calculating the difference between a hypothetical learned ranking (R) and a ground-truth ranking (R*). PEN (Permutation Entropy) measures the randomness of the ranking. CIN (Count Inversions) counts the number of inversions between the two rankings.  LIS (Longest Increasing Subsequence) finds the length of the longest increasing subsequence in the learned ranking compared to the ground truth ranking. Smaller values for PEN and CIN, and larger values for LIS indicate better alignment with human preferences.", "section": "2.1 Definition and Metrics"}, {"figure_path": "uNZpvFlsg9/figures/figures_3_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \u201cagree\u201d it.", "description": "The figure illustrates the PiCO framework, where multiple LLMs evaluate each other's responses to unlabeled questions.  Each LLM receives a score determined by the evaluations of other LLMs. A learnable capability parameter adjusts the final ranking, aiming to maximize consistency and align with a hypothetical human ranking.  The consistency optimization process seeks to create a final ranking where all LLMs largely agree.", "section": "The Proposed Approach"}, {"figure_path": "uNZpvFlsg9/figures/figures_6_1.jpg", "caption": "Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN, CIN, and LIS, respectively", "description": "This figure illustrates three metrics used to evaluate how well a learned LLM ranking aligns with human rankings.  PEN (Permutation Entropy) measures the randomness or irregularity of the ranking sequence. CIN (Count Inversions) counts the number of pairs in the ranking that are in reverse order compared to the ground truth. LIS (Longest Increasing Subsequence) finds the length of the longest sub-sequence that is in increasing order.  These metrics help quantify the differences between the learned ranking and the human ranking, providing insights into the quality of the LLM ranking.", "section": "2.1 Definition and Metrics"}, {"figure_path": "uNZpvFlsg9/figures/figures_7_1.jpg", "caption": "Figure 5: Performance comparison of the PiCO (Ours) and PRE[17] methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower values indicate better performance.", "description": "This figure shows the performance comparison of PiCO and PRE methods across three different datasets (Chatbot Arena, MT-Bench, and AlpacaEval). The x-axis represents the number of eliminated reviewers, while the y-axis represents the CIN (Count Inversions) metric. Lower CIN values indicate better performance, signifying a higher alignment with human preferences. The figure visualizes how the performance of both PiCO and PRE methods change as more weak reviewers are eliminated during the evaluation process.", "section": "3.3 Study of Elimination Mechanism"}, {"figure_path": "uNZpvFlsg9/figures/figures_12_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \"agree\" it.", "description": "This figure illustrates the PiCO framework, which uses peer review among LLMs to automatically evaluate their capabilities without human input.  Open- and closed-source LLMs answer unlabeled questions and rate each other's responses. Each LLM receives a score based on these peer reviews, weighted by a learnable capability parameter which is optimized to maximize score consistency and align with a hypothetical human ranking. The goal is to create a final ranking that all LLMs agree upon.", "section": "1 Introduction"}, {"figure_path": "uNZpvFlsg9/figures/figures_14_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \"agree\" it.", "description": "This figure illustrates the PiCO framework, which uses a peer-review mechanism for LLMs evaluation.  Both open and closed-source LLMs answer unlabeled questions and then evaluate each other's answers.  Each LLM is assigned a learnable capability parameter, used to optimize rankings based on the consistency assumption (higher-level LLMs evaluate more accurately). The goal is to create a final ranking of LLMs that reflects a consensus among the models themselves.", "section": "1 Introduction"}, {"figure_path": "uNZpvFlsg9/figures/figures_16_1.jpg", "caption": "Figure 5: Performance comparison of the PiCO (Ours) and PRE[17] methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower values indicate better performance.", "description": "This figure compares the performance of the PiCO and PRE methods across three datasets (Chatbot Arena, MT-Bench, and AlpacaEval) by varying the number of eliminated reviewers. The y-axis represents the CIN metric, where lower values signify better performance.  The figure helps demonstrate the effect of the elimination mechanism in PiCO and how it compares to PRE.", "section": "3.3 Study of Elimination Mechanism"}, {"figure_path": "uNZpvFlsg9/figures/figures_16_2.jpg", "caption": "Figure 5: Performance comparison of the PiCO (Ours) and PRE[17] methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower values indicate better performance.", "description": "This figure shows the performance comparison of PiCO and PRE methods across three datasets (Chatbot Arena, MT-Bench, and AlpacaEval) by varying the number of eliminated reviewers. The y-axis represents CIN (Count Inversions), a lower CIN value indicates better performance, showing how well the ranking aligns with human ranking.  The x-axis shows the number of eliminated reviewers.  The figure helps to illustrate the impact of eliminating less effective reviewers on the overall ranking performance of both methods.", "section": "3.3 Study of Elimination Mechanism"}, {"figure_path": "uNZpvFlsg9/figures/figures_17_1.jpg", "caption": "Figure 9: Comparison of performance on the CIN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.", "description": "This figure compares the performance of different LLM evaluation methods across three datasets (MT-Bench, Chatbot Arena, AlpacaEval) using the Count Inversions (CIN) metric.  It contrasts the performance of unsupervised methods (PiCO, PRD, PRE) with the performance of using single models for evaluation. The results visually demonstrate the superiority of unsupervised, multi-model approaches like PiCO in aligning more closely with human preferences compared to relying on individual model scores. The dotted line represents a baseline of using only single models.", "section": "3.1 Performance Comparison"}, {"figure_path": "uNZpvFlsg9/figures/figures_17_2.jpg", "caption": "Figure 4: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher values (above 0) indicate greater evaluation bias [17]. The first row shows original PG values in three datasets, while the second row displays PG values re-weighted using our learned confidence weights.", "description": "This figure shows heatmaps visualizing the preference gap (PG) metric for seven LLMs across three datasets (Chatbot Arena, MT-Bench, and AlpacaEval).  The preference gap measures the bias of each LLM in its evaluation of other LLMs. A higher PG value (above 0) indicates a greater bias. The figure displays two sets of heatmaps: one showing the original preference gaps and another showing the preference gaps after re-weighting using the confidence weights learned by the PiCO model. By comparing the two sets, we can see how well PiCO reduces the evaluation bias of the LLMs.", "section": "3.2 Exploring the Role of Confidence Weight"}, {"figure_path": "uNZpvFlsg9/figures/figures_18_1.jpg", "caption": "Figure 11: Comparison of performance on the LIS metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.", "description": "The figure compares the performance of different LLMs evaluation methods across three datasets (MT-Bench, Chatbot Arena, and AlpacaEval) using the LIS metric.  It contrasts the performance of unsupervised methods (PiCO, PRD, PRE) with that of using single models.  The LIS metric measures the length of the longest increasing subsequence, where higher values indicate better alignment with the expected ranking. The dotted line shows the average LIS score achieved using single models as a baseline.  The results show that PiCO generally outperforms single models and other unsupervised methods.", "section": "3.1 Performance Comparison"}, {"figure_path": "uNZpvFlsg9/figures/figures_19_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \u201cagree\u201d it.", "description": "The figure illustrates the PiCO framework, a peer-review mechanism for evaluating LLMs.  Open- and closed-source LLMs answer unlabeled questions and then evaluate each other's responses. Each LLM receives a score based on these evaluations, weighted by a learnable parameter reflecting its capability. The goal is to create a consistent ranking of LLMs that reflects their abilities, which is achieved through consistency optimization to reduce the entropy and achieve consensus in the peer reviews.", "section": "The Proposed Approach"}, {"figure_path": "uNZpvFlsg9/figures/figures_20_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \"agree\" it.", "description": "This figure illustrates the PiCO framework, which uses a peer-review mechanism for evaluating LLMs.  Open-source and closed-source LLMs answer unlabeled questions and anonymously evaluate each other's responses. Each LLM is assigned a learnable capability parameter to optimize the final ranking, aiming for consistency in capability and score among all LLMs.", "section": "The Proposed Approach"}, {"figure_path": "uNZpvFlsg9/figures/figures_21_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \"agree\" it.", "description": "This figure illustrates the PiCO framework.  It shows how multiple LLMs (both open and closed source) answer unlabeled questions and then act as peer reviewers, evaluating each other's responses. Each LLM receives a learnable capability parameter to help determine its final ranking. The goal of the system is to find a consistent ranking across all LLMs.  The process uses a consistency optimization to refine the ranking, aiming for maximum agreement amongst the LLMs.", "section": "2 The Proposed Approach"}, {"figure_path": "uNZpvFlsg9/figures/figures_22_1.jpg", "caption": "Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \"agree\" it.", "description": "This figure illustrates the PiCO framework, which uses a peer-review mechanism for evaluating LLMs.  Open and closed-source LLMs answer questions and evaluate each other's answers. Each LLM gets a score based on the evaluations of other LLMs. A learnable parameter is used to adjust the final ranking, aiming to maximize consistency among LLMs' capabilities and scores, and to align the ranking with human preferences. The process is unsupervised and involves minimizing entropy in the evaluation system.", "section": "1 Introduction"}]