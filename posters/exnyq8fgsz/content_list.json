[{"type": "text", "text": "Active Learning of General Halfspaces: Label Queries vs Membership Queries ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ilias Diakonikolas University of Wisconsin-Madison ilias@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Daniel M. Kane University of California, San Diego dakane@cs.ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Mingchen Ma University of Wisconsin-Madison mingchen@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of learning general (i.e., not necessarily homogeneous) halfspaces under the Gaussian distribution on $\\mathbb{R}^{d}$ in the presence of some form of query access. In the classical pool-based active learning model, where the algorithm is allowed to make adaptive label queries to previously sampled points, we establish a strong information-theoretic lower bound ruling out non-trivial improvements over the passive setting. Specifically, we show that any active learner requires label complexity of $\\tilde{\\Omega}(d/(\\log(m)\\epsilon))$ , where $m$ is the number of unlabeled examples. Specifically, to beat the passive label complexity of $\\tilde{O}(d/\\epsilon)$ , an active learner requires a pool of $2^{\\mathrm{poly}(d)}$ unlabeled samples. On the positive side, we show that this lower bound can be circumvented with membership query access, even in the agnostic model. Specifically, we give a computationally efficient learner with query complexity of $\\tilde{O}(\\operatorname*{min}\\{1/p,1/\\epsilon\\}+d\\mathrm{polylog}(1/\\epsilon))$ achieving error guarantee of $O(\\mathrm{opt}+\\dot{\\epsilon})$ . Here $p\\in[0,1/2]$ is the bias and opt is the 0-1 loss of the optimal halfspace. As a corollary, we obtain a strong separation between the active and membership query models. Taken together, our results characterize the complexity of learning general halfspaces under Gaussian marginals in these models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In Valiant\u2019s PAC learning model [29, 30], the learner is given access to random labeled examples and aims to find an accurate approximation to the function that generated the labels. The standard PAC model is \u201cpassive\u201d in the sense that the learner has no control over the selection of the training set. Here we focus on interactive learning between a learner and a domain expert that can potentially lead to significantly more efficient learning procedures. A standard such paradigm is (pool-based) active learning [26], where the learner has access to a large pool of unlabeled examples $S$ and has the ability to (adaptively) select a subset of $S$ and obtain their labels. We will henceforth refer to this type of data access as label query access. An even stronger interactive model is that of PAC learning with membership queries [1, 18]. A membership query $(M Q)$ allows the learner to obtain the value of the target function on any desired point in the support of the marginal distribution. This model captures the ability to perform experiments or the availability of expert advice. While in active learning, the learner is allowed to query the labels of previously sampled points from $S$ , in MQ learning the learner has black-box access to the target function. We refer the reader to Appendix A for formal definitions of these two learning models. Roughly speaking, when the size of $S$ becomes exponentially large (so that it is a good cover of the space), the model of active learning \u201cconverges\u201d to the model of learning with MQs. This intuitive connection will be useful in the proceeding discussion. ", "page_idx": 0}, {"type": "text", "text": "Active learning is motivated by the availability of large amounts of unlabeled data at low cost. As such, the typical goal in this model is to develop algorithms with qualitatively improved label complexity (compared to passive learning) at the expense of a larger \u2014 but, ideally, still reasonably bounded \u2014 set of unlabeled data. Over the past two decades, a large body of work in theoretical machine learning has studied the possibilities and limitations of active learning in a variety of natural and important settings; see, e.g. [20, 6, 7, 9, 3, 4, 21, 22, 17, 32]. ", "page_idx": 1}, {"type": "text", "text": "A prototypical setting where active learning leads to substantial savings is for the task of learning homogeneous Linear Threshold Functions (LTFs) or halfspaces. An LTF is any function $h:\\mathbb{R}^{d}\\stackrel{}{\\rightarrow}$ $\\{\\pm1\\}$ of the form $h(x)=\\mathrm{sign}(w\\cdot x+t)$ , where $w\\in S^{d-1}$ is called the weight vector and $t$ is called the threshold. If $t=0$ , the halfspace is called homogeneous. The problem of learning halfspaces is one the classical problems in machine learning, going back to the Perceptron algorithm [27] and has had a great impact on many other influential techniques, including SVMs [31] and AdaBoost [19]. ", "page_idx": 1}, {"type": "text", "text": "For the class of homogeneous halfspaces under well-behaved distributions (including the Gaussian and isotropic log-concave distributions), prior work has established that $O(d\\log(1/\\epsilon))$ label queries suffice, where $d$ is the dimension and $\\epsilon$ is the desired accuracy [3, 9, 5]. Moreover, there are computationally efficient algorithms with near-optimal label complexity for this task [2, 34, 28], even in the agnostic model that achieve error $O(\\mathrm{opt}+\\epsilon)$ . Unfortunately, this logarithmic dependence on $1/\\epsilon$ breaks down for general (potentially biased) halfspaces. Intuitively, this holds because if the bias of a halfspace (the probability mass of the small class) is $p$ , then we need to obtain at least $1/p$ labeled examples before we see the first point in the small class. This implies an information-theoretic label complexity lower bound of $\\Omega(\\bar{\\mathrm{min}}\\{1/p,1/\\epsilon\\}+d\\log(1/\\epsilon))$ [7], even for realizable PAC learning under the uniform distribution on the sphere. Hanneke et al. [4] showed an information-theoretic label complexity upper bound of $\\tilde{O}((1/p)d^{3/2}\\log(1/\\epsilon))$ for general halfspaces under the uniform distribution on the sphere (via an exponential-time algorithm). ", "page_idx": 1}, {"type": "text", "text": "In summary, prior to this work, the possibility that there is an active learner with label complexity $O(d\\log(1/\\epsilon)^{-}+\\operatorname*{min}\\{1/p,1/\\epsilon\\})$ and unlabeled sample complexity poly $(d/\\epsilon)$ remained open. Our first main result is an information-theoretic lower bound ruling out this possibility. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Main Lower Bound). Let $S$ be a set of $m$ i.i.d. points drawn from $N(0,I)$ . With probability at least $2/3,$ , for any active learning algorithm $\\mathcal{A},$ , there is a halfspace $h^{*}$ that labels $S$ with bias $p$ such that if $\\boldsymbol{\\mathcal{A}}$ makes less than $\\tilde{O}(d/(p\\log(m)))$ label queries over $S$ , then with probability at least $2/3$ the halfspace $\\hat{h}$ output by $\\boldsymbol{\\mathcal{A}}$ has error more than $p/2$ with respect to $h^{*}$ . ", "page_idx": 1}, {"type": "text", "text": "In particular, if $p$ is chosen as $\\Theta(\\epsilon\\log(1/\\epsilon))$ , learning a $p$ -bias halfspace with error $C\\epsilon$ (for any fixed constant $C$ ) would require a learning algorithm to either make $\\tilde{\\Omega}(d/\\epsilon)$ label queries or have a pool of $2^{d}$ unlabeled examples. Our information-theoretic lower bound essentially shows that the active setting does not provide non-trivial advantages for the class of general halfspaces, unless the learner is allowed to obtain exponentially many unlabeled examples. (As already mentioned, in this extreme setting, the active learning model approximates PAC learning with MQs.) This motivates the study of learning halfspaces in the stronger model with MQs, where better upper bounds may be attainable. ", "page_idx": 1}, {"type": "text", "text": "To circumvent the aforementioned lower bound, we consider the stronger model of PAC learning with MQs. We are interested in understanding the query complexity of learning general halfspaces under the Gaussian distribution. We study this question in the agnostic learning model and establish the following positive result, the proof of which can be found in Appendix G: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2 (Main Algorithmic Result). Consider the problem of agnostic PAC learning halfspaces with membership queries under the Gaussian distribution. There is an algorithm such that for every labeling function $y(x)$ and for every \u03f5 $,\\delta\\in(0,1)$ , it makes $M=\\tilde{O}_{\\delta}(\\operatorname*{min}\\{1/p,1/\\epsilon\\}\\!+\\!d\\mathrm{polylog}(1/\\epsilon))$ 1 memberships queries, runs in poly $(d,M)$ time, where $p$ is the bias of the optimal halfspace $h^{*}$ , and outputs an ${\\hat{h}}\\in H$ such that with probability at least $1-\\delta$ , $\\mathrm{err}(\\hat{h})\\leq O(\\mathrm{opt}+\\epsilon)$ . ", "page_idx": 1}, {"type": "text", "text": "In other words, we provide a computationally efficient constant factor agnostic query learner with query complexity $\\tilde{O}(\\operatorname*{min}\\{1/p,1/\\epsilon\\}+d\\mathrm{polylog}(1/\\epsilon))$ . Due to known $d^{\\mathrm{poly}(1/\\epsilon)}$ complexity lower bounds for achieving optimal error of $\\mathrm{opt}+\\epsilon$ [11, 13, 14], the majority of work [15, 16] in the passive PAC model has focused on designing efficient learners achieving a constant factor approximation of $O(\\mathrm{opt}+\\epsilon)$ These passive learning algorithms have sample complexity poly $(d,1/\\epsilon)$ . Note that, by ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1, it is impossible to modify these algorithms (for general halfspaces) to achieve an active learner with low label complexity. ", "page_idx": 2}, {"type": "text", "text": "In the realizable setting under the Gaussian distribution, a learner may query many points that are extremely far from the origin to find examples from the small class with few queries. However, such an algorithm is quite fragile to even a tiny amount of noise. In particular, the query complexity achieved by our algorithm establishing Theorem 1.2 is nearly optimal in the agnostic setting. ", "page_idx": 2}, {"type": "text", "text": "On the one hand, $\\Omega(d\\log(1/\\epsilon))$ queries are required because describing a halfspace up to error $\\epsilon$ requires $d\\log(1/\\epsilon)$ bits of information [25]. On the other hand, we argue that the overhead term of $\\Omega(\\operatorname*{min}\\{1/p,1/\\epsilon\\})$ cannot be avoided in the agnostic setting. Such a statement can be deduced from a lower bound of [23]: they showed that in the realizable setting, any algorithm requires at least $\\Omega((1/p)^{1-o(1)})$ MQs to see the first example from the small class (where $p$ is the bias of the target halfspace with respect to the uniform distribution on the unit ball); they also showed a similar lower bound of $\\Omega(1/p)$ if the underlying distribution is the uniform distribution over the unit sphere. As the dimension $d$ increases, the standard Gaussian distribution \u221ais very well approximated by the uniform distribution over a $d$ -dimensional sphere with radius $\\sim\\sqrt{d}$ . Thus, an exponentially small level of noise would make every query far from this sphere contain no useful information. This allows us to show that, under the Gaussian distribution with a tiny amount of label noise, $\\Omega((1/p)^{1-o(1)})$ queries are needed to see a single example from the small class. The proof of this statement is essentially identical to the argument in [23] for unit ball. The reader is referred to that work for the details. ", "page_idx": 2}, {"type": "text", "text": "1.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For a halfspace $h(x)=\\mathrm{sign}(w\\cdot x+t)$ , $w\\in S^{d-1},t>0$ , we use $p(t)=\\mathbf{Pr}_{x\\sim N(0,I)}(h(x)=-1)$ to denote its bias. For a halfspace $h(x)$ , we define its Chow-Parameter under the standard Gaussian distribution to be ${\\mathbf E}_{x\\sim N(0,I)}\\,x h(x)$ . Let $y(x):\\mathbb{R}^{d}\\rightarrow\\{\\pm1\\}$ be a (randomized) labeling function for examples in $\\mathbb{R}^{d}$ . We denote by $\\mathrm{err}(h)=\\mathbf{Pr}_{x\\sim N(0,I)}(h(x)\\neq y(x))$ to be the error of the hypothesis $h$ and $\\mathrm{opt}=\\operatorname*{min}_{h\\in H}\\mathrm{err}(h)$ , where $H$ is the class of halfspaces over $\\mathbb{R}^{d}$ . We will use $h^{*}$ to denote the halfspace with an error equal to opt. When there is no confusion, we will use $p$ to denote the bias of the optimal halfspace $h^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "Let $D_{x}$ be a distribution over $\\mathbb{R}^{d}$ , $y(x)$ be a labeling function over $\\mathbb{R}^{d}$ and ${\\cal{S}}=\\{(x_{i},y(x_{i}))\\}_{i=1}^{m}$ be a set of i.i.d. examples drawn from the distribution $D$ over $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ such that the marginal distribution of $D$ is $D_{x}$ . A membership query takes an $x$ in the support of $D_{x}$ as input and outputs $y(x)$ . A label query takes an $x_{i}$ , where $(x_{i},y(x_{i}))\\,\\in\\,S$ as input and outputs $y(x_{i})$ . A learning algorithm $\\boldsymbol{\\mathcal{A}}$ is allowed to use membership queries/label queries and aims to output a halfspace hypothesis $\\hat{h}$ such that $\\mathrm{err}(\\hat{h})\\leq O(\\mathrm{opt}+\\epsilon)$ by making as few queries as possible. ", "page_idx": 2}, {"type": "text", "text": "2 Nearly-Tight Lower Bound on Label Complexity: Proof of Theorem 1.1 ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we prove our information-theoretic lower bound on the label complexity of active learning general halfspaces under the Gaussian distribution. ", "page_idx": 2}, {"type": "text", "text": "Before presenting our proof, we provide high-level intuition behind Theorem 1.1 and the strategy of our proof. Previous work, see, e.g. [6, 9, 23], showed that if $S$ is a set of examples drawn uniformly from the unit sphere, and if $h^{*}$ is a halfspace with bias $p$ that is chosen uniformly, the following holds: no matter which query strategy a learning algorithm $\\boldsymbol{\\mathcal{A}}$ uses, for the first $r$ queries, in expectation only pr of them fall into the small cap on the sphere cut by $h^{*}$ . Thus, if $\\boldsymbol{\\mathcal{A}}$ makes less than $1/(2p)$ queries, it will with constant probability not see any negative examples; and it is therefore impossible to learn the target halfspace. ", "page_idx": 2}, {"type": "text", "text": "In the Gaussian case, we will use a similar but stronger idea. If we are able to learn $h^{*}$ up to error $p/2$ with few queries, then we can randomly partition $S$ into two sets, use the first set to learn the halfspace and use the second part to find $d$ negative examples by paying another $O(d)$ queries in expectation. Formally, we have the following statement, the proof of which can be found in Appendix B.1. ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.1. Suppose there is an active learning algorithm that can make r label queries over a pool $S$ of $m\\geq\\mathrm{poly}(d/p)$ examples drawn from $N(0,I)$ and learn any halfspace $h^{*}(x)=\\mathrm{sign}(w^{*}{\\cdot}x{+}t^{*})$ with bias $p$ up to error $p/2$ with probability at least $2/3$ . Then there is an algorithm such that given $a$ pool of 2m random examples $S$ drawn from the standard Gaussian distribution with hidden labels by some halfspace $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ with bias $p$ , it makes $r+O(d)$ queries and finds $d$ negative examples from $S$ with probability $1/2$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We will show that finding $d$ negative examples from $S$ requires many queries. The idea is that since $S$ is sampled from a standard Gaussian in high dimensions, every pair of examples is almost orthogonal unless $m$ is as large as $2^{d}$ . If we have made $1/\\epsilon$ queries over $S$ and found our first negative example, then this negative example will only provide us with very little knowledge to find the next negative example \u2014 as no example in the pool has a large correlation with it. Therefore, it will still take us another approximately $1/\\epsilon$ queries to find the next negative example. Such an issue only disappears after we have already found roughly $d$ negative examples; at which time, the average of the $d$ examples has a good correlation with $w^{*}$ . Therefore, it would take us roughly $d/\\epsilon$ queries in total. We remark that such an argument is hard to formalize, because, besides negative examples, the algorithm has also seen many positive examples in the process. It is thus challenging to argue that the algorithm cannot make good use of the information obtained from these positive examples. ", "page_idx": 3}, {"type": "text", "text": "To overcome this difficulty, our proof strategy works as follows. Each algorithm $\\boldsymbol{\\mathcal{A}}$ can be described as a decision tree. Each tree node represents the example queried in a given round. Every time the algorithm sees a negative example, it moves to the left; otherwise, it moves to the right. Suppose that $\\boldsymbol{\\mathcal{A}}$ wants to find $k$ negative examples with $r$ queries. Then there are at most $\\binom{r}{k}\\leq\\bar{(}e r/k)^{\\bar{k}}$ paths of the tree, where $\\boldsymbol{\\mathcal{A}}$ successfully finds $k$ negative examples, and for each of the paths there are exactly $k$ examples that are negative upon queried. For a $k$ -tuple of examples, we will derive a deterministic condition such that if the $k$ examples satisfy the condition, a random halfspace with bias $p$ will have only roughly $p^{k}$ probability to label all of the $k$ examples negative. ", "page_idx": 3}, {"type": "text", "text": "Formally, we establish the following technical lemma (see Appendix B.2 for the proof). ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.2. Let $A\\in\\mathbb{R}^{k\\times d}$ be a matrix with row vectors $x_{1},\\ldots,x_{k}$ . Let $t^{*}>C>0$ for some sufficiently large constant $C$ . Let $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ be a random halfspace with bias $p$ with $w^{\\ast}\\sim S^{d-1}$ chosen uniformly from $S^{d-1}$ . If $\\|A A^{\\top}-d I\\|_{2}\\leq O(d/(t^{*})^{2})$ , then with probability at most $O(p\\log(1/p))^{k}$ , where $p$ is the bias of $h^{*}$ under the Gaussian distribution, $h^{*}(x_{i})=-1$ for $i=1,\\ldots,k$ . ", "page_idx": 3}, {"type": "text", "text": "Thus, if the $\\binom{r}{k}$ tuples all satisfy such a condition, then $\\boldsymbol{\\mathcal{A}}$ will succeed with a fairly tiny probability unless $r$ is larger than $k/p$ . So, in the last step of the proof, we will show in Lemma 2.3 that by taking $k\\approx d/(\\log(m)\\mathrm{polylog}(1/p))$ , with high probability every $k$ -tuple of examples in $S$ will satisfy the deterministic condition. Thus, no algorithm can succeed with a constant probability, unless it makes $\\tilde{\\Omega}(d/(p\\log(m)))$ queries. The proof of Lemma 2.3 can be found in Appendix B.3. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.3. Let $S\\subseteq\\mathbb{R}^{d}$ be a set of m examples drawn i.i.d. from $N(0,I)$ . Let $t^{*}>C>0$ for a sufficiently large constant $C$ and $\\dot{k}\\,=\\,O(d/\\log(m)(t^{*})^{4})$ . Then, with probability at least $2/3$ , for every $k$ -tuple of examples $\\{x_{1},\\ldots,x_{k}\\}\\subseteq S$ , $\\left\\|A A^{\\top}-d I\\right\\|_{2}\\leq d/(t^{*})^{2}$ , where $A\\in\\mathbb{R}^{k\\times d}$ be $a$ matrix with row vectors x1, . . . , xk. ", "page_idx": 3}, {"type": "text", "text": "Proof of Theorem 1.1. We will start by showing that, given a set $S$ of $m$ points drawn i.i.d. from a Gaussian distribution, the following holds. With probability at least $2/3$ , for every algorithm $\\boldsymbol{\\mathcal{A}}$ there exists a halfspace $h^{*}=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ with bias $p$ such that if $\\boldsymbol{\\mathcal{A}}$ makes only $r=\\tilde{O}(d/p\\log(m))$ label queries over $S$ , then with probability at least $2/3$ it will not be able to find $k$ negative examples in $S$ for some $k\\leq d$ . By Yao\u2019s minimax principle, it is sufficient to show that there is a distribution over halfspaces $h^{*}$ such that for any deterministic active learning algorithm, the following holds: given $m$ random Gaussian examples, if the learning algorithm makes $r$ queries, with probability $2/3$ it cannot find $k$ negative examples. We will fix the threshold $t^{*}$ of $h^{*}$ and draw $w^{*}$ uniformly from the unit sphere. ", "page_idx": 3}, {"type": "text", "text": "By Lemma 2.3, we know that by choosing $k=O(d/\\log(m)(t^{*})^{4})$ , with probability at least $2/3$ , for every $k$ -tuple of examples $x_{1},\\ldots,x_{k}\\in S$ , $\\left\\|A A^{\\top}-d I\\right\\|_{2}\\leq d/(t^{*})^{2}$ , where $A\\in\\mathbb{R}^{k\\times d}$ is a matrix with row vectors $x_{1},\\ldots,x_{k}$ . By Lemma 2.2, we know that every $k$ -tuple of examples $x_{1},\\ldots,x_{k}\\in S$ has a probability $\\alpha^{k}$ , which is at most $O(p\\log p)^{k}$ to be labeled all negative by the random halfspace $h^{*}$ . Notice that every query algorithm can be expressed as a binary tree $T$ . Each node of the tree represents an example where the algorithm makes queries at a time. If the example at node $v$ is negative, then the algorithm will query the left child of $v$ , and otherwise it will query the right child of $v$ . The algorithm stops making queries when either it has queried $r$ examples or it has queried $k$ negative examples. In particular, for a given search algorithm, there are at most $\\binom{r}{k}$ different possible outcomes where it successfully finds $k$ negative examples. Furthermore, for each of the possible outcomes, there is a set of $k$ examples in $S$ that correspond to the $k$ negative examples the algorithm finds. Thus, the probability that the algorithm successfully finds $k$ negative examples is bounded above by the probability that there exists one of the ${\\binom{r}{k}}\\ k.$ -tuples of examples in $S$ that are all labeled negative by $h^{*}$ . Such a probability can be bounded above by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\binom{r}{k}}\\alpha^{k}\\leq\\left({\\frac{e r}{k}}O(p\\log(1/p))\\right)^{k}\\leq2/3\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "if $r\\,\\leq\\,O(k/p\\log(1/p))\\,=\\,O(d/(p\\log(m)\\mathrm{polylog}(1/p))$ . By Lemma 2.1, we know that if we can make $O(d/(p\\log(m){\\mathrm{polylog}}(1/p))$ label queries to learn a $p$ -biased halfspace up to error $p/2$ over a set $S$ of $m/2$ Gaussian examples, then we can use $O(d/(p\\log(m){\\mathrm{polylog}}(1/p))$ queries to find $d$ negative examples among $m$ Gaussian points. This leads to a contradiction. Thus, the label complexity of the learning problem is $\\tilde{\\Omega}(d/(p\\log(m)))$ , as desired. ", "page_idx": 4}, {"type": "text", "text": "3 Robust Learning of General Halfspaces with MQs: Proof of Theorem 1.2 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main algorithmic result, Theorem 1.2. We refer the readers to Appendix G for the full proof of Theorem 1.2. Throughout the paper, we will assume for convenience that the noise level opt $\\le\\epsilon$ . Such an assumption can be made without loss of generality, as discussed in Appendix C.1. We first present our main algorithm, Algorithm 1. Algorithm 1 will maintain a list of $\\mathrm{polylog}(1/\\epsilon)$ candidate hypotheses at least one of which has error $O(\\mathrm{opt}+\\epsilon)$ . We will then use a standard tournament approach to find an accurate hypothesis among them. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 QUERY LEARNING HALFSPACE(Efficient Agnostic Learning Halfspaces with Queries) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: error parameter $\\epsilon\\in(0,1)$ , confidence parameter $\\delta\\in(0,1)$   \nOutput: halspace $\\hat{h}(x)=\\mathrm{sign}(\\hat{w}\\cdot x+\\hat{t})$ , where $\\hat{w}\\in S^{d-1},\\hat{t}>0$   \n$\\mathcal{C}\\gets\\emptyset$ $\\triangleright$ Create a list of candidate hypothesises $\\mathcal{C}$   \nUse $\\tilde{O}(\\operatorname*{min}\\{1/p,1/\\epsilon\\})$ queries to estimate $p$ by some $\\hat{p}$ such that $\\hat{p}\\leq p\\leq2\\hat{p}$ (or verify $p<C\\epsilon$   \nand return $+1$ , the constant hypothesis).   \nLet $t_{a},t_{b}>0$ such that a halfspace with threshold $t_{a}$ has bias $2\\hat{p}$ and with threshold $t_{b}$ has bias $\\hat{p}$ .   \nBuild grid points $t_{a}=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{\\psi}=t_{b}$ such that $|t_{i+1}-t_{i}|=1/(2\\log(1/\\epsilon)),\\forall i\\leq\\psi\\!-\\!1$ .   \n$\\triangleright$ Guess the true threshold $t^{*}$ with $t^{\\prime}\\in\\{t_{0},t_{1},\\dotsc\\}$   \nfor $j=0,\\dots,\\psi$ do Repeat the following procedure $\\mathrm{polylog}(1/\\epsilon)\\log(1/\\delta)$ times $\\boldsymbol{v}_{0}\\gets\\mathrm{INITIALIZATION}(\\epsilon,t_{j},\\delta/\\mathrm{polylog}(1/\\epsilon))$ $\\triangleright$ Find a $w_{0}\\in S^{d-1}$ as a warm start $(w_{T},\\hat{t})\\gets\\mathrm{REFINE}(w_{0},t_{j},\\epsilon.\\delta/\\mathrm{polylog}(1/\\epsilon))\\,\\mathsf{v}$ Find a $w_{T}\\in S^{d-1}$ close enough to $w^{*}$ and   \n$\\hat{t}$ close enough to $t^{*}$ based on $w_{0}$ $\\mathcal{C}\\gets\\mathcal{C}\\cup\\{\\mathrm{sign}(w_{T}\\cdot x+\\hat{t})\\}$ $\\triangleright$ Add a new candidate hypothesis to $\\mathcal{C}$   \nFind a good hypothesis $\\hat{h}$ from $\\mathcal{C}$ using Lemma C.1, a standard tournament approach   \nreturn $\\bar{\\ h}$ ", "page_idx": 4}, {"type": "text", "text": "At the beginning of Algorithm 1, we will use random queries to approximately estimate the bias $p$ of the optimal halfspace up to a constant factor. As we will discuss in Appendix C.2, such an estimation can be done with only $\\bar{\\tilde{O}}(\\operatorname*{min}\\{1/p,1/\\epsilon\\})$ queries by applying a doubling trick to the coin estimation problem. In particular, if we find $p<C\\epsilon$ , we can directly output a constant hypothesis as it has error only $O(\\epsilon)$ . Since $t^{*}$ is unknown to us, such an approach can prevent us from using some $t^{\\prime}$ which is much larger than $t^{*}$ in the rest of the learning procedure, which will potentially lead to a larger query complexity. With such a $\\hat{p},t^{*}$ will fall into a reasonable range $[t_{a},t_{b}]$ . We next partition $[t_{a},t_{b}]$ into a grid of size ${\\cal O}(1/\\log(1/\\epsilon))$ and use each of the grid points as an initial guess of $t^{*}$ . In particular, at least one of these grid points $t_{j}$ is ${\\cal O}(1/\\log(1/\\epsilon))$ close to $t^{*}$ . Although such a $t_{j}$ is not accurate enough to be used in the final output hypothesis, as $t^{\\ast}\\leq\\sqrt{\\log(1/\\epsilon)}$ , we will show later that such a $t_{j}$ is enough for us to use it to learn $w^{\\ast},t^{\\ast}$ accurately. Suppose now we have such a good $t_{j}$ . ", "page_idx": 4}, {"type": "text", "text": "We will design two subroutines that make use of $t_{j}$ to produce a good hypothesis $\\mathrm{sign}(w_{T}\\cdot x+\\hat{t})$ . The first algorithm will take $t_{j}$ and the noise level $\\epsilon$ as its input and produce a unit vector $w_{0}$ as an initialization. We will show in Section 3.2 that as long as $|t_{j}-t^{*}|\\leq1/\\log(1/\\epsilon)$ , we can with probability at least $1/\\log(1/\\epsilon)$ produce some $w_{0}$ such that $\\theta(w_{0},w^{*})\\leq O(1/t_{j})$ . By repeating such an initialization algorithm $\\mathrm{polylog}(1/\\epsilon)$ times, with high probability one of these runs will succeed. In particular, such an algorithm has a query complexity of $\\bar{\\tilde{O}}(1/p+\\dot{d}\\mathrm{polylog}(1/\\epsilon))$ . Now assume we have such a $w_{0}$ as a warm-start. Our second subroutine is to refine the direction $w_{0}$ and the threshold $t_{j}$ . More specifically, we will maintain a unit vector $w_{i}$ such that $\\theta_{i}=\\theta(w_{i},w^{*})$ and an upper bound $\\sigma_{i}$ for $\\sin(\\theta_{i}/2)$ . In each round of the refining algorithm, we will use $\\tilde{O}(d)$ queries to update $w_{i}$ . In particular, in each round $\\sigma_{i}$ will decrease by a constant factor and thus after at most $T=\\tilde{O}(\\log(1/\\epsilon))$ rounds, we will have $\\sin(\\theta_{T}/2)\\leq\\sigma_{T}=\\dot{C}\\epsilon\\exp(t_{j}^{2}/2)$ . As we will show in Section 3.1, provided the correct $t^{*}$ , $\\mathrm{sign}(w_{T}\\cdot x+t^{*})$ is at most $O(\\epsilon)$ far from $h^{*}$ . However, to output a good hypothesis, we still need to learn $t^{*}$ up to a high accuracy. When $t^{*}$ is small, we even have to estimate $t^{*}$ up to error $O(\\epsilon)$ , which typically needs many queries. However, as we will show in Section 3.1, given $w_{T}$ close enough to $w^{*}$ , we are able to combine the localization technique used in [15] with this fact to learn $t^{*}$ using only ${\\cal O}(\\log(1/\\epsilon))$ queries. This gives an overview of Algorithm 1 and its query complexity. ", "page_idx": 5}, {"type": "text", "text": "3.1 Refining A Warm-Start ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We will start by discussing how to refine a warm start $w_{0}$ by proving the following theorem. The proof of the theorem and the main algorithm, Algorithm 3 can be found in Appendix D.5. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Let $h^{*}(x)\\;=\\;\\mathrm{sign}(w^{*}\\cdot x\\,+\\,t^{*})$ be a halfspace such that $\\mathrm{err}(h^{*})\\;=\\;\\mathrm{opt}\\;\\leq\\;\\epsilon.$ . Let $t^{\\prime}\\ \\leq\\ \\sqrt{\\log(1/\\epsilon)},w_{0}\\ \\in\\ S^{d-1}$ be inputs of Algorithm 3. If $t^{\\prime}\\,-\\,1/\\log(1/\\epsilon)\\;\\le\\;t^{*}\\;\\le\\;t^{\\prime}$ , $t^{\\prime}\\exp((t^{\\prime})^{2}/2)\\le1/(C\\epsilon)$ and $\\sin(\\theta(w_{0},w^{*})/2)\\leq\\sigma_{0}:=\\operatorname*{min}\\{1/t^{\\prime},1/2\\}$ , then Algorithm $3$ makes $M=\\tilde{O}_{\\delta}(d\\mathrm{polylog}(1/\\epsilon))$ membership queries, runs in $\\mathrm{poly}(d,M)$ time, and outputs $(w_{T},\\hat{t})$ such that with probability at least $1-O(\\delta)$ $),\\,\\mathrm{err}(\\mathrm{sign}(w_{T}\\cdot x+\\hat{t}))\\leq O(\\epsilon)$ . ", "page_idx": 5}, {"type": "text", "text": "As we discussed in Section 3, we will assume we have some $t^{\\prime}$ such that $t^{\\prime}-1/\\log(1/\\epsilon)\\leq t^{*}\\leq t^{\\prime}$ and some $w_{0}$ such that $\\sin(\\theta_{0}/2)\\leq\\sigma_{0}=\\operatorname*{min}\\{1/t^{\\prime},1/2\\}$ , i.e., some initial knowledge of $t^{*},w^{*}$ . Our algorithm runs in iterations and will maintain some $w_{i}$ in round $i$ . We will maintain some unit vector $w_{i}$ and use $\\lVert w_{i}-w^{*}\\rVert\\,=\\,2\\sin(\\theta_{i}/2)$ to measure the progress made by Algorithm 3. The method we use to update $w_{i}$ is a simple projected gradient descent algorithm. Specifically, we will construct a random vector $G_{i}$ over $\\mathbb{R}^{\\hat{d}}$ such that $G_{i}\\perp w_{i}$ and in expectation $g_{i}={\\bf E}\\,G_{i}$ has bounded length and a good correlation with respect to $w^{*}$ . We will show in the following lemma that by estimating $\\mathbf{E}\\,G_{i}$ up to constant error with $\\hat{g_{i}}$ and using the update rule $w_{i+1}=\\mathrm{proj}_{S^{d-1}}(w_{i}+\\mu_{i}\\hat{g}_{i})$ , we are able to significantly decrease $\\theta_{i}$ . The proof of Lemma 3.2 can be found in Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. Let $w^{*}$ , $w_{i}\\,\\in\\,S^{d-1}$ such that $w^{\\ast}=a_{i}w_{i}+b_{i}u,$ , where $u\\in S^{d-1},u\\,\\perp w_{i},a_{i},b_{i}>$ $0,a_{i}^{2}+b_{i}^{2}=1$ . Let $\\theta_{i}=\\theta(w_{i},w^{*})$ . Let $G_{i}$ be a random vector drawn from some distribution $\\mathcal{D}$ such that with probability 1, $G_{i}\\perp w_{i}$ . Let $g_{i}$ be the mean of $G_{i}$ . Let $\\hat{g_{i}}$ be the empirical mean of $G_{i}$ and $\\mu_{i}>0$ . The update rule $w_{i+1}=\\mathrm{proj}_{S^{d-1}}(w_{i}+\\mu_{i}\\hat{g}_{i})$ satisfies the following property, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|w_{i+1}-w^{*}\\right\\|^{2}\\leq\\left\\|w_{i}-w^{*}\\right\\|^{2}-2\\mu_{i}b_{i}g_{i}\\cdot u+2\\mu_{i}b_{i}\\left\\|\\hat{g}_{i}-g_{i}\\right\\|+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}^{2}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore, $i f\\sin(\\theta_{i}/2)\\,\\le\\,\\sigma_{i}\\,\\in\\,(0,1)$ and there exist constant $c_{1},c_{2}$ such that $g_{i}\\,\\cdot\\,u\\,\\geq\\,c_{1}/10,$ , $\\|\\hat{g}_{i}\\|\\;\\leq\\;c_{1}$ and $\\|g_{i}-\\hat{g_{i}}\\|\\;\\leq\\;c_{2}\\;\\leq\\;c_{1}/40,$ , then there exist constant $C_{1},C_{2}~>~8$ such that by taking $\\mu_{i}=\\sigma_{i}/C_{1}$ and $\\sigma_{i+1}=(1-1/C_{2})\\sigma_{i}$ , it holds that $\\sin\\left(\\theta_{i+1}/2\\right)\\leq\\sigma_{i+1}$ . In particular, $i f$ $\\sin(\\theta_{i}/2)\\le3\\sigma_{i}/4$ and $\\|\\hat{g}_{i}\\|\\leq c_{1}$ then s $\\sin\\left(\\theta_{i+1}/2\\right)\\le\\sigma_{i+1}$ always holds. ", "page_idx": 5}, {"type": "text", "text": "In the rest of the section, we will show that as long as $w_{i}$ is not good enough, we can always efficiently construct a random vector $G_{i}$ whose expectation points to the correct direction and we can use very few queries to estimate its expectation up to a desired accuracy. We adapt the localization technique used in [15] to achieve this goal. ", "page_idx": 5}, {"type": "text", "text": "3.1.1 Finding A Good Gradient via Localization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the $i$ -th round of Algorithm 3, we write $w^{\\ast}=a_{i}w_{i}+b_{i}u_{i}$ , where $u_{i}\\in S^{d-1},u_{i}\\perp w_{i},a_{i},b_{i}>$ $0,a_{i}^{2}+b_{i}^{2}=1$ . Recall that $\\sigma_{i}$ is an upper bound we maintain for $\\sin(\\theta_{i}/2)$ . We will construct the random gradient as follows ", "page_idx": 5}, {"type": "equation", "text": "$$\nG_{i}:=\\mathrm{proj}_{w_{i}^{\\perp}}z y(A_{i}^{1/2}z-\\tilde{t}w_{i}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z\\sim N(0,I)$ , $A_{i}=I-(1-\\sigma_{i}^{2})w_{i}w_{i}^{t}$ and $\\tilde{t}\\in(0,t^{\\prime})$ is a scalar. To see why $G_{i}$ is a good choice, we will start by analyzing $G_{i}$ assuming the noise rate opt $=0$ . To simplify the notation, denote by $\\ell_{i}(z)=\\mathrm{sign}((a_{i}{\\bar{w_{i}}}+\\bar{b}_{i}u_{i}/\\sigma_{i})z+(\\bar{t}^{*}-a\\tilde{t})/\\sigma_{i})$ and $\\bar{g}_{i}=\\mathbf{E}_{z\\in N(0,I)}\\,\\mathrm{proj}_{w_{i}^{\\perp}}z\\ell_{i}(z)$ . A simple calculation gives us the following result. ", "page_idx": 6}, {"type": "text", "text": "Fact 3.3. Let $h(x)=\\mathrm{sign}(w\\cdot x+t)$ be a halfspace. Let $v\\in S^{d-1}$ such that $w=a v+b u_{*}$ , where $a,b>0,a^{2}{+}b^{2}=1$ , $u\\in S^{d-1},u\\perp v.$ . Let $s,\\sigma>0$ be real numbers and define $A=I\\!-\\!(1\\!-\\!\\sigma^{2})v v^{t}$ . For each $z\\in\\mathbb{R}^{d}$ , define $\\tilde{z}:=A^{1/2}z-s v$ . Then $h(\\tilde{z})=\\ell(z)$ , where $\\ell$ is the following halfspace ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell(z)=\\mathrm{sign}((a v+b u/\\sigma)\\cdot z+(t-a s)/\\sigma)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Fact 3.3 implies that if $\\mathrm{opt}=0$ , then it always holds that $f_{i}(z):=y(A_{i}^{1/2}z-\\widetilde{t}w_{i})=\\ell_{i}(z),\\forall z\\in$ $\\forall z\\in\\mathbb{R}^{d}$ and we can view $z$ as examples labeled by a halfspace $\\ell_{i}(z)$ . In particular, ${\\bf E}_{z\\sim N(0,I)}\\,z f_{i}(z)$ is the Chow-Parameter of the halfspace $\\ell_{i}(z)$ under the standard Gaussian distribution. ", "page_idx": 6}, {"type": "text", "text": "Fact 3.4 (Lemma C.3 in [15]). Let $h(x)=\\mathrm{sign}(w\\cdot x+t),$ , where $w\\in S^{d-1}$ be a halfspace. Then $\\begin{array}{r}{{\\mathbf E}_{z\\sim N(0,I)}\\,z h(z)=\\sqrt{\\frac{2}{\\pi}}\\exp{(-t^{2}/2)}w.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "By Fact 3.4, in the noiseless case, ${\\bf E}_{z\\sim N(0,I)}\\,z\\,f_{i}(z)$ is parallel to $\\left(a_{i}w_{i}+b_{i}u_{i}/\\sigma_{i}\\right)$ with length $\\Theta(\\exp(-T_{i}^{2}))$ , where $\\begin{array}{r}{T_{i}\\,=\\,\\frac{t^{*}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}}\\end{array}$ and $g_{i}\\,=\\,{\\bar{g}}_{i}$ is exactly the $u_{i}$ component of the ChowParameter. In particular, if $T_{i}$ is constant, then by estimating $g_{i}$ using $\\hat{g_{i}}$ up to a small constant error using $\\tilde{O}(d)$ queries, we are able to use Lemma 3.2 to improve $w_{i}$ . Assuming we set $\\tilde{t}=t^{*}$ , as $\\sigma_{i}t^{\\prime}\\,\\leq\\,1$ and $b_{i}\\,\\leq\\,O(\\sigma_{i})$ , it is easy to check $T_{i}$ can be bounded by some universal constant. However, as we mentioned before, we only know $\\begin{array}{r}{|t^{\\prime}-t^{*}|\\leq\\frac{1}{\\log(1/\\epsilon)}}\\end{array}$ , when $w_{i}$ getting close to $w^{*}$ , $\\sigma_{i}$ could become very small and an error of $1/\\log(1/\\epsilon)$ could potentially blow up $T_{i}$ , making the signal we want quite small. Such an issue is problematic for the algorithm, especially when $\\bar{f}_{i}(z)$ is a noisy version of $\\ell_{i}(z)$ . To overcome such an issue, we prove the following structural lemma in Appendix D.2 showing that we can always check whether the choice of $\\tilde{t}$ is good or not, by looking at the bias of $\\ell(z)$ , using $\\tilde{O}(1)$ queries. Using this method, we can perform a binary search for $\\tilde{t}$ to find a correct choice in at most $\\log(1/\\epsilon)$ rounds. Furthermore, as long as we select the correct $\\tilde{t}$ , it must hold that $\\left|\\tilde{t}-t^{*}\\right|\\leq O(\\sigma_{i})$ . In particular, as $\\sigma_{T}=C\\epsilon\\exp((t^{\\prime})^{2}/2)$ , such a $\\tilde{t}$ is a good enough estimate for $t^{*}$ to be used in the final hypothesis. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.5. Let $w^{*}$ , $w_{i}\\in S^{d-1}$ such that $w^{\\ast}=a_{i}w_{i}+b_{i}u_{i}$ , where $u_{i}\\in S^{d-1},u\\perp w_{i},a_{i},b_{i}>$ $0,a_{i}^{2}+b_{i}^{2}=1$ . Let $t^{*},t^{\\prime},\\sigma_{i},\\epsilon$ be positive real numbers such that $0\\,\\leq\\,t^{*}\\,\\leq\\,t^{\\prime}$ , $\\sin(\\theta_{i}/2)\\,\\leq\\,\\sigma_{i}$ , and $\\sigma_{i}t^{\\prime}\\leq1$ . Define $\\begin{array}{r}{T_{i}:=\\frac{t^{\\ast}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}}\\end{array}$ , $\\ell_{i}(z)=\\mathrm{sign}((a_{i}w_{i}+b_{i}u_{i}/\\sigma_{i})z+(t^{*}-a\\tilde{t})/\\sigma_{i})$ and $\\bar{g}_{i}=\\mathbf{E}_{z\\in N(0,I)}\\,\\mathrm{proj}_{w_{i}^{\\perp}}z\\ell_{i}(z)$ for some $\\tilde{t}\\in[0,t^{\\prime}]$ . Then the following three properties hold. 1. There exists an interval $I_{t^{\\prime}}\\subseteq[0,t^{\\prime}]$ of length at least $\\sigma_{i}$ such that for every $\\tilde{t}\\in I_{t^{\\prime}},|T_{i}|\\leq5$ . 2. When $\\lvert T_{i}\\rvert\\leq6$ , it holds that $\\bar{g_{i}}\\cdot u_{i}=\\|\\bar{g_{i}}\\|$ and $e^{-19}b_{i}/\\sigma_{i}\\leq\\|\\bar{g}_{i}\\|\\leq2e^{-19}$ . 3. For every $\\left|\\tilde{t}-t^{*}\\right|>40\\sigma_{i}$ and $\\tilde{t}<t^{\\prime}$ , $\\left|T_{i}\\right|>10$ . ", "page_idx": 6}, {"type": "text", "text": "3.1.2 Robustness Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "So far, we have only considered the case when $\\mathrm{opt}=0$ . Due to the presence of noise, it is impossible for us to estimate $\\bar{g}_{i}\\,=\\,{\\bf E}_{z\\in N(0,I)}\\,\\mathrm{proj}_{w_{i}^{\\perp}}\\,z\\ell_{i}(z)$ because we only have a noisy version $f_{i}(z)$ of $\\ell_{i}(z)$ . In this section, we will show that as long as $w_{i}$ is close to $w^{*}$ and $|t^{\\prime}-t^{*}|\\leq1/\\log(1/\\epsilon)$ , the probability that for a Gaussian point $z$ , $\\ell_{i}(z)\\neq f_{i}(z)$ is at most a tiny constant. This is incomparable with the bias of $\\ell_{z}(z)$ if $\\tilde{t}$ is chosen correctly, and does not affect the algorithm too much. We start with the following lemma which bounds the probability of $\\ell_{i}(z)\\neq f_{i}(z)$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.6. Let $h^{*}(x)\\,=\\,\\mathrm{sign}(w^{*}\\cdot x\\,+\\,t^{*})$ be a halfspace such that $\\mathrm{err}(h^{*})\\,=\\,\\mathrm{opt}\\,\\leq\\,\\epsilon.$ . Let $\\tilde{t},\\sigma_{i},t^{\\prime}$ be real numbers such that $\\tilde{t}\\leq t^{\\prime}$ and $\\sigma_{i}t^{\\prime}\\le1,\\sigma_{i}\\,\\le\\,1/2$ . Let $w^{\\ast}=a_{i}w_{i}+b_{i}u_{i}$ , where $u_{i}\\in S^{d-1},u\\perp w_{i},a_{i},b_{i}>0,a_{i}^{2}+b_{i}^{2}=1$ . Define $\\ell_{i}(z)=\\mathrm{sign}((a_{i}w_{i}+b_{i}u_{i}/\\sigma_{i})z+(t^{*}-a\\tilde{t})/\\sigma_{i})$ and $f_{i}(z)=y(A_{i}^{1/2}z-\\tilde{t}w_{i})$ . Then $\\mathbf{Pr}_{z\\sim N(0,I)}(\\ell_{i}(z)\\neq f_{i}(z))\\leq\\epsilon\\exp(\\tilde{t}^{2}/2+4)/\\sigma_{i}$ . In particular, $i f\\,\\sigma_{i}\\,\\geq\\,C\\exp((t^{\\prime})^{2}/2)\\epsilon,$ , for some sufficient large constant $C$ , then there is a sufficiently small constant $c$ such that $\\mathbf{Pr}_{z\\sim N(0,I)}(\\ell_{i}(z)\\neq f_{i}(z))\\leq c\\leq e^{-40}$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Lemma 3.6 leverages the $(v,s,\\sigma)$ - rejection procedure introduced in [15] (see Appendix D.3). We will use Lemma 3.6 to analyze the gradient descent approach we described in the presence of noise. Formally, we establish the following lemma (see Appendix D.4 for the proof). ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.7. Let $w^{*}$ , $w_{i}\\in S^{d-1}$ such that $w^{\\ast}=a_{i}w_{i}+b_{i}u_{i}$ , where $u_{i}\\in S^{d-1},u\\perp w_{i},a_{i},b_{i}>$ $0,a_{i}^{2}+b_{i}^{2}=1$ . $L e t\\ t^{*},t^{\\prime},\\sigma_{i},\\epsilon$ be positive real numbers such that $0\\,\\leq\\,t^{*}\\,\\leq\\,t^{\\prime}$ , $\\sin(\\theta_{i}/2)\\,\\leq\\,\\sigma_{i}$ , $\\sigma_{i}\\,\\,\\stackrel{.}{\\geq}\\,C\\,\\dot{\\exp}((t^{\\prime})^{2}/2)\\epsilon,$ , and $\\sigma_{i}t^{\\prime}\\,\\leq\\,1$ . Let $h^{*}(x)\\,=\\,\\mathrm{sign}(w^{*}\\cdot x\\,+\\,t^{*})$ be a halfspace such that $\\mathrm{err}(h^{*})=\\mathrm{opt}\\leq\\epsilon.$ . Define $\\begin{array}{r}{T_{i}:=\\frac{t^{*}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}}\\end{array}$ , $\\ell_{i}(z)=\\mathrm{sign}((a_{i}w_{i}+b_{i}u_{i}/\\sigma_{i})z+(t^{*}-a\\tilde{t})/\\sigma_{i})$ , $\\bar{g}_{i}=\\mathbf{E}_{z\\in N(0,I)}\\,\\mathrm{proj}_{w_{i}^{\\perp}}z\\ell_{i}(z)$ and $g_{i}=\\mathbf{E}_{z\\in N(0,I)}\\operatorname{proj}_{w_{i}^{\\perp}}z f_{i}(z),$ , where $f_{i}(z)=y(A_{i}^{1/2}z-\\tilde{t}w_{i})$ for some $\\tilde{t}\\in[0,t^{\\prime}]$ . Let $\\eta_{i}:=\\mathbf{Pr}_{z\\sim N(0,I)}(\\ell_{i}(z)\\neq f_{i}(z))$ and $p_{i}$ be the probability that $f_{i}(z)=-1$ . Then the following two properties hold. ", "page_idx": 7}, {"type": "equation", "text": "$$\ng_{i}\\cdot u_{i}\\geq\\bar{g_{i}}\\cdot u_{i}-2\\sqrt{e}\\eta_{i}\\sqrt{\\log(1/\\eta_{i})}\\,a n d\\,||g_{i}||\\leq||\\bar{g_{i}}||+2\\sqrt{e}\\eta_{i}\\sqrt{\\log(1/\\eta_{i})}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 3.7 says as the noise level is small, it will not affect the structure lemma we established in Lemma 3.5 too much, and thus we are able to find the correct threshold $\\tilde{t}$ by checking the probability of $f_{i}(z)=-1$ . Furthermore, as long as we choose the correct threshold $\\tilde{t},\\dot{g_{i}}$ , the noisy version of $\\bar{g_{i}}$ still satisfies the conditions in the statement of Lemma 3.2 and thus can be used to improve $w_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "3.2 Finding A Good Initialization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 3.1, we have shown that given some $w_{0}$ non-trivially close to $w^{*}$ and some $t^{\\prime}$ such that $\\begin{array}{r}{t^{\\prime}-\\frac{1}{\\log(1/\\epsilon)}\\leq t^{*}\\leq t^{\\prime}}\\end{array}$ , we can use Algorithm 3 to learn a good hypothesis with high probability. In this section, we show how to find such a good initialization $w_{0}$ using a few membership queries. The most common way to get such a warm-start is by robustly estimating the Chow-Parameter (see for example [28, 34]) using Fact 3.4. Such an approach does not work for general halfspaces because the length of the length of the Chow-Parameter can be as small as ${\\tilde{O}}(p)$ , and thus needs roughly $d/p$ random queries to estimate. In this section, we show how to overcome such an issue using a label smoothing technique, which has been useful in related problems [12]. The main results in this step can be summarized as follows. The proof of Theorem 3.8 is deferred to Appendix E.2 ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.8. Let $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ and $y(x)$ be any labeling function such that $\\operatorname{err}(h^{*})=$ opt $\\le\\epsilon\\le1/C$ for some large enough constant $C$ . If $|t-t^{*}|\\leq1/\\log(1/\\epsilon)$ , then with probability at least $1/3$ , Algorithm 2 makes $M=\\tilde{O}(1/p+d\\log(1/\\epsilon))$ , runs in $\\mathrm{poly}(d,M)$ time, and outputs some $w_{0}$ such that $\\begin{array}{r}{\\sin(\\theta(w_{0},w^{*})/2)\\le\\operatorname*{max}\\{\\operatorname*{min}\\{1/t,1/2\\},O(\\eta\\sqrt{\\log(1/\\eta)}\\},}\\end{array}$ , where $\\eta=\\epsilon/p$ . ", "page_idx": 7}, {"type": "text", "text": "Due to the space limitations, here we only consider the case when $t^{*}$ is not extremely large, which roughly covers the regime when $\\eta\\sqrt{\\log(1/\\eta)}\\leq1/t$ . This suffices to capture some of the ideas and illustrate the power of the smoothed labeling. For the case when $\\eta\\sqrt{\\log(1/\\eta)}>1/t$ , we are still able to find such a warm start by leveraging the smoothed label method in combination with the technique used in Section 3.1 in a more complicated way. We postpone this analysis to Appendix F. Our algorithm, Algorithm 2, to find a warm start is presented as follows. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 INITIALIZATION 1(Finding a good initialization under unextreme threshold) Input: error parameter $\\epsilon\\in(0,1)$ , confidence parameter $\\delta\\in(0,1)$ , threshold $t>0$ Output: $w_{0}\\in S^{d-1}$ Keep some $x\\sim N(0,I)$ and query $y(x)$ until see some $x_{0}$ such that $y(x_{0})=-1$ for $i=1,\\dots,m=\\tilde{O}(d\\log(1/\\epsilon))$ do Sample $z_{i}\\sim N(0,I)$ and query $\\tilde{y}(x_{0}^{(i)}):=y(\\sqrt{1-\\rho^{2}}x_{0}+\\rho z_{i})$ with $\\rho:=\\operatorname*{min}\\{1/t,1\\}$ Let $\\begin{array}{r}{u_{0}:=\\frac{1}{m}\\sum_{i=1}^{m}z_{i}\\tilde{y}(x_{0}^{(i)})}\\end{array}$ return $w_{0}:=u_{0}/\\parallel\\!u_{0}\\parallel$ ", "page_idx": 7}, {"type": "text", "text": "To analyze Algorithm 2, we introduce the following definitions and notations. ", "page_idx": 7}, {"type": "text", "text": "Definition 3.9 (Smoothed Label). Let $x\\in\\mathbb{R}^{d}$ be a point and $y(x)$ be any labeling function. For $\\rho\\in[0,1]$ , define the random variable $\\tilde{x}=\\sqrt{1-\\rho^{2}}x+\\rho z$ , where $z\\sim N(0,I)$ . The smoothed label of $x$ with parameter $\\rho$ is defined as ${\\tilde{y}}(x):=y(\\tilde{x})$ . ", "page_idx": 8}, {"type": "text", "text": "We will require the following fact (whose proof follows via a direct calculation): ", "page_idx": 8}, {"type": "text", "text": "Fact 3.10. Let $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ be a halfspace. Let $\\boldsymbol{x},\\boldsymbol{z}\\in\\mathbb{R}^{d}$ and define $\\tilde{x}:=\\sqrt{1-\\rho^{2}}x+$ $\\rho z$ . Then $\\tilde{h}(z):=h^{*}(\\tilde{x})=\\mathrm{sign}(w^{*}\\cdot z+(t^{*}+\\sqrt{1-\\rho^{2}}w^{*}\\cdot x)/\\rho)$ is another halfspace for $z$ with threshold $(t^{*}+\\sqrt{1-\\rho^{2}}w^{*}\\cdot x)/\\rho$ . ", "page_idx": 8}, {"type": "text", "text": "Let $h^{*}=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ be an optimal halfspace and let $y(x)$ be any labeling function such that $\\mathrm{err}(h^{*})=\\mathrm{opt}\\le\\epsilon$ . For $x\\in\\mathbb{R}^{d}$ , we denote by $\\eta(x):=\\mathbf{Pr}(h^{*}(\\tilde{x})\\neq\\tilde{y}(x))$ , the noise level of the smoothed label. Assuming that we are given a random negative example $x_{0}$ , then with constant probability, it is close to the decision boundary, i.e., $\\begin{array}{r}{w^{*}\\cdot x_{0}^{\\bar{\\textrm{\\tiny{d}}}}(-t^{*}-\\bar{\\frac{1}{t^{*}}},-\\bar{t^{*}})}\\end{array}$ . This implies that the threshold of $\\bar{h}$ , the halfspace corresponding to the smoothed label at $x_{0}$ , is between $(-1,1)$ . Moreover, the Chow-Parameter of $\\bar{h}$ under the standard Gaussian distribution is parallel to $w^{*}$ with a constant length, by Fact 3.4. If $\\mathrm{opt}=0$ , then for every $t\\leq\\sqrt{\\log(1/\\epsilon)}$ , we only need another ${\\tilde{O}}(d\\log(1/\\epsilon))$ queries to estimate the Chow-Parameter of $\\bar{h}$ up to error $O(1/t)$ ; thus, we get a warm start $w_{0}$ such that $\\sin(\\theta_{0}/2)\\le1/t$ , given $|t-t^{*}|$ is small. Therefore, the total number of queries we use to run Algorithm 2 is $\\tilde{O}(1/p+d\\log(1/p))$ . However, in general, it is impossible to estimate $w^{*}$ up to arbitrary accuracy \u2014 even using an infinite number of queries \u2014 because of the presence of noise. In fact, using a random $x_{0}$ is important for Algorithm 2 to succeed. If we are given some adversarially selected $x_{0}$ , even if it is close to the decision boundary, the above method can easily fail. This is because almost all the queries we made are in a small neighborhood of $x_{0}$ and could be corrupted by noise arbitrarily. However, we show in Appendix E.1 that, with a probability at least $2/3$ , the noise level $\\eta(x_{0})$ of the smoothed label around $x_{0}$ is at most $O(\\epsilon/p)$ , if $x_{0}$ is a random example given $y(x_{0})=-1$ ; and thus we can still estimate $w^{*}$ to a desired accuracy provided $\\epsilon/p$ is not too large. ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.11. Let $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ be a halfspace and $y(x)$ be any labeling function such that $\\mathrm{err}(h^{*})=\\mathrm{opt}\\leq\\epsilon$ . Let $x\\sim N(0,I)$ conditioned on $y(x)=-1$ be a Gaussian example with $a$ negative label. If $p>C\\epsilon$ for some large enough constant $C$ , then with probability at least $1/2\\,w e$ have $\\eta(x)\\leq5\\epsilon/p$ and $w^{*}\\cdot x\\in(-t^{*}-1/t^{*},-t^{*})$ . ", "page_idx": 8}, {"type": "text", "text": "Finally, we briefly discuss how to obtain a warm start when the threshold $t^{*}$ is very large. The details of this method can be found in Appendix F. By Theorem 3.8, when $p$ is small, we are only able to get some $w_{0}$ such that $\\sin(\\theta(w_{0},w^{*}))\\leq O(\\eta\\sqrt{\\log(1/\\eta)})$ for $\\eta=\\epsilon/p$ . One possible approach is to use the localization technique we use in Section 3.1 to refine such $w_{0}$ . However, such an approach fails because after localization the noise rate would be possibly larger than the length of the Chow-Parameter that we want to estimate. This makes it impossible for us to learn the useful signal. On the other hand, [15] gave a randomized localization method that can make the expected noise level sufficiently smaller than the length of the Chow-Parameter we want to estimate; and thus will succeed with constant probability in each round of refinement. Unfortinately, such an approach cannot be used in a query-efficient manner, because to implement such a method we need to know $\\theta(w_{i},w^{*})$ up to an error $1/\\log(1/\\epsilon)$ , in each round of refinement. This implies that if we make a random guess of $\\theta(w_{i},w^{*})$ , the probability of success in each round drops to only $1/\\log(1/\\epsilon)$ , which requires to rerun the whole algorithm too many times in order to succeed once. ", "page_idx": 8}, {"type": "text", "text": "Such an issue could be addressed in a similar but more complicated way to the method we use in Lemma 3.5, by looking at the bias of the halfspace after localization. The second issue is that even the noise level is smaller than the length of the Chow-Parameter we want to estimate, the length of the Chow-Parameter is only $1/p^{c}$ , for some small constant $c$ , as we can only make $\\theta_{0}$ smaller than some small constant. This still requires us to use $d/p^{c}$ queries to estimate it. Such an issue can again be addressed using the smoothed label method, where we use only $1/p^{c}$ queries to search a small class example and use another $\\tilde{O}(d)$ queries to estimate the Chow-Parameter. Importantly, even such a method only succeeds with constant probability overall. As the refinement stage only runs for ${\\cal O}(\\log\\log(1/\\epsilon))$ rounds, we only need to rerun the entire algorithm ${\\cal O}(\\log(1/\\epsilon))$ times to succeed once. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ilias Diakonikolas was supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), a Sloan Research Fellowship, and a DARPA Learning with Less Labels (LwLL) grant. Daniel M. Kane was supported by NSF Medium Award CCF-2107547 and NSF Award CCF-1553288 (CAREER). Mingchen Ma was supported by NSF Award CCF-2144298 (CAREER). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] D. Angluin. Queries and concept learning. Machine learning, 2:319\u2013342, 1988.   \n[2] P. Awasthi, M. F. Balcan, and P. M. Long. The power of localization for efficiently learning linear separators with noise. Journal of the ACM (JACM), 63(6):1\u201327, 2017.   \n[3] M.-F. Balcan, A. Broder, and T. Zhang. Margin based active learning. In International Conference on Computational Learning Theory, pages 35\u201350. Springer, 2007.   \n[4] M.-F. Balcan, S. Hanneke, and J. W. Vaughan. The true sample complexity of active learning. Machine learning, 80:111\u2013139, 2010.   \n[5] M.-F. Balcan and P. Long. Active and passive learning of linear separators under log-concave distributions. In Conference on Learning Theory, pages 288\u2013316. PMLR, 2013.   \n[6] S. Dasgupta. Analysis of a greedy active learning strategy. Advances in neural information processing systems, 17, 2004.   \n[7] S. Dasgupta. Coarse sample complexity bounds for active learning. Advances in neural information processing systems, 18, 2005.   \n[8] S. Dasgupta and A. Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures & Algorithms, 22(1):60\u201365, 2003.   \n[9] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. In Learning Theory: 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005. Proceedings 18, pages 249\u2013263. Springer, 2005.   \n[10] I. Diakonikolas, D. Kane, V. Kontonis, S. Liu, and N. Zarifis. Efficient testable learning of halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] I. Diakonikolas, D. Kane, and N. Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.   \n[12] I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zarifis. Agnostically learning multi-index models with queries. arXiv preprint arXiv:2312.16616, 2023.   \n[13] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the SQ model. In Proceedings of The $34^{t h}$ Conference on Learning Theory, COLT, 2021.   \n[14] I. Diakonikolas, D. M. Kane, and L. Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. CoRR, abs/2302.06512, 2023.   \n[15] I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\u20131073, 2018.   \n[16] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\u20135141. PMLR, 2022.   \n[17] I. Diakonikolas, M. Ma, L. Ren, and C. Tzamos. Fast co-training under weak dependence via stream-based active learning. In Forty-first International Conference on Machine Learning, 2024.   \n[18] V. Feldman. On the power of membership queries in agnostic learning. The Journal of Machine Learning Research, 10:163\u2013182, 2009.   \n[19] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, 1997.   \n[20] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine learning, 28:133\u2013168, 1997.   \n[21] S. Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014.   \n[22] S. Hanneke and L. Yang. Minimax analysis of active learning. J. Mach. Learn. Res., 16(1):3487\u2013 3602, 2015.   \n[23] M. Hopkins, D. Kane, and S. Lovett. The power of comparisons for actively learning linear classifiers. Advances in Neural Information Processing Systems, 33:6342\u20136353, 2020.   \n[24] V. Kontonis, M. Ma, and C. Tzamos. The gain from ordering in online learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] S. R. Kulkarni, S. K. Mitter, and J. N. Tsitsiklis. Active learning using arbitrary binary valued queries. Machine Learning, 11:23\u201335, 1993.   \n[26] A. McCallum, K. Nigam, et al. Employing em and pool-based active learning for text classification. In ICML, volume 98, pages 350\u2013358. Citeseer, 1998.   \n[27] F. Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958.   \n[28] J. Shen. On the power of localized perceptron for label-optimal learning of halfspaces with adversarial noise. In International Conference on Machine Learning, pages 9503\u20139514. PMLR, 2021.   \n[29] L. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142, 1984.   \n[30] L. G. Valiant. A theory of the learnable. In Proc. 16th Annual ACM Symposium on Theory of Computing (STOC), pages 436\u2013445. ACM Press, 1984.   \n[31] V. N. Vapnik. The support vector method. In International conference on artificial neural networks, pages 261\u2013271. Springer, 1997.   \n[32] K. Vasilis, M. Mingchen, and T. Christos. Active learning with simple questions. In S. Agrawal and A. Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 3064\u20133098. PMLR, 30 Jun\u201303 Jul 2024.   \n[33] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.   \n[34] S. Yan and C. Zhang. Revisiting perceptron: Efficient and label-optimal learning of halfspaces. Advances in Neural Information Processing Systems, 30, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Here we give an organization of the supplementary material. In Appendix A, we present the formal definition of agnostic learning with membership queries and label queries. In Appendix B, we present the omitted proofs in Section 2 about the information-theoretical lower bound. In Appendix C, we discuss why we can without loss of generality assume the noise level opt $\\le\\epsilon$ and how to learn $p$ up to a constant factor with $\\tilde{O}(1/p)$ queries. In Appendix D, we present the omitted proofs in Section 3.1 about how to learn a good hypothesis given a good initialization. In Appendix E.2, we present the omitted proofs in Section 3.2 about how to find a good initialization when the threshold is not extremely large. In Appendix F, we design an algorithm that finds a good initialization when the threshold is very large. In Appendix G, we prove Theorem 1.2. ", "page_idx": 11}, {"type": "text", "text": "A Active Learning with Membership Queries and Label Queries ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Definition A.1 (Active Learning Halfspace with Membership Queries). Let $H=\\{h(x)=\\mathrm{sign}(w\\;\\cdot\\;$ $x+t):\\mathbb{R}^{d}\\rightarrow\\{\\pm1\\}\\mid w\\in S^{d-1},t\\geq0\\}$ be the class of halfspaces over $X=\\ensuremath{\\mathbb{R}}^{d}$ . The labeling function $y(x):X\\to\\{\\pm1\\}$ is a random function that maps each $x\\in X$ to an unknown binary random variable. For each $h\\in H$ , denote by $\\mathrm{err}(h)=\\mathbf{Pr}_{x\\sim N(0,I)}$ $(h(x)\\neq y(x))$ , opt $\\mathrel{\\mathop:}=\\operatorname*{min}_{h\\in H}\\operatorname{err}(h)$ and $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ any halfspace with error opt. A membership query takes $x\\in X$ as an input and returns a label $y\\sim y(x)$ . We say a learning algorithm $\\boldsymbol{\\mathcal{A}}$ is a constant-approximate learning algorithm if for every labeling function $y(x)$ , and for every $\\epsilon,\\delta\\in(0,1)$ , it outputs some $\\hat{h}\\;\\in\\;H$ by adaptively making memberships queries, such that with probability at least $1\\,-\\,\\delta$ , $\\mathrm{err}(\\hat{h})\\leq O(\\mathrm{opt}+\\epsilon)$ . The query complexity of $\\boldsymbol{\\mathcal{A}}$ is the total number of membership queries it uses during the learning process. ", "page_idx": 11}, {"type": "text", "text": "Definition A.2 (Active Learning Halfspace with Label Queries). Let $H=\\{h(x)=\\mathrm{sign}(w\\cdot x+t):$ $\\mathbb{R}^{d}\\rightarrow\\{\\pm1\\}\\mid w\\in S^{d-1},t\\geq0\\}$ be the class of halfspaces over $X=\\ensuremath{\\mathbb{R}}^{d}$ . Let $D$ be a distribution over $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ such that $D_{x}$ , the marginal distribution over $x$ is the standard Gaussian distribution $N(0,I)$ . For each $h\\in H$ , denote by $\\operatorname{err}(h)=\\mathbf{Pr}_{(x,y)\\sim N(0,I)}\\left(h(x)\\neq y\\right.$ , opt $\\mathrel{\\mathop:}=\\operatorname*{min}_{h\\in H}\\operatorname{err}(h)$ and $h^{*}(x)\\,=\\,\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ any halfspace with error opt. Let $S$ be a set of m i.i.d. labeled examples drawn from $D$ . An active learning algorithm with label query is given $S$ but with hidden labels and is allowed to make a label query for each $x\\in S$ and see its label $y$ . We say a learning algorithm $\\boldsymbol{\\mathcal{A}}$ is a constant-approximate learning algorithm if for every distribution $D$ and for every $\\epsilon,\\delta\\in(0,1)$ , it outputs some $\\hat{h}\\in H$ by adaptively making label queries over a set of m examples drawn i.i.d. from $D$ , such that with probability at least $1-\\delta$ , $\\mathrm{err}(\\hat{h})\\,\\leq\\,O(\\mathrm{opt}+\\epsilon)$ . The label complexity of $\\boldsymbol{\\mathcal{A}}$ is the total number label queries made over $S$ during the learning process ", "page_idx": 11}, {"type": "text", "text": "B Omitted Proofs in Section 2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof of Lemma 2.1. Let $\\boldsymbol{\\mathcal{A}}$ be such a learning algorithm. We select a random set of $m$ examples $S_{1}$ and give it to $\\boldsymbol{\\mathcal{A}}$ . With probability at least $2/3$ , $\\boldsymbol{\\mathcal{A}}$ makes $r$ queries and learns a halfspace $\\hat{h}$ with error $p/2$ with respect to $h^{*}$ . This implies that given a Gaussian example, with probability at least $p/2$ it will predict negative, and given it predicts negative, with probability at least $1/2$ it is actually negative. Since $m$ is at least poly $(d,1/p)$ , we know that with enough high probability, at least $\\Omega(\\dot{d})$ examples will be predicted by negative by $\\hat{h}$ and at least a constant fraction of these examples are actually negative. Thus, given such a $\\hat{h}$ with probability at least $3/4$ , we can find $d$ negative examples in $S$ by randomly querying $O(d)$ examples that are predicted as negative by $\\hat{h}$ . \u53e3 ", "page_idx": 11}, {"type": "text", "text": "B.2 Proof of Lemma 2.2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Let $v=A w^{*}=(w^{*}\\cdot x_{1},\\ldots,w^{*}\\cdot x_{k})^{\\top}$ . Consider the projection of $w^{*}$ over the subspace spanned by the row vectors of $A$ , $A^{\\top}(A A^{\\top})^{-1}A w^{*}$ . Assuming that $x_{1},\\ldots,x_{k}$ are all negative, then $\\left\\|v\\right\\|^{2}\\geq$ ", "page_idx": 11}, {"type": "text", "text": "$k(t^{*})^{2}$ . This implies that the square of the norm of the projection of $w^{*}$ onto the subspace is ", "page_idx": 12}, {"type": "equation", "text": "$$\nB:=(w^{*})^{\\top}A^{\\top}(A A^{\\top})^{-1}A w^{*}=v^{\\top}(A A^{\\top})^{-1}v\\geq\\|v\\|^{2}/\\left\\|A A^{\\top}\\right\\|_{2}\\geq k(t^{*})^{2}/\\left\\|A A^{\\top}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $w^{*}$ is uniformly chosen from the unit sphere, by Lemma B.1 in [24], the square norm of $w^{*}$ projected onto a fixed $k-$ dimensional subspace is a random variable drawn from a beta distribution $\\textstyle{\\bar{B}}({\\bar{\\frac{k}{2}}},{\\frac{d-k}{2}})$ . By Lemma 2.2 in [8], if $\\|A\\bar{A^{\\top}}\\|_{2}\\leq d(1-O(1/(t^{*})^{2}))$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Pr}\\left(B\\geq k(t^{*})^{2}/\\left\\Vert A A^{\\top}\\right\\Vert_{2}^{2}\\right)\\leq\\exp\\left(-\\frac{k}{2}(\\frac{d(t^{*})^{2}}{\\left\\Vert A A^{\\top}\\right\\Vert_{2}}-1-\\log(\\frac{d(t^{*})^{2}}{\\left\\Vert A A^{\\top}\\right\\Vert_{2}}))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\sqrt{\\frac{(t^{*})^{2}d}{\\left\\Vert A A^{\\top}\\right\\Vert_{2}}}\\exp(-\\frac{1}{2}(\\frac{d(t^{*})^{2}}{\\left\\Vert A A^{\\top}\\right\\Vert_{2}}-1))\\right)^{k}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(O((t^{*})\\exp(-\\frac{(t^{*})^{2}}{2}(1-O(1/(t^{*})^{2})))\\right)^{k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(O((t^{*})^{2}\\frac{1}{t^{*}}\\exp(-\\frac{(t^{*})^{2}}{2}))\\right)^{k}\\leq\\left(O(p\\log(1/p))\\right)^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The last inequality follows by Fact D.4. ", "page_idx": 12}, {"type": "text", "text": "B.3 Proof of Lemma 2.3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We will first show that for a given $A\\in\\mathbb{R}^{k\\times d}$ , $\\left\\|A A^{\\top}-d I\\right\\|_{2}$ is small with high probability if the rows of $A$ are drawn i.i.d. from $d$ -dimensional standard Gaussian. Let $\\mathcal{N}$ be an $1/4$ -net of $S^{k-1}$ . According to [33], we know that $|{\\mathcal{N}}|\\,\\leq\\,e^{3k}$ and $\\begin{array}{r}{\\|A A^{\\top}-d I\\|_{2}\\,\\leq\\,2\\operatorname*{sup}_{u\\in{\\mathcal{N}}}\\left|u^{t}(A A^{\\top}-d I)u\\right|}\\end{array}$ . Thus, to show that $\\left\\|A A^{\\top}-d I\\right\\|_{2}$ is small with high probability, it is equivalent to show with high probability for every $u\\in\\mathcal N$ , $|\\boldsymbol{u}^{\\Tilde{\\top}}(A A^{\\top}-d I)\\boldsymbol{u}|$ is small. ", "page_idx": 12}, {"type": "text", "text": "Fix $u\\in\\mathcal N$ to be a unit vector. We have ", "page_idx": 12}, {"type": "equation", "text": "$$\nu^{\\top}A A^{\\top}u-d u^{\\top}u=\\sum_{j=1}^{d}(u^{\\top}A_{j})^{2}-d.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Notice that each $u^{\\top}A_{j}$ is a standard Gaussian variable and thus $\\textstyle\\sum_{j=1}^{d}(u^{\\top}A_{j})^{2}$ is a chi-squared distribution with freedom $d$ . By [33], we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left(\\left|\\sum_{j=1}^{d}(\\boldsymbol{u}^{\\top}A_{j})^{2}-d\\right|\\geq2\\xi d\\right)\\leq2\\exp\\left(-d\\xi^{2}/2\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $|\\mathcal{N}|\\leq e^{3k}$ , we know that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left(\\left\\|A A^{\\top}-d I\\right\\|_{2}\\geq\\xi d\\right)\\mathbf{Pr}\\left(\\operatorname*{sup}_{u\\in{\\cal N}}\\left|u^{\\top}(A A^{\\top}-d)u\\right|\\geq2\\xi d\\right)\\leq2\\exp(3k-d\\xi^{2}/2).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since there are at most $\\binom{m}{k}$ such $k$ -tuples of examples, the probability that there exists a $k$ -tuple such that $\\left\\|A A^{\\top}-d I\\right\\|_{2}$ is larger than $\\xi d=O(d/(t^{*})^{2})$ is at most   \n$\\left\\langle\\binom{m}{k}\\exp(3k-d\\xi^{2}/2)\\leq2\\left(\\frac{e m}{k}\\right)^{k}\\exp(3k-d\\xi^{2}/2)\\leq2\\exp(-(d\\xi^{2}/2-3k-k\\log(e r/k)))\\leq2\\exp(-2k\\delta^{2}/2)>0\\right\\}$ /3, by choosing $k=d/(\\log(r)(t^{*})^{4})$ and $\\xi=O(1/(t^{*})^{2})$ . ", "page_idx": 12}, {"type": "text", "text": "C Omitted Details in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Discussion on the Noise Level opt ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Assuming we know some $\\alpha$ such that $\\epsilon\\leq\\alpha/2\\leq\\mathrm{opt}\\leq\\alpha$ , then learning $\\hat{h}$ upto error $O(\\mathrm{opt}+\\epsilon)$ is equivalent to learning it up to error $O(\\alpha)$ . By guessing $\\alpha=\\epsilon2^{i}$ for $i=0,\\dots,O(\\log(1/\\epsilon))$ , we can always obtain a desired $\\alpha$ and use it to run the learning algorithm and get a good hypothesis. Such an approach will generate a list of ${\\cal O}(\\log(1/\\epsilon))$ different hypotheses, finding a good enough hypothesis among them only costs polylog $\\left(1/\\epsilon\\right)$ queries using a standard tournament approach, such as the following lemma. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Lemma C.1 (Lemma 3.6 in [10]). Let $\\epsilon,\\delta\\in(0,1)$ and $D$ a distribution over $\\mathbb{R}^{d}\\times\\{0,1\\}$ . Given a list of hypothesises $\\{h^{(i)}\\}_{i=1}^{k}$ , there is an algorithm that draws $O(k^{2}\\log(k/\\delta)/\\epsilon)$ unlabeled examples from $D_{x}$ and performs $O(k^{2}\\log(d/\\delta))$ label queries runs in $\\mathrm{poly}(d,\\epsilon,\\delta)$ times and output a hypothesis $\\hat{h}$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{(x,y)\\sim D}(\\hat{h}(x)\\neq y)\\leq10\\operatorname*{min}_{i\\in[k]}\\operatorname*{Pr}_{(x,y)\\sim D}(h^{(i)}(x)\\neq y)+\\epsilon.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C.2 Approximate Bias Estimation Using Queries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this part, we describe a simple approach to estimate the bias $p$ up to a constant factor using $\\tilde{O}(1/p)$ queries. To do this we will estimate $\\bar{p}=\\mathbf{Pr}_{x\\sim N(0,I)}(y(x)=-1)$ , the noise version of $p$ as $|\\bar{p}-p|\\leq\\epsilon$ . If we can estimate $\\hat{p}$ such that $\\hat{p}/2\\le\\bar{p}\\le\\hat{p}$ or verify that $\\bar{p}\\leq(C-1)\\epsilon$ , then $\\hat{p}$ satisfies our purpose. ", "page_idx": 13}, {"type": "text", "text": "By Chebyshev\u2019s inequality, if $\\bar{p}\\leq3\\hat{p}/4$ , then taking $O(1/\\hat{p})$ random queries at $x$ and computing the empirical probability of $y(x)\\,=\\,-1$ , with probability $2/3$ , we are able to verify this fact by checking whether the empirical probability is less than $5\\hat{p}/6$ . On the other hand, if $4\\hat{p}/5\\leq\\bar{p}\\leq\\dot{\\hat{p}}$ , with probability $2/3$ we are able to verify this fact by checking whether the empirical probability is greater than $5\\hat{p}/6$ . Furthermore, by repeating this approach $O(\\log(1/\\delta))$ times and using a majority voting trick, we can boost the probability of success up to $1-\\delta$ . We will run the above approach for $\\hat{p}=(4/5)^{i}/2$ for $i=0,1,\\dots$ until we find $\\bar{p}\\geq(4/5\\bar{)}\\hat{p}$ or $\\hat{p}=C^{\\prime}\\epsilon$ for some constant $C^{\\prime}$ . In the first case $(4/5)\\hat{p}\\le\\bar{p}\\le(25/24)\\hat{p}$ and we find a good approximation for $\\bar{p}$ and thus for $p$ . In the second case, we can conclude that $p$ is smaller than $O(\\epsilon)$ ", "page_idx": 13}, {"type": "text", "text": "D Omitted Proofs from Section 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 3 REFINE(Learn the correct direction $w^{*}$ based on a warm start $w_{0}$ ) Input: Intial direction $w_{0}\\in S^{d-1}$ , $t^{\\prime}>0$ , an approximate threshold, error parameter $\\epsilon\\in(0,1)$ , confidence parameter $\\delta\\in(0,1)$ Output: $w_{T}\\in S^{d-1}$ , an approximation of $w^{*}$ , $\\hat{t}\\in\\mathbb{R}$ , an approximation of $t^{*}$ Let $\\epsilon^{\\prime}=C\\epsilon\\exp(t^{\\prime2}/2)\\;m=\\tilde{O}(d),T=O(\\log(1/\\epsilon^{\\prime}))\\;\\sigma_{0}\\gets\\operatorname*{min}\\{1/t^{\\prime},1/2\\}.$ Let $C_{1},C_{2}$ be large enough constants for $i=0,\\dots,T$ do $A_{i}\\leftarrow I-(1-\\sigma_{i}^{2})w_{i}w_{i}^{t}$ , $\\mu_{i}\\leftarrow\\sigma_{i}/C_{1}$ Find $\\tilde{t}\\in\\{0,\\epsilon,2\\epsilon,\\ldots,t^{\\prime}\\}$ using the following binary search method, if no such $\\tilde{t}$ is found, then stop the algorithm and return $w_{T}=0$ . \u25b7Find the correct threshold to construct the gradient. Draw $O(\\log(1/\\delta))$ Gaussian samples $z\\sim N(0,I)$ , query $A_{i}^{1/2}z-\\tilde{t}w_{i}$ and compute $p(\\tilde{t})$ , the empirical probability that a query returns $-1$ . If $p(\\tilde{t})<e^{-17}$ , properly decrease $\\tilde{t}$ , if $p(\\tilde{t})>e^{-17}$ , properly increase $\\tilde{t}$ . Otherwise, declare that $\\tilde{t}$ is found. for $j=1,\\dots,m$ do Draw $z_{j}\\sim N(0,I)$ , make queries at $\\tilde{z}_{j}:=A_{i}^{1/2}z_{j}-\\tilde{t}w_{i}$ and denote by $f_{i}(z_{j})$ the result $\\begin{array}{r}{\\hat{g}_{i}\\leftarrow\\frac{1}{m}\\sum_{j=1}^{m}\\mathrm{proj}_{w_{i}^{\\perp}}(z_{j}f_{i}(z_{j}))}\\end{array}$ $\\triangleright$ Construct the gradient wi+1 \u2190projSd\u22121(wi + \u00b5i g\u02c6i), \u03c3i+1 \u2190(1 \u22121/C2)\u03c3i $\\triangleright$ Gradient Descent t\u02c6 \u2190t\u02dc $\\triangleright$ Use the threshold found in the last round return wT , t\u02c6 ", "page_idx": 13}, {"type": "text", "text": "D.1 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.2. We first observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|w_{i+1}-w^{*}\\right\\|^{2}=\\left\\|{\\mathrm{proj}}_{S^{d-1}}(w_{i}+\\mu_{i}\\hat{g}_{i})-{\\mathrm{proj}}_{S^{d-1}}(w^{*})\\right\\|^{2}\\leq\\left\\|w_{i}+\\mu_{i}\\hat{g}_{i}-w^{*}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It remains to upper bound $\\left\\|w_{i}+\\mu_{i}\\hat{g}_{i}-w^{*}\\right\\|^{2}$ . We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|w_{i}+\\mu_{i}\\hat{g}_{i}-w^{*}\\right\\|^{2}=\\left\\|w_{i}-w^{*}\\right\\|^{2}+2\\mu_{i}\\hat{g}_{i}\\cdot\\left(w_{i}-w^{*}\\right)+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\|w_{i}-w^{*}\\right\\|^{2}-2\\mu_{i}\\hat{g}_{i}\\cdot w^{*}+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|w_{i}-w^{*}\\right\\|^{2}-2\\mu_{i}g_{i}\\cdot w^{*}+2\\mu_{i}(g_{i}-\\hat{g}_{i})\\cdot w^{*}+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}\\right\\|^{2}}\\\\ &{\\qquad\\qquad=\\left\\|w_{i}-w^{*}\\right\\|^{2}-2\\mu_{i}g_{i}\\cdot w^{*}+2\\mu_{i}(g_{i}-\\hat{g}_{i})\\cdot b_{i}u+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|w_{i}-w^{*}\\right\\|^{2}-2\\mu_{i}g_{i}\\cdot w^{*}+2\\mu_{i}b_{i}\\left\\|g_{i}-\\hat{g}_{i}\\right\\|+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}\\right\\|^{2}.}\\\\ &{\\qquad\\qquad=\\left\\|w_{i}-w^{*}\\right\\|^{2}-2\\mu_{i}b_{i}g_{i}\\cdot u+2\\mu_{i}b_{i}\\left\\|g_{i}-\\hat{g}_{i}\\right\\|+\\mu_{i}^{2}\\left\\|\\hat{g}_{i}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, in the second equality, we use the fact that $\\hat{g}_{i}\\perp w_{i}$ and in the fourth equality, we use the fact that $(g_{i}-\\hat{g_{i}})\\cdot w^{*}=\\bar{(}g_{i}-\\hat{g_{i}})\\cdot a_{i}w_{i}+(g_{i}-\\hat{g_{i}})\\cdot b_{i}u=(g_{i}-\\hat{g_{i}})\\cdot b_{i}u.$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we assume that $\\sin(\\theta_{i}/2)\\le\\sigma_{i}$ and show that we can carefully choose parameter $\\mu_{i},\\sigma_{i+1}$ to make $\\sin(\\theta_{i+1}/2)\\le\\sigma_{i+1}$ . We consider two cases. In the first case, we assume $3\\sigma_{i}/4\\sin(\\theta_{i}/2)\\le\\sigma_{i}$ . Since $\\begin{array}{r}{\\|w_{i}-w^{*}\\|=2\\sin\\frac{\\theta_{i}}{2}}\\end{array}$ , by Lemma 3.2, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(2\\sin\\frac{\\theta_{i+1}}{2})^{2}\\le(2\\sin\\frac{\\theta_{i}}{2})^{2}-5\\mu_{i}b_{i}+2\\mu_{i}c_{2}b_{i}+\\mu_{i}^{2}c_{1}^{2}}\\\\ &{\\phantom{2s c}\\leq4\\sigma_{i}^{2}-15\\sigma_{i}^{2}c_{1}/(2C_{1})+4c_{2}\\sigma_{i}^{2}/C_{1}+c_{1}^{2}\\sigma_{i}^{2}/C_{1}^{2}}\\\\ &{\\phantom{2s c}\\leq4\\sigma_{i}^{2}-5\\sigma_{i}^{2}c_{1}/(2C_{1})+\\sigma_{i}^{2}c_{1}/(10C_{1})+c_{1}^{2}\\sigma_{i}^{2}/C_{1}^{2}}\\\\ &{\\phantom{2s c}\\leq4(1-5c_{1}/(8C_{1})+c_{1}/(80C_{1}^{2})+c_{1}^{2}/(2C_{1}^{2}))\\sigma_{i}^{2}:=4(1-1/C_{2})^{2}\\sigma_{i}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where use the fact that $b_{i}\\leq2\\sin(\\theta_{i}/2)\\leq3\\sigma_{i}/2$ and the fact that $C_{1}$ can be made large enough. ", "page_idx": 14}, {"type": "text", "text": "In the second case, we assume $\\sin(\\theta_{i}/2)<3\\sigma_{i}/4$ . In this case, using the fact that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\sin(\\frac{\\theta_{i+1}}{2})-\\sin(\\frac{\\theta_{i}}{2}))=\\|w_{i+1}-w^{*}\\|-\\|w_{i}-w^{*}\\|\\leq\\|w_{i+1}-w_{i}\\|\\leq\\|w_{i}+\\mu_{i}\\hat{g}_{i}-w_{i}\\|=\\mu_{i}\\,\\|\\hat{g}_{i}\\|_{L^{2}((0,1)\\times(0,1))}\\leq C\\epsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\sigma_{i+1}-\\sin(\\displaystyle\\frac{\\theta_{i+1}}{2})=\\sigma_{i+1}-\\sin(\\displaystyle\\frac{\\theta_{i}}{2})-(\\sin(\\displaystyle\\frac{\\theta_{i+1}}{2})-\\sin(\\displaystyle\\frac{\\theta_{i}}{2}))}\\\\ {\\geq\\sigma_{i+1}-\\displaystyle\\frac{3\\sigma_{i}}{4}-\\displaystyle\\frac{\\sigma_{i}\\,\\|\\hat{g}_{i}\\|}{C_{1}}\\geq(\\displaystyle\\frac{1}{4}-\\displaystyle\\frac{1}{C_{2}}-\\displaystyle\\frac{1}{C_{1}})\\sigma_{i}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds because the parameter $C_{1},C_{2}$ can be chosen larger than 8. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3.2 implies that if $g_{i}$ has enough correlation with respect to $w_{*}$ but is also not too long, then by estimating $g_{i}$ up to some error, we can ensure $\\lVert\\boldsymbol{w}_{i}-\\boldsymbol{w}^{*}\\rVert$ drops significantly each round. Formally, we have the following corollary. ", "page_idx": 14}, {"type": "text", "text": "Corollary D.1. In Algorithm 3, denote by $\\theta_{i}=\\theta(w^{*},w_{i})$ . Assume that sin $\\textstyle{\\frac{\\theta_{i}}{2}}\\leq\\sigma_{i}$ . If there exist a suitable constant $c_{1}$ and a small enough constant $c_{2}$ such that $g_{i}\\cdot w^{*}\\geq c_{1}\\bar{\\sigma}_{i}/10,$ , $\\|\\hat{g}_{i}\\|\\leq c_{1}$ and $\\left\\|g_{i}-\\hat{g_{i}}\\right\\|\\leq c_{2}$ . Then there exists large enough constant $C_{1},C_{2}$ such that by taking $\\mu_{i}=\\sigma_{i}/C_{1}$ , $i t$ holds that sin $\\begin{array}{r}{\\frac{\\theta_{i+1}}{2}\\leq(1-1/C_{2})\\sigma_{i}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Corollary $D.I$ . Since $\\begin{array}{r}{\\|w_{i}-w^{*}\\|=2\\sin\\frac{\\theta_{i}}{2}}\\end{array}$ , by Lemma 3.2, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(2\\sin\\frac{\\theta_{i+1}}{2})^{2}\\le(2\\sin\\frac{\\theta_{i}}{2})^{2}-5\\mu_{i}\\sigma_{i}+2\\mu_{i}c_{2}\\sigma_{i}+\\mu_{i}^{2}c_{1}^{2}}\\\\ &{\\phantom{2s c}\\leq4\\sigma_{i}^{2}-5\\sigma_{i}^{2}c_{1}/C_{1}+2c_{2}\\sigma^{2}/C_{1}+c_{1}^{2}\\sigma_{i}^{2}/C_{1}^{2}}\\\\ &{\\phantom{2s c}\\leq4\\sigma_{i}^{2}-5\\sigma_{i}^{2}c_{1}/C_{1}+2\\sigma^{2}/C_{1}^{2}+c_{1}^{2}\\sigma_{i}^{2}/C_{1}^{2}}\\\\ &{\\phantom{2s c}\\leq4(1-5c_{1}/(4C_{1})+1/(2C_{1}^{2})+c_{1}^{2}/(2C_{1}^{2}))\\sigma_{i}^{2}:=4(1-1/C_{2})^{2}\\sigma_{i}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the third and the fourth inequalities hold when $C_{1}$ is large enough. ", "page_idx": 14}, {"type": "text", "text": "D.2 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 3.5. We first prove Item 1. Since $T_{i}$ is a monotone decreasing function on $\\tilde{t}$ , and $t^{\\prime}>t^{*}$ , it remains to show that for every $\\tilde{t}$ such that $\\left|\\tilde{t}-t^{*}\\right|\\leq\\sigma_{i}$ , $|T_{i}|\\leq5$ . Notice that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|T_{i}|=\\left|\\frac{t^{*}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}\\right|\\le\\left|\\frac{t^{*}-a_{i}\\tilde{t}}{\\sigma_{i}}\\right|\\le\\left|\\frac{\\tilde{t}-a_{i}\\tilde{t}}{\\sigma_{i}}\\right|+\\left|\\frac{\\tilde{t}-t*}{\\sigma_{i}}\\right|\\le\\frac{b_{i}^{2}\\tilde{t}}{\\sigma_{i}}+1\\le5.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Fact 3.3, we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{g_{i}}=\\sqrt{\\frac{2}{\\pi}}\\exp(-T_{i}^{2}/2)\\frac{b_{i}u_{i}/\\sigma_{i}}{\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}\\le\\sqrt{5}$ and $|T_{i}|\\leq5$ , we immediately obatin Item 2. Finally, we prove Item 3. Using the monotone property of $T_{i}$ , we prove the case where $\\tilde{t}<t^{*}-40\\sigma_{i}$ and the case $\\tilde{t}>t^{*}+40\\sigma_{i}$ can proved symmetrically. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{i}=\\frac{t^{*}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}=\\frac{t^{*}-\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}+\\frac{\\tilde{t}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}\\geq\\frac{40\\sigma_{i}}{\\sqrt{5}\\sigma_{i}}-4\\geq10,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality holds because of Equation (1). ", "page_idx": 15}, {"type": "text", "text": "D.3 Proof of Lemma 3.6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To prove Lemma 3.6, we first introduce the following definition called $(v,s,\\sigma)$ - rejection procedure. ", "page_idx": 15}, {"type": "text", "text": "Definition D.2 $((v,s,\\sigma)$ - rejection procedure). Let $v\\in\\mathbb{R}^{d}$ be a unit vector and $s,\\sigma$ be real numbers such that $\\sigma<1$ . Given a point $x\\in\\mathbb{R}^{d}$ , $(v,s,\\sigma)$ - rejection procedure accepts it with probability ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\exp\\left(-(\\sigma^{-2}-1)(v\\cdot x+s/(1-\\sigma^{2}))^{2}/2\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and rejects it otherwise. ", "page_idx": 15}, {"type": "text", "text": "$(v,s,\\sigma)$ - rejection procedure satisfies the following property. ", "page_idx": 15}, {"type": "text", "text": "Lemma D.3 (Lemma C.7, Lemma C.8 in [15]). If $x\\sim N(0,I)$ is fed into the $(v,s,\\sigma)$ - rejection procedure, then it is accepted with probability $\\sigma\\exp(-s^{2}/(2(1-\\sigma^{2})))$ ). In particular, when $\\sigma s\\leq2$ and $\\sigma\\leq1/2$ , the accepted probability is at least $\\sigma\\exp(-s^{2}/2-4)$ . Moreover, the distribution on $x$ conditioned on acceptance is that of $N(-s v,A_{v,\\sigma})$ , where $\\overset{\\cdot}{A}_{v,\\sigma}=I-(1-\\sigma^{2})v v^{t}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 3.6. Let $\\tilde{z}:=A_{i}^{1/2}z-\\tilde{t}w_{i}$ . By Fact 3.3, we know that $\\ell_{i}(z)=h^{\\ast}(\\tilde{z}),\\forall z\\in\\mathbb{R}^{d}$ . By Lemma 3.6, we know that if $z\\sim N(0,I)$ , then $\\tilde{z}\\sim N(-\\tilde{t}w_{i},A_{i})$ , which can be seen by feeding a Gaussian random vector into the $(w_{i},\\tilde{t},\\sigma_{i})$ \u2212rejection procedure conditioned on acceptance. Since $\\mathrm{err}(h^{*})=\\mathrm{opt}\\le\\epsilon$ and the accepted rate is at least $\\bar{\\sigma}\\exp(-s^{2}/2-4)$ , we know from Lemma 3.6 that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{z\\sim N(0,I)}(\\ell_{i}(z)\\neq f_{i}(z))=\\operatorname*{Pr}_{z\\sim N(0,I)}(h^{*}(\\tilde{z})\\neq f_{i}(z))\\leq\\epsilon\\exp(\\tilde{t}^{2}/2+4)/\\sigma_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, if $\\sigma_{i}\\geq C\\exp((t^{\\prime})^{2}/2)\\epsilon$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon\\exp(\\tilde{t}^{2}/2+4)/\\sigma_{i}\\leq\\left(\\epsilon\\exp(\\tilde{t}^{2}/2+4)\\right)/\\left(C\\epsilon\\exp((t^{\\prime})^{2}/2)\\right)\\leq e^{4}/C:=c\\leq e^{-40}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.4 Proof of Lemma 3.7 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before presenting the proof, we state the following two facts that will be used in our proof. ", "page_idx": 15}, {"type": "text", "text": "Fact D.4 (Komatsu\u2019s Inequality). For any $t\\in\\mathbb R$ the bias $p$ of a halfspace $h(x)=\\mathrm{sign}(w^{*}\\cdot x+t)$ can be bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{2}{\\pi}}\\frac{\\exp(-t^{2}/2)}{t+\\sqrt{t^{2}+4}}\\leq p\\leq\\sqrt{\\frac{2}{\\pi}}\\frac{\\exp(-t^{2}/2)}{t+\\sqrt{t^{2}+2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Fact D.5 (Lemma B.4 in [16]). Let $D$ be a distribution on $\\mathbb{R}^{d}\\,\\times\\,\\{\\pm1\\}$ with standard normal $x-$ margin and let $w,u$ be two orthogonal unit vectors. Let $B$ be any interval over $\\mathbb{R}$ and let $S(x,y)$ be any event over $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ , such that $S(x,y)\\subseteq\\{w\\cdot x\\in B\\}$ then it holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left(\\left|u\\cdot x\\right|\\mathbf{1}\\{S(x,y)\\}\\right)\\leq2{\\sqrt{e}}\\,\\mathbf{Pr}(S(x,y)){\\sqrt{\\log({\\frac{\\mathbf{Pr}(w\\cdot x\\in B)}{\\mathbf{Pr}(S(x,y))}})}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.7. We start by proving the first part of Lemma 3.7. By Lemma 3.6, we know that $\\eta_{i}:=\\mathbf{Pr}_{z\\sim N(0,I)}(\\ell_{i}(z)\\neq\\bar{f}_{i}(z))\\leq\\bar{e}^{-40}$ . This implies that $\\left|{\\bf P r}_{z\\sim N(0,I)}(\\ell_{i}(z)=-1)-p_{i}\\right|\\leq$ $e^{-40}$ . We first show that when $p_{i}$ is in a reasonable range, $|T_{i}|<6$ . Assuming by contradiction that $\\left|T_{i}\\right|\\geq6$ , then by Fact D.4, the bias of $\\ell_{i}(z)$ must be at most $\\exp(-T_{i}^{2}/2)\\bar{/}(2\\bar{T}_{i})\\leq e^{-20}$ , which implies that it cannot be the case where $p_{i}^{\\,\\,\\,}\\in\\,(e^{-18},1\\,-\\,e^{-18})$ . Similarly, if $|T_{i}|\\,<\\,5$ , then by Fact D.4, the bias of $\\ell_{i}(z)$ must be at least $\\exp(-T_{i}^{2}/2)/20\\geq e^{-15.5}$ . As the noise level $\\eta_{i}\\leq e^{-4\\tilde{0}}$ , we have $p_{i}\\in(e^{-18},1-\\overset{.}{e}^{-18})$ . ", "page_idx": 16}, {"type": "text", "text": "Next, we prove the second part of Lemma 3.7. We start by bounding the correlation between $g_{i}$ and $u_{i}$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{i}\\cdot u_{i}=\\underset{z\\in N(0,I)}{\\mathbf{E}}\\mathrm{~\\normalfont~\\proj}_{w_{i}^{\\perp}}z(\\ell_{i}(z)+f_{i}(z)-\\ell_{i}(z))\\cdot u_{i}}\\\\ &{\\qquad=\\bar{g_{i}}\\cdot u_{i}-\\underset{z\\in N(0,I)}{\\mathbf{E}}\\mathrm{~\\normalfont~\\proj}_{w_{i}^{\\perp}}z(\\ell_{i}(z)-f_{i}(z))\\cdot u_{i}}\\\\ &{\\qquad\\ge\\bar{g_{i}}\\cdot u_{i}-\\underset{z\\in N(0,I)}{\\mathbf{E}}|u_{i}\\cdot z|\\mathbf{1}\\{\\ell_{i}(z)\\ne f_{i}(z)\\}}\\\\ &{\\qquad\\ge\\bar{g_{i}}\\cdot u_{i}-2\\sqrt{e}\\eta_{i}\\sqrt{\\mathrm{log}(1/\\eta_{i})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the third and the last inequalities hold because $u_{i}\\perp w_{i}$ and Fact D.5. ", "page_idx": 16}, {"type": "text", "text": "We next bound the norm of $g_{i}$ . Since both $\\bar{g_{i}}$ and $g_{i}$ \u221aare orthogonal to $w_{i}$ . It is sufficient to show that for every unit vector $u\\perp w_{i}$ , $|g_{i}\\cdot u|\\leq|\\bar{g_{i}}\\cdot u|+4\\sqrt{e}\\eta_{i}\\sqrt{\\log(1/\\eta_{i})}$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|g_{i}\\cdot u\\right|=\\left|\\underset{z\\in N(0,I)}{\\mathbf{E}}\\mathrm{proj}_{w_{i}^{\\perp}}z(\\ell_{i}(z)+f_{i}(z)-\\ell_{i}(z))\\cdot u\\right|}\\\\ &{\\qquad=\\left|\\bar{g_{i}}\\cdot u_{i}-\\underset{z\\in N(0,I)}{\\mathbf{E}}\\mathrm{proj}_{w_{i}^{\\perp}}z(\\ell_{i}(z)-f_{i}(z))\\cdot u_{i}\\right|}\\\\ &{\\qquad\\leq|\\bar{g_{i}}\\cdot u_{i}|+\\underset{z\\in N(0,I)}{\\mathbf{E}}|u_{i}\\cdot z|\\mathbf{1}\\{\\ell_{i}(z)\\neq f_{i}(z)\\}}\\\\ &{\\qquad\\leq|\\bar{g_{i}}\\cdot u_{i}|+2\\sqrt{e}\\eta_{i}\\sqrt{\\mathrm{log}(1/\\eta_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D.5 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the proof of Theorem 3.1. Before presenting the proof, we present the following fact that will be a crucial part of our proof. ", "page_idx": 16}, {"type": "text", "text": "Fact D.6 (Lemma 4.2 in [15]). Under the standard normal distribution for every pair of unit vector $w,w^{*}$ and real number $t$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Pr}(\\operatorname{sign}(w^{*}\\cdot x+t)\\neq\\operatorname{sign}(w\\cdot x+t))\\leq{\\frac{\\sin(\\theta(w,w^{*}))}{2}}\\exp(-t^{2}/2).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3.1. Denote by $\\theta_{i}:=\\theta(w_{i},w^{*})$ . We will first show by induction that with high probability in the $i$ -th round of Algorithm 3, $\\sin(\\theta_{i}/2)\\ \\leq\\ \\sigma_{i}$ . Assuming this is correct, since $\\dot{\\sigma}_{T}={C}\\epsilon\\,\\mathrm{exp}\\,(t^{\\prime})^{2}/2$ for some large constant $C$ . We will have ", "page_idx": 16}, {"type": "equation", "text": "$$\n>\\mathbf{r}(\\mathrm{sign}(w^{*}\\cdot x+t^{*})\\neq\\mathrm{sign}(w\\cdot x+t^{*}))\\leq C\\epsilon\\exp(\\frac{(t^{\\prime}+t^{*})(t^{\\prime}-t^{*})}{2})\\leq C\\epsilon\\exp(\\frac{1}{\\sqrt{\\log(1/\\epsilon)}})=C\\epsilon\\exp(-\\frac{1}{\\sqrt{\\log(1/\\epsilon)}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since opt $\\le\\epsilon$ , this implies that by providing a good enough estimation of $t^{*}$ , we found a hypothesis with error at most $O(\\epsilon)$ . Now we show that this is actually true. For $i=0$ , $\\sin(\\theta_{0}/2)\\le\\sigma_{0}\\$ holds by our assumption. ", "page_idx": 17}, {"type": "text", "text": "Now, we assume this is correct for the $i$ -round and we show this holds with high probability for the $i+1$ -th round. We will show that with high probability the gradient $\\hat{g_{i}}$ we use in the update $w_{i+1}=\\mathrm{proj}_{w_{i}^{\\perp}}(w_{i}+\\mu_{i}\\hat{g}_{i})$ satisfies the condition of Lemma 3.2. ", "page_idx": 17}, {"type": "text", "text": "Recall that we have the following notations. $w^{*}~=~a_{i}w_{i}\\;+\\;b_{i}u_{i}$ . $\\begin{array}{r l r}{T_{i}}&{{}:=}&{\\frac{t^{*}-a_{i}\\tilde{t}}{\\sigma_{i}\\sqrt{a_{i}^{2}+b_{i}^{2}/\\sigma_{i}^{2}}}}\\end{array}$ $\\ell_{i}(z)\\;=\\;\\mathrm{sign}((a_{i}w_{i}\\,+\\,b_{i}u_{i}/\\sigma_{i})z\\,+\\,(t^{*}\\,-\\,a\\tilde{t})/\\sigma_{i}).$ , $\\bar{g_{i}}~=~\\mathbf{E}_{z\\in N(0,I)}\\operatorname{proj}_{w_{i}^{\\perp}}z\\ell_{i}(z)$ and $g_{i}=$ $\\mathbf{E}_{z\\in N(0,I)}\\operatorname{proj}_{w_{i}^{\\perp}}z f_{i}(z)$ , where $f_{i}(z)=y(A_{i}^{1/2}z-\\tilde{t}w_{i})$ . And $\\eta_{i}:=\\mathbf{Pr}_{z\\sim N(0,I)}(\\ell_{i}(z)\\neq f_{i}(z))<$ $e^{-40}$ by Lemma 3.6. ", "page_idx": 17}, {"type": "text", "text": "We first show that with high probability, Algorithm 3 must be able to select a correct threshold $\\tilde{t}\\leq t^{\\prime}$ such that $\\lvert T_{i}\\rvert\\leq6$ . Denote by $p_{i}$ the probability that $f_{i}(z)=-1$ . We notice that for each fixed $\\tilde{t}$ by randomly querying $O(\\log(1/\\delta))\\ f_{i}(z)$ , we can with high probability check if $p_{i}\\in(e^{-17},1-e^{-17})$ or not. This can be done using the same method we used in Appendix C.2. ", "page_idx": 17}, {"type": "text", "text": "Since $b_{i}/2=\\sin\\theta_{i}/2\\le\\sin(\\theta_{i}/2)\\le\\sigma_{i}$ , we know from Lemma 3.7 that as long as we find some $\\tilde{t}$ such that $p_{i}\\in(e^{-17},1-e^{-17})$ , we have have $\\lvert T_{i}\\rvert\\leq6$ . By Lemma 3.5 and Lemma 3.7, we know that there exists an interval $I_{i}\\subseteq[0,t^{\\prime}]$ of length at least $\\sigma_{i}>\\epsilon$ such that for every $\\tilde{t}\\in I_{i}$ , $|T_{i}|<5$ and thus $p_{i}\\,\\in\\,(e^{-16},1-e^{-16})$ . Thus, by performing a binary search at most $O(\\log(1/\\epsilon))$ times, with high probability, we are able to find such a $\\tilde{t}$ such that $|T_{i}|<6$ . Given that we find such a correct $\\tilde{t}$ , we will consider two cases. ", "page_idx": 17}, {"type": "text", "text": "First, we assume that $3\\sigma_{i}/4\\,\\le\\,\\sin(\\theta_{i}/2)\\,\\le\\,\\sigma_{i}$ . We will show that with high probability $g_{i}$ and its empirical estimation $\\hat{g}_{i}$ satisfy the condition in the statement of Corollary D.1 and thus prove $\\sin(\\theta_{i+1}/2)\\le\\sigma_{i+1}$ . Since $\\mathrm{proj}_{w_{i}^{\\perp}}z f_{i}(z)$ is 1-subgaussian random vector, by Hoeffding\u2019s inequality, we know that with $\\tilde{O}(d)$ samples of $z$ , with high probability we have $\\|g_{i}-\\hat{g_{i}}\\|\\leq c_{2}\\leq e^{-40}$ . ", "page_idx": 17}, {"type": "text", "text": "By Lemma 3.7 and Lemma 3.5, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{_{t}\\cdot u_{i}=\\bar{g}_{i}\\cdot u_{i}-2\\sqrt{e}\\eta_{i}\\sqrt{\\log(1/\\eta_{i})}\\ge\\frac{b_{i}\\|\\bar{g}_{i}\\|}{\\sigma_{i}}e^{-19}-100e^{-40}\\ge e^{-19}-100e^{-40}\\ge e^{-20}:=c_{1}.}\\\\ &{\\|\\hat{g}_{i}\\|\\le\\|g_{i}\\|+\\|g_{i}-\\hat{g}_{i}\\|\\le\\|\\bar{g_{i}}\\|+2\\sqrt{e}\\eta_{i}\\sqrt{\\log(1/\\eta_{i})})+e^{-40}\\le3e^{-19}\\le10c_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, by Corollary D.1, we can conclude that $\\sin(\\theta_{i+1}/2)\\,\\le\\,(1-1/C_{2})\\sigma_{i}\\,=\\,\\sigma_{i+1}$ , for a large constant $C_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Next, we consider the case where $\\sin(\\theta_{i}/2)<3\\sigma_{i}/4$ . In this case, as we have shown that $\\|\\hat{g}_{i}\\|$ is bounded by some universal constant, the condition of Lemma 3.2 is fulfliled automatically and thus $\\sin(\\theta_{i+1}/2)\\le(1-1/C_{2})\\sigma_{i}=\\sigma_{i+1}$ , for a large constant $C_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "By induction, with a high probability for each $i$ , we have $\\sin(\\theta_{i}/2)\\,\\leq\\,\\sigma_{i}$ and thus $w_{T}$ is a good approximation of $w^{*}$ . It remains to show that $\\hat{t}$ is also a good approximation of $t^{*}$ . Recall that $\\hat{t}=\\tilde{t}<t^{\\prime}$ such that $|T_{T}|<6$ . Lemma 3.5 implies that $\\left|\\hat{t}-t^{*}\\right|\\leq40\\sigma_{T}=40C\\epsilon\\exp{(t^{\\prime})^{2}/2}$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\sim N(0,I)}{\\mathbf{Pr}}\\left(\\mathrm{sign}(w_{T}\\cdot x+t^{*})\\neq\\mathrm{sign}(w_{T}\\cdot x+\\hat{t})\\right)\\leq(2\\pi)^{-1}\\left|t^{*}-\\hat{t}\\right|\\exp(-\\frac{(t^{*}-\\left|t^{*}-\\hat{t}\\right|)^{2}}{2})}&{}\\\\ {\\leq(2\\pi)^{-1}40C\\epsilon\\exp(\\frac{(t^{\\prime})^{2}-(t^{*}-\\left|t^{*}-\\hat{t}\\right|)^{2}}{2})}&{}\\\\ {\\leq(2\\pi)^{-1}40C\\epsilon\\exp(2t^{\\prime}(t^{\\prime}-t^{*}+40\\sigma_{T}))}&{}\\\\ {\\leq(2\\pi)^{-1}40C\\epsilon\\exp(2t^{\\prime}(40\\sigma_{T}+\\frac{1}{\\log(1/\\epsilon)}))}&{}\\\\ {=O(\\epsilon\\exp(80t^{\\prime}\\sigma_{T})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\sigma_{T}t^{\\prime}}&{{}=}&{O(\\epsilon t^{\\prime}\\exp((t^{\\prime})^{2}/2))}\\end{array}$ and $\\begin{array}{r l r}{t^{\\prime}\\exp((t^{\\prime})^{2}/2)}&{{}\\le}&{1/(C\\epsilon)}\\end{array}$ , we can conclude that $\\mathbf{Pr}_{x\\sim N\\left(0,I\\right)}$ $\\begin{array}{r l r}{\\mathrm{sign}(w_{T}\\cdot x+t^{*})\\neq\\mathrm{sign}(w_{T}\\cdot x+\\hat{t}))}&{\\leq}&{O(\\epsilon)}\\end{array}$ . Thus, with high probability $\\mathrm{err}(\\mathrm{sign}(w_{T}\\cdot x+\\hat{t}))\\leq O(\\epsilon)$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, we count the number of queries used by Algorithm 3. In each round of the algorithm, we perform ${\\cal O}(\\log(1/\\epsilon))$ binary searches to find the correct parameter $\\tilde{t}$ , each of which takes us only $\\bar{O}(1)$ queries. We also make $\\tilde{O}(d)$ queries to construct $\\hat{g_{i}}$ in each round of the algorithm. Thus, each round of Algorithm 3 takes ${\\tilde{O}}(d+\\log(1/\\epsilon))$ queries. Since there are at most ${\\cal O}(\\log(1/\\epsilon))$ rounds, the query complexity of Algorithm 3 is $\\tilde{O}(d\\mathrm{polylog}(1/\\epsilon))$ . ", "page_idx": 18}, {"type": "text", "text": "E Omitted Proofs from Section 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Proof of Lemma 3.11 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 3.11. Denote by $D^{-}$ the conditional distribution of $x\\sim N(0,I)$ given $y(x)=-1$ . Recall that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim N(0,I)}(y(x)=-1)\\ge\\operatorname*{Pr}_{x\\sim N(0,I)}(h^{*}(x)=-1)-\\epsilon\\ge(1-1/C)p.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will first show that $E_{x\\sim D^{-}}\\eta(x)\\,\\leq\\,O(\\epsilon/p)$ . Denote by $Z:={\\sqrt{1-\\rho^{2}}}x\\,+\\,\\rho z$ , where $x\\sim$ $D^{-},z\\sim N(0,I)$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{\\sim D^{-}}{\\mathbf{E}}\\eta(x)=\\underset{x\\sim D^{-},z\\sim N(0,I)}{\\mathbf{Pr}}\\mathbf{1}\\{h^{*}(\\sqrt{1-\\rho^{2}}x+\\rho z)\\neq y(\\sqrt{1-\\rho^{2}}x+\\rho z)\\}=\\mathbf{Pr}\\mathbf{1}\\{h^{*}(Z)\\neq y(Z)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $z$ and $x$ are independent, we notice that $Z$ can be simulated via the following reject sampling process. We draw $x\\sim N(0,I)$ and $Z\\sim N(0,I)$ to construct $Z={\\sqrt{1-\\rho^{2}}}x+\\rho z$ and accepted $Z$ when $y(x)=-1$ . Since $\\sqrt{1-\\rho^{2}}N(0,I)+\\rho N(0,I)=N(0,I)$ , $Z$ can be seen as a reject sampling process with an accepted rate at least $(1-1/C)p$ . Since the noise rate opt $\\le\\epsilon$ , we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{E}\\quad\\eta(x)=\\mathbf{Pr}\\,\\mathbf{1}\\{h^{*}(Z)\\neq y(Z)\\}\\leq(1-1/C)^{-1}\\epsilon/p.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Markov\u2019s inequality, we know that with probability at least $3/4,\\eta(x)\\leq5\\epsilon/p$ , with $x\\sim D^{-}$ . ", "page_idx": 18}, {"type": "text", "text": "Next, we show that with a constant probability a negative example $x$ must be close to the decision boundary of $h^{*}$ . We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\textbf{P r}}{c\\sim N-D}\\left(w^{*}\\cdot x<-t^{*}-\\frac{1}{t^{*}}\\right)}\\\\ &{=\\frac{\\textbf{P r}}{x\\sim N-1}(h^{*}(x)=-1)_{x\\sim D^{-1}\\mid\\{h^{*}(x)=-1\\}}\\left(w^{*}\\cdot x<-t^{*}-\\frac{1}{t^{*}}\\right)}\\\\ &{+\\frac{\\textbf{P r}}{x\\sim D^{-1}}(h^{*}(x)=+1)_{x\\sim D^{-1}\\mid\\{h^{*}(x)=+1\\}}\\left(w^{*}\\cdot x<-t^{*}-\\frac{1}{t^{*}}\\right)}\\\\ &{\\overset{\\leq}\\frac{\\textbf{P r}}{x\\sim D^{-1}\\{1\\}^{(k*)}(x)=-1}\\left(w^{*}\\cdot x<-t^{*}-\\frac{1}{t^{*}}\\right)+1/C\\leq\\underset{x\\sim N(0,I\\mid\\{h^{*}(x)=-1\\}}{\\textbf{P r}}\\left(w^{*}\\cdot x<-t^{*}-\\frac{1}{t^{*}}\\right)+\\right.}\\\\ &{=\\left.\\int_{t^{*}+1/t^{*}}^{\\infty}\\exp(-s^{2}/2)d s/\\int_{t^{*}}^{\\infty}\\exp(-s^{2}/2)d s+2/C\\leq\\exp(-\\frac{(t^{*}+\\frac{1}{t^{*}})^{2}-\\binom{t^{*}}{t^{*}}}{2})+2/C}\\\\ &{=\\exp(-\\frac{(2t^{*}+1/t^{*})/t^{*}}{2})+2/C\\leq e^{-1}+2/C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the third inequality, we use Fact D.4. ", "page_idx": 18}, {"type": "text", "text": "Thus, by union bound, with probability at least $1/2$ , it simultaneously holds that $\\eta(x)\\leq5\\epsilon/p$ and $w^{*}\\cdot x\\leq-t^{*}-1/t^{*}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3.8. We consider two cases. First, if $t^{\\prime}<1$ , then each $\\boldsymbol{x}_{0}^{(i)}=\\boldsymbol{z}_{i}$ is drawn from the standard Gaussian. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\bigg|\\bigg|=\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)+\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\bigg\\|}&{}\\\\ {\\leq\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)\\bigg\\|+\\bigg\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\bigg\\|}&{}\\\\ {\\leq\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)\\bigg\\|+\\underset{u\\in S^{d-1}}{\\operatorname*{sup}}\\underset{z\\sim N(0,I)}{\\mathbf{E}}|u\\cdot z|\\,\\mathbf{1}(y(z)\\neq h^{*}(z))}&{}\\\\ {\\leq\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)\\bigg\\|+2\\sqrt{e}\\epsilon\\sqrt{\\log(1/\\epsilon)},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality holds because of Fact D.5. Since each $z_{i}y(z_{i})$ is a standard Gaussian, by Hoeffding\u2019s inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{Pr}(\\left\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z y(z)\\right\\|\\geq r\\leq2\\exp(-\\frac{m r^{2}}{d})\\leq\\mathrm{{polylog}}(\\delta),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "when $m\\geq\\tilde{\\Omega}(d/r^{2})$ . By taking $r=(20\\log(1/\\epsilon))^{-1}$ , we obtain that $\\begin{array}{r l}{\\lefteqn{\\big\\|u_{0}-\\mathbf{E}_{z\\sim N(0,I)}\\,z y(z)\\big\\|\\,\\le\\,}}\\end{array}$ $(20\\log(1/\\epsilon))^{-1}$ with high probability. Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\right\\|\\leq(20\\log(1/\\epsilon))^{-1}+2\\sqrt{e}\\epsilon\\sqrt{\\log(1/\\epsilon)}\\leq O(\\log(1/\\epsilon)^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Fact 3.4, we know that ${\\bf E}_{z\\sim N(0,I)}\\,z h^{*}(z)=\\xi w^{*}$ for some $\\xi\\geq e^{-1}$ , which also implies that $\\|u_{0}\\|\\geq e^{-1}/2$ , because $u_{0}$ sufficiently close to ${\\bf E}_{z\\sim N\\left(0,I\\right)}\\,z h^{*}(z)$ . ", "page_idx": 19}, {"type": "text", "text": "Since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\bigg|\\bigg|^{2}=\\|u_{0}\\|^{2}+\\bigg\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\bigg\\|^{2}-2\\left\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\right\\|\\|u_{0}\\|^{2}\\cos\\theta(w_{0},w^{*})}&{}\\\\ {\\geq2\\left\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\right\\|\\|u_{0}\\|\\left(1-\\cos\\theta(w_{0},w^{*})\\right)}&{}\\\\ &{=4\\left\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\right\\|\\|u_{0}\\|\\sin^{2}(\\theta(w_{0},w^{*})/2),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we get $\\begin{array}{r l r}{\\sin(\\theta(w_{0},w^{*})/2)}&{{}\\le}&{\\sqrt{\\left\\|u_{0}-\\mathbf{E}_{z\\sim N(0,I)}\\,z h^{*}(z)\\right\\|^{2}/4\\left\\|\\mathbf{E}_{z\\sim N(0,I)}\\,z h^{*}(z)\\right\\|\\left\\|u_{0}\\right\\|}\\quad\\le}\\end{array}$ ${\\cal O}(1/\\log(1/\\epsilon))$ . In particular as $\\epsilon\\mathrm{~\\ensuremath~{~<~}~}1/C$ for some large enough $C$ , we conclude that $\\sin(\\theta(w_{0},w^{*})/2)\\leq\\operatorname*{max}\\{\\operatorname*{min}\\{1/t,1/2\\},O(\\eta\\sqrt{\\log(1/\\eta)}\\}.$ ", "page_idx": 19}, {"type": "text", "text": "We next address the case when $t>1$ . By Lemma 3.11, we know that with probability at least $1/2$ , we have $\\eta(x)\\leq5\\epsilon/p$ and $w^{*}\\cdot x\\in(-t^{*}-1/t^{*},-t^{*})$ . We will assume these two events happen in the rest of the proof. Let $z\\sim N(0,I)$ and by Fact 3.10, define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{h}(z):=h^{*}(\\tilde{x_{0}})=\\mathrm{sign}(w^{*}\\cdot z+\\frac{t^{*}+\\sqrt{1-\\rho^{2}}w^{*}\\cdot x_{0}}{\\rho})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 3.11, we know that $\\mathbf{Pr}_{z\\sim N(0,I)}\\,\\tilde{h}(z)\\neq\\tilde{y}(x_{0})=\\eta(x_{0})\\leq5\\epsilon/p$ . Similar to the first case, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\hat{h}(z)\\bigg\\|=\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{y}(x_{0})+\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{y}(x_{0})-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{h}(z)\\bigg\\|}&{}\\\\ {\\leq\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{h}(z)\\bigg\\|+\\bigg\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{h}(z)-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{h}(z)\\bigg\\|}&{}\\\\ {\\leq\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{y}(x_{0})\\bigg\\|+\\underset{u\\in S^{d-1}}{\\mathbf{u}}z\\frac{\\mathbf{E}}{\\nu\\sim N(0,I)}\\bigg|u:z\\big|\\mathbf{1}(\\tilde{y}(x_{0})\\neq\\tilde{h}(z))}\\\\ {\\leq\\bigg\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{y}(x_{0})\\bigg\\|+2\\sqrt{e}\\eta(x_{0})\\sqrt{\\log(1/\\eta(x_{0}))}}&{}\\\\ {\\leq\\operatorname*{max}\\{O(\\eta(x_{0})\\sqrt{\\log(1/\\eta(x_{0}))}),1/(50\\sqrt{\\log(1/\\epsilon)})\\}}&{}\\\\ {\\leq\\operatorname*{max}\\{O(\\eta\\sqrt{\\log(1/\\eta)}),1/(50\\sqrt{\\log(1/\\epsilon)})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the second last inequality we use the fact that $\\begin{array}{r l}{\\left\\|u_{0}-\\mathbf{E}_{z\\sim N(0,I)}\\,z\\tilde{y}(x_{0})\\right\\|}&{{}\\leq}\\end{array}$ $1/(100\\sqrt{\\log(1/\\epsilon)})\\,\\le\\,1/(100t)$ with high probability\u221a. Since $\\rho=1/t$ , $|t-t^{*}|\\,\\leq\\,1/\\log(1/\\epsilon)$ and $t^{*}\\le\\sqrt{\\log(1/\\epsilon)}\\ll\\log(1/\\epsilon)$ , the threshold $\\begin{array}{r}{T_{\\rho}=\\frac{t^{*}+\\sqrt{1-\\rho^{2}}w^{*}\\cdot x_{0}}{\\rho}}\\end{array}$ can be bounded as follows. ", "page_idx": 20}, {"type": "equation", "text": "$$\n-1\\le\\frac{t^{*}-\\sqrt{1-\\rho^{2}}(t^{*}+\\frac{1}{t^{*}})}{\\rho}\\le T_{\\rho}\\le\\frac{t^{*}(1-\\sqrt{1-\\rho^{2}})}{\\rho}\\le t t^{*}/(t)^{2}\\le1+o(1).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Fact 3.4 implies that ${\\bf E}_{z\\sim N(0,I)}\\,z\\tilde{h}(z)=\\xi w^{*}$ for some $\\xi\\geq e^{-1}$ . Since $u_{0}$ is close to ${\\bf E}_{z\\sim N\\left(0,I\\right)}\\,z$ , $\\|u_{0}\\|\\geq e^{-1}/2$ . Thus, we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sin(\\theta(w_{0},w^{*})/2)\\leq\\sqrt{\\left\\|u_{0}-\\underset{z\\sim N(0,I)}{\\mathbf{E}}z\\tilde{h}(z)\\right\\|^{2}/4\\left\\|\\underset{z\\sim N(0,I)}{\\mathbf{E}}z h^{*}(z)\\right\\|\\|u_{0}\\|}}\\\\ &{\\leq\\operatorname*{max}\\{\\operatorname*{min}\\{1/t,1/2\\},O(\\eta\\sqrt{\\log(1/\\eta)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F Finding a Good Initialization with an Extreme Threshold ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By Theorem 3.8, we know that Algorithm 2 can only find some $w_{0}$ such that $\\sin(\\theta_{0}/2)\\ \\le$ $O(\\eta\\sqrt{\\log(1/\\eta)})$ , where $\\eta=\\epsilon/p$ when $p$ is small such that $\\eta\\sqrt{\\log(1/\\eta)}>O(1/t)$ . In this section, we design an algorithm that finds a warm start with a non-negligible probability of success when the threshold $t^{*}$ falls in this range. Formally, we prove the following theorem. ", "page_idx": 20}, {"type": "text", "text": "Theorem F.1. Let $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ be a halfspace with bias $p$ and $y(x)$ be any labeling function such that $\\operatorname{err}(h^{*})=\\operatorname{opt}\\leq\\epsilon\\leq1/C$ for some large enough constant $C$ . Let t be a scalar such that $t-1/\\log(1/\\epsilon)\\leq t^{*}\\leq t$ and $1/(400t)\\le\\eta\\sqrt{\\log(1/\\eta)}\\le1/C$ for some large enough constant $C$ , where $\\eta=\\epsilon/p,$ , Algorithm $5$ makes $M=\\tilde{O}(1/p+d\\log(1/\\epsilon))$ membership queries, runs in poly $(d,M)$ time and with probability at least $1/\\mathrm{polylog}(1/\\epsilon)$ , outputs some $w_{0}$ such that $\\sin(\\theta(w_{0},w^{*})/2)\\le1/t$ . ", "page_idx": 20}, {"type": "text", "text": "The high-level idea of our algorithm is as follows. Although Algorithm 2 will not provide us a $w_{0}$ such that $\\bar{\\theta_{0}}\\le{\\cal O}(1/t)$ , $\\theta_{0}$ is still smaller than a sufficiently small constant. We want to use the localization technique to refine $w_{0}$ so that after $T$ rounds of refinement, $\\sin(\\theta_{T}/2)\\,\\le\\,\\sigma_{T}\\,=\\,1/t$ . Recall in Appendix D, we introduce Definition D.2, $(v,s,\\sigma)$ -rejection procedure, which can be simulated using membership query. Passing a Gaussian random point to the $(v,s,\\sigma)$ -rejection procedure, according to Lemma D.3, we will get a another distribution over $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ that behaves the same as another halfspace $h^{\\prime}$ . ", "page_idx": 20}, {"type": "text", "text": "In this section, we want to design a $(v,s,\\sigma)$ -rejection procedure such that the direction of the halfspace $h^{\\prime}$ has a constant correlation with respect to $w^{*}$ and the noise level after the rejection procedure is much smaller than the length of the Chow-Parameter of $h^{\\prime}$ . Write $w^{\\ast}=a_{i}w_{i}+b_{i}u_{i}$ . We want to set up $v=w_{i}$ , $\\sigma=1/t$ and $s\\sim(a_{i}t,a_{i}t+b_{i})$ uniformly. Such a method is called the randomized threshold method in [15]. This method has the following property. ", "page_idx": 20}, {"type": "text", "text": "Lemma F.2 (Proposition C.11 in [15]). Let a, $b,t>0$ such that $a^{2}+b^{2}=1$ and $t$ larger than some constant $C$ . Let $\\bar{w}\\in S^{d-1}$ . Let $s\\sim[a t,a t+b]$ uniformly. For each $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , the expected probability that $x$ is accepted by the $(w,s,\\sigma)$ -rejection procedure is at most $\\sigma/b$ , where $\\sigma=1/t$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma F.2 implies that in expectation over the randomness of $s$ , only $\\sigma/b_{i}$ -fraction of the noisy points will pass the $(w_{i},s,\\sigma)$ -rejection procedure. If we use query to simulate such a rejection procedure, by Lemma D.3, with a constant probability, the noise rate among our queries would be $\\bar{O}(\\epsilon\\exp(s^{2}/\\bar{2})/b_{i})$ . However, as we do not know $b_{i}$ , using some $b$ that is slightly far from $b_{i}$ would make the noise level too high for us to learn the signal we want. To overcome this, we design the following test approach to show that given a $b$ , we can with high probability check if it can be used to construct the rejection procedure or not and in particular, when $\\bar{b^{+}}-1/\\log(1/\\epsilon)<b_{i}<b$ , such a $b$ is guaranteed to pass our test. ", "page_idx": 21}, {"type": "text", "text": "Lemma F.3. Let $h^{*}(x)=\\mathrm{sign}(w^{*}\\cdot x+t^{*})$ be a halfspace and $y(x)$ be any labeling function such that $\\mathrm{err}(h^{*})=\\mathrm{opt}\\le\\epsilon$ . Let $w\\in S^{d-1}$ be unit vector such that $w^{*}=a^{*}w+b^{*}u,\\,a^{*},b^{*}>0$ and $(a^{*})^{2}+^{\"}(b^{*})^{2}=^{\"}1,$ , $b<1/4.$ . Let $t\\,>\\,0$ such that $t\\exp(t^{2}/2)\\,\\leq\\,1/(C\\epsilon)$ for a sufficiently large constant $C$ . Let $a,b\\,\\in\\,(0,1)$ such that $a^{2}+b^{2}\\,=\\,1\\qquad$ . Let $b,t,w,\\delta$ be input of Algorithm 4. Let $s\\sim(a t,a t+b)$ uniformly. Denote by $p(b,s)$ be bias of a halfspace with threshold $T_{b s}:=(t-a s)/b$ . Let $\\ell(z)=\\mathrm{sign}((a^{*}\\sigma w+b^{*}u)\\cdot z+t^{*}-a^{*}s)$ be a halfspace with bias $p_{s}$ , where If the probability that $p_{s}>p(b,s)/4$ is at most $1/2$ , then with probability at least $1-\\delta$ , Algorithm 4 output \u201cNo\u201d. If the probability that $p_{s}>p(b,s)/2$ is at least 29/30, then with probability at least $1-\\delta$ , Algorithm 4 output \u201cYes\u201d. Furthermore, the query complexity of Algorithm $^{4}$ is $\\tilde{O}_{\\delta}(1/p^{2}(b,a t))=\\tilde{O}_{\\delta}(1/\\sqrt{p})$ ", "page_idx": 21}, {"type": "text", "text": "In particular, when $b^{*}~\\geq~1.5/t~\\geq~1.5/\\sqrt{\\log(1/\\epsilon)},~|b-b^{*}|~\\leq~1/\\log(1/\\epsilon)$ and $|t-t^{*}|\\ \\leq$ $1/\\log(1/\\epsilon)$ , with probability at least $1-\\delta$ , Algorithm 4 will output \u201cYes\u201d. ", "page_idx": 21}, {"type": "text", "text": "Algorithm 4 ANGLE TEST(Check if $b$ is a good approximation for $\\sin\\theta(w^{*},w))$ ) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input: A direction $w$ , confidence parameter $\\delta\\in(0,1)$ , threshold $t>0$ , parameter $b$   \nOutput: \u201cYes\u201d or $\\mathbf{\\delta}^{6}\\mathbf{N_{0}}^{,,}$   \nCount $\\gets0$ .   \n$A\\leftarrow I-(1-\\sigma^{2})w w^{t},\\sigma=1/t$   \nCompute $a={\\sqrt{1-b^{2}}}$   \nLet $T_{b s}=(t-a s)/b$ and $p(b,s)$ be the bias of a halfspace with threshold $T_{b s}$   \nfor $i=1\\ldots T=O(\\log(1/\\delta))$ do Draw $s\\sim[a t,a t+b]$ uniformly Draw $m=\\Tilde{O}(1/p^{2}(b,s))\\ z\\sim N(0,I)$ and query $y(A z-s w)$ . Compute $\\hat{p_{s}}$ the empirical probability of $y(A z-s w)=-1$ if $\\hat{p_{s}}>p(b,s)/3$ then Count $\\leftarrow\\mathrm{Count}{+1}$   \nif Count $>3T/4$ then return \u201cYes\u201d   \nelsereturn \u201cNo\u201d ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma F.3. By Lemma D.3 and Lemma F.2, we know that over the randomness of $s$ , with probability at least $5/6$ , $\\eta:=\\mathbf{Pr}_{z\\sim N(0,I)}(h^{\\ast}(A z-s w)\\neq y(A z-s w))\\leq6\\epsilon\\exp(s^{2}/2)/b$ . We assume, for now, such an event happens. We first show that such a noise rate is much smaller than $p(b,s)$ . Write $s=a t+\\xi$ , where $\\xi\\in[0,b]$ , then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\epsilon\\exp(s^{2}/2)/b}{\\frac{1}{T_{b s}}\\exp(-T_{b s}^{2}/2)}=\\frac{T_{b s}\\epsilon}{b}\\exp(\\frac{s^{2}+T_{b s}^{2}}{2})\\le t\\epsilon\\exp(\\frac{s^{2}}{2}+\\frac{(t-a s)^{2}}{2b^{2}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le t(t\\exp(t^{2}/2))^{-1}\\exp(\\frac{s^{2}}{2}+\\frac{(t-a s)^{2}}{2b^{2}})/C}\\\\ &{\\qquad\\qquad\\qquad\\qquad=C^{-1}\\exp(-\\frac{t^{2}}{2}+\\frac{(a t+\\xi)^{2}}{2}+\\frac{(b^{2}t-a\\xi)^{2}}{2b^{2}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=C^{-1}\\exp(\\frac{1}{2}(\\xi^{2}+\\frac{a^{2}\\xi^{2}}{b^{2}}))\\le C^{-1}e:=(C^{\\prime})^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, in the first inequality, we use the fact that $T_{b s}\\leq b t$ , in the second inequality, we use the fact that $t\\exp(t^{2}/2)\\le1/(C\\epsilon)$ for a sufficiently large constant $C$ , and in the last inequality, we use the fact that $a^{2}+b^{2}={\\mathrm{1}},\\xi^{2}<b^{2}$ . By Fact D.4, we know that $\\exp(-T_{b s}^{2}/2)/T_{b s}$ is at most 3 times $p(b,s)$ , and thus $\\eta\\le p(b,s)/C^{\\prime}$ for a large enough constant $C^{\\prime}$ . ", "page_idx": 22}, {"type": "text", "text": "By Fact 3.3, we know that the ground truth label $\\ell(z)=h^{\\ast}(A z-s w)=\\operatorname{sign}((a^{\\ast}\\sigma w+b^{\\ast}u)\\cdot z+$ $t^{*}-a^{*}s)$ . By Hoeffding\u2019s inequality, with high probability, we are able to estimate the probability of $y(A z\\,-\\,s w)\\;=\\;-1$ up to error $p(b,s)/20$ using $\\tilde{O}(1/p^{2}(b,s))$ queries. In particular, since $T_{b s}\\leq t b<1/4$ , by Fact D.4, we know that $p(b,s)>p^{1/4}$ and will cost us only $\\tilde{O}(1/\\sqrt{p})$ queries. ", "page_idx": 22}, {"type": "text", "text": "If the probability that $p_{s}\\,>\\,p(b,s)/4$ is at most $1/2$ , then in each round $i$ of Algorithm 4, with probability at least $1/3$ it holds simultaneously that $p_{s}<p(b,s)/4$ and $\\eta\\le p(b,s)/C^{\\prime}$ . In this case, with high probability $\\hat{p_{s}}<p(b,s)/3$ and Count does not increase. Thus, with probability at least $1-\\delta$ , after $T=O(\\log(1/\\delta))$ rounds, Count $<3T/4$ by Hoeffding\u2019s inequality. ", "page_idx": 22}, {"type": "text", "text": "Similarly, if the probability that $p_{s}>p(b,s)/2$ is at least $29/30$ , then in each round $i$ of Algorithm 4, with probability at least $4/5$ it holds simultaneously that $p_{s}^{\\star}>p(b,s)/2$ and $\\eta\\le p(b,s)/\\bar{C}^{\\prime}$ . In this case, with high probability $\\hat{p_{s}}>p(b,s)/3$ and Count increases. Thus, with probability at least $1-\\delta$ , after $T=O(\\log(1/\\delta))$ rounds, Count $>3T/4$ by Hoeffding\u2019s inequality. ", "page_idx": 22}, {"type": "text", "text": "Finally, we show that when $|b^{*}-b|\\leq1/\\log(1/\\epsilon)$ and when $|t-t^{*}|\\leq1/\\log(1/\\epsilon).$ , Algorithm 4 with high probability outputs \u201cYes\u201d. To do this, we will show the true threshold of $\\ell(z)$ is close to $T_{b s}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{t^{*}-a^{*}s}{\\sqrt{(b^{*})^{2}+(a^{*}\\sigma)^{2}}}-T_{b s}\\leq\\frac{t^{*}-a^{*}s}{b^{*}}-\\frac{t-a s}{b}\\leq\\frac{O(\\log(1/\\epsilon)^{-1})}{b^{*}}+|t-a s|\\left|\\frac{1}{b^{*}}-\\frac{1}{b}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq O(\\log(1/\\epsilon)^{-1/2})+\\left|\\frac{b^{2}t(b-b^{*})}{b^{*}b}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=O(\\log(1/\\epsilon)^{-1/2})+O(t\\log(1/\\epsilon)^{-1})=O(\\log(1/\\epsilon)^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Fact D.4, it holds with probability 1 that $p_{s}>p(b,s)/2$ . ", "page_idx": 22}, {"type": "text", "text": "Now assume that we have $\\sin(\\theta_{i}/2)\\ \\le\\ \\sigma_{i}$ , then $b_{i}~\\leq~2\\sigma_{i}$ . This implies that by testing $b\\,=$ $\\begin{array}{r}{2\\sigma_{i}-\\frac{j}{\\log\\left(1/\\epsilon\\right)}}\\end{array}$ for $j=0,1,\\ldots$ , we only need ${\\cal O}(\\log(1/\\epsilon))$ rounds to find the correct $b$ . With this fact, we have the following Algorithm 5. ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem F.1. Let $\\theta_{i}=\\theta(w_{i},w^{*})$ and write $w^{\\ast}=a_{i}w_{i}\\{b_{i}u_{i}$ , where $a_{i},b_{i}>0,a_{i}^{2}{+}b_{i}^{2}=1$ . By Algorithm 2, we know that with probability at least $./3,\\sin(\\theta_{0}/2)\\leq O(\\epsilon/p\\sqrt{\\log(p/\\epsilon)})$ . We will assume $\\sin(\\theta_{0}/2)\\;\\leq\\;\\epsilon/p\\sqrt{\\log(p/\\epsilon)}$ holds throughout the proof, since the constant before $\\epsilon/p{\\sqrt{\\log(p/\\epsilon)}}$ can always be assumed to be normalized as $1/C$ is large enough. In round $i$ of the algorithm, we write $w^{\\ast}\\,=\\,a_{i}w_{i}+b_{i}u_{i}$ where $a_{i},b_{i}\\,>\\,0,a_{i}^{2}+b_{i}^{2}\\,=\\,1$ . Similar to the analysis of Algorithm 3, we will show that if $\\sin(\\theta_{i}/2)\\,\\leq\\,\\sigma_{i}$ then with probability $1/3$ it also holds that $\\sin(\\theta_{i+1}/2)\\le\\sigma_{i+1}$ . If this is true then since $1/t>1/\\sqrt{\\log(1/\\epsilon)}$ after ${\\cal O}(\\log\\log(1/\\epsilon))$ rounds, we have $\\sin(\\theta_{T}/2)\\le1/t$ with probability at least $1/\\mathrm{polylog}(1/\\epsilon)$ . ", "page_idx": 22}, {"type": "text", "text": "Recall the notation in the proof of Lemma F.3. Given $\\hat{b}$ , we define $p(\\hat{b},s)$ to be the bias of a halfspace with a threshold $T_{\\hat{b},s}\\,=\\,(t-\\hat{a}s)/\\hat{b}$ . By Fact 3.3, we define $\\ell(z)\\,=\\,h^{*}(A z\\,-\\,s w_{i})\\,=$ $\\mathrm{sign}((a_{i}\\sigma w_{i}+b_{i}u)\\cdot z+t^{*}-a_{i}s)$ the ground truth label of $y(A z-s w_{i}),$ $t_{s}$ to be its threshold and $p_{s}$ to be the bias of $\\ell(z)$ . ", "page_idx": 22}, {"type": "text", "text": "By Lemma F.3, we know that as long as $b_{i}>1.5/t$ , with high probability Algorithm 4 will output \u201cYes\u201d for some $\\hat{b}$ such that with probability at least $1/2$ , $p_{s}>p(\\hat{b},s)/2>p^{1/4}$ . On the other hand, by Equation (3), we know that with probability at least $5/6,\\eta:=\\mathbf{Pr}_{z\\sim N(0,I)}(\\ell(z)\\neq y(A z-s w_{i}))\\leq$ $p(b,s)/C^{\\prime}$ for a sufficiently large constant $C^{\\prime}$ . Thus, with a probability at least $1/3,p_{s}>p(\\hat{b},s)/2$ and $\\eta\\leq p(\\hat{b},s)/C^{\\prime}$ hold simultaneously. For now, we assume this happens and we will analyze the smoothed label around some $z_{0}$ such that $y(A z_{0}-s w_{i})=-1$ . By Fact 3.10, the smoothed label ", "page_idx": 22}, {"type": "text", "text": "Input: error parameter $\\epsilon\\in(0,1)$ , confidence parameter $\\delta\\in(0,1)$ , threshold $t>0$   \nOutput: $w_{0}\\in S^{d-1}$   \nRun Algorithm 2 to get a $w_{0}\\in S^{d-1}$ . Let $\\sigma_{0}=\\epsilon/p\\sqrt{\\log(p/\\epsilon)}$ be a parameter   \nfor $i=0,\\ldots,T=O(\\log\\log(1/\\epsilon))$ do Run Algorithm 4 with input $w_{i}$ and $\\begin{array}{r}{\\hat{b}=2\\sigma_{i}-\\frac{j}{\\log(1/\\epsilon)},j=0,\\dots,\\sigma_{i}\\log(1/\\epsilon)}\\end{array}$ Let $\\hat{b}$ be the first parameter such that Algorithm 4 outputs \u201cYes\u201d If Algorithm 4 outputs \u201cNo\u201d for all $\\hat{b}$ or the $\\hat{b}$ we use less than $1/t$ , then return $w_{i}$ . Let $\\hat{a}=\\sqrt{1-\\hat{b}^{2}}$ and $T_{\\hat{b},s}=(t-\\hat{a}s)/\\hat{b}$ , where $s\\sim[\\hat{a}t,\\hat{a}t+\\hat{b}]$ . Estimate the probability $\\hat{p_{s}}$ of $p_{s}=y(A z-s w_{i})=-1$ for $z\\sim N(0,I)$ up to error $p^{1/4}/100$   \nusing $\\tilde{O}(\\sqrt{1/p})$ queries. Let $\\hat{t_{s}}$ be the threshold of a halfspace with bias $\\hat{p_{s}}$ $A\\leftarrow\\dot{I}-(1-\\sigma^{2})w_{i}w_{i}^{t},$ $\\sigma=1/t$ Draw $z\\sim N(0,I)$ and query $y(A z\\mathrm{~-~}s w_{i})$ until some $z_{0}$ such that $y(A z_{0}-s w_{i})\\,=\\,-1$ is   \ndrawn Draw $z_{i}\\sim N(0,I)$ , for $i\\in[m],m=\\tilde{O}(d)$ and query $f_{i}(z_{i}):=y(A(\\sqrt{1-\\rho}z_{0}\\!+\\!\\rho z_{i})\\!-\\!s w_{i})$ ,   \nwhere $\\rho=1/\\hat{t_{s}}$ $\\begin{array}{r l}&{g_{i}\\leftarrow\\frac{1}{m}\\overleftarrow{\\sum_{i=1}^{m}\\mathrm{proj}}_{w_{i}^{\\perp}}z_{i}f_{i}(z_{i}),w_{i+1}\\leftarrow\\mathrm{proj}_{S^{d-1}}(w_{i}+\\mu_{i}g_{i})}\\\\ &{\\sigma_{i+1}\\leftarrow(1-1/C_{2})\\sigma_{i}\\;\\mu_{i+1}=(1-1/C_{1})\\sigma_{i+1}}\\end{array}$   \nreturn $w_{T}$ ", "page_idx": 23}, {"type": "text", "text": "around $z_{0}$ with respect to halfspace $\\ell(z)$ can be seen as a halfspace ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ell_{z_{0}}(z_{i})=\\mathrm{sign}(W_{i}\\cdot z_{i}+T_{\\rho,s}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $W_{i}:=(a_{i}\\sigma w_{i}+b_{i}u_{i})/\\sqrt{(a_{i}\\sigma)^{2}+b_{i}^{2}}$ and $\\begin{array}{r}{T_{\\rho,s}=\\frac{t_{s}+\\sqrt{1-\\rho^{2}}W_{i}\\cdot z_{0}}{\\rho}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "By Fact 3.10 and Lemma 3.11, we know that with probability at least $1/2$ , such a $z_{0}$ satisfies ", "page_idx": 23}, {"type": "text", "text": "2. the noise level of the smoothed label is at most $5\\eta/p_{s}\\,\\le\\,1/C^{\\prime\\prime}$ for some large enough constant C\u2032\u2032 ", "page_idx": 23}, {"type": "text", "text": "Since $p_{s}>p(\\hat{b},s)/2$ , we can bound the threshold $T_{\\rho,s}$ by ", "page_idx": 23}, {"type": "equation", "text": "$$\n-2\\le\\frac{t_{s}-\\sqrt{1-\\rho^{2}}(t_{s}+\\frac{1}{t_{s}})}{\\rho}\\le T_{\\rho,s}\\le\\frac{t_{s}(1-\\sqrt{1-\\rho^{2}})}{\\rho}\\le t_{s}/\\hat{t_{s}}\\le2,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "because $\\hat{t_{s}}$ is at least close to $t_{s}$ up to a small constant factor, otherwise $\\hat{p_{s}}$ would be far from $p_{s}$ . Combine Equation (4) and Fact 3.4, we know that $\\begin{array}{r}{\\mathbf{E}_{z^{\\prime}\\sim N(0,I)}\\operatorname{proj}_{w_{i}^{\\perp}}z^{\\prime}\\ell_{z_{0}}(z^{\\prime})=\\phi\\frac{u_{i}b_{i}}{\\sqrt{(a_{i}\\sigma)^{2}+b_{i}^{2}}}}\\end{array}$ , for some $\\phi\\in(e^{-2},1)$ . Since the noise level of the smoothed label around $z_{0}$ is as small as $1/C^{\\prime\\prime}$ for some large enough constant $C^{\\prime\\prime}$ , by Hoeffding\u2019s inequality, we know that $\\begin{array}{r}{\\left\\|g_{i}-\\mathbf{E}_{z^{\\prime}\\sim N(0,I)}\\operatorname{proj}_{w_{i}^{\\perp}}z^{\\prime}\\ell_{z_{0}}(z^{\\prime})\\right\\|}\\end{array}$ can be smaller than some tiny constant with high probability. ", "page_idx": 23}, {"type": "text", "text": "As it always holds that $\\sigma_{i}\\geq1/t$ for each $i$ , we will consider two cases. In the first case, $\\sin(\\theta_{i}/2)\\le$ $3\\sigma_{i}/4$ and $\\lVert g_{i}\\rVert$ is bounded by some universal constant. ", "page_idx": 23}, {"type": "text", "text": "In the second case, we have $3\\sigma_{i}/4\\;\\;\\le\\;\\;\\sin(\\theta_{i}/2)\\;\\;\\le\\;\\;\\sigma_{i}$ . In this case we know that $\\begin{array}{r l r}{\\mathrm{\\bfE}_{z^{\\prime}\\sim N(0,I)}\\,\\mathrm{proj}_{w_{i}^{\\perp}}z^{\\prime}\\ell_{z_{0}}(z^{\\prime})\\;=\\;\\phi\\frac{u_{i}b_{i}}{\\sqrt{(a_{i}\\sigma)^{2}+b_{i}^{2}}}\\;=\\;\\psi u_{i}}\\end{array}$ for some $\\psi\\ \\geq\\ e^{-4}$ , which implies that $\\begin{array}{r l}&{\\mathbf{E}_{z^{\\prime}\\sim N(0,I)}\\operatorname{proj}_{w_{i}^{\\perp}}z^{\\prime}\\ell_{z_{0}}(z^{\\prime})\\ \\cdot\\ u_{i}\\ \\geq\\ \\psi\\frac{b_{i}}{\\sqrt{(a_{i}\\sigma)^{2}=b_{i}^{2}}}\\ \\geq\\ e^{-5}}\\\\ &{\\sin(\\theta_{i+1}/2)\\leq(1-1/C_{1})\\sigma_{i}=\\sigma_{i+1}.}\\end{array}$ . Using Lemma 3.2, we know that ", "page_idx": 23}, {"type": "text", "text": "Finally, we prove the query complexity of Algorithm 5. By Theorem 3.8, it takes us ${\\tilde{O}}(1/p+$ $d\\log(1/\\epsilon))$ queries to get some $w_{0}$ by running Algorithm 2. After obtaining $w_{0}$ , in each round of ", "page_idx": 23}, {"type": "text", "text": "Algorithm 4, we will run Algorithm 4 ${\\cal O}(\\log(1/\\epsilon))$ times to find a desired $\\hat{b}$ and each round takes us $\\tilde{O}(1/p^{2}(\\hat{b}))\\,\\leq\\,1/p^{2c}\\,\\leq\\,1/\\sqrt{p}$ queries, because $p(\\hat{b})$ is the bias of a halfspace with threshold $T_{\\hat{b}}=\\hat{b}t$ , which is smaller than $t$ by a tiny constant factor. Furthermore, after obtaining $\\hat{b}$ it takes us $\\tilde{O}(1/p(\\hat{b})+d\\log(1/\\epsilon))$ queries to perform the gradient descent update. So, in total Algorithm 5 has query complexity at most $\\tilde{O}(1/p+d\\log(1/\\epsilon))$ . ", "page_idx": 24}, {"type": "text", "text": "G Proof of Theorem 1.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 1.2. We first show the correctness of Algorithm 1. When we run Algorithm 1, we will start with some interval $[t_{a},t_{b}]$ such that any halfspace with a threshold $t\\in[t_{a},t_{b}]$ must have bias $\\Theta(p)$ . Next, Algorithm 1 partition $[t_{a},t_{b}]$ into grid such that $|t_{j+1}-t_{j}|\\leq1/\\log(1/\\bar{\\epsilon})$ . This implies that there must be some $t_{j}\\in[t_{a},\\bar{t}_{b}]$ such that $t_{j}-1/\\log(\\log(1/\\epsilon))\\stackrel{*}{\\leq}t^{*}\\stackrel{}{\\leq}t_{j}$ . By Theorem 3.8 and Algorithm 5, as long as $p>C\\epsilon$ , with probability at least $1/\\mathrm{polylog}(1/\\epsilon)$ , we can find some $w_{0}$ such that $\\sin(\\theta_{0}/2)\\leq\\operatorname*{min}\\{1/t_{j},1/2\\}$ . In particular, by running Algorithm 2 or Algorithm 5 $\\mathrm{polylog}(1/\\epsilon)$ times, at least one of these $w_{0}$ satisfies the condition. Furthermore, with such a $w_{0}$ , we know from Theorem 3.1 that we can with high probability get some $\\hat{h}$ such that $\\mathrm{err}(\\hat{h})\\leq O(\\mathrm{opt}+\\epsilon)$ . Thus within the list $\\mathcal{C}$ of the candidate hypotheses maintained by Algorithm 1 at least one of them has error $O(\\mathrm{opt}+\\epsilon)$ . By Lemma C.1, we can with high probability find a hypothesis among $\\mathcal{C}$ , whose error is at most 10 times the error of the best hypothesis in $\\mathcal{C}$ and thus has error $O(\\mathrm{opt}+\\epsilon)$ . ", "page_idx": 24}, {"type": "text", "text": "Next, we prove the query complexity of Algorithm 1. According to Appendix C.2, we know that finding an interval $[t_{a},t_{b}]$ costs us $\\tilde{O}(\\operatorname*{min}\\{1/p,1/\\epsilon\\})$ queries. If we find $p<C\\epsilon$ then we are done. Otherwise, we will run the initialization algorithm and the refinement algorithm. By Theorem 3.8 and Algorithm 5, each time we run an initialization algorithm, it takes us $\\tilde{O}(1/p+d\\mathrm{polylog}(1/\\epsilon))$ queries. By Algorithm 3, each time we run Algorithm 3, it takes us $\\tilde{O}(d\\log(1/\\epsilon)$ queries. Since we will run these algorithms at most polylog $(1/\\epsilon)$ times. We will in total make $\\tilde{O}(1/p+d\\mathrm{polylog}(1/\\epsilon))$ queries. Finally, by Lemma C.1, finding a good hypothesis from the list of candidate hypotheses will only take us $\\mathrm{{olylog}}(1/\\epsilon)$ queries. Thus, we conclude the query complexity of Algorithm 1 is $\\tilde{O}(\\operatorname*{min}\\{1/p,1/\\epsilon\\}+d\\mathrm{polylog}(1/\\epsilon))$ . ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The abstract summarizes the results provided in Theorem 1.1 and Theorem 1.2. The introduction summarizes the motivations of this paper and describes prior work\u2019s contributions. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The limitations of this paper are discussed in the introduction of the paper. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The statement of each theorem provides all the assumptions and we provide complete proofs for all statements that are either in the main body of the paper or in the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is theoretical and does not contain experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper is theoretical and does not contain experiments. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper is theoretical and does not contain experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper is theoretical and does not contain experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper is theoretical and does not contain experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our research conforms in every respect with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work is theoretical and we do not see any immediate implications on society. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper is theoretical. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work does not use any assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work does not use any assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]