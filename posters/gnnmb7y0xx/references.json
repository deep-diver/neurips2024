{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper introduces the concept of In-Context Learning (ICL) in large language models (LLMs), which is the central focus of the current paper."}, {"fullname_first_author": "Roee Hendel", "paper_title": "In-context learning creates task vectors", "publication_date": "2023-10-26", "reason": "This paper introduces the concept of task vectors within LLMs, which the current paper expands on by proposing the use of state vectors."}, {"fullname_first_author": "Eric Todd", "paper_title": "Function vectors in large language models", "publication_date": "2023-10-26", "reason": "This paper also explores compressed vectors representing ICL functionality within LLMs, providing another foundation for the current paper's state vector concept."}, {"fullname_first_author": "Damai Dai", "paper_title": "Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers", "publication_date": "2023-07-11", "reason": "This paper's analysis of LLMs' ICL mechanisms as gradient descent informs the current paper's optimization methods for state vectors."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "publication_date": "2022-07-17", "reason": "This paper's model soup approach, which averages model weights, inspires the current paper's inner optimization method for state vector refinement."}]}