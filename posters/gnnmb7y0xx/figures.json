[{"figure_path": "gnnmB7y0Xx/figures/figures_3_1.jpg", "caption": "Figure 1: The overall framework of the proposed state vector. The state vectors are extracted from the output activations of attention heads. These state vectors are progressively optimized by inner optimization and momentum optimization, or be aggregated through a divide-and-conquer (D&C) aggregation. Finally, the processed state vector is utilized to intervene the inference forward pass.", "description": "This figure illustrates the proposed framework for processing state vectors in the context of in-context learning. It shows how state vectors are extracted from attention head outputs, progressively optimized using inner and momentum methods or aggregated via a divide-and-conquer strategy, and finally used to intervene in the inference forward pass.", "section": "4 Method"}, {"figure_path": "gnnmB7y0Xx/figures/figures_6_1.jpg", "caption": "Figure 2: Performance of aggregation across number of examples. Avg. denotes the average aggregation baseline and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and the Y axis represents the accuracy.", "description": "The figure shows the performance of two aggregation methods (average and divide-and-conquer) compared to baselines (regular and ICL) across different numbers of examples. The x-axis shows the number of examples used for aggregation, while the y-axis represents the accuracy achieved.  It demonstrates how the performance of the aggregation methods improves as the number of examples increases, eventually surpassing the baselines in many cases.  This highlights the effectiveness of the divide-and-conquer approach in handling a large number of examples in in-context learning.", "section": "5.4 Momentum Optimization"}, {"figure_path": "gnnmB7y0Xx/figures/figures_7_1.jpg", "caption": "Figure 3: Average zero-shot performance across six datasets for each choice of the intermediate layer L. The solid line means the average value, while the shaded area indicates the standard deviation.", "description": "This figure shows the average zero-shot performance across six datasets for different choices of the intermediate layer L, a hyperparameter in the proposed model.  The x-axis represents the number of layers (L), and the y-axis shows the accuracy.  The solid line represents the average accuracy across the six datasets, and the shaded region indicates the standard deviation, illustrating the variability of the performance across different datasets at each layer. This visualization helps determine the optimal number of layers for extracting the state vector for optimal model performance in a zero-shot setting.", "section": "6.2 Layer Selection"}, {"figure_path": "gnnmB7y0Xx/figures/figures_8_1.jpg", "caption": "Figure 4: The 2D PCA visualization of the state vector in the Antonym, English-French, and Product-Company tasks, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration, with the position of each point indicated by the adjacent number.", "description": "This figure shows the results of applying Principal Component Analysis (PCA) to reduce the dimensionality of the state vectors from three different tasks: Antonym, English-French translation, and Product-Company. Each point in the plots represents a state vector extracted from the transformer model's attention mechanism, specifically focusing on the final separate token, the last layer.  The color of each point corresponds to the example position in the demonstration sequence.  The plot helps to visualize the clustering of state vectors based on their position within the demonstration, revealing how the model encodes information from the examples into the compressed vectors.", "section": "6.3 Qualitative Study"}, {"figure_path": "gnnmB7y0Xx/figures/figures_13_1.jpg", "caption": "Figure 5: Time efficiency analysis of Llama-2-7B and GPT-J-6B. Inn denotes our state vector with inner optimization. Mom denotes our state vector with momentum optimization", "description": "This figure shows the speedup achieved by the proposed inner and momentum optimization methods compared to regular ICL.  The speedup is calculated as the ratio of the inference time of regular ICL to the inference time of the optimized methods. The results are shown separately for Llama-2-7B and GPT-J-6B, and for both zero-shot and few-shot settings.  The figure demonstrates that the proposed optimization methods significantly speed up the inference process while maintaining a high level of accuracy, as reported in Table 1.", "section": "5. Experiment"}, {"figure_path": "gnnmB7y0Xx/figures/figures_16_1.jpg", "caption": "Figure 2: Performance of aggregation across number of examples. Avg. denotes the average aggregation baseline and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and the Y axis represents the accuracy.", "description": "This figure displays the performance of two different aggregation methods (average and divide-and-conquer) on the accuracy of in-context learning (ICL) tasks as the number of examples increases.  It showcases how the performance of both methods improves with more examples, with divide-and-conquer eventually surpassing the average aggregation method. The results are presented separately for zero-shot and few-shot learning settings and for different datasets (AG News, Antonym, English-French, and Product-Company). The figure helps to visually understand and compare the effectiveness of different aggregation strategies in ICL.", "section": "5.4 Divide-and-Conquer Aggregation (RQ3)"}, {"figure_path": "gnnmB7y0Xx/figures/figures_17_1.jpg", "caption": "Figure 2: Performance of aggregation across number of examples. Avg. denotes the average aggregation baseline and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and the Y axis represents the accuracy.", "description": "The figure shows the performance of two aggregation methods (average and divide-and-conquer) compared to baselines (regular and ICL) across different numbers of examples for four tasks using two different LLMs.  The X-axis represents the number of examples used, and the Y-axis represents the accuracy achieved.  The results illustrate how the performance of both methods initially trails behind ICL, but they improve as the number of examples increases, showcasing the efficiency of aggregation, especially the divide-and-conquer approach.", "section": "5.4 Momentum Optimization (RQ2)"}, {"figure_path": "gnnmB7y0Xx/figures/figures_17_2.jpg", "caption": "Figure 9: The 2D PCA visualization of the state vector in the Antonym task and English-French task of GPT-J, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration and the outlier is of the first order.", "description": "This figure shows the result of applying Principal Component Analysis (PCA) to reduce the dimensionality of the state vectors for the Antonym and English-French tasks using GPT-J.  Each color represents a different example's position within the demonstration sequence. The clustering suggests that state vectors from the same demonstration position are similar, and there is a clear separation between the first example and subsequent examples, potentially indicating how ICL develops incrementally.  The outlier is indicated as a first-order outlier.", "section": "Qualitative Study"}, {"figure_path": "gnnmB7y0Xx/figures/figures_17_3.jpg", "caption": "Figure 10: Standard deviation of performance on Llama-2 across three datasets.", "description": "This figure displays the standard deviation of performance results for the task vector and the inner optimized state vector on three datasets (Antonym, Person-Instrument, English-French) in both zero-shot and few-shot settings.  It illustrates the robustness of the methods against variations in demonstrations (a) and dummy queries (b) during the experiment. Lower standard deviations indicate more consistent performance.", "section": "J Robustness Analysis"}]