[{"type": "text", "text": "Learning Spatially-Aware Language and Audio Embeddings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bhavika Devnani1,\u2217 Skyler Seto2 Zakaria Aldeneh2 Alessandro Toso2 Elena Menyaylenko2 Barry-John Theobald2 Jonathan Sheaffer2 Miguel Sarabia2 ", "page_idx": 0}, {"type": "text", "text": "1 Georgia Institute of Technology 2 Apple bdevnani3@gatech.edu, {sseto, zaldeneh, atoso}@apple.com {elenam, bjtheobald, sheaffer, miguelsdc}@apple.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans can picture a sound scene given an imprecise natural language description. For example, it is easy to imagine an acoustic environment given a phrase like \u201cthe lion roar came from right behind me!\u201d. For a machine to have the same degree of comprehension, the machine must know what a lion is (semantic attribute), what the concept of \u201cbehind\u201d is (spatial attribute) and how these pieces of linguistic information align with the semantic and spatial attributes of the sound (what a roar sounds like when its coming from behind). State-of-the-art audio foundation models, such as CLAP [7, 44], which learn to map between audio scenes and natural textual descriptions, are trained on non-spatial audio and text pairs, and hence lack spatial awareness. In contrast, sound event localization and detection models are limited to recognizing sounds from a fixed number of classes, and they localize the source to absolute position (e.g., $0.2\\mathrm{m})$ ) rather than a position described using natural language (e.g., \u201cnext to me\u201d). To address these gaps, we present ELSA (Embeddings for Language and Spatial Audio), a spatially awareaudio and text embedding model trained using multimodal contrastive learning. ELSA supports non-spatial audio, spatial audio, and open vocabulary text captions describing both the spatial and semantic components of sound. To train ELSA: (a) we spatially augment the audio and captions of three open-source audio datasets totaling 4,738 hours and 890,038 samples of audio comprised from 8,972 simulated spatial configurations, and (b) we design an encoder to capture the semantics of non-spatial audio, and the semantics and spatial attributes of spatial audio using contrastive learning. ELSA is a single model that is competitive with state-of-theart for both semantic retrieval and 3D source localization. In particular, ELSA achieves $+2.8\\%$ mean audio-to-text and text-to-audio $\\mathbf{R}\\@1$ above the LAIONCLAP [44] baseline, and outperforms by $-11.6^{\\circ}$ mean-absolute-error in 3D source localization over the SeldNET [40] baseline on the TUT Sound Events 2018 benchmark [1]. Moreover, we show that the representation-space of ELSA is structured, enabling swapping of direction of audio via vector arithmetic of two directional text embeddings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans use implicit context when communicating about and comprehending sounds in their environment. For instance, the instruction \u201cPull over if you hear a siren from behind you\u201d is easily understood by most humans. However, a machine would need to not only recognize the source the sound, i.e., the siren (a semantic cue), but also interpret the spatial reference implied by \u201cbehind\u201d ", "page_idx": 0}, {"type": "text", "text": "relative to its own position (a spatial cue). The machine must then translate these linguistic cues into its understanding of spatial audio to accurately identify, locate, and conditionally respond to the sound. This degree of alignment between spatial audio and natural language is understudied in prior work. ", "page_idx": 1}, {"type": "text", "text": "Audio foundation models (AFMs), such as LAION-CLAP [44], have been used for multiple downstream applications, such as language guided audio editing [42, 17], language guided audio and music generation [16, 11, 45], audio representations for image and text [14, 46], setting a precedent for the wide applicability of audio representations aligned with natural language. However, these models, and similar state-of-the-art AFMs, such as Pengi [5], LTU [12], and SALMONN [37], cannot capture the spatial attributes as the models are trained only on single-channel/non-spatial audio. Conversely, models such as SELDNet [1], and PILOT [33] are capable of precise spatial attribute classification and regression, but lack capability to generalize to natural language descriptions of spatial and semantic attributes. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we introduce ELSA, a multimodal foundation model that learns a joint representation space for the spatial attributes and semantics of audio aligned with natural language descriptions of the audio scene. Learning a joint contrastive representation model enables and improves several tasks including: retrieval, multimodal QA, captioning, and generation [44]. Prior work has shown the beneftis of learning well-aligned encoders for multimodal tasks within the vision domain [23, 35]. In contrast, mapping between audio and natural language via a large language model, as in [48], can significantly improve language reasoning tasks, especially for zero-shot generalization of pre-trained models, however yields worse performance in classification and QA tasks when finetuned in the language domain [41]. ELSA enables similar spatially-aware downstream applications, for instance, one can expand traditional language-guided audio editing to manipulate spatial elements using natural language commands like: \u201cRemove the sound of the plane flying above\u201d or \u201cmove the sound of the dog barking from left to right\u201d. In this paper we focus, for the first time, on devising and analyzing multimodal task-agnostic representations that capture both the semantics and the spatial attributes of audio aligned with natural language. ", "page_idx": 1}, {"type": "text", "text": "Noting a lack of paired spatial audio and language data that can enable training spatially aware audio-language models at scale, to train ELSA, we synthesize a spatial audio corpus consisting of 890,038 samples that span a variety of acoustic room properties, such as size and reverberation, from audio clips of the AudioSet [9] and Freesound [8] corpora. We also synthesize natural language spatial audio captions to match the spatial audio using a large-language model (LLM) to rephrase the initial captions. We demonstrate that ELSA captures spatial attributes and semantics of audio by identifying a set of tasks on which a standard AFM, such as LAION-CLAP, fails. Moreover, we show that ELSA achieves better zero-shot classification of spatial attributes than models trained only for that task. Finally, we show that ELSA maintains the ability to represent non-spatial audio by demonstrating performance competitive with existing state-of-the-art for a number of tasks. ", "page_idx": 1}, {"type": "text", "text": "Our key contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present and release a new synthetic dataset of 4738.55 hours, with 890,038 samples and corresponding spatial captions across 8,972 simulated rooms with accurate parametric labels for the room properties and sound source locations. Additionally, we also record a small spatial real-world dataset to verify transfer to the real-world (cf. Section 3). ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide ELSA, a multimodal spatial audio-language model that jointly performs semantic classification (sound detection, retrieval), spatial localization, and direction of arrival. ELSA consists of an audio encoder paired with a text encoder that jointly learns semantic and spatial attributes via contrastive learning (cf. Section 4).   \n\u2022 We show that ELSA effectively captures spatial attributes and semantics competitive with baselines. ELSA improves by $-11.6^{\\circ}$ mean-absolute-error on 3D source localization, and by $+2.9\\%$ on text-to-audio and audio-to-text mAP $@10$ scores. (cf. Section 5).   \n\u2022 Further, we show that the representation-space of ELSA is structured, allowing for transposition of spatial sound direction via addition or subtraction of two spatially descriptive text embeddings. (cf. Section 5.4). ", "page_idx": 1}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/530a3461f77a460cf251e6fc372768706d4f13ec410e0f25af7d2c01493b10fc.jpg", "img_caption": ["(a) Our spatial audio pipeline uses (b) We augment the original cap- (c) We encode the spatially augsimulated rooms with different di- tions by adding properties from mented captions and audio, and then mensions, materials, and reverber- the room simulations and prompt a align the representations using a ation, and with sources located at LLM to rewrite the sentence. CLIP objective (see Fig. A.F.1 for different spatial locations. full architecture). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide an overview of the architecture choices and corresponding datasets for training and evaluation of models that can capture the semantics and spatial attributes of audio, including AFMs. ", "page_idx": 2}, {"type": "text", "text": "Audio-language Approaches Prior works (e.g., CLAP [7], LAION-CLAP [44], MULAN [15]) have extended the image-text contrastive pre-training approach introduced by CLIP [27] to link audio representations to textual descriptions. These models use two encoders, one for audio and another for text, to project the representations from the two modalities into a common embedding space. Once trained, the models enable zero-shot prediction and retrieval capabilities on unseen sounds and textual descriptions. Despite their utility, the methods do not capture the spatial attributes of the modeled signals, rather they capture only their semantics. Another line of work (e.g., Pengi [5], LTU [12], and SALMONN [37]) extends LLMs to enable audio understanding in open-vocabulary settings (e.g., audio captioning and audio question answering). Such models learn audio encoders to provide a prefix token to prompt a frozen pre-trained autoregressive LLM, which is then used to generate unconstrained text. These prior methods do not explicitly model the spatial attributes of the audio. Zheng et al. [48] introduced BAT, an audio-based LLM that combines binaural spatial sound perception with natural language understanding with an accompanying question-and-answer dataset that enables model training. BAT focuses on enabling LLMs to reason about binaural spatial audio, which depends on the head-related transfer-function. In contrast our focus is on a task-agnostic and device-agnostic representation of spatial audio aligned with text. ", "page_idx": 2}, {"type": "text", "text": "Audio-language Datasets Learning audio-language models requires access to datasets that link the two modalities. Clotho [6] and AudioCaps [18] are popular audio captioning datasets for which the textual descriptions were collected by annotating sound event datasets (e.g., AudioSet [9], or Freesound [8]) through crowd-sourcing platforms. LAION-Audio-630K [44] is a large-scale audiotext dataset collected by downloading audio and relevant textual descriptions from publicly available websites. All three datasets focus on the semantic attributes of the audio signal and do not have labels for the spatial attributes. SPATIALSOUNDQA [48] is a dataset that consists of simulated binaural audio samples and question-answer pairs, which was used to train BAT [48]. The audio samples were sourced from AudioSet [9], and the question-answer pairs were paraphrased using GPT-4. In contrast with the our environmental description captions, the text in SPATIALSOUNDQA is geared towards question-and-answer tasks. In addition, the dataset employs a binaural representation of spatial audio, rendering the data incompatible with ELSA. STARSS23 [36] is a dataset of real-world multi-channel audio annotated with semantic labels for overlapping sound sources, and the equivalent annotations for the spatial attributes. However, a limitation is that the dataset lacks natural language descriptions of the sound scenes, which are required for aligning the spatial attributes with language descriptions. ", "page_idx": 2}, {"type": "text", "text": "3 Paired Spatial Audio and Text Datasets ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multimodal contrastive learning approaches, e.g., CLIP [27] and CLAP [7], use large amounts of multimodal data pairs: 413M and 634k for the LAION versions of both models [32, 44]. Training a model capable of understanding spatial audio as natural language requires a spatial audio dataset annotated with natural language spatial descriptions (e.g., \u201ca dog barking in the far left corner of a room\u201d). To the best of our knowledge, no such dataset is available. Thus, we use a spatial augmentation pipeline composed of two steps: simulating spatial audio in synthetic rooms (cf. Section 3.2 and Fig. 1a), and caption rephrasing using terms that refer to spatial audio attributes (cf. Section 3.2 and Fig. 1b). We use AudioCaps [18], Clotho [6], and Freesound [8] as base datasets for our augmentation pipeline. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The training set ensures at least two spatial augmentations per data point, allowing for the model to see the same audio with at least two different spatial augmentations per epoch. We generate two different sized versions of the evaluation and test sets. The larger version consists, once more, of at least two augmentations per audio sample, whilst the smaller version has no repeated samples and, consequently, is the same size as the original test set. The smaller dataset allows reporting retrieval results on the same sized dataset as the original, as size uniformity is key to consistency in retrieval metrics. The size of the respective datasets is reported in Appendix A.1. For all datasets, we use first-order ambisonics (FOA) as the encoding of spatial audio, which we describe next. ", "page_idx": 3}, {"type": "text", "text": "3.1 Spatial Audio Encoding: First Order Ambisonics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Monophonic, non-spatial audio, captures the spectral and temporal nature of sound, which carries a significant portion of context. Spatial audio provides additional context as it contains both spectraltemporal information and directional attributes, characterized by azimuth and elevation $(\\theta,\\dot{\\phi})\\in\\mathbb{S}^{2}$ , and distance. Binaural audio, a common spatial audio distribution format, mimics the signal entering the ear canals. Whilst binaural audio may be a natural choice for playback over headphones, it presents challenges for encoding, storing, and processing spatial information due to the presence of head-related transfer-functions in the signal [2]. To facilitate more processing flexibility, microphone array signals are often encoded as ambisonics [10]. This is accomplished by taking the spherical Fourier transform of the microphone signals and removing their radial component, which is equivalent to representing the spatial signal as a phase-coincident, infinite series in a spherical harmonic basis [30]. In practice, to avoid spatial aliasing, this series is truncated at an order proportional to the number of microphones in the array, with higher orders corresponding to a higher spatial resolution. Ambisonics are linearly mappable into a variety of audio playback formats, including binaural. Firstorder ambisonics (FOA) can be recorded using readily available four-channel microphone arrays, and have been shown to carry significant spatial information [50]. As such, we develop our models to ingest FOA signals. We leave generalization to higher orders for future work. It is worthwhile noting that once microphone array signals have been encoded into ambisonics, no a-priori knowledge on the structure of the capturing array is needed in order to perform any downstream spatial processing. Thus, ambisonics are agnostic to both recording and playback devices, making any embeddings derived from them equally generalizable. ", "page_idx": 3}, {"type": "text", "text": "3.2 Spatial Augmentation of the Audio and Captions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Like TUT Sounds Events 2018 [1] and BAT [48], we use a simulator to spatially augment nonspatial audio. The augmentation pipeline mirrors that of Spatial LibriSpeech [31]. We specify room configurations parameterized by size, shape, and reverberation time, where reverberation time is a function of the room structure and materials with characteristic absorption and scattering coefficients. The simulator further allows specification of the placement and direction of the receiver microphones relative to the source of the sound (see Fig. 1a). For each sample we remove leading and trailing silences, and repeat the audio signal to ensure that samples are at least four seconds long before simulation. A randomly chosen room, placement for the microphone, and placement for the static source is then selected. We ensure that the room augmentations do not overlap between the train, evaluation, and test datasets. The rooms vary in size between $13.3\\mathrm{m}^{2}$ and $277.\\dot{4}\\mathrm{m}^{2}$ , their full-band T30 reverberance ranges from $114.5\\mathrm{ms}$ to $2671.9\\mathrm{ms}$ . The full statistics of these synthetic rooms can be found in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Our caption augmentation pipeline converts raw numerical values associated with the spatial audio attributes of the room simulator (e.g., distance to the microphone) into natural language descriptors (e.g., \u201cnear\u201d or \u201cfar\u201d). Our caption augmentation pipeline is shown in Fig. 1b. The full mapping from spatial audio attributes to natural language is given in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "The original caption augmented with the spatial information makes up the input to LLaMA-13B [38], which is prompted to rephrase in the form of a spatially augmented caption. The prompt is: ", "page_idx": 4}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/4022d9f815475b01ff47e61ac8202b78984f7443771ad4dfc09de07ca05f9c1a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "This template prompt overcomes challenges like non-English language in the original caption, missing spatial descriptors in the generated caption, and hallucinations that changed the meaning of the caption. We set the inference temperature of the LLM to 0.9 and the maximum tokens to 1,024. Appendix A.3 contains examples of the obtained spatial captions. We note that the caption re-writes can lead to hallucinations, which is discussed further in Appendix A.4. We leave the quantification and mitigation of hallucinations for future work. ", "page_idx": 4}, {"type": "text", "text": "3.3 Spatial Real-World Dataset ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our training data consists of synthetically-augmented audio and captions, so we also recorded a small dataset to verify generalization to real-world data (refer to Sections 5.2 and 5.3 for analysis). Our spatial real-world dataset was recorded using a Zylia 19 microphone spherical array at $48\\mathrm{kHz}$ with a bit-depth of 24-bits per sample. The dataset contains environmental sounds typically found in an apartment. In total, we recorded 70 samples of spatial audio in five rooms. Each spatial audio sample in the dataset was captioned with the semantic content (e.g., \u201csound of a vacuum\u201d), and the direction {\u201cleft\u201d, \u201cright\u201d, \u201cfront\u201d, \u201cback\u201d}, distance {\u201cfar\u201d, \u201cnear\u201d}, and elevation {\u201cup\u201d, \u201cdown\u201d, \u201clevel\u201d}. For privacy, no personally identifiable information was included in the dataset. ", "page_idx": 4}, {"type": "text", "text": "4 ELSA Pretraining for Spatial Audio and Language ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our architecture is derived from LAION-CLAP [44], which is composed of an audio encoder and a text encoder that aligns embeddings for similar samples across modalities whilst maintaining the original representational capabilities of the individual modalities. ", "page_idx": 4}, {"type": "text", "text": "4.1 Audio Input Features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The audio encoder must capture both the semantics of the audio (e.g., \u201cthe sound of a fire alarm\u201d) and the spatial attributes (e.g., \u201cthe upper right of a reverberant room\u201d). Following LAION-CLAP [7] and BAT [48], we translate the raw audio into the frequency domain. Consider a FOA signal represented by tensor, $\\mathbf{A}\\in\\mathbb{C}^{T\\times F\\times(N+1)^{2}}$ , where $N=1$ is the spherical-harmonics order, $T$ the number of time frames and $F$ the number of frequency bins. More information on the derivation of $\\mathbf{A}$ can be found in Appendix A.5. The corresponding real-valued log-mel spectrogram feature can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{MEL}(t,\\nu)=\\log\\left(\\left|\\mathbf{A}(t,f)\\right|^{2}\\cdot\\mathbf{W}_{\\mathrm{mel}}(f,\\nu)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{\\mathrm{mel}}$ is the corresponding filter, $\\nu$ is the filter index, $t$ is time, and $f$ is frequency. As summarized in Table 1 of SALSA [25], both mel-spectrograms and intensity vectors (IVs) are effective spatial features for FOAs. We extract the IVs, $I(t,\\bar{f})$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{\\mathrm{active}}(t,f)=\\Re\\left[A_{0,0}^{*}(t,f)\\left(A_{1,1}^{A_{1,-1}(t,f)}\\right)\\right],\\quad I_{\\mathrm{reactive}}(t,f)=\\Re\\left[A_{0,0}^{*}(t,f)\\left(A_{1,0}^{A_{1,-1}(t,f)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A_{n,m}$ are the $n^{\\mathrm{th}}$ and $m^{\\mathrm{th}}$ order and mode of the ambisonics signal corresponding to its omnidirectional $(W)$ and three dipole $(Z,Y,X)$ components, and $(\\cdot)^{\\ast}$ denotes complex conjugation. Physical normalization constants are omitted here for brevity as IVs are scaled to unit-norm [25]. ", "page_idx": 4}, {"type": "text", "text": "For ELSA to use semantic features from both non-spatial audio and FOAs, during training we use sample from both the spatially-augmented datasets and the original non-spatial dataset. Since first-order ambisonics has four channels, and non-spatial audio only one, we copy the single-channel non-spatial signal across all channels. Intensity vectors normalize the dipoles by the omni channel, and result in identical IVs for non-spatial audio. We let the model learn this condition. We ablate the effect of using both spatial audio and non-spatial audio in Appendix A.6 and find that using both improves semantic retrieval. ", "page_idx": 4}, {"type": "text", "text": "4.2 Audio and Text Encoders ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our architecture is composed of an audio encoder and a text encoder. The audio encoder consists of two branches: the semantic audio branch, and the spatial attributes branch. See Appendix A.7 for a visualization of the full architecture. ", "page_idx": 5}, {"type": "text", "text": "For the semantic audio branch, we use HTSAT [3] since it was found to perform best in the LAIONCLAP evaluation [44]. HTSAT is a transformer-based audio encoder with self-attention blocks to achieve high performance in audio classification tasks. We initialize HTSAT with weights provided by LAION-CLAP2. For spatial-audio input, we feed only the mel-spectrogram of the omni channel from the first-order ambisonics encoding. The omni channel does not contain spatial characteristics, so its role is equivalent to single channel, non-spatial audio. This branch has 30M parameters. ", "page_idx": 5}, {"type": "text", "text": "As far as we are aware, there is no existing established feature encoder for spatial audio. Thus, for our spatial attributes branch we propose a two-branched CNN based on the architecture of [31] that was trained on a multi-task regression loss for azimuth, elevation, distance, and third-octave direct-to-reverberant ratio. The branch was trained for 100 epochs on Spatial LibriSpeech, which uses FOA spatial audio and has enough samples to train the spatial attributes branch. Further details, along with the full training hyper-parameters are discussed in Appendix A.8. This branch is fed the active and reactive intensity vector features described in Eq. (2). This branch has $486\\mathrm{k}$ parameters. ", "page_idx": 5}, {"type": "text", "text": "The outputs of both the semantic (768-dimensional) and the spatial attributes (192-dimensional) branches are concatenated to form a 960-dimensional embedding. Using a two-layer multi-layer perceptron (MLP), they are subsequently projected down to a 512-dimensional embedding. ", "page_idx": 5}, {"type": "text", "text": "For the text branch, we follow the best performing model in LAION-CLAP [44], and use RoBERTabase [22]. RoBERTa is a general purpose bidirectional transformer [39], pretrained on a dynamically masked token prediction task, which employs byte-pair encoding [34] for tokenization. We use the same pre-trained model as [44] as the starting point3. The text encoder has 125M parameters, and the final embedding has a dimensionality of 712, which also is projected down to 512 by a two-layer MLP, matching the size of the audio encoder output. ", "page_idx": 5}, {"type": "text", "text": "4.3 Pretraining Objectives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We learn aligned representations using batched contrastive loss (popularized by CLIP [27]). The loss function rewards the alignment of representations from the same sample but different modalities, and penalizes the alignment of representations from different samples (see Fig. 1c). Our loss (in common with CLIP [27], CLAP [7], and LAION-CLAP [44]) is derived from the InfoNCE loss [26], as we now describe. Given a set of embeddings of any modality $X\\in\\mathbb{R}^{N\\times D}$ where the $i^{\\mathrm{th}}$ entry, $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{D}$ is to be matched with $y\\in\\mathbb{R}^{D}$ , the following InfoNCE sample loss maximizes the similarity between the pair $x_{i}$ and $y$ , and minimizes the similarity between all other $x$ and $y$ pairs: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{InfoNCE}}(X,x_{i},y)=-\\log\\frac{f_{\\mathrm{sim}}(x_{i},y)}{\\sum_{x_{j}\\in X}f_{\\mathrm{sim}}(x_{j},y)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{\\mathrm{sim}}(a,b)=\\mathrm{exp}(a\\cdot b/\\tau)$ is a similarity function with a learnable temperature parameter $\\tau$ . Taking the average across all audio-text pairs in the batch, where entries at the $i^{\\mathrm{th}}$ position match each other, we arrive at the CLIP loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{\\displaystyle\\mathcal{L}_{\\mathrm{CLIP}}=\\frac{1}{2}\\left(\\frac{1}{N}\\sum_{i=0}^{N}\\mathcal{L}_{\\mathrm{InfoNCE}}(Z^{a},z_{i}^{a},z_{i}^{t})+\\frac{1}{N}\\sum_{i=0}^{N}\\mathcal{L}_{\\mathrm{InfoNCE}}(Z^{t},z_{i}^{t},z_{i}^{a})\\right),}\\\\ {\\displaystyle=-\\,\\frac{1}{2N}\\sum_{i=0}^{N}\\left(\\log\\frac{f_{\\mathrm{sim}}(z_{i}^{a},z_{i}^{t})}{\\sum_{j=0}^{N}f_{\\mathrm{sim}}(z_{j}^{a},z_{i}^{t})}+\\log\\frac{f_{\\mathrm{sim}}(z_{i}^{t},z_{i}^{a})}{\\sum_{j=0}^{N}f_{\\mathrm{sim}}(z_{j}^{t},z_{i}^{a})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the rooms we use to spatially-augment the audio are parametric, we have accurate labels associated with spatial features of the audio source. We take advantage of these labels by adding three additional spatial regression objectives. We feed the generated 512 dimension audio embedding into ", "page_idx": 5}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/5cf07b6477e37cb107b2a2459b4d57f894036e838e974b6b169ec0366b0af270.jpg", "table_caption": ["Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "three 2-layer MLPs of $33\\mathbf{k}$ parameters, which respectively regress the direction of arrival (azimuth and elevation) of sound in 3D space, distance of the source to the receiver, and room floor area. These objectives, along with the CLIP loss in Eq. (4), define our final loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ELSA}}=\\mathcal{L}_{\\mathrm{CLIP}}+\\mathcal{L}_{\\mathrm{dir}}+\\mathcal{L}_{\\mathrm{dist}}+\\mathcal{L}_{\\mathrm{area}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\mathcal{L}}_{\\mathrm{dir}}$ is the cosine similarity between the predicted and target angles, and $\\mathcal{L}_{\\mathrm{dist}}$ and $\\mathcal{L}_{\\mathrm{area}}$ is the mean-squared error between the predicted and target distances and room floor area respectively. We ablate the differences between $\\mathcal{L}_{\\mathrm{ELSA}}$ and $\\mathcal{L}_{\\mathrm{CLIP}}$ in Appendix A.6 and find that at a negligible cost $(0.4\\%)$ to semantic retrieval, we get a $15.3\\%$ improvement to 3D localization capability and $12.3\\%$ improvement in distance estimation when using $\\mathcal{L}_{\\mathrm{ELSA}}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments, Results, and Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We demonstrate that ELSA jointly captures the semantics and spatial attributes of sound with either audio or text inputs by answering the following research questions: ", "page_idx": 6}, {"type": "text", "text": "RQ1 Does ELSA capture spatial attributes in spatial audio (Section 5.2)? ", "page_idx": 6}, {"type": "text", "text": "RQ2 Does ELSA capture semantic information in both text and audio (Section 5.3)? RQ3 Does ELSA transfer to our real-world dataset? (Sections 5.2 and 5.3)? RQ4 Does ELSA provide interpretable multimodal representations (Section 5.4)? RQ5 Are ELSA embeddings capable of driving automatic captioning (Section 5.5)? ", "page_idx": 6}, {"type": "text", "text": "5.1 Training and Evaluation of ELSA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As indicated in Section 4.2, we use pretrained weights for the semantic audio encoder, the spatial attributes encoder, and the text encoder. All components of the model are fine-tuned, which corresponds to 158M trainable parameters, an increase of $0.86\\%$ over LAION-CLAP[3]. ", "page_idx": 6}, {"type": "text", "text": "For our best model, we train for 40 epochs on 12 nodes, each with 8 NVIDIA A100 GPUs and 96 CPU cores with a batch size of 2,304. Training converges within 17 hours. We use the Adam optimizer with a learning rate of $5\\times10^{-5}$ and cosine scheduling. We select the checkpoint with the lowest $\\operatorname*{mAP}@10$ retrieval on the spatially augmented captions. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Zero-shot classification accuracy using the cosine similarity between test set audio embeddings and templated probe caption embeddings. The template is \u201cA sound coming from <spatial attribute>\u201d and a value for <spatial attribute> is substituted into the template representing the desired class (e.g., \u201cnear\u201d or \u201cfar\u201d for distance). A classification is correct if the attribute in the closest test sample matches the attribute in the template. We cannot provide comparisons with baselines since this is a new task. ", "page_idx": 6}, {"type": "text", "text": "5.2 Spatial Attributes Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/aafe3448afcc20fe26701ef9ac14f7597131f09cdd9bd20e1cb31011b33f6a07.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We show that ELSA captures the spatial attributes of sound (RQ1) by carrying out downstream regression and zero-shot spatial prompt classification. For regression to 3D sound localization, we a train two-layer MLP with 32,768 parameters using the ELSA audio embeddings generated from the training set. We then evaluate on the REAL component of the TUT Sound Events 2018 dataset [1]. Table 1 confirms that CLAP cannot encode spatial attributes $(95.29^{\\circ})$ , whereas ELSA achieves $14.97^{\\circ}$ mean-absolute error (MAE) and maintains a higher mAP $@10$ for semantic retrieval tasks than CLAP. Appendix A.9 shows that there is little variability in the direction-of-arrival error across various spatial attributes. However, we note the errors tend to be higher at the extrema of the dimensions. When compared to methods designed explicitly for 3D sound localization, ELSA performs better than SeldNET (the baseline included with TUT Sound Events 2018) by $+11.6^{\\circ}$ , and that achieves only $-2.6^{\\circ}\\mathrm{MAE}$ compared to the model in Spatial LibriSpeech4. ELSA does not reach the performance of PILOT $(4.3^{\\circ})$ , but this model was specifically-tuned only for 3D sound localization on data derived from TUT Sound Events 2018 [33]. ", "page_idx": 6}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/26e71c533d4f915d6bf90f3e41b0e59eff616b086c82f662ed2b4bbf2f0ce855.jpg", "table_caption": ["Table 3: Semantic retrieval $(\\mathbf{R}@1,\\mathbf{R}@5$ , and ${\\tt R}\\ @10\\,\\$ ) for CLAP and ELSA calculated over the original (non-spatial) versions of Clotho and AudioCaps. Although ELSA is trained using a mixture of nonspatial and spatial audio, it conserves the retrieval performance on non-spatial audio of LAION-AI CLAP, which was trained on only non-spatial data. For the training data, read $C$ as Clotho, $A C$ as AudioCaps, $L A$ as LAION-Audio-630K and $F S$ as Freesound. A superscript S denotes the spatially-augmented equivalent dataset. We use Freesound, a subset of LAION-Audio-630K due to its more permissive licensing. For a fair comparison, we train a version of CLAP locally with Clotho, Audiocaps and Freesound, which is not reported in the CLAP paper. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "To verify that the spatial attributes are aligned with language, we create new captions using the template: \u201cA sound coming from <spatial attribute >\u201d, where <spatial attribute > can be distance, direction, elevation, room size, and reverberation. For instance, a caption for distance might be \u201cA sound coming from far away\u201d. The ELSA text embeddings for such captions are extracted from the pre-trained encoder and compared in a zero-shot fashion with ELSA audio embeddings for samples from the test set using cosine similarity. We classify the match as correct if the spatial attribute in the closest audio sample matches the spatial attribute of the query caption, and we report accuracy in Table 2. ELSA achieves ${>}90\\%$ correct retrieval for most spatial attributes. For room area, ELSA achieves $74.7\\%$ correct retrieval, which we hypothesize is due to the relatively small perceptual differences between small $(<\\!50\\mathrm{m^{2}})$ and large rooms $(>\\!100\\mathrm{m}^{2})$ ). We observe a transfer gap on retrieval scores when evaluating on our spatial real-world dataset, with ELSA achieving $67.1\\%$ (distance), $35.8\\%$ (direction) and $72.1\\%$ (elevation) correct retrieval. Part of this performance difference is because the spatial attributes were only estimates by the annotators during data capture. On the other hand, performance using pre-trained LAION-CLAP is close to random for all tasks (Appendix A.10), which is expected as CLAP was not trained with spatial audio or captions. ", "page_idx": 7}, {"type": "text", "text": "5.3 Semantics Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following LAION-CLAP [44], we calculate retrieval results when finding matches from audio-to-text and text-to-audio. To compute retrieval, we encode the test set for each modality, and for every sample we check whether the corresponding sample in the other modality has the closest cosine distance $(\\mathbf{R}@1)$ , is within the five closest samples $(\\mathbf{R}@5)$ , or within the ten closest samples $(\\mathbf{R}@10)$ . The results in Table 3 show that in addition to learning representations of spatial captions and spatial audio, ELSA also performs on par with LAION-CLAP on non-spatial tasks. Table A.T.8 in Appendix A.12 shows the retrieval results when using spatially-augmented versions of AudioCaps and Clotho. We remark that adding Freesound to the training set decreases the retrieval scores in Spatial AudioCaps, but improves retrieval scores in Clotho, due to Clotho being a differently-captioned subset of Freesound. We note that the spatial retrieval performance of ELSA is lower than the non-spatial retrieval performance (for instance, $-9.4\\%$ and $-13.3\\%$ on audio-to-text and text-to-audio $\\mathbf{R}\\@10$ ", "page_idx": 7}, {"type": "text", "text": "AudioCaps). This reflects the fact that spatial captions are harder to match, since there is a larger number of attributes and since there are hard-negatives (same semantics, different spatial attributes). Still, ELSA achieves the highest retrieval scores on the spatial real-world dataset (Table A.T.9 in Appendix A.12), showcasing its ability to transfer to the real-world without fine-tuning. Note that we cannot provide comparisons with prior models since using these spatial augmentations is a new task. ", "page_idx": 8}, {"type": "text", "text": "5.4 Interpreting the representation structure of ELSA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To confirm that directional characteristics in ELSA spatial caption embeddings are encoded in the same feature space as those in ELSA spatial audio embeddings, we train a direction regressor with a two-layer MLP using the spatial audio embeddings in the training split. We subsequently regress the spatial text embeddings to azimuth values using our trained regressor and affix direction labels (\u201cleft\u201d, \u201cright\u201d, \u201cfront\u201d, \u201cback\u201d) to each sample based on the azimuth values. We obtain $64.3\\%$ accuracy on the four-class problem, indicating alignment between the encoding of the modalities. Similarly, we obtain an accuracy of $76.5\\%$ for when classifying over distance labels (\u201cfar\u201d, \u201cnear\u201d) and $55.1\\%$ over elevation labels (\u201cup\u201d, \u201cdown\u201d). ", "page_idx": 8}, {"type": "text", "text": "Besides using regression to confirm that ELSA embeddings capture spatial direction, we verify whether the ELSA embeddings can be clustered by spatial attributes. Fig. 2 shows a UMAP projection of the ELSA embeddings from the test sets of Spatial-AudioCaps and Spatial-Clotho. Note that the UMAP projection was guided with the embeddings and labels of the training sets of both datasets. The figure shows the embeddings cluster well with the direction labels, though there is some degree of confusion between \u201cback\u201d and \u201cfront\u201d. This is corroborated by the analysis in Appendix A.13, where we compute Wassertein distances directly in the 512-dimensional space. We carried out a similar analysis for spatial distance and found the embeddings cluster clearly between \u201cnear\u201d and \u201cfar\u201d. ", "page_idx": 8}, {"type": "text", "text": "We validate that ELSA audio embeddings capture implicit spatial attributes that are latent in the textencoder by first training a classifier using the spatial audio in our training data, where the classes are broad directions, such as above and below. We use LLaMA-13B [38] to generate descriptions of sounds that would typically come from each of these directions, e.g., \u201cthe rhythmic drumming of raindrops on a skylight\u201d (for above) and \u201cthe faint creaking of an old house settling\u201d (for below). Appendix A.11 lists all generated captions. Finally, we use the classifier trained on audio samples to classify ELSA embeddings for these generated captions with implicit directionality. We find that the classifier can correctly identify $68\\%$ of the sounds typically heard from above as being from above, showing that the latent space of the text encoder for ELSA is capturing directionality. ", "page_idx": 8}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/cf6fe110d1eeb6819a25cd2c4b2527894b0bda62b0a4e7e13a04f74c6e6e47dd.jpg", "img_caption": ["Figure 2: UMAP projection of ELSA embeddings of the test splits of Spatial-Clotho and Spatial-AudioCaps. Filled markers are obtained from spatial audio, and hollow markers are obtained from spatial captions. The UMAP projection was fitted with the train splits of Spatial-Clotho and Spatial-Audio caps, and we made use of supervised dimension reduction to highlight the direction differences rather than the semantic differences in the embeddings. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Lastly, we show that we can swap the spatial direction encoded by an ELSA audio embedding with a simple text caption. We first obtain ELSA prototypes for four directions (\u201cleft\u201d, \u201cright\u201d, \u201cfront\u201d, \u201cback\u201d) with the template: \u201cA sound coming from the direction \u201d. Next, we train a 4-class direction classifier with a two-layer MLP using the spatial audio in the training splits of our spatially-augmented datasets. To swap the direction of the sound, we subtract the text prototype of the original direction and add prototype for the new direction. For evaluation, we swap the spatial direction of every sample in our spatially-augmented test set that was correctly classified by the 4-class direction classifier $(96.7\\%$ of the audio embeddings). Our results show that $99.7\\%$ of the swapped samples are classified correctly with the new spatial direction, which highlights the strong alignment of spatial features across modalities, resulting in the ability to edit spatial attributes of existing spatial audio using text in embedding space. Further details about this experiment are described in Appendix A.14. These results also point to exciting avenues wherein text can condition the manipulation and generation of spatial characteristics of audio. We leave this application for future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.5 Spatial Audio Caption Generation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Decoding multimodal embeddings into natural language can be achieved by prefixing an autoregressive causal language model [24, 13, 19, 4] , where the prefix is constructed from a projection of the multimodal embeddings. To facilitate audio captioning using ELSA, we fine-tune a GPT-2 model [28] with 12 attention layers each having 12 heads (with 163M parameters). The ELSA embeddings are projected onto the prefix using a single dense layer ( $393\\mathrm{k}$ parameters). With the ELSA encoder frozen, we train the GPT-2 model on $150\\mathrm{k}$ spatial-audio embedding and caption pairs from Spatial-Clotho and Spatial-AudioCaps. We report caption generation metrics in Table 4 and show three generation samples in Appendix A.15. Overall, we find ", "page_idx": 9}, {"type": "text", "text": "Table 4: Evaluation of Spatial Audio Caption Generation. Metrics were obtained from the Audio Captioning task of the DCASE Challenge5 by comparing the generated captions produced from spatial audio and the ground-truth captions from the test splits of Spatial-AudioCaps (S-AC) and Spatial-Clotho. ", "page_idx": 9}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/9fc8bdd51fde9e2159c77fb4eae06d037ce62d2063c35affdc4d39108707a73b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "that automatic spatial audio captioning systems are viable though more work is needed to increase the vocabulary size of the generations. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions, Limitations, and Further Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented ELSA, an AFM that aligns representations of spatial audio and equivalent text descriptions. To train such representations we built a pipeline to spatially augmented the audio in existing non-spatial audio-text datasets, such as Clotho [6] and AudioCaps [18], and added spatial information to their respective captions. Our results show that ELSA embeddings capture both the semantic contents and the spatial attributes of the audio, with ELSA achieving $+2.8\\%$ higher scores in audio-to-text and text-to-audio retrieval scores than the state-of-the-art, and obtaining $-2.6^{\\circ}\\mathrm{MAE}$ in direction of arrival error with respect to an equivalent baseline. Interestingly, by mixing spatial and non-spatial audio and caption pairs, ELSA is able to represent non-spatial audio as well, Finally, we show that the representation space of ELSA is structured in that the directionality of a spatial audio sample can be transposed by simple addition or subtraction of two text representations. Future work will explore acoustic scenarios with overlapping sound sources and sound sources that are moving in the scene. ELSA will also benefti from advances in spatial attributes encoders. In this work, we used the augmented spatial captions as is, but further work should ensure consistency with the semantics before and after augmentation, which will further improve the representational power of ELSA. ", "page_idx": 9}, {"type": "text", "text": "Perceiving spatial audio is a fundamental aspect of human nature. As is linking perception with language. Using spatial audio and a contrastive multimodal training approach, ELSA bridges the gap between feature rich spatial audio and language, paving the way for more intuitive and effective human-machine interactions by allowing for richer understanding of the users\u2019 environment and generation of immersive sound scenes from natural language. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact Our research has the potential to be used in creation of immersive augmented or virtual reality environments. If not controlled well, these immersive experiences have the potential to become addictive, and thus impact the mental health of individuals or even society as a whole. Another danger is possibility of creating deepfakes of soundscapes, thus making it possible for generated 3D environments to sound very realistic. The proliferation of deepfake soundscapes could lead to misinformation and manipulation, undermining trust in audio media. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Nicholas Apostoloff, Masha Fedzechkina, Rin Metcalf, Russ Webb, Megan Maher Welsh, and Luca Zappella for their insightful input and discussions on earlier versions of this paper. Moreover, we are thankful to Denise Hui and David Koski for technical support. Names are in alphabetical order by last name within group. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sharath Adavanne, Archontis Politis, Joonas Nikunen, and Tuomas Virtanen. \u201cSound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks\u201d. In: Journal of Selected Topics in Signal Processing 13.1 (2019).   \n[2] Jens Blauert. \u201cThe Technology of Binaural Listening\u201d. Springer, 2013.   \n[3] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. \u201cHTS-AT: A Hierarchical Token-semantic Audio Transformer for Sound Classification and Detection\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2022, pp. 646\u2013650.   \n[4] Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, and Huaming Wang. \u201cTraining Audio Captioning Models without Audio\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2024, pp. 371\u2013375. [5] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. \u201cPengi: An Audio Language Model for Audio Tasks\u201d. In: Advances in Neural Information Processing Systems. Vol. 36. 2023, pp. 18090\u201318108. [6] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. \u201cClotho: An Audio Captioning Dataset\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2020, pp. 736\u2013740.   \n[7] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. \u201cCLAP: Learning Audio Concepts from Natural Language Supervision\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2023, pp. 1\u20135. [8] Frederic Font, Gerard Roma, and Xavier Serra. \u201cFreesound Technical Demo\u201d. In: International Conference on Multimedia. ACM. 2013, pp. 411\u2013412.   \n[9] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. \u201cAudio Set: An Ontology and Human-labeled Dataset for Audio Events\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2017, pp. 776\u2013780.   \n[10] Michael J. Gerzon. \u201cPeriphone (with Height Sound Reproduction)\u201d. In: Journal of the Audio Engineering Society M07 (1972).   \n[11] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. arXiv: 2304.13731 [eess.AS].   \n[12] Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. \u201cListen, Think, and Understand\u201d. In: International Conference on Learning Representations. 2024.   \n[13] Sophia Gu, Christopher Clark, and Aniruddha Kembhavi. \u201cI can\u2019t believe there\u2019s no images!: Learning Visual Tasks Using Only Language Supervision\u201d. In: International Conference on Computer Vision. IEEE. 2023, pp. 2672\u20132683.   \n[14] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, and Andreas Dengel. \u201cAudioCLIP: Extending CLIP to Image, Text and Audio\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2022, pp. 976\u2013980.   \n[15] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P W Ellis. \u201cMuLan: A Joint Embedding of Music Audio and Natural Language\u201d. In: International Society for Music Information Retrieval Conference. ISMIR. 2022, pp. 559\u2013566.   \n[16] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. \u201cMake-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models\u201d. In: International Conference on Machine Learning. PMLR, 2023, pp. 13916\u201313932.   \n[17] Xilin Jiang, Cong Han, Yinghao Aaron Li, and Nima Mesgarani. Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience. 2024. arXiv: 2402. 03710 [eess.AS].   \n[18] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. \u201cAudiocaps: Generating Captions for Audios in the Wild\u201d. In: Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. 2019, pp. 119\u2013132.   \n[19] Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh. \u201cPrefix Tuning for Automated Audio Captioning\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2023, pp. 1\u20135.   \n[20] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. \u201cAn intriguing failing of convolutional neural networks and the coordconv solution\u201d. In: Advances in Neural information Processing Systems 31 (2018).   \n[21] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. \u201cImproved Image Captioning via Policy Gradient optimization of SPIDEr\u201d. In: International Conference on Computer Vision. IEEE. 2017, pp. 873\u2013881.   \n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019. arXiv: 1907.11692 [cs.CL].   \n[23] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. \u201cLinearly Mapping from Image to Text Space\u201d. In: International Conference on Learning Representations. 2022.   \n[24] Ron Mokady, Amir Hertz, and Amit H. Bermano. ClipCap: CLIP Prefix for Image Captioning. 2021. arXiv: 2111.09734 [cs.CV].   \n[25] Thi Ngoc Tho Nguyen, Karn N. Watcharasupat, Ngoc Khanh Nguyen, Douglas L. Jones, and Woon-Seng Gan. \u201cSALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection\u201d. In: Transactions on Audio, Speech, and Language Processing 30 (2022), pp. 1749\u20131762.   \n[26] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. 2018. arXiv: 1807.03748 [cs.LG].   \n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. \u201cLearning Transferable Visual Models from Natural Language Supervision\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 8748\u20138763.   \n[28] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.   \n[29] Boaz Rafaely. \u201cAnalysis and Design of Spherical Microphone Arrays\u201d. In: Transactions on Speech and Audio Processing 13.1 (2004), pp. 135\u2013143.   \n[30] Boaz Rafaely. \u201cFundamentals of Spherical Array Processing\u201d. Springer, 2019.   \n[31] Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, and Jonathan Sheaffer. \u201cSpatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning\u201d. In: Interspeech. ISCA, 2023, pp. 3724\u20133728.   \n[32] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. \u201cLAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs\u201d. In: Data Centric AI NeurIPS Workshop. 2021.   \n[33] Christopher Schymura, Benedikt B\u00f6nninghoff, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani, Shoko Araki, and Dorothea Kolossa. \u201cPILOT: Introducing Transformers for Probabilistic Sound Event Localization\u201d. In: Interspeech. ISCA, 2021.   \n[34] Rico Sennrich, Barry Haddow, and Alexandra Birch. \u201cNeural Machine Translation of Rare Words with Subword Units\u201d. In: Annual Meeting of the Association for Computational Linguistics. 2016, pp. 1715\u20131725.   \n[35] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. \u201cHow Much Can CLIP Benefti Vision-and-Language Tasks?\u201d In: International Conference on Learning Representations. 2022.   \n[36] Kazuki Shimada, Archontis Politis, Parthasaarathy Sudarsanam, Daniel A Krause, Kengo Uchida, Sharath Adavanne, Aapo Hakala, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, et al. \u201cSTARSS23: An Audio-visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events\u201d. In: Advances in Neural Information Processing Systems Datasets and Benchmarks Track. Vol. 36. 2024, pp. 72931\u201372957.   \n[37] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. \u201cSALMONN: Towards Generic Hearing Abilities for Large Language Models\u201d. In: International Conference on Learning Representations. 2024.   \n[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language Models. 2023. arXiv: 2302.13971 [cs.CL].   \n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. \u201cAttention Is All You Need\u201d. In: Advances in Neural Information Processing Systems. 2017, pp. 6000\u20136010.   \n[40] Qing Wang et al. The NERC-SLIP System for Sound Event Localization and Detection of DCASE2023 Challenge. Tech. rep. DCASE2023 Challenge, 2023.   \n[41] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. \u201cWhat Language Model Architecture and Pretraining Objective Works Best for Zero-shot Generalization?\u201d In: International Conference on Machine Learning. PMLR. 2022, pp. 22964\u201322984.   \n[42] Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, and Sheng Zhao. \u201cAUDIT: Audio Editing by Following Instructions with Latent Diffusion Models\u201d. In: Advances in Neural Information Processing Systems 36 (2023), pp. 71340\u201371357.   \n[43] Earl G Williams. \u201cFourier Acoustics: Sound Radiation and Nearfield Acoustical Holography\u201d. Academic Press, 1999.   \n[44] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. \u201cLarge-scale Contrastive Language-audio Pretraining with Feature Fusion and Keywordto-Caption Augmentation\u201d. In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2023, pp. 1\u20135.   \n[45] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. \u201cDiffsound: Discrete Diffusion Model for Text-to-sound Generation\u201d. In: Transactions on Audio, Speech, and Language Processing 31 (2023), pp. 1720\u20131733.   \n[46] Guy Yariv, Itai Gat, Lior Wolf, Yossi Adi, and Idan Schwartz. AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation. 2023. arXiv: 2305.13050 [cs.SD].   \n[47] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. \u201cLarge Batch Optimization for Deep Learning: Training BERT in 76 minutes\u201d. In: International Conference on Learning Representations. 2020.   \n[48] Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol Choi, and David Harwath. \u201cBAT: Learning to Reason about Spatial Sounds with Large Language Models\u201d. In: International Conference on Machine Learning. PMLR. 2024, pp. 61454\u201361469.   \n[49] Zelin Zhou, Zhiling Zhang, Xuenan Xu, Zeyu Xie, Mengyue Wu, and Kenny Q. Zhu. \u201cCan Audio Captions Be Evaluated With Image Caption Metrics?\u201d In: International Conference on Acoustics, Speech and Signal Processing. IEEE. 2022, pp. 981\u2013985.   \n[50] Franz Zotter and Matthias Frank. \u201cAmbisonics: A Practical 3D Audio Theory for Recording, Studio Production, Sound Reinforcement, and Virtual Reality\u201d. Springer, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Dataset statistics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table A.T.1 presents a summary of all the paired audio and text datasets we use for training and evaluation. ", "page_idx": 13}, {"type": "text", "text": "Table A.T.1: Audio-caption dataset descriptions. The first three rows correspond to the original publicly available datasets, and the subsequent rows correspond to our spatially-augmented variants. For each spatially augmented dataset, there are at least two spatial augmentations per original sample in the train split. ", "page_idx": 13}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/8e66744ec6d284b1ac13cded27d6abcb1544cb7c7578fc64f726b43abb0005f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "For the spatially-augmented versions of Clotho, AudioCaps, and Freesound, we use 8,972 parametric rooms with the statistic described in Table A.T.2. Note that the parametric rooms in the test set are a subset of the rooms in the training set, however the sources locations on those rooms do not overlap. ", "page_idx": 13}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/5018c6af87d6facfd544f262d196ec5dad9239f18f4a88ae57db81aa3df03e84.jpg", "table_caption": ["Table A.T.2: Spatial attributes of room simulations used to spatially-augmented audio and language datasets "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Mapping of spatial attributes to natural language ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As part of the spatial-augmentation pipeline (described in Section 3.2), we use the mappings in Table A.T.3 to convert spatial attributes to natural language. ", "page_idx": 13}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/cc90d5ff03c321ce20a22bb1bca76fd2a8df7163b21c06f8a8dd6edaf67efc3a.jpg", "table_caption": ["Table A.T.3: Mapping between spatial features and natural language descriptors "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Audio dataset captions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report the mapping from raw spatial values to spatial captions in Appendix A.2. We select these bounds based on how an audio would be percieved by human ears, e.g. a sound higher than 40 degrees elevation sounds like its coming from a height. ", "page_idx": 14}, {"type": "text", "text": "We also present a few random samples of regular to spatial text rewrites by the LLM: ", "page_idx": 14}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/4285f1def7b6513672194ce7ff951a1168763f998bd3c54dce97e30891858e4e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Hallucinations in LLMs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We note that the caption re-writes can lead to hallucinations in captions. For instance, the phrase \u201cthe purr of a carerra\u201d describing the sound of the engine of a Porsche Carerra was rephrased as \u201cthe purring cat named Carerra\u201d. Another example, \u201cthe sounds of papers turning\u201d with direction below was rewritten as \u201cthe sound of someone shuffling cards in the basement\u201d. It is not clear what is the overall effect of the hallucinations; as in the first example it changes the semantics of the audio, but in the second example adds plausible and welcome diversity to our caption set. As mentioned in the main section, we leave the quantification and mitigation of sub optimal hallucinations for future work. ", "page_idx": 15}, {"type": "text", "text": "A.5 Further background on first-order ambisonics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider a continuum of plane-waves impinging on the surface of a sphere, $p(k r,\\Omega)$ , where $p$ is acoustic pressure, $k=2\\bar{\\pi}f c^{-1}$ is the spatial frequency, $r$ is radial distance and $\\Omega\\equiv(\\theta,\\phi)\\in\\mathbb{S}^{2}$ is direction in terms of elevation $\\theta$ and azimuth $\\phi$ . The expansion of this function in a spherical harmonics basis, $p_{n m}(k)$ can be written as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p_{n m}(\\boldsymbol{k},\\boldsymbol{r})=b_{n}(\\boldsymbol{k}\\boldsymbol{r})\\int_{\\Omega\\in S^{2}}a(\\boldsymbol{k},\\Omega)[Y_{n}^{m}(\\Omega)]^{*}\\mathrm{d}\\Omega}}\\\\ &{}&{=b_{n}(\\boldsymbol{k}\\boldsymbol{r})A_{n m}(\\boldsymbol{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $a(k,\\Omega)$ denotes the plane-wave density function in the spatial domain, $Y_{n}^{m}(\\Omega)$ are the sphericalharmonics basis function for order $n$ and mode $m$ , and $b_{n}(k r)$ is the radial function given for a rigid sphere by [43], ", "page_idx": 15}, {"type": "equation", "text": "$$\nb_{n}(k r)=4\\pi i^{n}\\left[j_{n}(k r)-\\frac{j_{n}^{\\prime}(k r_{0})}{h_{n}^{\\prime}(k r_{0})}h_{n}(k r)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $j_{n}(k r)$ and $h_{n}(k r)$ are the spherical Bessel and Hankel function, respectively, and $(\\cdot)^{\\prime}$ denotes their first derivative with respect to the argument. In this work we employ a real-valued spherical harmonics basis and radial functions corresponding to a rigid sphere. We denote $A_{n m}(k)$ as the spherical Fourier transform of the plane-wave density function, $a(k,\\Omega)$ , and refer to it as an ambisonics signal. We further denote the inverse spherical Fourier transform of the ambisonics signal as, ", "page_idx": 15}, {"type": "equation", "text": "$$\na(k,\\Omega)=\\sum_{n=0}^{\\infty}\\sum_{m=-n}^{n}A_{n m}(k)Y_{n}^{m}(\\Omega),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Considering now a microphone array with $Q$ sensors, the integral in (6) becomes a weighted finite summation. In order to avoid spatial aliasing, we conform to $Q=(N+1)^{2}$ with optimal spatial sampling of $Q$ [29] sensors, where $N$ is the spherical harmonics order. Accordingly the outer sum in (8) is truncated at order $N$ , and the transformation between the pressure $p(k r,\\Omega)$ and ambisonics function $A_{n m}(k)$ is approximated by, ", "page_idx": 15}, {"type": "equation", "text": "$$\np(k r,\\Omega)\\approx\\sum_{n=0}^{N}b_{n}(k r)\\sum_{m=-n}^{n}A_{n m}(k)Y_{n}^{m}(\\Omega),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "rewriting (9) in matrix form and solving for $A_{n m}$ , the linear encoding the microphone signals into ambisonics becomes, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{a}_{n m}=\\mathbf{Y}^{H}\\mathrm{diag}(\\mathbf{b}_{n})^{-1}\\mathbf{p}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{Y}$ is a $Q\\times(N+1)^{2}$ matrix of spherical harmonics, ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf Y}=\\left[\\!\\!\\begin{array}{c c c c}{Y_{0}^{0}(\\Omega_{1})}&{Y_{1}^{-1}(\\Omega_{1})}&{\\cdot\\cdot\\cdot}&{Y_{N}^{N}(\\Omega_{1})}\\\\ {Y_{0}^{0}(\\Omega_{2})}&{Y_{1}^{-1}(\\Omega_{2})}&{\\cdot\\cdot}&{Y_{N}^{N}(\\Omega_{2})}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {Y_{0}^{0}(\\Omega_{Q})}&{Y_{1}^{-1}(\\Omega_{Q})}&{\\cdot\\cdot}&{Y_{N}^{N}(\\Omega_{Q})}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "${\\bf b}_{n}$ is the radial function vector, and dependency on the spatial frequency $k$ is omitted for brevity. As in this paper we consider signals to be outputs of a short-time Fourier transform, we further denote our ambisonics features as $\\mathbf{A}\\in\\mathbb{C}^{T\\times F\\times(N+1)^{2}}$ . More specifically, for $N=1$ and a given time frame $t$ and frequency bin $f=k c/(2\\pi)$ our features are contained in the following ambisonics channels: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}(t,f)=\\left[\\begin{array}{c}{A_{0,0}(t,f)}\\\\ {A_{1,1}(t,f)}\\\\ {A_{1,0}(t,f)}\\\\ {A_{1,1}(t,f)}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "these correspond to the $W$ (omnidirectional) and $Y,Z,X$ (dipole) components of the first order ambisonics approximation. It is worthwhile noting that once microphone array signals have been encoded into ambisonics, no a-priori knowledge on the structure of the capturing array is needed in order to perform any downstream spatial processing. Thus, ambisonics are effectively agnostic to both recording and playback devices, making any embeddings derived from them equally generalizable. ", "page_idx": 16}, {"type": "text", "text": "A.6 Ablations across model architectures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We ablate over using static intensity vectors in place of the learned encoder, just the learned encoder without spatial regressors, and compare them with our current architecture. Results are shown in Table A.T.4. ", "page_idx": 16}, {"type": "text", "text": "Table A.T.4: Comparison of semantic and spatial retrieval performance across data input ablations. mAP $@10$ refers to the text-to-audio mean average precision $@$ 10 and audio-to-text mean average precision $@\\ 10$ . It is a summary metric capturing the model\u2019s semantic retrieval capabilities. ", "page_idx": 16}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/51cd467b0cf487d86abde63f7b81071e8619c87cb3bfab6376c27c60dbb40aeb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/e62c7fd9ee94864ddfec22d998c57f5c44996580c8380944593aacc8710bb1d0.jpg", "table_caption": ["Table A.T.5: The retrieval metrics here demonstrate why mixing both mono and spatial audio is required to achieve the best possible retrieval performance. We see that the model needs to see both the regular and spatially augmented versions of the audio and captions to achieve best performance. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.7 ELSA architecture ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The full architecture of the ELSA model is shown in Fig. A.F.1. ", "page_idx": 16}, {"type": "text", "text": "A.8 Spatial attributes branch of audio encoder details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The full architecture for the spatial attributes branch of the audio encoder is shown below in Fig. A.F.2. ", "page_idx": 16}, {"type": "text", "text": "The spatial attributes branch has 485,828 parameters, and was pre-trained with a learning rate of $10^{-3}$ on the LAMB optimizer [47] with weight decay factor of 0.01 and without scheduling the learning rate. The batch size was 1024 and the model was trained for 100 epochs on a single node with 8 NVIDIA V100 GPUs and 80CPUs. Training took 12h 20min. The training set was composed of 134,712 10-second segments from the first-order ambisonics samples of the Spatial LibriSpeech train set. We used a multi-task regression loss, ${\\mathcal{L}}_{\\mathrm{pre}}$ , to jointly learn the azimuth, elevation, distance, room volume, and 20-third octave bins (between $100\\mathrm{Hz}$ and 8kHz) for direct-to-reverberant ratio and T30. ", "page_idx": 16}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/1a0cb17eb20757d62da6fb68ba7f7ec7f6a0e29fe1e7f560deefd8a087c120cc.jpg", "img_caption": ["Figure A.F.1: Full architecture diagram for ELSA. Filled blocks include trainable parameters. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/fdec706d0c1143d9d9ed194b8f98846538ec35ded1cb9e4230329cb03308de0f.jpg", "img_caption": ["Figure A.F.2: Architecture diagram for Spatial Attributes Branch. Filled blocks include trainable parameters. The AddCoords2D block is described in [20]. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "${\\mathcal{L}}_{\\mathrm{pre}}$ is the sum of the cosine loss for azimuth and elevation and the mean squared error for all other predictions. ", "page_idx": 17}, {"type": "text", "text": "A.9 Fine-grained of direction-of-arrival error analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We analyze the errors of a two-layer MLP trained to regress the direction-of-arrival (same setting as the last column in Table 1). We observe how the errors vary along the following dimensions: source azimuth, source elevation, source distance, room floor area, room mean T30, and TUT Sound Events 2018 semantic classes. Results are rendered as boxplots in Fig. A.F.3 below. ", "page_idx": 17}, {"type": "text", "text": "Table A.T.6 further report mean, standard deviation, and number of samples per bin about the direction-of-arrival errors across the previously analysed dimensions. ", "page_idx": 17}, {"type": "text", "text": "A.10 Spatial attributes retrieval for LAION-CLAP ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Section 5.2 showed that ELSA embeddings can be classified in a zero-shot fashion by using a templated probe caption. Table A.T.7 shows the same experiment applied to LAION-CLAP (with the ELSA results retained for context) by feeding LAION-CLAP the omni-channel of the spatial datasets. As expected the performance of LAION-CLAP in this setting is close to random, for instance LAION-CLAP achieves $48\\%$ accuracy on the two class distance classification task and $28.2\\%$ on the four-class direction classification task. ", "page_idx": 17}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/ef13f1e8bb5e5b666970b165b2748f6bb1ea044e1da09f267344f31ca1b4c7f8.jpg", "img_caption": ["(f) TUT Sound Events 2018 Semantic classes "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "", "img_caption": ["Figure A.F.3: Boxplots of absolute direction-of arrival errors predicted by 2-layer MLP. Figs. (a)\u2013(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different categories. Fig. (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes. For all figures, boxes represent the interquartile range, solid orange lines are the median, and dashed green lines are the mean. ", "(e) Mean T30 (ms) "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table A.T.6: Mean and standard deviation of absolute direction-of arrival errors (in radians) predicted by 2-layer MLP. Tables (a)\u2013(e) show the Spatial Audiocaps and Spatial Clotho test sets errors by different dimensions. Table (f) shows the predictions of the test set of TUT Sounds 2018 by different semantic classes. ", "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/52c2d2ad28856ecf3797da7a376d955f445d5dd4035125671b1b4422889f5e16.jpg", "table_caption": ["(a) DOA error by azimuth "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/b22edb41b92b5e5742408c80ae6335e536454172dcd44ab88582b8bf7ccf9d65.jpg", "table_caption": ["(b) DOA error by elevation "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/2afdbe1ebcadc0b86bcdf3728832b3a7b65dbdc7933970e90ed3282d5052cb14.jpg", "table_caption": ["(c) DOA error by distance "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/a70b071636a09aba67b3cea98f285c62118fbb71b1da3c0801211046f86b6625.jpg", "table_caption": ["(d) DOA error by room floor area "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/ae58b39f592e6af26b87acf11c2a0e840bb51f1d2cfdbb29c63e36660852d0e0.jpg", "table_caption": ["(e) DOA error by T30 "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/d2d65afe1bdfba5b0f2560685a60c5b110bfe9b6725e6943291ce557575640a6.jpg", "table_caption": ["(f) DOA error by TUT Sound Events 2018 semantic class "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/7bcda217d6e365357a92357a2b3c21aca78ff23dfd202598b2ca61239e812634.jpg", "table_caption": ["Table A.T.7: Complete version of Table 2 with LAION-CLAP results. LAION-CLAP results are obtained by passing omni channel of spatial dataset through pre-trained model. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.11 Text corpus used when testing ELSA\u2019s implicitly learned spatial attributes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As mentioned in main text, we validate that ELSA\u2019s audio embeddings can capture implicit spatial attributes, which are latent in the text-encoder. We use LLaMA-13B [38] to generate descriptions of 50 sounds that typically come from each of above and below. The sentences do not necessarily include explicit spatial descriptions (e.g., \"A fire alarm going off\", a sound typically from above but not explicitly stated). The original prompt and generated sentences can be found below. We then train a two-class classifier (top vs bottom) using the spatial audio in the train sets of our spatial-augmented datasets, and classify each of the sentences. Results show an above-random classification accuracy of $68.75\\%$ for above sentences, $58.82\\%$ for below sentences. This experiment shows that, to a degree, the text-encoder (RoBERTa) is able to leverage its semantic bias of placement of objects in the real world and encode it as spatial features that the spatial encoder understands. This task is made quite hard by the observation that our simulated dataset does not reflect the natural world (an aeroplane sound could be simulated from below) and the fact that RoBERTa-base has significantly fewer parameters and a smaller training set more recent large-language models such as LLaMA-13B. This experiment is purely qualitative, and we leave a more in-depth exploration of knowledge sharing between the pretrained spatial and text encoders to future work. ", "page_idx": 20}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/a9ae379b83fbbb5fe8303337a9d00a3cc66a9209630b18e2a06e5e0c4abb5d1b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "\u2022 The rhythmic pounding of rain on a metal roof.   \n\u2022 The rhythmic chirping of birds waking you up at dawn.   \n\u2022 The muffled conversation of people walking on a floor above.   \n\u2022 The muffled snoring of someone sleeping upstairs.   \n\u2022 The chirping and squawking of a flock of birds taking flight.   \n\u2022 The rhythmic click-clack of tap shoes dancing on a floor above.   \n\u2022 The rhythmic tapping of a woodpecker searching for insects. ", "page_idx": 21}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/b6df6b30d47d41b690dbc294c1e5eed9bb03745c9f4b07cbe41e736a481a9ceb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.12 Semantic retrieval for spatial data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We report ELSA\u2019s semantic retrieval on Spatial-Clotho, and Spatial-Audiocaps in Table A.T.8.   \nLikewise, Table A.T.9 shows the retrieval scores of ELSA on our spatial real-world dataset. ", "page_idx": 22}, {"type": "text", "text": "Table A.T.8: Semantic Retrieval Metrics calculated over spatially augmented version of Clotho and AudioCaps eval sets \u2013 identical in size as the non-spatial sets. ", "page_idx": 22}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/2a0299d3cf96556d8775f6defb039740e0be9b7d28a78ed97900639f650c2c59.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/38c676008a0e6c4a7659c7541c36279303a8a5ddff43b245530992c8d59a112d.jpg", "table_caption": ["Table A.T.9: Semantic Retrieval Metrics calculated over our spatial real-world dataset. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.13 Further analysis on embedding clusters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Section 5.4 we analysed the UMAP projection of the ELSA embeddings of the test set of Spatial AudioCaps and Spatial Clotho. Table A.T.10 (a) shows the Wasserstein distances computed directly in the 512-dimensional space, where we see the data clusters by direction with lower Wasserstein distances between \u201cfront\u201d and \u201cback\u201d. Similarly, Fig. A.F.4 and Table A.T.10 (b) show the ELSA embeddings can be clustered according to spatial distance characteristics. ", "page_idx": 22}, {"type": "text", "text": "Table A.T.10: Wasserstein distances of 512-dimensional ELSA embeddings, clustered by either (a) direction or (b) distance. ", "page_idx": 22}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/ae2ae7cf18ff47d03d1b931b189b93a8b2f6e9ce5bd3d3392eb53039edb321ff.jpg", "table_caption": ["(a) Direction clustering distances "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/df309ea96c5b38dce38733173a14ba94d8d26e05a1ef5fceeb3058d54fe5fabd.jpg", "table_caption": ["(b) Distance clustering distances "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/044f2cf7a2696c76d1c5eef3c4dfd6166d03bbf066112bc1d6fe77f429edaf1a.jpg", "img_caption": ["Figure A.F.4: UMAP projection of ELSA embeddings of the test splits of Spatial-Clotho and SpatialAudioCaps. Filled markers are obtained from spatial audio, and hollow markers are obtained from spatial captions. The UMAP projection was ftited with the train splits of Spatial-Clotho and SpatialAudio caps, and we made use of supervised dimension reduction to highlight the distance differences rather than the semantic differences in the embeddings. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.14 Swapping of Spatial Direction Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As already introduced in Section 5.4. Our spatial direction swapping pipeline consists of the following steps: ", "page_idx": 23}, {"type": "text", "text": "1. We obtain the ELSA embeddings four directions (\u201cleft\u201d, \u201cright\u201d, \u201cfront\u201d, \u201cback\u201d) with the template: \u201cA sound coming from the direction \u201d. These are our direction prototypes.   \n2. Train a 4-class direction classifier with a 2-layer MLP $33\\mathbf{k}$ parameters) on the training set of Spatial-AudioCaps and Spatial-Clotho. We obtained a $96.7\\%$ classification accuracy of the test set of Spatial-AudioCaps and Spatial-Clotho.   \n3. For every correctly-classified sample in the test sets, we obtain their ELSA embedding, subtract the prototype of the original direction, and add a prototype for the new direction. ", "page_idx": 23}, {"type": "text", "text": "Additionally, we measure any changes in sound semantics by computing the difference in recall $@10$ between the ELSA audio embedding and the ELSA embedding of the sample\u2019s non-spatial description. ", "page_idx": 23}, {"type": "text", "text": "Detailed results are shown in table Table A.T.11. Overall, we find an average $99.7\\%$ of the samples are correctly classified with the new direction after transposition, and an average change of - $-0.2\\%$ in recall $@10$ . These results show ELSA directional attributes can be linearly swapped without affecting the semantics of sound. ", "page_idx": 23}, {"type": "text", "text": "Table A.T.11: Direction swapping of ELSA embeddings. See Appendix A.14 for a detailed explanation of how we swapped the embedding directions. $\\nless$ is the number of test samples misclassified by our direction classifier, and subsequently excluded. N is the number of samples that were used for direction transposition. $\\mathbf{R}\\@10$ is the recall $@10$ computed over the corresponding non-spatial captions. $\\theta$ is the classification accuracy of the transposed sample. $\\Delta_{R@10}$ is the change in recall $@10$ after performing the change of direction. ", "page_idx": 23}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/2047ad9b069e57e689e9e640dfad101a08a62e0ae085d95b5c4bdcc79aa66fb2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Furthermore, we wanted to verify what happened if we removed the original direction but did not add back a new direction. Table A.T.12 shows this ablation. Interestingly, the classification does not result in random classification accuracy but rather $0\\%$ accuracy for all four original directions. ", "page_idx": 24}, {"type": "text", "text": "Table A.T.12: Direction removal of ELSA embeddings. See Appendix A.14 for a detailed explanation of how we swapped the embedding directions. $\\notin$ is the number of test samples misclassified by our direction classifier, and subsequently excluded. $_\\mathrm{N}$ is the number of samples that were used for direction transposition. $\\mathbf{R}\\@10$ is the recall $@10$ computed over the corresponding non-spatial captions. $\\theta$ is the classification accuracy of the transposed sample. $\\Delta_{R@10}$ is the change in recall $@10$ after performing the change of direction. ", "page_idx": 24}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/a542114927cb1d5ca389cb1e77987431f583090310f02da95bae36733af1b49d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "A.15 Further details on Spatial Audio Caption Generation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Section 5.5 introduced a spatial audio caption generation system. In what follows, we illustrate some of the generations produced by the system as well as the corresponding ground-truth annotation. Below that, in Fig. A.F.5, we include an architecture diagram for the spatial audio caption generation system. ", "page_idx": 24}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/262db267af01982ba78638530995e7271a8b7453e28adcca72a14e7e863edada.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/6d3fc1fab44439f2cd19e2b61abee892a58dc7ddea577e7d46ab549f89349724.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "wDDvJzvvBR/tmp/d717f0e8c79a0eed6ffda2868a940542a45322007c6b50a55c36da2d6adb0f67.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "wDDvJzvvBR/tmp/29a8986eaa003aae5cac9a1cf61e5b41a0b49b21efd2f33b3debaea29b6df446.jpg", "img_caption": ["Figure A.F.5: Architecture diagram for spatial audio caption generation. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We propose a new dataset, discuss the dataset in Section 3, and demonstrate results of ELSA trained on this dataset in Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the limitations of our work in terms of some pitfalls of using synthetically augmented captions in the conclusion \u2014 Section 6 of the main paper as well as Section A.9 of the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not contain any novel theoretical results. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Dataset creation details are included in Section 3 including details of prompts, and dataset creation. Hyperparameter details are included in Section 5.1 and the appendix. Implementation details of our architecture are included in Section 4. Furthermore, we will release our code and models to aid reproducibility. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Code, datasets, and models will be made publicly available at https://github.com/apple/ml-spatial-audio-elsa. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We highlight training hyperparameters in Section 5.1. Details of the dataset creation are included in Section 3.1, and details of our dataset evaluations are included in Sections 5.2-5.4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: The original datasets provide only a single partition for training/testing/evaluation splits. We base our experiments on these splits so that results are directly comparable with other state of the art methods. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The training dataset took 2 weeks to generate, and utilized 96 CPUs and 16T of disk space. Each full training run of the model takes roughly 1600 A100 GPUhs. This was tested on multi-node GPU machines with up to 12 nodes. There were a total of 40 estimated runs to completion. For the version of our model only trained on the smaller datasets Clotho and AudioCaps, convergence takes 4 hours otherwise takes 17 hours. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: For training we use publicly available datasets, for which we augment with only spatial information. The real-world dataset that we captured for our experiments was created without capturing personally identifiable information. Furthermore, the dataset contains sounds of everyday items, e.g., a coffee grinder or paper rustling, and we know of no concerns regarding demographics of end-users. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Positive impact is detailed in the introduction(Section 1) and conclusion(Section 6), potential negative impacts are in the broader impacts paragraph (Section 6). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our societal risks only apply for generation of soundscapes, which is out of the scope for the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All datasets and model architectures are appropriately and explicitly cited.   \nGithub links are also included for code used. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: New assets including models and datasets are included in Sections 3 and 4 and plan to be released for reproducibility. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not conduct any experiments involving human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not conduct any experiments involving human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]