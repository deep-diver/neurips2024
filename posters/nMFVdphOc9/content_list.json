[{"type": "text", "text": "Rule Based Learning with Dynamic (Graph) Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 A common problem of classical neural network architectures is that additional   \n2 information or expert knowledge cannot be naturally integrated into the learning   \n3 process. To overcome this limitation, we propose a two-step approach consisting   \n4 of (1) generating formal rules from knowledge and (2) using these rules to define   \n5 rule based layers \u2013 a new type of dynamic neural network layer. The focus of this   \n6 work is on the second step, i.e., rule based layers that are designed to dynamically   \n7 arrange learnable parameters in the weight matrices and bias vectors for each input   \n8 sample following a formal rule. Indeed, we prove that our approach generalizes   \n9 classical feed-forward layers such as fully connected and convolutional layers by   \n10 choosing appropriate rules. As a concrete application we present rule based graph   \n11 neural networks (RuleGNNs) that are by definition permutation equivariant and   \n2 able to handle graphs of arbitrary sizes. Our experiments show that RuleGNNs   \n13 are comparable to state-of-the-art graph classifiers using simple rules based on   \n14 the Weisfeiler-Leman labeling and pattern counting. Moreover, we introduce new   \n15 synthetic benchmark graph datasets to show how to integrate expert knowledge   \n16 into RuleGNNs making them more powerful than ordinary graph neural networks. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Using expert knowledge to increase the efficiency, interpretability or predictive performance of   \n19 a neural network is an evolving research direction in machine learning [21, 23]. Many ordinary   \n20 neural network architectures are not capable of using external and structural information such as   \n21 expert knowledge or meta-data, e.g., graph structures in a dynamic way. We would like to motivate   \n22 the importance of \u201cexpert knowledge\u201d by considering the following example. Maybe one of the   \n23 best studied examples based on knowledge integration are convolutional neural networks [12].   \n24 Convolutional neural networks for images use at least two extra pieces of \u201cexpert knowledge\u201d that is:   \n25 neighbored pixels correlate, and the structure of images is homogeneous. The consequence of this   \n26 knowledge is the use of receptive fields and weight sharing. It is a common fact that the usage of   \n27 this information about images has highly improved the predictive performance over fully connected   \n28 neural networks. But what if expert knowledge suggests that rectangular convolutional kernels are   \n29 not suitable to solve the task? In this case the ordinary convolutional neural network architecture   \n30 is too static to adapt to the new information. Dynamic neural networks are not only applicable to   \n31 images but also to other data types such as video [25], text [10], or graphs [19]. The limitation   \n32 of such approaches is that expert knowledge is somehow implicit and not directly encoded in the   \n33 network structure, i.e., for each new information a new architecture has to be designed. Thus, our   \n34 goal is to extract the essence of dynamic neural networks by defining a new type of neural network   \n35 layer that is on the one side able to use expert knowledge in a dynamic way and on the other side   \n36 easily configurable. Our solution to this problem are rule based layers that are able to encode expert ", "page_idx": 0}, {"type": "image", "img_path": "nMFVdphOc9/tmp/e2cb3083657010cd9607d254d80f633a212847403787f055eddeea93a851cdbd.jpg", "img_caption": ["(a) Learned weights and bias for the best model of (b) Learned weights for the best model of the IMDBthe DHFR dataset. BINARY dataset. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Visualization of the learnable parameters of our RuleGNN on DHFR (a) and IMDBBINARY (b) for three different graphs. Positive weights are denoted by red arrows and negative weights by blue arrows. The arrow thicknesss and color corresponds to the absolute value of the weight. The bias is denoted by the size of the node. The second image of (a) resp. (b) shows the weights the 10 resp. 5 largest positive and negative weights. ", "page_idx": 1}, {"type": "text", "text": "37 knowledge directly in the network structure. As far as we know, this is the first work that defines a   \n38 dynamic neural network layer in this generality.   \n39 Main Idea We simplify and unify the integration of expert knowledge and additional informa  \n40 tion into neural networks by proposing a two-step approach and show how to encode given extra   \n41 information directly into the structure of a neural network in a dynamic way. In the first step the   \n42 extra information or expert knowledge is formalized using appropriate rules (e.g., certain pixels in   \n43 images are important, only nodes in a graph of type A and B interact, some patterns, e.g., cycles   \n44 or cliques, in a graph are important, etc.). In the second step the rules are used to manipulate the   \n45 structure of the neural network. More precisely, the rules determine the positions of the weights in   \n46 the weight matrix and the bias terms. We note that the focus of this work is on the second step as we   \n47 show how to use given rules to dynamically adapt the layers. In fact, we do not provide a general   \n4 instruction for deriving formal rules from given expert knowledge. In difference to ordinary network   \n49 layers we consider a set $\\mathcal{W}$ of learnable parameters instead of fixed weight matrices. The weight   \n50 matrices and bias terms are then constructed for each input sample independently using the learnable   \n51 parameters from $\\mathcal{W}$ . Indeed, each learnable parameter in $\\mathcal{W}$ is associated with a specific relation   \n52 between an input and output feature of a layer. As an example consider Figure 1 where each input and   \n53 output feature corresponds to a specific node in the graph. The input samples are (a) molecule graphs   \n54 respectively (b) snippets of social networks and the task is to predict the graph class. Each colored   \n55 arrow in the figure corresponds to a learned parameter from $\\mathcal{W}$ , i.e., a specific relation between two   \n56 atoms in the molecules or two nodes in the social network. Considering only the weights with the   \n57 largest absolute values, see the second image of (a) respectively (b), our approach has learned how to   \n58 propagate information from outer atoms to the rings respectively from the nodes to the \u201cimportant\u201d   \n59 nodes of the social network. This example shows several advantages of our approach: (1) rule based   \n60 layer type has a much more flexible structure than layers in classical architectures and allow to deal   \n61 with arbitrary input dimensions, (2) the layers are easily integrable into existing architectures, and   \n62 (3) the learned parameters, hence the model, is interpretable and can possibly be used to extract new   \n63 knowledge from the data or to improve the existing rules.   \n64 Main Contributions We define a new type of neural network layer called rule based layer. This   \n65 new layer can be integrated into arbitrary architectures making them dynamic, i.e., the structure   \n66 of the network changes based on the input data and predefined rules. We prove that rule based   \n67 layers generalize classical feed-forward layers such as fully connected and convolutional layers.   \n68 Additionally, we show that rule based layers can be applied to graph classification tasks, by introducing   \n69 RuleGNNs, a new type of graph neural networks. In this way we are able to extend the concept of   \n70 dynamic neural networks to graph neural networks together with all the advantages of dynamic neural   \n71 networks, e.g., that RuleGNNs are by definition permutation equivariant and able to handle graphs   \n72 of arbitrary sizes. Considering various real-world graph datasets, we demonstrate that RuleGNNs   \n73 are competitive with state-of-the-art graph neural networks and other graph classification methods.   \n74 Using synthetic graph datasets we show that \u201cexpert knowledge\u201d is easily integrable into our neural   \n75 network and also necessary for classification1   \n76 The rest of the paper is organized as follows: We introduce the concept of rule based layers in Section 2   \n77 and prove in Section 3 that rule based layers generalize fully connected and convolutional layers.   \n78 In Section 4 we present RuleGNNs and apply them in Section 5 to different benchmark datasets   \n79 and compare the results with state-of the art graph neural networks. Finally, we discuss limitations,   \n80 related work and conclude the paper in Section 6. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "81 2 Rule Based Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "82 Introducing the concept of rule based learning we first present some basic definitions followed by the   \n83 formal definition of rule based layers.   \n84 Preliminaries For some $n\\in\\mathbb N$ we denote by $[n]$ the set $\\{1,\\ldots,n\\}$ . A neural network is denoted   \n85 by a function $\\mathbf{f}\\left(-,\\Theta\\right):\\mathbb{R}^{n}\\,\\longrightarrow\\,\\mathbb{R}^{m}$ with the learnable parameters $\\Theta$ . We extend this notation   \n86 introducing an additional parameter $\\mathcal{R}$ , that is the set of formal rules $\\mathcal{R}\\,=\\,\\{{\\mathbf{R}}^{1},\\ldots,{\\mathbf{R}}^{k}\\}$ . The   \n87 exact definition of these rules is given in the next paragraph. Informally, a rule $\\mathbf{R}$ is a function   \n88 that determines the distribution of the weights in the weight matrix or the bias vector of a layer. A   \n89 rule $\\mathbf{R}$ is called dynamic if it is a function in the input samples $x\\in\\mathbb{R}^{n}$ otherwise it is called static.   \n90 An example of a static rule is the one used to define convolutional layers, see Proposition 2. An   \n91 example of a dynamic rule can be found in Section 4. In our setting, a neural network is a function   \n92 $\\mathbf{f}(-,\\bar{\\Theta},\\mathcal{R}):\\bar{\\mathbb{R}}^{*}\\longrightarrow\\mathbb{R}^{*}$ that depends on a set of learnable parameters denoted by $\\Theta$ and some   \n93 rule set $\\mathcal{R}$ derived from expert knowledge or additional information. The notation $^*$ in the domain   \n94 and codomain of $\\mathbf{f}$ indicates that the input and output can be of arbitrary or variable dimension. As   \n95 usual f is a concatenation of sub-functions $f^{1},\\ldots\\;\\stackrel{\\cdot}{,}f^{l}$ called the layers of the neural network. More   \n96 precisely, the $i$ -th layer is a function $f^{i}(-,\\vec{\\Theta}^{i},\\mathbf{R}^{i}):\\mathbb{R}^{*}\\longrightarrow\\mathbb{R}^{*}$ where $\\Theta^{i}$ is a subset of the learnable   \n97 parameters $\\Theta$ and $\\mathbf{R}^{i}$ is an element of the ruleset $\\mathcal{R}$ . We call a layer $f^{i}$ static if $\\mathbf{R}^{i}$ is a static rule and   \n98 dynamic if $\\mathbf{R}^{i}$ is a dynamic rule. The input data is a triple $(\\mathbf{D},\\mathbf{L},\\mathbf{I})$ , where $\\mathbf{D}=\\{x_{1}\\ldots,x_{k}\\}$ with   \n99 $x_{i}\\in\\mathbb{R}^{*}$ is the set of examples drawn from some unknown distribution. The labels are denoted by   \n100 $\\mathbf{L}=\\left(y_{1}\\ldots,y_{k}\\right)$ with $y_{i}\\in\\mathbb{R}^{*}$ and I is some additional information known about the input data $\\mathbf{D}$ .   \n101 This can be for example knowledge about the graph structure, node or edge labels, importance of   \n102 neighborhoods and many more. One main assumption of this paper is that I can be used to derive a   \n103 set of static or dynamic rules $\\mathcal{R}$ . Again we would like to mention that we concentrate on the analysis   \n104 of the effects of applying different rules $\\mathbf{R}$ and not on the very interesting but also wide field of   \n105 deriving the best rules $\\mathcal{R}$ from $I$ , see some discussion in Section 6. Nonetheless, we always motivate   \n106 the choice of the rules derived by I.   \n107 Rule Based Layers We now give a formal definition of rule based layers. Given some dataset   \n108 $(\\mathbf{D},\\mathbf{L},\\mathbf{I})$ defined as before and the rule set $\\mathcal{R}$ derived from I, the task is to learn the weights $\\Theta$ of   \n109 the neural network f to predict the labels of unseen examples drawn from an unknown distribution.   \n110 Our contribution concentrates on single layers and is fully compatible with other layers such as   \n111 linear layers, convolutional layers Hence, in the following we restrict to the $i$ -th layer $f^{i}\\bar{(-,\\Theta^{i},\\mathbf{R}^{i})}:$   \n112 $\\mathbb{R}^{*}\\longrightarrow\\mathbb{R}^{*}$ of a network f. For simplicity, we assume $i=1$ and omit the indices, i.e., we write   \n113 $f:=f^{i},\\Theta:=\\Theta^{i}$ and $\\mathbf{R}:=\\mathbf{R}^{i}$ . The forward propagation step of the rule based layer $f$ which will be   \n114 a generalization of certain known layers as shown in Section 3 is as follows. Fix some input sample   \n115 $x\\in\\mathbf{D}$ with $x\\in\\mathbb{R}^{n}$ . Then $f(-,\\Theta,\\mathbf{R}):\\mathbb{R}^{n}\\longrightarrow\\mathbb{R}^{m}$ for $n,m\\in\\mathbb{N}$ is given by ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x,\\Theta,{\\bf R})=\\sigma(W_{{\\bf R}_{W}(x)}\\cdot x+b_{{\\bf R}_{b}(x)})\\,\\,\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "116 Here $\\sigma$ denotes an arbitrary activation function and $W_{\\mathbf{R}_{W}(x)}\\in\\mathbb{R}^{m\\times n}$ rsp. $b_{\\mathbf{R}_{b}(x)}\\in\\mathbb{R}^{m}$ is some   \n117 weight matrix rsp. weight vector depending on the input vector $x$ and the rule $\\mathbf{R}$ . The set $\\Theta:=$   \n118 $\\{w_{1},\\dots,w_{N},b_{1},\\dots,b_{M}\\}$ consists of all possible learnable parameters of the layer. The parameters   \n119 $\\{w_{1},\\dots,w_{N}\\}$ are possible entries of the weight matrix while $\\{b_{1},\\dotsc,b_{M}\\}$ are possible entries of   \n120 the bias vector. The key point here is that the rule $\\mathbf{R}$ determines the choices and the positions of   \n121 the weights from $\\Theta$ in the weight matrix $W_{\\mathbf{R}_{W}(x)}$ and the bias vector $b_{{\\bf R}_{b}(x)}$ depending on the input   \n122 sample $x$ . More precisely, not all learnable parameters must be used in the weight matrix and the   \n123 bias vector for some input sample $x$ . Note that for two samples $x,y\\in\\mathbf{D}$ of different dimensionality,   \n124 e.g., $x\\in\\mathbb{R}^{n}$ and $\\boldsymbol{y}\\in\\dot{\\mathbb{R}}^{k}$ with $n\\not=k$ the weight matrices $W_{\\mathbf{R}_{W}(x)}$ and $W_{\\mathbf{R}_{W}\\left(y\\right)}$ also have different   \n125 dimensions and the learnable parameters can be in totally different positions in the weight matrix.   \n126 This is where the rules $\\mathbf{R}$ and their associated rule functions, see (2) below, come into play.   \n127 Given the set of learnable parameters $\\Theta:=\\{w_{1},\\dots,w_{N},b_{1},\\dots,b_{M}\\}$ , for each input $x\\in\\mathbb{R}^{n}$ the   \n128 rule $\\mathbf{R}$ induces the following two rule functions ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{R}_{W}(x):[m]\\times[n]\\longrightarrow\\{0\\}\\cup[N]\\quad{\\mathrm{~and~}}\\quad\\mathbf{R}_{b}(x):[m]\\longrightarrow\\{0\\}\\cup[M]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 where $m\\in\\mathbb{N}$ is the output dimension of the layer that can also depend on $x$ . In the following we   \n130 abbreviate $\\mathbf{R}_{W}(x)(i,j)$ by $\\mathbf{R}_{W}(x,i,j)$ and ${\\mathbf{R}}_{b}(x)(i)$ by $\\mathbf{R}_{b}(x,i)$ . We note that for simplicity we   \n131 assume that the matrix and vector indices start at 1 and not at 0. Using the associated rule functions (2)   \n132 we can construct the weight matrix resp. bias vector by defining the entry $(i,j)\\in\\mathbb{R}^{m\\times n}$ in the $i$ -th   \n133 row and the $j$ -th column of the weight matrix $W_{\\mathbf{R}(x)}\\in\\mathbb{R}^{m\\times n}$ via ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{{\\bf R}_{W}(x)}(i,j):=\\left\\{0\\atop w_{{\\bf R}_{W}(x,i,j)}\\right.\\quad\\mathrm{o.w.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 and the entry at position $k$ in the bias vector $b_{\\mathbf{R}_{b}(x)}\\in\\mathbb{R}^{m}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\nb_{\\mathbf{R}_{b}(x)}(k):=\\left\\{0\\begin{array}{l l}{\\mathrm{if}\\;\\mathbf{R}_{b}(x,k)=0}\\\\ {\\mathrm{~}b_{\\mathbf{R}_{b}(x,k)}}&{\\mathrm{~}0.\\mathrm{w}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 Summarizing, the rule based layer defined in (1) is a standard feed-forward layer with the difference   \n136 that the weights in the weight matrix and the bias vector are determined by a predifined rule $\\mathbf{R}$ .   \n137 In fact, weight matrix and bias vector depend on the input and can contain shared weights. More   \n138 precisely, the rule controls the connection between the $i$ -th input and the $j$ -th output feature in the   \n139 weight matrix. A rule $\\mathbf{R}$ is called static if it is independent of the input $x\\in\\mathbf{D}$ , i.e., $\\mathbf{R}(x)\\equiv\\mathbf{R}(y)$   \n140 for all inputs $x,y\\in\\mathbb{R}\\in\\mathbf{D}$ otherwise it is called dynamic. We call a rule based layer as defined in (1)   \n141 static if it is based on a static rule $\\mathbf{R}$ and dynamic otherwise. We will show in Section 3 that rule   \n142 based layers generalize known concepts of neural network layers for specific rules $\\mathbf{R}$ . In fact, we   \n143 show that fully connected layers and convolution layers are static rule based layers. Examples of   \n44 dynamic rule based layers are given later on in Section 4. The back-propagation of such a layer can   \n145 be done as usual enrolling the computation graph of the forward step and applying iteratively the   \n146 chain rule to all the computation steps. We will not go into the details of this computation as it is   \n147 similar to many other computations using backpropagation with shared weights. For the experiments   \n148 we use the automatic backpropagation tool of PyTorch [16] which fully meets our requirements.   \n149 Assumptions and Examples Rule based learning relies on the following two main assumptions:   \n150 $A1$ ) There is a connection between the additional information or expert knowledge I and the used   \n151 rule R and $A2$ ) The distribution of weights given by the rule $\\mathbf{R}$ in the weight matrix $W_{\\mathbf{R}(x)}$ improves   \n152 the predictive performance or increases the interpretability of the neural network. As stated before   \n153 we concentrate on the second assumption and consider different distribution of weights in the weight   \n154 matrix given by different rules. In fact, we assume without further consideration that it is possible to   \n155 derive a meaningful ruleset R from the additional information or expert knowledge I. For example if   \n156 the dataset consists of images we can derive the \u201cinformal\u201d rule that neighboured pixels are more   \n157 important than pixels far away and in case of chemical data there exists, e.g., the ortho-para rule for   \n158 benzene rings that makes assumptions about the influence of atoms for specific positions regarding   \n159 the ring. This rule was already learned by a neural network in [28]. It is another very interesting task   \n160 which is beyond the scope of this work how to formalize these \u201cinformal\u201d rules or to learn the \u201cbest\u201d   \n161 formal rules from the additional information I.   \n162 In the following sections we focus on the concept of rule based layers and therefore for simplicity   \n163 and space reasons only consider the rule function of weight matrices. The rule function associated   \n164 with the bias vector can be constructed similarly. For simplicity, we write $\\mathbf{R}$ instead of ${\\bf R}_{W}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "165 3 Theoretical Aspects of Rule Based Layers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 In this section we provide a theoretical analysis of rule based layers and show that they generalize fully   \n167 connected and convolutional layers. More precisely, we define two static rules $\\mathbf{R}_{\\mathrm{FC}}$ and $\\mathbf{R}_{\\mathrm{CNN}}$ and   \n168 show that the rule based layer as defined in (1) based on $\\mathbf{R}_{\\mathrm{FC}}$ is a fully connected layer and the rule   \n169 based layer based on $\\mathbf{R}_{\\mathrm{CNN}}$ is a convolutional layer. All the proofs can be found in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "170 Proposition 1 Let $f:\\mathbb{R}^{n}\\longrightarrow\\mathbb{R}^{m}$ with ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(y,\\Theta,\\mathbf{R}_{\\mathrm{FC}})=\\sigma(W_{\\mathbf{R}_{\\mathrm{FC}}(x)}\\cdot y)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 be a rule based layer of a neural network as defined in (1) (without bias term) with learnable   \n172 parameters $\\Theta=\\{w_{1},\\dots,w_{n\\cdot m}\\}$ and $y=\\mathbf{f}^{i}(x)$ is the result of the first $i-1$ layers. Then for the   \n173 rule function $\\mathbf{R}_{\\mathrm{FC}}(x):[m]\\times[n]\\rightarrow[m\\cdot n]$ defined for all inputs x as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{R}_{\\mathrm{FC}}:=\\mathbf{R}_{\\mathrm{FC}}(x)(i,j):=(i-1)\\cdot n+j,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 the rule based layer $f$ is equivalent to a fully connected layer with activation function $\\sigma$ . ", "page_idx": 4}, {"type": "text", "text": "175 Proposition 1 shows that rule based layers generalize fully connected layers of arbitrary size without   \n176 bias vector and can be easily adapted to include the bias vector. Hence, this shows that rule based   \n177 layers generalize arbitrary fully connected layers. Moreover, fully connected layers are static rule   \n178 based layers as the rule $\\mathbf{R}_{\\mathrm{FC}}$ is static because it does not depend on the particular input $x$ . ", "page_idx": 4}, {"type": "text", "text": "179 Proposition 2 Let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f:\\mathbb R^{n\\cdot m}\\longrightarrow\\mathbb R^{(n-N+1)\\cdot(m-N+1)}~w i t h}&{{}}\\\\ {f(y,\\Theta,\\mathbf{R}_{\\mathrm{CNN}})=\\sigma(W_{\\mathbf{R}_{\\mathrm{CNN}}(x)}\\cdot y)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "180 be a rule based layer of a neural network as defined in (1) (without bias term) and $W^{i}\\ =$   \n181 $\\{w_{1},\\dots,w_{N^{2}}\\}$ be the set of learnable parameters. Then for the rule function ${\\bf R}_{\\mathrm{CNN}}~:~[(n~-$   \n182 $\\bar{N}+1)\\cdot(m-N+1)]\\times[n\\cdot m]\\to[N^{2}]$ defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{R}_{\\mathrm{CNN}}:=\\mathbf{R}_{\\mathrm{CNN}}(x)(i,j):=\\left\\{{\\begin{array}{l l}{\\!\\!\\!\\sigma(i,j)}&{\\!\\!\\!i f\\ 0<\\gamma(i,j)<N\\cdot n\\,a n d}\\\\ {\\!\\!\\!0<j\\,(\\!\\!{\\bmod{n}})-j+\\gamma(i,j)<N}\\\\ {\\!\\!\\!0.}&{\\!\\!\\!o.w.}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 ", "page_idx": 4}, {"type": "text", "text": "184 the rule based layer $f$ is equivalent to a convolution layer with quadratic kernel of size $N$ $\\prime N<n,$   \n185 $N<m,$ ) and a stride of one over a two-dimensional image of size $n\\times m$ (without padding and bias   \n186 vector) with activation function $\\sigma$ . The notation $a//b$ denotes the integer division of two integers $a$   \n187 and $b$ .   \n188 Proposition 2 shows that rule based layers generalize 2D-image convolution without padding and   \n189 bias term. By adaption of the rule function it is possible to include the bias vector and padding.   \n190 Moreover, the result can be generalized to higher dimensions kernels, non-quadratic kernels and   \n191 arbitrary input and output channels. Hence, rule based layers also generalize arbitrary convolutional   \n192 layers. Convolutional layers are static rule based layers as the rule ${\\bf R}_{\\mathrm{CNN}}$ is static because it is   \n193 independent of the input. The following result is a direct implication from Propositions 1 and 2.   \n194 Theorem 1 Rule based layers generalize fully connected and convolutional feed-forward layers.   \n195 Moreover, both layers are static rule based layers.   \n196 We claim that also other types of feed-forward layers can be generalized by rule based layers using   \n197 appropriate rule functions. Because of space limitations we would rather present a specific application   \n198 of dynamic rule based layers on graphs. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "199 4 Rule Based Learning on Graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "200 One of the main advantages of rule based layers as introduced in this work is that they give rise to   \n201 a dynamic neural network architecture that is freely configurable using different rules. In fact, the   \n202 network is independent of the dimension and structure of the input samples. Hence, a natural applica  \n203 tion of our approach is graph classification. We would like to emphasize that graph classification is   \n204 only one of many possible applications of rule based layers. Other possible applications are node   \n205 classification, regression tasks, graph embeddings or completely different data-structures.   \n206 Graph Preliminaries By a graph we mean a pair $G=(V,E)$ with $V$ denoting the set of nodes of   \n207 $G$ and $E\\subseteq\\{\\{i,j\\}\\mid i,j\\in V\\}$ the set of edges. We assume that the graph is undirected and does   \n208 not contain self-loops or parallel edges. In case that it is clear from the context we omit $G$ and only   \n209 use $V$ and $E$ . The distance between two nodes $i,j\\in V$ in a graph, i.e., the length of the shortest   \n210 path between $i$ and $j$ , is denoted by $d(i,j)$ . A labeled graph is a graph $G=(V,\\mathbf{\\bar{{E}}})$ equipped with   \n211 a function $l:V\\to{\\mathcal{L}}$ that assigns to each node a label from the set ${\\mathcal{L}}\\subseteq\\mathbb{N}$ . In this paper the input   \n212 samples corresponding to a graph $(V,E)$ are always vectors of length equal to $|V|$ . In particular, the   \n213 input vectors can be interpreted as signals over the graph and each dimension of the input vector   \n214 corresponds to the one-dimensional input signal of a graph node. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "215 4.1 Graph Rules ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "216 The example on molecule graphs in Figure 2 and Appendix A.4 motivates the intuition behind   \n217 different graph specific rules that can be used to define a graph neural network based on rule layers.   \n218 The underlying general scheme to define a rule based layer on graphs is as follows: Let $G=(\\dot{V},E)$   \n219 be a graph and $l:V\\to{\\mathcal{L}}$ a permutation equivariant labeling function of the nodes, i.e., for some   \n220 permutation $\\pi$ of $V$ it holds $\\bar{l}(\\pi(V))=\\pi(l(\\dot{V}))$ . Assuming that input and output dimension of the   \n221 layer is equal to $|V|$ the rule functions $\\mathbf{R}$ as defined in (2) map each pair of nodes $(i,j)\\in V\\times V$   \n222 to an integer which is the index of the learnable parameter in the set of all learnable parameters.   \n223 The mapping is injective based on the labels $l(i),{\\bar{l}}(j)$ and an additionally defined shared property   \n224 between the nodes $i$ and $j$ . Examples for such shared properties can be the distance between $i$ and   \n225 $j$ , the type of the edge connecting $i$ and $j$ or the information, that $i$ and $j$ are in one circle. As an   \n226 example ${\\bf R}_{\\mathrm{Mol}}$ as defined in Appendix A.4 is induced by the permutation equivariant function $l$ that   \n227 maps each node to its atom label and the shared property between two nodes is the type of the edge   \n228 connecting the nodes or the absence of an edge. Besides ${\\bf R}_{\\mathrm{Mol}}$ the simple rule that is based on the   \n229 given node labels in this paper we focus on three different rule based layers for graphs.   \n230 Proposition 3 Let \u03c0 be some permutation of the nodes of $G=(V,E)$ and $x$ its corresponding input   \n231 vector. If R permutation equivariant, i.e., ${\\bf R}(\\pi(x))(i,\\stackrel{.}{j})={\\bf R}(x)(\\bar{\\pi}(i),\\pi(j))$ then the rule based   \n232 layer is also equivariant under node permutations, i.e., $f(\\pi(x),\\Theta,{\\bf R}_{\\mathrm{Mol}})=\\pi(f(x,\\Theta,{\\bf R}_{\\mathrm{Mol}}))$ .   \n233 Weisfeiler-Leman Rule Recent research has shown that the Weisfeiler-Leman labeling is a powerful   \n234 tool for graph classification [18, 14, 2, 22]. Thus, we propose to use Weisfeiler-Leman labels as   \n235 one option to define the rule based layer for graph classification. The Weisfeiler-Leman algorithm   \n236 assigns in the $k$ -th iteration to each node of a graph a label based on the structure of its local $k$ -hop   \n237 neighborhood, see [18]. Let $l(v)$ be the result of the $k$ -th iteration of the Weisfeiler-Leman algorithm   \n238 for some node $v\\in V$ . Then the Weisfeiler-Leman Rule $\\mathbf{R}_{W L_{k,d}}$ assigns to each node pair $(i,j)$ an   \n239 integer or zero based on the Weisfeiler-Leman labels $l(i),l(j)$ and the distance between the nodes $i$   \n240 and $j$ . The result is zero if the distance between $i$ and $j$ is not between 1 and $d$ . Note that we are not   \n241 restricted to look at consecutive distances from 1 to $d$ . It is also possible to look at certain distances   \n242 only if the expert knowledge suggests it. In fact, $(i,j)$ and $(k,l)$ are mapped to the same integer if   \n243 and only if $l(\\bar{i})=l(k),l(j)=l(\\bar{l})$ and the distance between $i$ and $j$ is equal to the distance between   \n244 $k$ and $l$ . The layer defined by this rule is related to ordinary message passing but messages can pass   \n245 between nodes of arbitrary distances. For computational reasons in the experiments we restrict the   \n246 maximum number of different Weisfeiler-Leman labels considered by some bound $L$ . We relabel   \n247 the most frequent $l-1$ labels to $1,\\cdot\\cdot\\cdot\\,,l-1$ and set all other labels to $l$ . The corresponding layer is   \n248 denoted by fW Lk,d,L.   \n249 Pattern Counting Rule Beyond labeling nodes via the Weisfeiler-Leman algorithm, it is a common   \n250 approach to use subgraph isomorphism counting to distinguish graphs [3]. This is in fact necessary   \n251 as the 1-Weisfeiler-Leman algorithm is not able to distinguish some types of graphs, for example   \n252 circular skip link graphs [4] and strongly regular graphs [2, 3]. Thus, we propose the pattern counting   \n253 rule and show in Section 5 that RuleGNNs based on this rule are able to perform well on synthetic   \n254 benchmark datasets while message passing models based on the Weisfeiler-Leman algorithm fail. In   \n255 general, subgraph isomorphis counting is a hard problem [5], but for the real-world and synthetic   \n256 benchmark graph datasets that are usually considered, subgraphs of size $k\\,\\in\\,\\{3,4,5,6\\}$ can be   \n257 enumerated in a preprocessing step in a reasonable time, see Table 5. Given a set of patterns, say   \n258 $\\mathcal{P}$ , we compute all possible embeddings of these patterns in the graph dataset in a preprocessing   \n259 step. Then for each pattern $P\\in\\mathcal P$ and each node $i\\in V$ we count how often the node $i$ is part of an   \n260 embedding of $P$ . Using those counts we define a labeling function $l:V\\to{\\mathcal{L}}$ . Two nodes $i,j\\in V$   \n261 are mapped to the same label if and only if their counts are equal for all patterns in $\\mathcal{P}$ . Patterns   \n262 that are often used in practice are small cycles, cliques, stars, paths, etc. The Pattern Counting Rule   \n263 $\\mathbf{R}_{\\mathcal{P}_{d}}$ assigns each node pair $(i,j)$ an integer or zero based on the values of $l(i),l(j)$ and the distance   \n264 between $i$ and $j$ . As for the Weisfeiler-Leman Rule we restrict the maximum number of different   \n265 labels to some number $L$ . The corresponding layer is denoted by $f_{\\mathcal{P}_{d,L}}$ .   \n266 Summary Rule The summary rule $\\mathbf{R}_{\\mathrm{Out}}^{N}$ can be used as the output layer as its output is a fixed   \n267 dimensional vector of size independent of the size of the input data and the output is invariant   \n226689 iunntdeegre rn. odTeh epne rtmheu tsautimonms.a rAy grauilne, $\\mathbf{R}_{\\mathrm{Out}}^{N}$ $l:V\\to{\\mathcal{L}}$ beea ca hf upnacirti $(n,i)$ a t wmitahp $i\\in V$ annodd $n\\in[N]$ pahn t ion steogmeer   \n270 or zero based on $n$ and $l(i)$ . In fact, for each element of $\\mathcal{L}$ the rule defines $n$ different learnable   \n271 parameters. The corresponding layer is denoted by fRONut.   \n272 All the above rules define dynamic rule based neural network layers because the weight matrix and   \n273 bias terms defined by the rules depend on the input vectors $x$ corresponding to different graphs. Note   \n274 that the layers defined by the above rules are permutation equivariant as the node labeling function $l$   \n275 used to define the rule is equivariant under node permutations. Thus, using the layers corresponding   \n276 to the above defined rules we can build a graph classification architecture that by definition does not   \n277 depend on the order of the nodes in the input graphs. Moreover, a layer is able to pass information   \n278 between nodes of arbitrary distances in the graph. Thus, as shown in the experiments below, it is not   \n279 necessary to use deep networks to achieve good performance on the real-world benchmark datasets. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "280 4.2 Rule Graph Neural Networks (RuleGNNs) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "281 The layers derived from the above rules are the building blocks of the RuleGNNs. Each RuleGNN is   \n282 a concatenation of different rule based layers from Weisfeiler-Leman rules and pattern counting rules   \n283 followed by a summary rule using arbitrary activation functions. To define the learnable parameters   \n284 of the bias term we also use the summary rule. The input of the network is a signal $\\boldsymbol{x}\\in\\mathbb{R}^{|V|}$   \n285 corresponding to a graph $G=(V,E)$ . We note that for simplicity we focus on one-dimensional   \n286 signals but also multidimensional signals, i.e., $\\boldsymbol{x}\\in\\mathbb{R}^{|V|\\times d}$ are possible. The output of the network   \n287 is a vector of fixed size $N\\in\\mathbb{N}$ determined by the summary rule where $N$ is usually the number   \n288 of classes of the graph classification task. The output can be also used as an intermediate vectorial   \n289 representation of the graph or for regression tasks. ", "page_idx": 6}, {"type": "text", "text": "290 5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "291 We evaluate the performance of RuleGNNs on different real-world and synthetic benchmark graph   \n292 dataset and compare the results to the state-of-the-art graph classification algorithms. For comparabil  \n293 ity and reproducibility of the results, also with future algorithms, we make use of the experimental   \n294 setup from [7]. That means, for each graph dataset we perform a 10-fold cross validation, i.e., we use   \n295 fixed splits of the dataset into 10 equally sized parts (the splits can be found in our repository), and   \n296 use 9 of them for training, parameter tuning and validation. We then use the model that performs   \n297 best on the validation set and report the performance on the previously unseen test set. We train the   \n298 best model 3 times and average the results on each fold to decrease random effects. The standard   \n299 deviation reported in the tables is computed over the results on the 10 folds.   \n300 Data and Algorithm Selection A problem of several heavily used graph benchmark datasets   \n301 like MUTAG or PTC [13] is that node and edge labels seems to be more important than the graph   \n302 structure itself, i.e., there is no significant improvement over simple baselines [17]. Moreover, in   \n303 case of MUTAG the performance of the model is highly dependent on the data split because of the   \n304 small number of samples. Thus, in this work for benchmarking we choose DHFR, Mutagenicity,   \n305 NCI1, NCI109, IMDB-BINARY and IMDB-MULTI from the TU Dortmund Benchmark Graphs   \n306 repository [13] because the structure of the graphs seems to play an important role, i.e., the simple   \n307 baselines presented in [17, 7] are significantly worse than the state-of-the-art graph classification   \n308 algorithms. Additionally, we consider circular skip link graphs CSL [4] and constructed some new   \n309 synthetic benchmark graph datasets called LongRings, EvenOddRings and Snowflakes [15] to show   \n310 the advantages of RuleGNNs on more complex graph structures with given expert knowledge. For   \n311 more details on the datasets see Appendix A.5. For NCI1, IMDB-BINARY and IMDB-MULTI we   \n312 use the same splits as in [7] and for CSL we use the splits as in [6] and a 5-fold cross validation. We   \n313 evaluate the performance of the RuleGNNs on these datasets and compare the results to the baselines   \n314 from [7] and [17] and the Weisfeiler-Leman subtree kernel (WL-Kernel) [18] which is one of the best   \n315 performing graph classification algorithm besides the graph neural networks. For comparison with   \n316 state-of-the-art graph classification algorithms we follow [7] and compare to DGCNN [27], GIN [26]   \n317 and GraphSAGE [8]. Additionally, we compare to the results of some newer state-of-the-art graph   \n318 classification algorithms [3, 1, 2, 22]. For the latter we use the results from the respective papers that   \n319 might be obtained with different splits of the datasets.   \n320 Experimental Settings and Resources All experiments were conducted on a AMD Ryzen 9 7950X   \n321 16-Core Processor with 128 GB of RAM. For the competitors we use the implementations from [7].   \n322 For the real-world datasets we were not aware of expert-knowledge, hence we tested different rules   \n323 and combinations of the layers defined in Section 4.1. More details on the tested hyperparameters   \n324 can be found in Appendix A.7. We always use tanh for activation and the Adam optimizer [11] with   \n325 a learning rate of 0.05 (real-world datasets) resp. 0.1 (synthetic datasets). For the real-world datasets   \n326 the learning rate was decreased by a factor of 0.5 after each 10 epochs. For the loss function we use   \n327 the cross entropy loss. All models are trained for 50 (real-world) resp. 200 (synthetic) epochs and the   \n328 batch size was set to 128. We stopped if the validation accuracy did not improve for 25 epochs.   \n329 Real-World Datasets The results on the real-world datasets (Table 1) show that RuleGNNs are   \n330 able to outperform the state-of-the-art graph classification algorithms in the setting of [7] even if   \n331 we add all the additional label information that RuleGNNs use to the input features of the graph   \n332 neural networks (see the (features) results in Table 1). This shows that the structural encoding of   \n333 the additional label information is crucial for the performance of the graph neural networks and not   \n334 replacable by using more input features. Moreover, the results show that the Weisfeiler-Leman subtree   \n335 kernel is the best performing graph classification algorithm on NC1, NCI109 and Mutagenicity. For   \n336 IMDB-BINARY and IMDB-MULTI our approach performs worse than the state-of-the-art graph   \n337 classification algorithms that are not evaluated within the same experimental setup.   \n338 Synthetic Datasets The results on the synthetic benchmark graph dataset show that RuleGNNs   \n339 outperform the state-of-the-art graph classification algorithms if expert knowledge is available even   \n340 in the case that mesage passing is enough to solve the task. In fact, CLS and Snowflakes are not   \n341 solvable by the message passing model because they are not distinguishable by the 1-WL test. The   \n342 results on LongRings show that long range dependencies can be easily captured by RuleGNNs and   \n343 also dependencies between nodes of different distances as in case of the EvenOddRings dataset can   \n344 be encoded by appropriate rules.   \n345 Interpretability of the Rule Based Layers Each learnable parameter of RuleGNNs can be inter  \n346 preted in terms of the importance of a connection between two nodes in a graph with respect to their   \n347 labels and their shared property (in our case the distance). In Figures 1 and6 we see how the network   \n348 has learned the importance of different connections between nodes for different distances and labels. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "nMFVdphOc9/tmp/a82c56ee1e73fbb10031baf976b1bea2c19baa9264baec0eaf60ccc8b7bd1f81.jpg", "table_caption": [], "table_footnote": ["Table 1: Test set performance of several state-of-the-art graph classification algorithms averaged over three different runs and 10 folds. The $\\pm$ values report the standard deviation over the 10 folds. The overall best results are colored red and the best ones obtained for the fair comparison from [7] are in bold. The (features) variant of the algorithms uses the same information as the RuleGNN as input features additionally to node labels. The (paper) results are taken from the respective papers and might be obtained with different splits of the datasets. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "nMFVdphOc9/tmp/46e6d738c685ba8077555df02b7f2ac0f8155325ffa1d6f65a10d51cff999be1.jpg", "table_caption": [], "table_footnote": ["Table 2: Test set performance of several state-of-the-art graph classification algorithms averaged over three different runs and 10 folds. The $\\pm$ values report the standard deviation over the 10 folds. The best results and our algorithm are highlighted in bold. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "349 6 Related Work, Limitations and Concluding Remarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "350 Dynamic neural networks have been proven to be more efficient, have more representation power   \n351 and better interpretability than static neural networks [9]. Our approach can be seen as a sample   \n352 dependent dynamic neural network as for each input sample the network structure is adapted. In   \n353 contrast to other sample dependent dynamic neural networks [20, 24], our approach changes the   \n354 layer structure based on a predefined rule instead of the whole architecture. The rule based layers   \n355 of RuleGNNs use the Weissfeiler-Leman labeling algorithm and subgraph isomorphism counting   \n356 which are both recently used concepts in graph classification algorithms [18, 3, 2, 1]. The challenge   \n357 for graph neural networks is the heterogenicity of the input data and the lack of a fixed order of the   \n358 input data. [19] proposes a dynamic neural network for graph classification that uses node and edge   \n359 labels and is similar to our approach. In fact, they also show that their approach generalizes CNNs.   \n360 In contrast, they do not provide a general scheme to encode expert knowledge into the network.   \n361 Moreover, their approach is not able to encode long range dependencies in the graph using only   \n362 one layer. There exist graph neural networks that have learned the ortho-para rule for molecules   \n363 [28]. While the additional information used in these algorithms is mostly hard-coded, we are able to   \n364 integrate arbitrary rules.   \n365 Limitations Input Features: So far we have only considered 1-dimensional input signals and   \n366 node labels, i.e., our experimental results are restricted to graphs that have no multi-dimensional   \n367 node features. Additionally, we have not considered edge features in our rules. In principle, multi  \n368 dimensional node features and edge labels can be handled by our approach with the cost of increased   \n369 complexity. Space: For each graph we need to precompute the pairwise distances and store the   \n370 positions of the weights in the weight-matrix. This is a disadvantage for large and dense graphs   \n371 as we need to store a large number of positions. For dense graphs the number of positions can be   \n372 quadratic in the number of nodes. Structure: To define a meaningful rule for a layer the input and   \n373 output features need to be logically connected. Fortunately, this is the case for graphs but this fact can   \n374 be a limitation for other structures. Combinatorics: If it is not possible to define a formal rule given   \n375 some informal expert knowledge the number of possible rules that have to be tested can be very large.   \n376 Thus, it is an interesting question if it is possible to automatically learn a rule that captures the expert   \n377 knowledge in the best way. Implementation: As stated in [9] there is a \u201cgap between theoretical &   \n378 practical efficiency\u201d regarding dynamic neural networks, i.e., common libraries such as PyTorch or   \n379 TensorFlow are not optimized for dynamic neural networks.   \n380 Concluding Remarks We have introduced a new type of neural network layer that dynamically   \n381 arranges the learnable parameters in the weight matrices and bias vectors according to a formal   \n382 rule. On the one hand our approach generalizes classical neural network components such as fully   \n383 connected layers and convolutional layers. On the other hand we are able to apply rule based layers   \n384 to the task of graph classification showing that expert knowledge can be integrated into the learning   \n385 process. Moreover, our approach gives rise to a more interpretable neural network architecture as   \n386 every learnable parameter is related to a specific connection between input and output features. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "387 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "388 [1] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Li\\`o, Guido F. Mont\u00b4ufar,   \n389 and Michael M. Bronstein. Weisfeiler and lehman go cellular: CW networks. In Marc\u2019Aurelio   \n390 Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan,   \n391 editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural   \n392 Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages   \n393 2625\u20132640, 2021.   \n394 [2] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F. Mont\u00b4ufar, Pietro Li\u00b4o,   \n395 and Michael M. Bronstein. Weisfeiler and lehman go topological: Message passing simplicial   \n396 networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International   \n397 Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of   \n398 Proceedings of Machine Learning Research, pages 1026\u20131037. PMLR, 2021.   \n399 [3] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving   \n400 graph neural network expressivity via subgraph isomorphism counting. IEEE Trans. Pattern   \n401 Anal. Mach. Intell., 45(1):657\u2013668, 2023.   \n402 [4] Jin-yi Cai, Martin F\u00a8urer, and Neil Immerman. An optimal lower bound on the number of   \n403 variables for graph identification. Comb., 12(4):389\u2013410, 1992.   \n404 [5] Stephen A. Cook. The complexity of theorem-proving procedures. In Michael A. Harrison,   \n405 Ranan B. Banerji, and Jeffrey D. Ullman, editors, Proceedings of the 3rd Annual ACM Sym  \n406 posium on Theory of Computing, May 3-5, 1971, Shaker Heights, Ohio, USA, pages 151\u2013158.   \n407 ACM, 1971.   \n408 [6] Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio,   \n409 and Xavier Bresson. Benchmarking graph neural networks. J. Mach. Learn. Res., 24:43:1\u201343:48,   \n410 2023.   \n411 [7] Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph   \n412 neural networks for graph classification. ArXiv, abs/1912.09893, 2019.   \n413 [8] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on   \n414 large graphs. In Neural Information Processing Systems, 2017.   \n415 [9] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic   \n416 neural networks: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):7436\u20137456, 2022.   \n417 [10] Yacine Jernite, Edouard Grave, Armand Joulin, and Tom\u00b4as Mikolov. Variable computation in   \n418 recurrent neural networks. In 5th International Conference on Learning Representations, ICLR   \n419 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,   \n420 2017.   \n421 [11] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua   \n422 Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,   \n423 ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.   \n424 [12] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard,   \n425 Wayne E. Hubbard, and Lawrence D. Jackel. Handwritten digit recognition with a back  \n426 propagation network. In David S. Touretzky, editor, Advances in Neural Information Processing   \n427 Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 396\u2013404.   \n428 Morgan Kaufmann, 1989.   \n429 [13] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion   \n430 Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML   \n431 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020.   \n432 [14] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen,   \n433 Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural   \n434 networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The   \n435 Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth   \n436 AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,   \n437 Hawaii, USA, January 27 - February 1, 2019, pages 4602\u20134609. AAAI Press, 2019.   \n438 [15] Harish G. Naik, Jan Polster, Raj Shekhar, Tam\u00b4as Horv\u00b4ath, and Gy\u00a8orgy Tur\u00b4an. Iterative graph   \n439 neural network enhancement via frequent subgraph mining of explanations, 2024.   \n440 [16] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n441 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas   \n442 Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,   \n443 Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,   \n444 High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer,   \n445 F. d\u2019Alch\u00b4e Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing   \n446 Systems 32, pages 8024\u20138035. Curran Associates, Inc., 2019.   \n447 [17] Till Hendrik Schulz and Pascal Welke. On the necessity of graph kernel baselines. 2019.   \n448 [18] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.   \n449 Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12:2539\u20132561, 2011.   \n450 [19] Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned fliters in convolutional   \n451 neural networks on graphs. In 2017 IEEE Conference on Computer Vision and Pattern Recogni  \n452 tion, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 29\u201338. IEEE Computer Society,   \n453 2017.   \n454 [20] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early   \n455 exiting from deep neural networks. In 23rd International Conference on Pattern Recognition,   \n456 ICPR 2016, Canc\u00b4un, Mexico, December 4-8, 2016, pages 2464\u20132469. IEEE, 2016.   \n457 [21] Geoffrey G. Towell and Jude W. Shavlik. Knowledge-based artificial neural networks. Artif.   \n458 Intell., 70(1-2):119\u2013165, 1994.   \n459 [22] Quang Truong and Peter Chin. Weisfeiler and lehman go paths: Learning topological features   \n460 via path complexes. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors,   \n461 Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference   \n462 on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on   \n463 Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,   \n464 Canada, pages 15382\u201315391. AAAI Press, 2024.   \n465 [23] Laura von R\u00a8uden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach,   \n466 Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, Michal   \n467 Walczak, Jochen Garcke, Christian Bauckhage, and Jannis Schuecker. Informed machine   \n468 learning - A taxonomy and survey of integrating prior knowledge into learning systems. IEEE   \n469 Trans. Knowl. Data Eng., 35(1):614\u2013633, 2023.   \n470 [24] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. Skipnet: Learning   \n471 dynamic routing in convolutional networks. In Vittorio Ferrari, Martial Hebert, Cristian Smin  \n472 chisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference,   \n473 Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, volume 11217 of Lecture   \n474 Notes in Computer Science, pages 420\u2013436. Springer, 2018.   \n475 [25] Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, and Gao Huang. Adaptive   \n476 focus for efficient video recognition. In 2021 IEEE/CVF International Conference on Computer   \n477 Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 16229\u201316238. IEEE,   \n478 2021.   \n479 [26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural   \n480 networks? In 7th International Conference on Learning Representations, ICLR 2019, New   \n481 Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n482 [27] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning   \n483 architecture for graph classification. In Sheila A. McIlraith and Kilian Q. Weinberger, editors,   \n484 Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the   \n485 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium   \n486 on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA,   \n487 February 2-7, 2018, pages 4438\u20134445. AAAI Press, 2018.   \n488 [28] Zhenpeng Zhou and Xiaocheng Li. Graph convolution: A high-order and adaptive approach.   \n489 arXiv: Learning, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "image", "img_path": "nMFVdphOc9/tmp/9f027ace37f7c5c96c830bf694654d03cc06225fd9a2cce60cc899b86fcb4d56.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "image", "img_path": "nMFVdphOc9/tmp/e5561d7b6bc06a5b5afaa58a2ff2ce5ba23c6eacdc3aa5d988e7b16c3e80e5c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of ethylene (left) and cyclopropenylidene (right) and the rules ${\\bf R}_{\\mathrm{Mol}}$ (5) and $\\mathbf{R}_{\\mathrm{Out}}$ (6). The input signal is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges of the same color denote shared weights in a layer. For more details see Appendix A.4. ", "page_idx": 11}, {"type": "text", "text": "490 A Appendix / supplemental material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "491 A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "492 To show the equivalence between the two layers it suffices to show that their weight matrices coincide.   \n493 In case of fully connected layers we have to show that the weight matrix $W_{\\mathbf{R}_{\\mathrm{FC}}(x)}\\in\\mathbb{R}^{m\\times n}$ is fliled   \n494 with $n\\cdot m$ distinct weights. This can be easily checked by computing $W_{\\mathbf{R}_{\\mathrm{FC}}(x)}$ using the definition   \n495 of the weight distribution based on the rule function in (3). ", "page_idx": 11}, {"type": "text", "text": "496 A.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "497 Instead of the original two-dimensional image of size $n\\times m$ we consider a reshaped vector $\\boldsymbol{x}\\in\\mathbb{R}^{n\\cdot m}$   \n498 as our definition of rule based layers is restricted to simple vector matrix multiplication. The output   \n499 vector of dimension $(n{-}N{+}1){\\cdot}(m{-}N{+}1)$ can then again be reshaped into a two-dimensional image   \n500 of size $(n-N+1)\\times(m-N+1)$ . Unfortunately, the reshaping makes the rule function complicated   \n501 as the indices of the reshaped vector have to be mapped to the indices of the two-dimensional image.   \n502 First note that convolution with a $N\\times N$ kernel corresponds to matrix-vector multiplication of a   \n503 doubly block circulant matrix that is a special case of a block Toeplitz matrix. Hence, to show the   \n504 equivalence between the layers we have to compare the weight matrices and show that the entries in   \n505 $\\bar{W}_{\\mathbf{R}_{\\mathrm{CNN}}(x)}\\in\\mathbb{R}^{(n-N+1)\\cdot(\\dot{m}-N+1)\\times n\\cdot m}$ exactly matches the entries in the block Toeplitz matrix of   \n506 the same dimension that corresponds to the convolution kernel. Comparing the definition of block   \n507 Toeplitz matrices with the above given rule shows that the rule exactly returns the entries of the block   \n508 Toeplitz matrix. Hence, the multiplication of $x$ with $W_{\\mathbf{R}_{\\mathrm{CNN}}}(x)$ is equivalent to multiplication of $x$   \n509 with the block Toeplitz matrix that is equivalent to the convolution of $x$ with a kernel of size $N\\times N$ . ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "510 A.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "511 The proof of Proposition 3 follows directly from the definitions of the rule based layers, see (1), and   \n512 the rule functions, see (2). If the order of the nodes in the graph is permuted and rule function is   \n513 permutation equivariant, then the node labels are permuted accordingly. Hence, the positions of the   \n514 weights in the weight matrix and the bias term are permuted in the same way as the node labels. Thus,   \n515 the result of $f$ , i.e., the multiplication of permuted weight matrix with the permuted input signal, is   \n516 the same as the permutation of the result of the multiplication of the original weight matrix with the   \n517 original input signal. ", "page_idx": 11}, {"type": "text", "text": "518 A.4 Example: RuleGNN for Molecule Graphs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "519 Assume the task is to learn a property of a molecule based on its graph structure. In this example we   \n520 present a RuleGNN that is a concatenation of two very simple rule based layers. The advantage of   \n521 rule based layers and hence also RuleGNNs is that they encode the graph structure (in this example   \n522 the structure of two molecules) directly into the neural network. Moreover, the input samples can be   \n523 arbitrary molecule graphs and the output is a vector of fixed size $k$ that encodes the property of the   \n524 molecule or some intermediate vectorial representation. In this example we consider the molecule   \n525 graphs of ethylene and cyclopropenylidene given in Figure 3 together with their corresponding input ", "page_idx": 11}, {"type": "image", "img_path": "nMFVdphOc9/tmp/daade021961a7fdd8d9bf7fe91f5a935afda8bf5fb142426e2a85bee2ed58c20.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "nMFVdphOc9/tmp/4a26a6947b31161765d6dcf47fc15a2db77f3ede0a3ee56f0aeac51904610be5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 3: Molecule graphs of ethylene (left) and cyclopropenylidene (right). The indices denote the order of the nodes. ", "page_idx": 12}, {"type": "text", "text": "526 signals $x\\in\\mathbb{R}^{6}$ and $y\\in\\mathbb{R}^{5}$ . The atoms of the molecules (hydrogen $H$ and carbon $C$ ) correspond   \n527 to the nodes of a graph and the bond types (single and double) correspond to the edges. The atom   \n528 labels and the atom bond types can be seen as additional information I that is known about the input   \n529 samples. The graph nodes are indexed via integers in some arbitrary but fixed order and the atom   \n530 corresponding to a graph node are given by the labeling function $l:\\dot{V}\\rightarrow\\{H,C\\}$ .   \n531 The RuleGNN consists of two rule based layers $f_{1}(,\\Theta_{1},\\mathbf{R}_{\\mathrm{Mol}})$ and $f(,\\Theta_{2},\\mathbf{R}_{\\mathrm{Out}})$ with learnable   \n532 parameters $\\Theta_{1}=\\{w_{1},\\dots,w_{6}\\}$ and $\\Theta_{2}=\\{w_{1}^{\\prime},\\ldots,w_{2\\cdot k}^{\\prime}\\}$ and the following rule functions ${\\bf R}_{\\mathrm{Mol}}$   \n533 and $\\mathbf{R}_{\\mathrm{Out}}$ . For some graph $G=(V,E)$ and its corresponding input signal $z$ we define ${\\bf R}_{\\mathrm{Mol}}$ as   \n534 follows: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mathbf{R}_{\\mathrm{Mol}}(z):}&{[|V|]\\times[|V|]}&{\\longrightarrow}&{\\{0\\}\\cup[6]}\\\\ &&{}&&{\\left(\\begin{array}{l l}{1}&{\\mathrm{if~}i=j\\mathrm{~and~}l(i)=H}\\\\ {2}&{\\mathrm{if~}i=j\\mathrm{~and~}l(i)=C}\\\\ {3}&{\\mathrm{if~}(i,j)\\mathrm{~is~an~edge~}(-),l(i)=H,l(j)=C}\\\\ {4}&{\\mathrm{if~}(i,j)\\mathrm{~is~an~edge~}(-),l(i)=C,l(j)=H}\\\\ {5}&{\\mathrm{if~}(i,j)\\mathrm{~is~an~edge~}(-),l(i)=l(j)=C}\\\\ {6}&{\\mathrm{if~}(i,j)\\mathrm{~is~an~edge~}(=),l(i)=l(j)=C}\\\\ {0}&{\\mathrm{o.w.}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "535 For some graph $G=(V,E)$ and its corresponding input signal $z$ we define $\\mathbf{R}_{\\mathrm{Out}}$ as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l l l l}{\\mathbf{R}_{\\mathrm{Out}}(z):}&{[|V|]\\times[k]}&{\\longrightarrow}&{\\{0\\}\\cup[2\\cdot k]}\\\\ &&&\\\\ &&{(i,j)}&{\\mapsto}&{\\left\\{\\begin{array}{l l}{1\\cdot j}&{l(i)=H}\\\\ {2\\cdot j}&{l(i)=C}\\\\ {0}&{0.\\mathrm{w}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "536 Note that ${\\bf R}_{\\mathrm{Mol}}$ and $\\mathbf{R}_{\\mathrm{Out}}$ are not restricted to the two molecules from above but can be applied   \n537 to arbitrary molecule graphs. Indeed, applying it to molecules with atom labels different from $H$   \n538 or $C$ makes the rules less powerful, i.e., it should be adapted to the type of molecules. Using the   \n539 definition (3) of weight distribution defined by the rule function we can construct the weight matrices   \n540 $W_{\\mathbf{R}_{\\mathrm{Mol}}(x)},W_{\\mathbf{R}_{\\mathrm{Out}}(x)}$ for the ethylene graph and $W_{\\mathbf{R}_{\\mathrm{Mol}}(y)},W_{\\mathbf{R}_{\\mathrm{Out}}(y)}$ for the cyclopropenylidene   \n541 graph as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nW_{\\mathbf{R}_{\\mathrm{Mol}}(x)}=\\left(\\begin{array}{l l l l l}{w_{1}}&{0}&{0}&{w_{3}}&{0}\\\\ {0}&{w_{1}}&{0}&{w_{3}}&{0}\\\\ {0}&{0}&{w_{1}}&{0}&{w_{3}}\\\\ {w_{4}}&{w_{4}}&{0}&{w_{2}}&{w_{5}}\\\\ {0}&{0}&{w_{4}}&{w_{4}}&{w_{5}}&{w_{2}}\\end{array}\\right)\\quad\\;W_{\\mathbf{R}_{\\mathrm{Out}}(x)}=\\left(\\begin{array}{l l l l l}{w_{1}^{\\prime}}&{w_{1}^{\\prime}}&{w_{1}^{\\prime}}&{w_{1}^{\\prime}}&{w_{2}^{\\prime}}&{w_{2}^{\\prime}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {w_{2k-1}^{\\prime}\\;w_{2k-1}^{\\prime}\\;w_{2k-1}^{\\prime}\\;w_{2k-1}^{\\prime}\\;w_{2k-1}^{\\prime}\\;w_{2k}^{\\prime}\\;w_{2k}^{\\prime}\\;w_{2k}^{\\prime}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nW_{\\mathbf{R}_{\\mathrm{Mol}}(y)}=\\left(\\!\\!\\begin{array}{c c c c}{\\!w_{1}\\!}&{\\!0\\!}&{\\!w_{3}\\!}&{\\!0\\!}&{\\!0}\\\\ {\\!0\\!}&{\\!w_{1}\\!}&{\\!0\\!}&{\\!w_{3}\\!}&{\\!0}\\\\ {\\!w_{4}\\!}&{\\!0\\!}&{\\!w_{2}\\!}&{\\!w_{6}\\!}&{\\!w_{5}}\\\\ {\\!0\\!}&{\\!w_{3}\\!}&{\\!w_{6}\\!}&{\\!w_{5}\\!}\\\\ {\\!0\\!}&{\\!0\\!}&{\\!w_{5}\\!}&{\\!w_{2}\\!}\\end{array}\\!\\!\\right)\\!\\!\\qquad\\qquad W_{\\mathbf{R}_{\\mathrm{Out}}(y)}=\\left(\\!\\!\\begin{array}{c c c c}{\\!w_{1}^{\\prime}}&{\\!w_{1}^{\\prime}}&{\\!w_{2}^{\\prime}}&{\\!w_{2}^{\\prime}}&{\\!w_{2}^{\\prime}}\\\\ {\\!\\vdots}&{\\!\\vdots}&{\\!\\vdots}&{\\!\\vdots}\\\\ {\\!w_{2k-1}^{\\prime}\\!}&{\\!w_{2k-1}^{\\prime}\\!}&{\\!w_{2k}^{\\prime}\\!}&{\\!w_{2k}^{\\prime}\\!}&{\\!w_{2k}^{\\prime}}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "542 Combining the two rule based layers we obtain the RuleGNN and the forward propagation is given   \n543 by $\\sigma(W_{\\mathrm{{ROut}}\\,(x)}\\cdot\\sigma(W_{\\mathrm{{RMol}}\\,(x)}\\cdot x))$ for the ethylene graph and $\\sigma(W_{\\mathbf{ROut}(y)}\\cdot\\sigma(W_{\\mathbf{R}_{\\mathrm{Mol}}(y)}\\cdot y))$ for the   \n544 cyclopropenylidene graph.   \n545 Note that the forward propagation of the layer corresponding to the rule ${\\bf R}_{\\mathrm{Mol}}$ is kind of a multiplica  \n546 tion with a weighted adjacency matrix of the graph where the weights of the adjacency matrix are   \n547 given by the learnable parameters, see also Figure 2. In contrast to adjacency matrices the weight   \n548 matrix is not necessary symmetric. The computation graph induced by the weight matrix exactly   \n549 represent the graph structure while the edge weights are shared across the network using the rule, see   \n550 Figure 2. Note that the above defined rule is very flexible as also edge labels (e.g., atomic bonds)   \n551 can be taken into account by increasing the size of the weight set. Moreover, it is possible to include   \n552 bigger neighbourhoods, i.e., all nodes reachable by $k$ -hops. Of course using other information of   \n553 the graph (e.g., substructures (such as circles or cliques), node degrees, connections not depicted by   \n554 edges) more complicated rules can be defined. ", "page_idx": 12}, {"type": "table", "img_path": "nMFVdphOc9/tmp/0f984b5af32ac8d4f159a256fe5c99effe4f4b7f5f090883094c1936570d0b22.jpg", "table_caption": [], "table_footnote": ["Table 3: Details on the real-world datasets used in the experiments. The datasets are from the TU Dortmund Graph Database [13]. "], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "555 A.5 Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "556 In this section we provide additional details on the datasets used in the experiments. Table 3 shows   \n557 an overview of the real-world datasets and Table 4 provides an overview of the synthetic datasets.   \n558 We consider the following synthetic datasets. The CSL dataset is from []. We constructed the other   \n559 datasets to demonstrate the strength of our approach to encode expert knowledge into the neural   \n560 network.   \n561 LongRings LongRings consists of 1200 cycles of 100 nodes each. Four nodes are labeled by   \n562 $1,2,3,4$ and all other nodes are labeled by 0. The distance between each pair of the four nodes is   \n563 exactly 25 or 50. The label of the graph is 0 if 1 and 2 have distance 50, 1 if 1 and 3 have distance   \n564 50 and 2 if 1 and 4 have distance 50. There are 400 graphs for each class. The difficulty of the   \n565 classification task is that information has to be propagated over a long distance. Regarding RuleGNNs   \n566 this is very easy because if the expert knows that distance 50 is relevant we can define an appropriate   \n567 rule.   \n568 EvenOddRings EvenOddRings consists of 1200 cycles of 16 nodes each. The nodes in each graph   \n569 are labeled from 0 to 15. The graph label is based on the labels of the nodes that have distance 8   \n570 respectively 4 to the node with label 0. We denote them by $x$ resp. $y,z$ . We distinct four cases: $x$ is   \n571 even and $y+z$ is even, $x$ is even and $y+z$ is odd, $x$ is odd and $y+z$ is even, $x$ is odd and $y+z$ is   \n572 odd. There are 300 graphs for each class, i.e., each of the four cases. The expert knowledge we use is   \n573 that the information has to be collected from nodes of distance 8 and 4.   \n574 EvenOddRingsCount EvenOddRingsCount consists of the same graphs as EvenOddRings but   \n575 the graph labels are different. For all nodes and their opposite node in the circle the sum of the   \n576 labels is computed. If there are more even sums than odd sums the graph is labeled by 0 and by 1   \n577 otherwise. There are 600 graphs for each class. The expert knowledge we use is the information that   \n578 only distance 8 is relevant.   \n579 Snowflakes Snowflakes is a dataset consisting of graphs proposed by [15] that are not distinguish  \n580 able by the 1-WL test, see Figure 4 for an example. The dataset consists of circles of length 3 to 12   \n581 and at each circle node a graph from $M_{0},M_{1},M_{2}$ or $M_{3}$ is attached, see Figure 5 and [15] for the   \n582 details. $M_{0},M_{1},M_{2}$ and $M_{3}$ are non-isomorphic graphs that are not distinguishable by the 1-WL   \n583 test. One label in the circle is labeled by 1 and all other nodes are labeled by 0. The label of the graph   \n584 is determined by the graph $M_{0},M_{1},M_{2}$ or $M_{3}$ that is attached to the circle node with label 1. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "nMFVdphOc9/tmp/5cf97c731d319b4651ebb0a592f3b0c2e476f2f065b38f6636a272af6a112a6e.jpg", "img_caption": ["Figure 4: Example graphs from the Snowflakes dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "nMFVdphOc9/tmp/7a36ec0c97056a2ab34579303569e03b184ea5236f63f355f8499a44ed034893.jpg", "img_caption": ["Figure 5: The graphs $M_{0},M_{1},M_{2},M_{3}$ from [15] that are not distinguishable by the 1-WL test. "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "nMFVdphOc9/tmp/4113495ef8a20c2d470f81c867a96dea2149ea04b3a3bcf8d7d88b4e7cd3cd76.jpg", "table_caption": [], "table_footnote": ["Table 4: Details of the synthetic datasets used in the experiments. The CSL dataset is from [4]. "], "page_idx": 14}, {"type": "text", "text": "585 A.6 RuleGNNs: Runtimes ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "586 Table 5 shows more details of the RuleGNN model. In particular, we see that except for the DHFR   \n87 dataset we need less than 12 epochs on average to reach the best result. This shows that our approach   \n88 is very efficient and converges quickly. At the first glance the average time per epoch seems to be very   \n89 high. This has two reasons. One is also mentioned in [9] that there is a gap between the theoretical   \n90 and practical runtime of dynamic neural networks because the implementation in PyTorch is not   \n9 optimized for dynamic neural networks. The other reason is that we parallelized the computation, i.e.,   \n92 we are able to run all the three runs and 10 folds in parallel on the same machine. Of course, this   \n93 produces some overhead. As stated above the preprocessing times are not relevant for the experiments   \n94 as they are only needed once. The third column shows the time needed to compute all the pairwise   \n95 distances between the nodes of the graph. The fourth column shows the time needed to compute the   \n96 node labels used for the best model. The most preprocessing time is needed for IMDB-BINARY and   \n97 IMDB-MULTI because the graphs are much denser than the other datasets. For the synthetic datasets   \n598 except for CSL we do not need any label preprocessing time as the original node labels are used. ", "page_idx": 14}, {"type": "text", "text": "599 A.7 RuleGNNs: Architectures and Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "600 Table 6 provides an overview of the different architectures used in the experiments that achieved   \n601 the best results. One advantage of our approach is that messages can be passed over long distances.   \n602 Hence, except for the EvenOddRings dataset we used only one layer and the output layer. In case   \n603 of NCI1, NCI109, Mutagenicity it turns out that the best model uses the Weisfeiler-Leman rule   \n604 with $k\\,=\\,2$ iterations. We restricted the number of maximum labels considered to 500 which   \n605 results in 250000 learnable parameters for the weight matrix and 500 for the bias vector. For the   \n606 output layer we used the bound of 50000 learnable parameters which was larger than the number of   \n607 different Weisfeiler-Leman labels in the second iteration. Interestingly, for NCI1 and NCI109 the   \n608 best validation accuracy was achieved if considering node pairs with maximum distance 10. In case   \n609 of Mutagenicity the best model uses only node pairs with distance 3 although we also considered   \n610 the hyperparameter $d=10$ . We also tested different small patterns, e.g., simple cycles, but they   \n611 did not improve the results. For DHFR this was different as the best model uses the pattern (simple   \n612 cycles with length at most 10) for the output layer. We also tested the Weisfeiler-Leman rule in this   \n613 case but the validation accuracy was lower. For IMDB-BINARY and IMDB-MULTI the best model   \n614 uses the pattern (simple cycles with length at most 10, triangle, edge). Note that the embedding of   \n615 one edge as a pattern is equivalent to the degree of the node. We also tested the Weisfeiler-Leman   \n616 rule but the validation accuracy was lower. All in all we considered many different rules from type   \n617 Weisfeiler-Leman and patterns but of course we did not test all possible rules. A full list of tested   \n618 hyperparameters can be found here. As a next step it would be interesting to consider more rules,   \n619 rules that come from expert knowledge or also deeper architectures with more rule based layers   \n620 concatenated. Regarding the number of learnable parameters we would like to mention that the   \n621 number is relatively high but lots of parameters are not used in the weight matrix. Hence, it might be   \n622 possible to prune the set of learnable parameters by removing those that are not used or those that   \n623 have a small absolute value.   \n624 For the synthetic datasets we use \u201cexpert knowledge\u201d to define the rules. Hence we did not tested   \n625 other rules than those in Table 6. For LongRings, EvenOddRings and EvenOddRingsCount we used   \n626 the original node labels for the rule based layers. Moreover, instead considering learnable parameters   \n627 for all node pairs of certain labels with distance smaller or equal to $d$ we considered only the node   \n628 pairs with distance $d$ (denoted by \u201conly: $d^{\\ast}$ ). In case of EvenOddRings we used two layers. The first   \n629 layer that considers only node pairs with distance 8 collects all the necessary information of opposite   \n630 nodes. The second layer that considers only node pairs with distance 4 collects the information of   \n631 the nodes that are 4 hops away from the nodes with label 0, see also Figure 6. For CSL we used as   \n632 patterns all simple cycles with length at most 10. For the Snowflakes dataset we used the patterns   \n633 cycle of length 4 and 5 and collect the information of the nodes that have pairwise distance 3. In this   \n634 way the RuleGNN is able to distinguish the graphs $M_{0},M_{1},M_{2}$ and $M_{3}$ that are not distinguishable   \n635 by the 1-WL test. In the output layer we used the Weisfeiler-Leman rule with $k=2$ iterations to   \n636 collect the relevant information from nodes with different Weisfeiler-Leman labels. ", "page_idx": 14}, {"type": "table", "img_path": "nMFVdphOc9/tmp/5cbd86844bcaf7506c11902757e1bdd1d79aeea4126e272dbc71c23afa6691ab.jpg", "table_caption": [], "table_footnote": ["Table 5: Runtimes and preprocessing times of the different datasets used in the experiments. All values are averaged over the best runs. The first column shows the best epoch (highest validation accuracy), the second column shows the average time per epoch, the third column shows the time needed to compute all the pairwise distances between the nodes of the graph, the fourth column shows the time needed to compute the node labels used for the best model and the last column shows the number of graphs in the dataset. "], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "637 A.8 RuleGNNs: Interpretability ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "638 One advantage of our approach is that each weight can be interpreted, i.e., we can see the relevance   \n639 of two nodes $i,j$ in a graph with labels $l(i),l(j)$ and distance $d(i,j)$ . Figure 6 shows an example of   \n640 the learned parameters for some synthetic dataset. Figure 1 shows an example of the relevance of the   \n641 weights for a graph from the DHFR dataset using the weights of the best model. Considering Figure 6b   \n642 we can see that in the first layer the RuleGNN passes the messages between opposite nodes as given   \n643 by the rule. In the second layer it has learned to collect the information from the nodes that have   \n644 distance 4 to the node with label 0 (dark blue node) all other connections of distance 4 have a smaller   \n645 weight. ", "page_idx": 15}, {"type": "table", "img_path": "nMFVdphOc9/tmp/3c081e0489f6b28fec4c69187f35ab627ce5bd41d3519dfb3964d37a5810894b.jpg", "table_caption": ["Table 6: Overview over the hyperparameters of the best models. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "nMFVdphOc9/tmp/ec9c464f0c4c023d9e1cf2526438aac9d470a1a27d3a8d16ac332c3bae28a350.jpg", "img_caption": ["Figure 6: Visualization of the learned weights and biases for the RuleGNN on the EvenOddRingsCount (a), EvenOddRings (b) and Snowflakes (c) dataset. The first column shows the graphs and the colors of the nodes represent the different node labels. The other columns show the learned weights and biases of the RuleGNN for the respective rule based layer. The message passing weights are visualized by arrows (thicker for higher absolute values) and the biases are visualized by the size of the node (red for positive and blue for negative weights). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "646 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Justification: The theoretical and experimentally claims made in the abstract are consistent with the results presented in the paper and reflect the contributions made. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In the conclusion and also in the experiments section we discuss the limitations of the approach. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "02 \u2022 The answer NA means that the paper does not include theoretical results.   \n03 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n04 referenced.   \n05 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n06 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n07 they appear in the supplemental material, the authors are encouraged to provide a short   \n08 proof sketch to provide intuition.   \n09 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n10 by formal proofs provided in appendix or supplemental material.   \n1 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "712 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "713 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n714 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n715 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Justification: We give all information that is needed to reproduce the experimental results and also provide the code and data which is not online. ", "page_idx": 18}, {"type": "text", "text": "0 \u2022 The answer NA means that the paper does not include experiments.   \n1 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n2 well by the reviewers: Making the paper reproducible is important, regardless of   \n3 whether the code and data are provided or not.   \n4 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n5 to make their results reproducible or verifiable.   \n6 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully   \n8 might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same   \n0 dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed   \n2 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n3 of a large language model), releasing of a model checkpoint, or other means that are   \n34 appropriate to the research performed.   \n35 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n6 sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n8 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n9 to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n2 (c) If the contribution is a new model (e.g., a large language model), then there should   \n3 either be a way to access this model for reproducing the results or a way to reproduce   \n4 the model (e.g., with an open-source dataset or instructions for how to construct   \n5 the dataset).   \n6 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n7 authors are welcome to describe the particular way they provide for reproducibility.   \n8 In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers   \n0 to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "751 5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "752 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n753 tions to faithfully reproduce the main experimental results, as described in supplemental   \n754 material?   \n755 Answer: [Yes]   \n756 Justification: We provide open access to the code, datasplits and synthetic datasets used in   \n757 the paper.   \n758 Guidelines:   \n759 \u2022 The answer NA means that paper does not include experiments requiring code.   \n760 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n761 public/guides/CodeSubmissionPolicy) for more details.   \n762 \u2022 While we encourage the release of code and data, we understand that this might not be   \n763 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n764 including code, unless this is central to the contribution (e.g., for a new open-source   \n765 benchmark).   \n766 \u2022 The instructions should contain the exact command and environment needed to run to   \n767 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n768 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n769 \u2022 The authors should provide instructions on data access and preparation, including how   \n770 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n771 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n772 proposed method and baselines. If only a subset of experiments are reproducible, they   \n773 should state which ones are omitted from the script and why.   \n774 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n775 versions (if applicable).   \n776 \u2022 Providing as much information as possible in supplemental material (appended to the   \n777 paper) is recommended, but including URLs to data and code is permitted.   \n778 6. Experimental Setting/Details   \n779 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n780 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n781 results?   \n782 Answer: [Yes]   \n783 Justification: We provide all data splits and hyperparameter choices for our algorithm in   \n784 the paper. Moreover, in the code we have understandable config files that contain all the   \n785 hyperparameters used in the experiments.   \n786 Guidelines:   \n787 \u2022 The answer NA means that the paper does not include experiments.   \n788 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n789 that is necessary to appreciate the results and make sense of them.   \n790 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n791 material.   \n792 7. Experiment Statistical Significance   \n793 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n794 information about the statistical significance of the experiments?   \n795 Answer: [Yes]   \n796 Justification: We use standard deviation to show the variability of the results. We do   \n797 not use statistical significance tests because our main claim is not that our method is   \n798 significantly better than state-of-the-art methods, but that it is an interesting new approach   \n799 that is applicable to a wide range of tasks.   \n800 Guidelines:   \n801 \u2022 The answer NA means that the paper does not include experiments.   \n802 \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confi  \n803 dence intervals, or statistical significance tests, at least for the experiments that support   \n804 the main claims of the paper.   \n805 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n806 example, train/test split, initialization, random drawing of some parameter, or overall   \n807 run with given experimental conditions).   \n808 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n809 call to a library function, bootstrap, etc.)   \n810 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n811 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n812 of the mean.   \n813 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n814 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n815 of Normality of errors is not verified.   \n816 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n817 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n818 error rates).   \n819 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n820 they were calculated and reference the corresponding figures or tables in the text.   \n821 8. Experiments Compute Resources   \n822 Question: For each experiment, does the paper provide sufficient information on the com  \n823 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n824 the experiments?   \n825 Answer: [Yes]   \n826 Justification: We specify the computer resources and the time needed to run the experiments   \n827 in the paper.   \n828 Guidelines:   \n829 \u2022 The answer NA means that the paper does not include experiments.   \n830 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n831 or cloud provider, including relevant memory and storage.   \n832 \u2022 The paper should provide the amount of compute required for each of the individual   \n833 experimental runs as well as estimate the total compute.   \n834 \u2022 The paper should disclose whether the full research project required more compute   \n835 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n836 didn\u2019t make it into the paper).   \n837 9. Code Of Ethics   \n838 Question: Does the research conducted in the paper conform, in every respect, with the   \n839 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n840 Answer: [Yes]   \n841 Justification: The research conducted in the paper conforms with the NeurIPS Code of   \n842 Ethics.   \n843 Guidelines:   \n844 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n845 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n846 deviation from the Code of Ethics.   \n847 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n848 eration due to laws or regulations in their jurisdiction).   \n849 10. Broader Impacts   \n850 Question: Does the paper discuss both potential positive societal impacts and negative   \n851 societal impacts of the work performed?   \n852 Answer: [NA]   \n853 Justification: The paper does not address societal impact because we present a very basic   \n854 approach that is not directly applicable to any specific societal problem.   \n855 Guidelines:   \n856 \u2022 The answer NA means that there is no societal impact of the work performed.   \n857 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n858 impact or why the paper does not address societal impact.   \n859 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n860 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n861 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n862 groups), privacy considerations, and security considerations.   \n863 \u2022 The conference expects that many papers will be foundational research and not tied   \n864 to particular applications, let alone deployments. However, if there is a direct path to   \n865 any negative applications, the authors should point it out. For example, it is legitimate   \n866 to point out that an improvement in the quality of generative models could be used to   \n867 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n868 that a generic algorithm for optimizing neural networks could enable people to train   \n869 models that generate Deepfakes faster.   \n870 \u2022 The authors should consider possible harms that could arise when the technology is   \n871 being used as intended and functioning correctly, harms that could arise when the   \n872 technology is being used as intended but gives incorrect results, and harms following   \n873 from (intentional or unintentional) misuse of the technology.   \n874 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n875 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n876 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n877 feedback over time, improving the efficiency and accessibility of ML).   \n878 11. Safeguards   \n879 Question: Does the paper describe safeguards that have been put in place for responsible   \n880 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n881 image generators, or scraped datasets)?   \n882 Answer: [NA]   \n883 Justification: We do not use scraped datasets or models that have a high risk for misuse.   \n884 Guidelines:   \n885 \u2022 The answer NA means that the paper poses no such risks.   \n886 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n887 necessary safeguards to allow for controlled use of the model, for example by requiring   \n888 that users adhere to usage guidelines or restrictions to access the model or implementing   \n889 safety filters.   \n890 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n891 should describe how they avoided releasing unsafe images.   \n892 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n893 not require this, but we encourage authors to take this into account and make a best   \n894 faith effort.   \n895 12. Licenses for existing assets   \n896 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n897 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n898 properly respected?   \n899 Answer: [Yes]   \n900 Justification: We mention all creators and original owners of code and data we use in the   \n901 paper.   \n902 Guidelines:   \n903 \u2022 The answer NA means that the paper does not use existing assets.   \n904 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n905 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n906 URL.   \n907 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n908 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n909 service of that source should be provided.   \n910 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n911 package should be provided. For popular datasets, paperswithcode.com/datasets   \n912 has curated licenses for some datasets. Their licensing guide can help determine the   \n913 license of a dataset.   \n914 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n915 the derived asset (if it has changed) should be provided.   \n916 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n917 the asset\u2019s creators.   \n918 13. New Assets   \n919 Question: Are new assets introduced in the paper well documented and is the documentation   \n920 provided alongside the assets?   \n921 Answer: [NA]   \n922 Justification: The paper does not introduce new assets.   \n923 Guidelines:   \n924 \u2022 The answer NA means that the paper does not release new assets.   \n925 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n926 submissions via structured templates. This includes details about training, license,   \n927 limitations, etc.   \n928 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n929 asset is used.   \n930 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n931 create an anonymized URL or include an anonymized zip file.   \n932 14. Crowdsourcing and Research with Human Subjects   \n933 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n934 include the full text of instructions given to participants and screenshots, if applicable, as   \n935 well as details about compensation (if any)?   \n936 Answer: [NA]   \n937 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n938 Guidelines:   \n939 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n940 human subjects.   \n941 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n942 tion of the paper involves human subjects, then as much detail as possible should be   \n943 included in the main paper.   \n944 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n945 or other labor should be paid at least the minimum wage in the country of the data   \n946 collector.   \n947 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n948 Subjects   \n949 Question: Does the paper describe potential risks incurred by study participants, whether   \n950 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n951 approvals (or an equivalent approval/review based on the requirements of your country or   \n952 institution) were obtained?   \n953 Answer: [NA]   \n954 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n955 Guidelines:   \n956 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n957 human subjects. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]