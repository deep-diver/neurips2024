[{"figure_path": "CwNevJONgq/figures/figures_1_1.jpg", "caption": "Figure 1: Overview: World models are commonly used to predict latent trajectories, predict sequences of pixel observations, and perform planning. We propose an architecture together with an information bottleneck for learning simple and parsimonious world models. Our method relies on a query network that extracts a sparse representation ht for predicting latent transition dynamics. Combining our method with auxiliary loss functions for i) contrastive learning ii) planning and iii) and model-free RL, we see consistent performance improvement in all domains. Lines and bars show mean performance from three sets of RL benchmarks. Error bars represent 95% confidence interval.", "description": "This figure provides a high-level overview of the Parsimonious Latent Space Model (PLSM) and its application in various reinforcement learning scenarios. It illustrates how PLSM, combined with different model classes (Contrastive World Models, Self-Predictive Representations), improves performance across multiple tasks such as model-free RL (Atari), continuous control (DeepMind Control Suite), and control with visual distractions. The figure showcases the architecture's components (encoder, query network, and dynamics), along with performance results demonstrated through graphs.", "section": "1 Introduction"}, {"figure_path": "CwNevJONgq/figures/figures_2_1.jpg", "caption": "Figure 2: The heart (left) can appear on any x, y coordinate in a two-dimensional latent space with boundaries, on which it can transition in 9 different ways (moving in eight directions and standing still, for instance when moving into a boundary). Encouraging dynamics to be parsimonious recovers these 9 different possible transitions (see right), whereas an unconstrained model (see center) does not.", "description": "This figure visually demonstrates the effect of the Parsimonious Latent Space Model (PLSM) on latent dynamics.  The left panel shows a simple 2D latent space where an agent (represented by a heart) can move in nine possible directions, including staying still. The center panel illustrates the distribution of changes in latent states (\u0394z) for an unconstrained model. This distribution is diffuse, indicating that the model doesn't capture the systematic nature of the agent's actions. The right panel shows the same distribution for the PLSM. Here, the distribution is much more concentrated, revealing the improved predictability of the latent dynamics due to regularization.", "section": "2 Latent dynamics"}, {"figure_path": "CwNevJONgq/figures/figures_4_1.jpg", "caption": "Figure 3: PLSM, when incorporated into either the TD-MPC algorithm (A), or RePo (B), improves planning in continuous control tasks with high-dimensional and complex dynamics, and visual distractions, respectively. Lines show the average return attained across 15 evaluation episodes, averaged over five seeds. The shaded region represents the 95% confidence interval.", "description": "This figure displays the results of experiments comparing the performance of TD-MPC and RePo algorithms with and without the proposed PLSM method in continuous control tasks.  Subfigure (A) shows results for five DeepMind Control Suite tasks, while subfigure (B) shows results for five Distracting Control Suite tasks.  The y-axis represents the average episode return, while the x-axis represents the number of environment steps.  Shaded areas represent 95% confidence intervals. The results demonstrate that incorporating PLSM improves the average return in most of the tasks, indicating the benefit of the proposed method for improving the efficiency and generalization of planning algorithms in complex scenarios.", "section": "3 Parsimonious dynamics for Reinforcement Learning"}, {"figure_path": "CwNevJONgq/figures/figures_5_1.jpg", "caption": "Figure 4: Changing the dynamics model in SPR to PLSM increases score in several Atari games, with little implementation overhead. On average, human normalized scores are higher when using PLSM dynamics. Bars show difference in human normalized score between SPR with and without PLSM dynamics, averaged over five seeds.", "description": "This figure shows the results of applying the Parsimonious Latent Space Model (PLSM) to several Atari games using the Self-Predictive Representations (SPR) model. The PLSM model improves the performance of the SPR model in most games, as indicated by the higher human-normalized scores. The bars in the figure show the difference in human-normalized scores between the SPR model with and without PLSM, averaged over five random seeds. The results indicate that incorporating PLSM into SPR improves the performance of the model, highlighting the effectiveness of the proposed method.", "section": "3.3 Model-free RL"}, {"figure_path": "CwNevJONgq/figures/figures_6_1.jpg", "caption": "Figure 5: PLSM improves contrastive world models' accuracy in long-horizon latent prediction in five out of six environments. In the cubes and shapes dataset, the PLSM is close to perfect even when predicting as far as 10 timesteps in the future. Lines show accuracy on entire test data averaged over five random seeds. The shaded region corresponds to the standard error of the mean.", "description": "This figure compares the performance of contrastive world models (CWM) and parsimonious latent space models (PLSM) on long-horizon latent prediction tasks across six different environments. The results demonstrate that PLSM significantly improves prediction accuracy, particularly in the cubes and shapes datasets, where it achieves near-perfect accuracy even at prediction horizons of 10 timesteps. The shaded regions represent the standard error of the mean, showing the consistency of the results.", "section": "4 Future state prediction"}, {"figure_path": "CwNevJONgq/figures/figures_7_1.jpg", "caption": "Figure 6: PLSM improves generalization and robustness in contrastive models: When exposed to scenes with fewer objects than trained on (cubes and shapes environment), or corrupted data (dSprite environment) from the test set, PLSM improves accuracy over the CWM. Lines represent the average of models trained across five seeds. Shaded regions and bars reflect the standard error of the mean.", "description": "This figure compares the performance of PLSM and CWM models on generalization and robustness tests.  The \"Novel cubes\" and \"Novel shapes\" plots show that PLSM generalizes better to unseen data with fewer objects than it was trained on, while the \"Noise \u03c3 = 0.1\" and \"Noise \u03c3 = 0.2\" plots demonstrate that PLSM is more robust to noisy data. The shaded areas and error bars represent confidence intervals.", "section": "4.2 Generalization and robustness"}, {"figure_path": "CwNevJONgq/figures/figures_7_2.jpg", "caption": "Figure 7: Latent states zt carry decodable information about the data generating factors, whereas query states ht do not. When conditioning on an action at, query states carry more information about the object that the action changes.", "description": "This figure displays a bar chart comparing the decodability (R-squared value) of generative factors from latent states (zt), object-matched query states (when the query is conditioned on the action affecting a specific object), and query states (ht) in general.  The results show that latent states (zt) have the highest decodability, meaning they retain the most information about the generative factors. Object-matched query states have intermediate decodability, while general query states (ht) have the lowest decodability. This demonstrates that the query network effectively filters out irrelevant information from the latent states, leaving only the information necessary to predict the effect of the action on the object it affects.", "section": "4.2 Generalization and robustness"}, {"figure_path": "CwNevJONgq/figures/figures_14_1.jpg", "caption": "Figure 8: To illustrate how our mutual information minimization impacts the latent space, we trained PLSM to learn the dynamics of a dot moving in four directions on a gridworld with a wall in the middle. We compared PLSM to contrastive models that regularize the L\u2081 and L2 norms of the latent space, respectively. We observe that PLSM regularization leads to a more regular representation of the states in the gridworld, whereas no regularization leads to a warped representation. Moreover, L\u2081 and L\u2082 regularization simply shrinks the latent space. This type of shrinkage is not present in PLSM.", "description": "This figure compares the latent space representations learned by PLSM and two other methods that use L1 and L2 regularization.  PLSM shows a more regular representation, while the others show a shrunken or distorted space.  It highlights how PLSM's mutual information minimization leads to better organization of the latent space.", "section": "B PLSM vs L\u2081 and L\u2082 norm regularization"}, {"figure_path": "CwNevJONgq/figures/figures_15_1.jpg", "caption": "Figure 5: PLSM improves contrastive world models' accuracy in long-horizon latent prediction in five out of six environments. In the cubes and shapes dataset, the PLSM is close to perfect even when predicting as far as 10 timesteps in the future. Lines show accuracy on entire test data averaged over five random seeds. The shaded region corresponds to the standard error of the mean.", "description": "This figure compares the performance of the proposed Parsimonious Latent Space Model (PLSM) against a baseline contrastive world model (CWM) in predicting future latent states.  It shows that PLSM significantly improves accuracy, especially at longer prediction horizons (up to 10 timesteps), across various datasets (spaceinvaders, pong, cubes with 9 objects, shapes with 9 objects, dSprites with 4 factors, and MOVi-e). The results demonstrate PLSM's effectiveness in learning more predictable and generalizable latent dynamics.", "section": "4 Future state prediction"}, {"figure_path": "CwNevJONgq/figures/figures_15_2.jpg", "caption": "Figure 5: PLSM improves contrastive world models' accuracy in long-horizon latent prediction in five out of six environments. In the cubes and shapes dataset, the PLSM is close to perfect even when predicting as far as 10 timesteps in the future. Lines show accuracy on entire test data averaged over five random seeds. The shaded region corresponds to the standard error of the mean.", "description": "This figure displays the results of long-horizon latent prediction experiments comparing the performance of the Parsimonious Latent Space Model (PLSM) against a standard contrastive world model (CWM) across six different environments.  The results show that PLSM significantly improves prediction accuracy, particularly in the cubes and shapes environment, where it achieves near-perfect accuracy even when predicting 10 timesteps into the future. The figure plots the accuracy across different prediction horizons for each environment, with error bars representing the standard error of the mean across five random seeds.", "section": "Future state prediction"}, {"figure_path": "CwNevJONgq/figures/figures_16_1.jpg", "caption": "Figure 3: PLSM, when incorporated into either the TD-MPC algorithm (A), or RePo (B), improves planning in continuous control tasks with high-dimensional and complex dynamics, and visual distractions, respectively. Lines show the average return attained across 15 evaluation episodes, averaged over five seeds. The shaded region represents the 95% confidence interval.", "description": "This figure shows the results of experiments comparing the performance of TD-MPC and RePo algorithms with and without PLSM in continuous control tasks.  Panel A displays results for TD-MPC across five different DeepMind Control Suite environments, demonstrating the improvement in average return achieved by using PLSM.  Panel B shows similar results for RePo across a different set of  environments, which also include distracting visual elements. The shaded areas represent 95% confidence intervals, highlighting the statistical significance of the observed performance differences.", "section": "3 Parsimonious dynamics for Reinforcement Learning"}, {"figure_path": "CwNevJONgq/figures/figures_16_2.jpg", "caption": "Figure 3: PLSM, when incorporated into either the TD-MPC algorithm (A), or RePo (B), improves planning in continuous control tasks with high-dimensional and complex dynamics, and visual distractions, respectively. Lines show the average return attained across 15 evaluation episodes, averaged over five seeds. The shaded region represents the 95% confidence interval.", "description": "This figure shows the results of using PLSM with two different model-based RL algorithms, TD-MPC and RePo.  The x-axis represents the number of steps, and the y-axis represents the average return across five different runs of each experiment.  The shaded regions indicate the 95% confidence interval.  In both (A) and (B), PLSM improves the performance of the respective baseline algorithm across a variety of tasks.  The results indicate that PLSM enhances the ability of these algorithms to learn effective policies in continuous control environments with complex dynamics and visual distractions.", "section": "3 Parsimonious dynamics for Reinforcement Learning"}, {"figure_path": "CwNevJONgq/figures/figures_16_3.jpg", "caption": "Figure 1: Overview: World models are commonly used to predict latent trajectories, predict sequences of pixel observations, and perform planning. We propose an architecture together with an information bottleneck for learning simple and parsimonious world models. Our method relies on a query network that extracts a sparse representation ht for predicting latent transition dynamics. Combining our method with auxiliary loss functions for i) contrastive learning ii) planning and iii) and model-free RL, we see consistent performance improvement in all domains. Lines and bars show mean performance from three sets of RL benchmarks. Error bars represent 95% confidence interval.", "description": "This figure shows an overview of the proposed architecture and results. The architecture consists of a world model with a query network that extracts a sparse representation for predicting latent transition dynamics.  Three sets of RL benchmarks show performance improvements when combining the proposed method with contrastive learning, planning, and model-free RL. The lines and bars in the graphs represent the mean performance, with error bars indicating the 95% confidence interval.", "section": "1 Introduction"}, {"figure_path": "CwNevJONgq/figures/figures_17_1.jpg", "caption": "Figure 5: PLSM improves contrastive world models\u2019 accuracy in long-horizon latent prediction in five out of six environments. In the cubes and shapes dataset, the PLSM is close to perfect even when predicting as far as 10 timesteps in the future. Lines show accuracy on entire test data averaged over five random seeds. The shaded region corresponds to the standard error of the mean.", "description": "This figure compares the long-horizon prediction accuracy of contrastive world models with and without the proposed PLSM method across six different environments.  The results demonstrate that PLSM significantly improves accuracy, particularly in the cubes and shapes dataset, where it achieves near-perfect accuracy even when predicting 10 time steps into the future.  Error bars represent the standard error of the mean, indicating confidence in the results.", "section": "4 Future state prediction"}, {"figure_path": "CwNevJONgq/figures/figures_17_2.jpg", "caption": "Figure 5: PLSM improves contrastive world models' accuracy in long-horizon latent prediction in five out of six environments. In the cubes and shapes dataset, the PLSM is close to perfect even when predicting as far as 10 timesteps in the future. Lines show accuracy on entire test data averaged over five random seeds. The shaded region corresponds to the standard error of the mean.", "description": "The figure shows the performance of contrastive world models with and without parsimonious latent space model (PLSM) regularization on six different datasets. The x-axis represents the prediction horizon (how many steps into the future the model is predicting), and the y-axis represents the accuracy of the prediction. For most datasets, PLSM improves accuracy, particularly for longer prediction horizons. The error bars indicate the standard error of the mean.", "section": "4 Future state prediction"}]