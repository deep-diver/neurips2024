[{"type": "text", "text": "Learning Mixtures of Unknown Causal Interventions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Abhinav Kumar LIDS, Massachusetts Institute of Technology Broad Institute of MIT and Harvard akumar03@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Kirankumar Shiragur Microsoft Research kshiragur@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Caroline Uhler LIDS, Massachusetts Institute of Technology Broad Institute of MIT and Harvard ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability to conduct interventions plays a pivotal role in learning causal relationships among variables, thus facilitating applications across diverse scientific disciplines such as genomics, economics, and machine learning. However, in many instances within these applications, the process of generating interventional data is subject to noise: rather than data being sampled directly from the intended interventional distribution, interventions often yield data sampled from a blend of both intended and unintended interventional distributions. ", "page_idx": 0}, {"type": "text", "text": "We consider the fundamental challenge of disentangling mixed interventional and observational data within linear Structural Equation Models (SEMs) with Gaussian additive noise without the knowledge of the true causal graph. We demonstrate that conducting interventions, whether do or soft, yields distributions with sufficient diversity and properties conducive to efficiently recovering each component within the mixture. Furthermore, we establish that the sample complexity required to disentangle mixed data inversely correlates with the extent of change induced by an intervention in the equations governing the affected variable values. As a result, the causal graph can be identified up to its interventional Markov Equivalence Class, similar to scenarios where no noise influences the generation of interventional data. We further support our theoretical findings by conducting simulations wherein we perform causal discovery from such mixed data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Interventions are experiments that can help us understand the mechanisms governing complex systems and also modify these systems to achieve desired outcomes [1, 5, 21]. For example, in causal discovery, interventions are used to infer causal relationships between variables of interest, which has applications in various fields such as biology [5, 12], economics [8], and psychology [13, 21]. ", "page_idx": 0}, {"type": "text", "text": "Given their extensive applications, there has been significant research in developing methods and experimental design strategies to conduct interventions under different scenarios [32, 24]. Despite significant efforts to develop sophisticated experimental techniques to perform interventions, they often encounter noise [4, 6, 30]. For example, the CRISPR technology, extensively used to perform gene perturbations (or interventions), is known to have off-target effects, meaning the interventions do not always occur on the intended genes [6, 30]. Consequently, in many applications, performing interventions generates data from a mixture of intended and unintended interventional distributions. Analyzing such mixed data directly can lead to incorrect conclusions, adversely affecting downstream applications. Hence, it is essential to disentangle the mixture and recover the components corresponding to each individual intervention for further use in downstream tasks like causal discovery. ", "page_idx": 0}, {"type": "text", "text": "In our work, we formally address the challenge of disentangling mixtures of unknown interventional and observational distributions within the framework of linear structural equation models (LinearSEM) with additive Gaussian noise. Given iid samples from a mixture with a fixed number of components as input, we present an efficient algorithm that can learn each individual component. Our results are applicable to both do and more general soft interventions. ", "page_idx": 1}, {"type": "text", "text": "We chose to study our problem in the Linear-SEM with additive Gaussian noise framework for its fundamental importance in the causal discovery literature. Shimizu et al. [25] showed that observational data is sufficient for learning the underlying causal graph when the data-generating process is a Linear-SEM with additive non-Gaussian noise with no latent confounders. However, in the same setting with Gaussian noise, the causal graph is only identifiable up to its Markov Equivalence Class (MEC) [18, 25]. Thus, performing interventions (possibly noisy) is necessary to identify the causal graph, making it an interesting framework for our problem. ", "page_idx": 1}, {"type": "text", "text": "Our contributions First, we show that given samples from a mixture of unknown interventions within the framework of Linear-SEM with additive Gaussian noise, there exists an efficient algorithm to uniquely recover the individual components of the mixture. The sample complexity of our procedure scales polynomially with the dimensionality of the problem and inversely polynomially with the accuracy parameter and the magnitude of changes induced by each intervention. Our findings indicate that the recovery error for each individual interventional distribution approaches zero as the number of samples increases. Therefore, in the infinite sample regime, we can recover the true interventional distributions, even when the targets of the interventions are unknown. Second, if the input distributions satisfy a strong interventional faithfulness assumption (as defined in Squires et al. [26]), we can utilize the results from [26] to identify the targets of the interventions, thereby enabling causal discovery using these accurately recovered interventional distributions. Finally, we conduct a simulation study to validate our theoretical findings. We show that as sample size increases, one can recover the mixture parameters, identify the unknown intervention targets, and learn the underlying causal graph with high accuracy. ", "page_idx": 1}, {"type": "text", "text": "2 Prior Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Mixture of DAGs and Interventions. There has been a lot of interest in understanding the mixture distribution arising from a collection of directed acyclic graphs (DAGs) [27, 23, 29, 14, 9, 28]. Saeed et al. [23] studied the distribution arising from a mixture of multiple DAGs with common topological order, thus a generalization of our problem. Their method identifies the variables whose conditional distribution across DAGs varies. However, there is no theoretical guarantee for the identifiability of the mixture\u2019s components. Like us, Kumar and Sinha [14] also studied the mixture arising from interventions on a causal graph and gives an algorithm to identify the mixture component. However, they assume knowledge of the correct topological order and sample access to the observational distribution. Thiesson et al. [28] also studied the problem of learning a mixture of DAGs using the Expectation-Maximization (EM) framework. However, there is no theoretical guarantee about the identifiability of individual components. ", "page_idx": 1}, {"type": "text", "text": "Learning Mixture of Gaussians. Learning a mixture of Gaussians is a heavily studied problem [2, 7, 15, 11]. There exist efficient algorithms both in terms of runtime and sample complexity for a fixed number of components in the mixture [11, 15\u221a, 2]. Ge et al. [7] gave an efficient algorithm for the case when the number of components is almost $\\sqrt{n}$ where $n$ is the dimension of the variables. However, this method only guarantees identifiability in the perturbative setting, where the true parameters are randomly perturbed before the samples are generated. ", "page_idx": 1}, {"type": "text", "text": "Causal Discovery with Unknown Interventions. Recent years have seen the development of methods that perform causal discovery with observational and multiple unknown interventional data [16, 26, 10]. Squires et al. [26] takes multiple but segregated datasets from unknown interventional distributions and aims to identify the unknown interventions and learn the underlying causal graph up to its interventional MEC (I-MEC). Jaber et al. [10] considers the same problem of causal discovery with unknown soft interventions in non-Markovian systems (i.e., latent common cause models). However, they also assume that the interventional and/or observational data is already segregated. To our knowledge, no prior works consider directly learning the causal graph from a mixture of interventions. ", "page_idx": 1}, {"type": "text", "text": "3 Notation and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We use the upper-case letter $X$ to denote a random variable and lower-case \u201c $x'^{*}$ to denote the value taken by the random variable $X$ . Let, the uppercase bold-face letter $\\mathbf{\\deltaX}$ denote a set of random variables and the lowercase bold-face letter $\\textbf{\\em x}$ denote the corresponding value taken by $\\mathbf{\\deltaX}$ . Let the conditional probability function $\\mathbb{P}(X\\,=\\,x|Y\\,=\\,y)$ be denoted by $\\mathbb{P}(\\pmb{x}|\\pmb{y})$ . We use the calligraphic letter $\\boldsymbol{S}$ to denote a set and $|{\\cal S}|$ to denote the cardinality of the set $\\boldsymbol{S}$ . Let [n] denote the set of natural numbers $\\{1,\\ldots,n\\}$ . For any vector $\\pmb{v}$ , we use the notation $[{\\pmb v}]_{j}$ to denote it\u2019s $j^{t h}$ entry and for any matrix $M$ we use $[M]_{i,j}$ to denote the entry in the $i^{t h}$ row and $j^{t h}$ column of $M$ . We use $\\mathbb{R}_{+}$ to denote positive scalars. ", "page_idx": 2}, {"type": "text", "text": "Structural Equation Model (SEM) Following Definition 7.1.1 in Pearl [18], let a causal model (or SEM) be defined by a 3-tuple $\\mathcal{M}=\\langle V,U,\\mathcal{F}\\rangle$ , where $V=\\{V_{1},\\ldots,V_{n}\\}$ denotes the set of observed (endogenous) variables and $U=\\{U_{1},\\ldots,U_{n}\\}$ denotes the set of unobserved (exogenous) variables that represent noise, anomalies or assumptions. Next, $\\mathcal{F}$ denotes a set of $n$ functions $\\{f_{1},\\ldots,f_{n}\\}$ , each describing the causal relationships between the random variables having the form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{i}=f_{i}(p\\pmb{a}(V_{i}),\\pmb{u}_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $U_{i}\\subseteq U$ and $p a(V_{i})\\subseteq V$ are such that the associated causal graph (defined next) is acyclic. A causal graph $\\mathcal{G_{M}}^{1}$ is a directed acyclic graph (DAG), where the nodes are the variables $V$ and the edges $U$ with edges pointing from ${p a}(V_{i})$ to $V_{i}$ for all $i\\in[n]$ . ", "page_idx": 2}, {"type": "text", "text": "Linear-SEM (with causal sufficiency) In this work, we study a special class of such causal models (Gaussian Linear-SEMs) where the function class of each $f_{i}$ is restricted to be linear and of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{i}=f_{i}(\\pmb{p}\\pmb{a}(V_{i}),\\pmb{u}_{i})=\\sum_{v_{j}\\in\\pmb{p}\\pmb{a}(V_{i})}\\alpha_{i j}v_{j}+u_{i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha_{i j}\\neq0,\\forall V_{j}\\in{\\pmb p}{\\pmb a}(V_{i})$ . The causal Sufficiency assumption states that $U_{i}=\\{U_{i}\\}$ , i.e., $U_{i}$ is the only exogenous variable that causally affects the endogenous variable $V_{i}$ . This is equivalent to the absence of any latent confounder (Chapter 9 in [20]). In our work, we consider causally-sufficient Linear-SEMs; with a slight abuse of nomeclature, we will call them Linear-SEMs. The functional relationship between the exogenous and endogenous variables is deterministic, and the system\u2019s stochasticity comes from a probability distribution over the exogenous noise variables $U$ . Thus, the probability distribution over the exogenous variable $\\mathbb{P}(U)$ defines a probability distribution over the endogenous variable $\\mathbb{P}(V)$ . Without loss of generality, let the nodes $\\{V_{1},\\ldots,V_{n}\\}$ of the underlying causal graph be topologically ordered. Then, we can equivalently write the above set of equations as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{v}=A\\pmb{v}+\\pmb{u}\\implies\\pmb{v}=(I-A)^{-1}\\pmb{u},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A$ , with $A_{i j}=\\alpha_{i j}$ ,contains the causal effects between the endogenous variables. Thus, the matrix $A$ , hereafter described as the adjacency matrix, characterizes the causal relationships between the endogenous variables $(V)$ , where $A_{i j}\\neq0$ denotes an edge between the variable $V_{i}$ and $V_{j}$ in $\\mathcal{G}$ . ", "page_idx": 2}, {"type": "text", "text": "Linear-SEM with additive Gaussian Noise We further specialize the exogenous variable $u_{i}$ (henceforth referred to as noise variable) to be Gaussian with mean $\\mu_{i}$ and variance $\\sigma_{i}$ , i.e. $u_{i}\\sim$ $\\mathcal{N}(\\mu_{i},\\sigma_{i})$ . Thus, the joint distribution of the exogenous variables is given by a multivariate Gaussian distribution $\\pmb{u}\\sim\\mathcal{N}(\\bar{\\pmb{\\mu}},D)$ where $[\\pmb{\\mu}]_{i}=\\mu_{i}$ and the covariance is given by a diagonal matrix $D$ with $[D]_{i i}=\\sigma_{i}$ . Thus, the endogenous variables also follow a multivariate Gaussian distribution with $\\mathbb{P}(\\pmb{v})=\\mathcal{N}(\\pmb{m},S)$ , where $\\pmb{m}\\triangleq B\\pmb{\\mu}_{i}\\ S\\triangleq B D B^{T}$ and $B\\triangleq(I-A)^{-1}$ . Causal discovery aims to identify the unknown adjacency matrix $A$ given observational or other auxiliary data. ", "page_idx": 2}, {"type": "text", "text": "Interventions. Following Definition 7.1.2 from Pearl [18], the new causal model describing the interventional distribution, where the variables in a set $\\boldsymbol{\\mathit{I}}$ are set to a particular value, is given by $M_{I}=\\langle{\\pmb U},V,\\mathcal{F}_{I}\\rangle$ , where ${\\mathcal{F}}_{I}=\\{f_{j}:V_{j}\\notin I\\}\\cup\\{f_{i}^{\\prime}:V_{i}\\in I\\}$ and the functional relationship of every node $V_{i}\\in{I}$ with their parents and corresponding exogenous variable $U_{i}$ is changed from $f_{i}$ to $f_{i}^{\\prime}$ . In particular, the functional relationship of node $V_{i}$ is changed to ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{i}=\\sum_{v_{j}\\in p\\mathbf{a}(V_{i})}\\alpha_{i j}'v_{j}+{u_{i}}^{\\prime}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $u_{i}^{\\phantom{\\dagger}}\\sim\\mathcal{N}(\\mu_{i}^{\\phantom{\\dagger}},\\sigma_{i}^{\\phantom{\\dagger}})$ . Such interventions are broadly referred to as \u201csoft\". Several other kinds of interventions are also defined in the literature, e.g., $d o$ , uncertain, soft etc. [3]. We consider three different types of widely studied specializations of soft interventions in our work: ", "page_idx": 3}, {"type": "text", "text": "(1) shift: the mean of the noise distribution is shifted by a particular value, i.e., $\\mu_{i}^{\\,\\,^{\\prime}}=\\mu_{i}+\\kappa$ for some $\\kappa\\in\\mathbb{R}$ , and everything else remains the same, i.e., $\\sigma_{i}^{\\prime}=\\sigma_{i}$ and ${a_{i j}}^{\\prime}={a_{i j}},\\forall j\\in[n]$ .   \n(2) stochastic do (henceforth referred as stochastic): where all the incoming edges from parents are broken, i.e., ${\\alpha_{i j}}^{\\prime}=0$ , and $u_{i}^{\\phantom{\\dagger}}\\sim\\mathcal{N}(\\mu_{i}^{\\phantom{\\dagger}},\\sigma_{i}^{\\phantom{\\dagger}})$ .   \n(3) do: in addition to breaking all incoming edges, i.e., ${\\alpha_{i j}}^{\\prime}=0$ , we also set the variance of the noise distribution to 0 and the mean to any value of choice, i.e., $u_{i}\\sim\\mathcal{N}(\\mu_{i}^{\\prime},0)$ . ", "page_idx": 3}, {"type": "text", "text": "Atomic Interventions In this work, we consider soft interventions where only one node is intervened at a time, i.e., $|I|=1$ . Thus after a soft intervention on node $V_{i}$ , the adjacency matrix is modified such that $A_{i}\\triangleq A-e_{i}(\\pmb{a}_{i}-\\pmb{a}_{i}^{\\prime})^{T}=A-e_{i}\\pmb{c}_{i}^{T}:$ 2, where $\\pmb{c}_{i}^{T}\\triangleq(\\pmb{a}_{i}-\\pmb{a}_{i}^{\\prime})^{T}$ , $\\pmb{a}_{i}^{T}$ is the $i^{t h}$ row of matrix A and ${\\pmb a}_{i}^{'T}$ is the new row after intervention such that $[\\mathbf{a}_{i}]_{k}=0,\\forall k\\geq i$ , and $e_{i}$ is the unit vector with entry 1 at the $i^{t h}$ position and 0 otherwise. Thus, the linear SEM from Eq. 1 is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{v}_{i}=(I-A_{i})^{-1}\\pmb{u}_{i}=(I-A+\\pmb{e}_{i}\\pmb{c}_{i}^{T})^{-1}\\pmb{u}_{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{u}_{i}\\sim\\mathcal{N}(\\pmb{\\mu}_{i},D_{i})$ , $\\pmb{\\mu}_{i}=\\pmb{\\mu}+\\gamma_{i}\\pmb{e}_{i}$ for some $\\gamma_{i}\\in\\mathbb{R},D_{i}=D-\\delta_{i}e_{i}e_{i}^{T}$ where $\\delta_{i}\\triangleq(\\sigma_{i}-\\sigma_{i}^{'})$ , is a diagonal matrix with the $i^{t h}$ diagonal entry as $\\sigma_{i}$ and rest is same as $D$ . Thus, the interventional distribution of the endogenous variables is also a multivariate Gaussian distribution, i.e., $\\mathbb{P}_{i}(V)=$ $\\mathcal{N}(\\pmb{m}_{i},\\boldsymbol{S}_{i})$ where $m_{i}\\triangleq B_{i}\\pmb{\\mu}_{i}$ , $S_{i}\\triangleq B_{i}D_{i}B_{i}^{T}$ and $B_{i}\\triangleq(I-A+e_{i}\\pmb{c}_{i}^{T})^{-1}$ . ", "page_idx": 3}, {"type": "text", "text": "4 Problem Formulation and Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In $\\S4.1$ , we begin by formulating the problem of learning the mixture of interventions and then state our main result on the identifiability of parameters of the mixture. As a consequence of our identifiability result, under an interventional faithfulness assumption (Squires et al. [26]), we show in $\\S4.2$ that the underlying true causal graph can be identified up to its I-MEC using a mixture of unknown interventions, thereby obtaining the same identifiability results as in the unmixed setting. ", "page_idx": 3}, {"type": "text", "text": "4.1 Learning a Mixture of Interventions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by formally defining the mixture of interventions over Linear-SEM with additive Gaussian noise and then state the main result of our paper \u2014 a mixture of interventions can be uniquely identified under a mild assumption discussed below. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Mixture of Soft Atomic Interventions). Let ${\\mathcal{M}}\\,=\\,\\langle V,U,{\\mathcal{F}}\\rangle$ be an unknown Gaussian Linear SEM where the distribution of the endogenous variables is given by $\\mathbb{P}(V)$ (see \u00a73). Let $\\mathcal{T}=\\{i_{1},\\dotsc,i_{K}\\}$ , hereafter referred to as intervention target set, be a set of unknown soft atomic interventions where each $i_{k}$ generates a new interventional distribution $\\mathbb{P}_{i}(V)$ . Then, the mixture of soft atomic intervention is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{m i x}(\\pmb{V})=\\sum_{i_{k}\\in\\mathcal{T}}\\pi_{i_{k}}\\mathbb{P}_{i_{k}}(\\pmb{V})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi_{i_{k}}\\in\\mathbb{R}_{+}$ , henceforth referred to as mixing weight, is a positive scalar such that $\\textstyle\\sum_{i_{k}\\in{\\mathcal{I}}}\\pi_{i_{k}}=$ 1, $i_{k}\\in[n]\\cup\\{0\\}$ where $n=|V|$ is the number of endogenous variables. We also allow $i_{k}=0$ , which denotes the setting when none of the nodes is intervened, i.e., $P_{0}(V)\\triangleq\\mathbb{P}(V)$ . Using Eq. $^{\\,l}$ and 3, a mixture defined over a linear SEM with additive Gaussian noise is a mixture of Gaussians with parameters $\\theta=\\{(\\pmb{m}_{i_{k}},S_{i_{k}},\\pi_{i_{k}})\\}_{i_{k}\\in\\mathbb{Z}}$ where ${\\mathbb P}_{i_{k}}(V)=\\mathcal N(m_{i_{k}},S_{i_{k}})$ . ", "page_idx": 3}, {"type": "text", "text": "Having defined a mixture of interventions, we then aim to answer the following questions: (1) Does there exist an algorithm that can uniquely identify the parameters $\\left(\\theta\\right)$ of the mixture of interventions under an infinite sample limit? (2) What is the run time and sample complexity of such an algorithm? It is immediate that if the intervention doesn\u2019t change the causal mechanism in any way, then the interventional distribution is equal to the observational distribution, and we would not be able to distinguish between them. This discussion suggests that it is necessary to put an additional constraint on the interventions performed. Below, we formally state the assumption that will ensure this and that it is sufficient for the identifiability of mixture distribution. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1 (Effective Intervention). Let $\\mathbb{P}_{i}(\\pmb{V})=\\mathcal{N}(\\pmb{m}_{i},S_{i})$ be an interventional distribution after intervening on node $v_{i}$ , where ${\\pmb m}_{i}\\,=\\,B_{i}{\\pmb\\mu}_{i}\\,=\\,B_{i}({\\pmb\\mu}+\\gamma_{k}{\\pmb e}_{i})$ , $B_{i}\\,=\\,(I-A+e_{i}c_{i}^{T})$ and ${\\pmb{c}}_{i}=({\\pmb{a}}_{i}-{\\pmb{a}}_{i}^{\\prime})$ , $S_{k}=B_{i}D_{i}B_{i}^{T}$ and $D_{i}=D-\\delta_{i}e_{i}e_{i}^{T}$ (see atomic intervention paragraph in $\\S3$ ). Then, at least one of the following holds: $\\gamma_{i}\\neq0$ or $\\|c_{i}\\|\\neq0$ or $\\delta_{i}\\neq0$ . ", "page_idx": 4}, {"type": "text", "text": "Now, we are ready to state the main result of our work that will help us answer the above questions.   \nFor an exact expression of sample complexity and runtime, see Lemma 5.1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Identifiability of Mixture Parameters). Let $\\mathbb{P}_{m i x}(V)$ be a mixture of soft atomic interventions defined over a Linear-SEM with additive Gaussian noise with \u201c $^n$ \u201d endogenous variables (Definition 4.1) such that the number of components $|{\\mathcal{T}}|$ is fixed. Given Assumption 4.1 is satisfied, then there exists an efficient algorithm that runs in time polynomial in $n$ , requires $\\begin{array}{r l}&{p o l y\\bigg(\\bar{n},\\frac{1}{\\epsilon},\\frac{1}{\\delta},\\frac{1}{\\operatorname*{min}\\big(\\big\\{p o l y(\\|c_{i_{k}}\\|,\\delta_{i},\\gamma_{i},\\frac{1}{\\|A\\|_{F}})\\big\\}_{i_{k}\\in\\mathbb{Z}}\\big)}\\bigg)}\\end{array}$ samples where $A$ is the adjacency matrix of underlying graph and with probability greater than $(1\\,-\\,\\delta)$ recovers the mixture parameters $\\hat{\\theta}=\\{(\\hat{m}_{1},\\hat{S}_{1},\\hat{\\pi}_{1}),\\dots,(\\hat{m}_{|\\mathbb{Z}|},\\hat{S}_{|\\mathbb{Z}|},\\hat{\\pi}_{|\\mathbb{Z}|})\\}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{i_{k}\\in{\\cal Z}}\\left(\\|m_{i_{k}}-\\hat{m}_{\\rho(i_{k})}\\|^{2}+\\|S_{i_{k}}-\\hat{S}_{\\rho(i_{k})}\\|^{2}+|\\pi_{i_{k}}-\\hat{\\pi}_{\\rho(i_{k})}|^{2}\\right)\\leq\\epsilon^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some permutation $\\rho:\\{1,2,\\ldots,|\\mathcal{Z}|\\}\\to\\{1,2,\\ldots,|\\mathcal{Z}|\\}$ and arbitrarily small $\\epsilon>0$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Causal Discovery with Mixture of Interventions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 helps us separate the mixture of interventions $\\mathbb{P}_{m i x}(V)$ and provides us with the parameters $\\{(m_{i},S_{i})\\}_{i\\in\\bar{\\mathcal{T}}}$ of the distribution of all the components in the mixture. However, it does not reveal which nodes were intervened, corresponding to the different components recovered from the mixture. There has been recent progress in performing causal discovery with a disentangled set of unknown interventional distributions [26, 16]. Specifically, Squires et al. [26] proposes an algorithm (UT-IGSP) that greedily searches over the space of permutations to determine the I-MEC and the unknown intervention target of each component. UT-IGSP is consistent, i.e., it will output the correct I-MEC as the sample size goes to infinity. Thus, combining Theorem 4.1 with the UT-IGSP algorithm implies that, as sample size goes to infinity, we can recover the underlying causal graph up to its I-MEC given a mixture of interventions over a Linear-SEM with additive Gaussian noise: ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.1.1 (Mixture-MEC). Given samples from a mixture of interventions $\\mathcal{P}_{m i x}(V)$ over a linear-SEM with additive Gaussian noise and samples from the observational distribution $\\mathbb{P}(V)$ , there exists a consistent algorithm that will identify the I-MEC of the underlying causal graph under the I-faithfulness assumption (defined in Squires et al. [26]) (and restated in $\\S A.I$ ). ", "page_idx": 4}, {"type": "text", "text": "Proof. The proof follows from the identifiability of the parameters of the mixture distribution (Theorem 4.1) and the consistency of UT-IGSP given by Squires et al. [26]. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Remark. The I-faithfulness assumption imposes certain restrictions on both observational and interventional distributions. However, as noted by Squires et al. [26], in the case of Linear Gaussian distributions, the set of distributions excluded by this assumption is of measure zero. This is because the Linear Gaussian distributions, defined by a matrix $A$ , that do not meet this assumption are subject to multiple polynomial constraints of the form $p(A)=0$ . It is a well-known result that for a random matrix $A$ , the set of matrices that satisfy such polynomial equalities has measure zero [17]. ", "page_idx": 4}, {"type": "text", "text": "5 Proof Sketch of Theorem 4.1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we provide an overview of the proof of Theorem 4.1. Definition 4.1 tells us that the mixture of interventions defined over a Linear-SEM with additive Gaussian noise is a mixture of Gaussians. Learning mixtures of Gaussian is well-studied in the literature [2, 15, 7]. Since most of these approaches require some form of separability between the distribution of components or the parameters of the distributions, in the following lemma we will first show that the covariance matrix and the mean of any interventional or observational distribution taken pairwise is well-separated when Assumption 4.1 holds. In particular, this seperation of parameters will ensure that the Gaussian mixture can be uniquely identified using the results from Belkin and Sinha [2]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Lemma 5.1. [Parameter Separation] Let $\\mathbb{P}_{0}(V)$ denote the observational distribution of a linear SEM with additive Gaussian noise \u201c $\\mathcal{M}^{\\ast}$ (see $\\S3$ ) with $^{\\ast}n^{\\prime\\prime}$ endogenous variables. For some $i,j\\in$ $[n]\\cup\\{0\\}$ , let ${\\mathbb P}_{i}(\\pmb{V})=\\mathcal{N}(\\pmb{m}_{i},S_{i})$ and $\\mathbb{P}_{j}(\\pmb{V})=\\mathcal{N}(\\pmb{m}_{j},S_{j})$ be two interventional distributions (observational if one of $\"i\"$ \u201d or ${\\bf\\ddot{\\omega}}_{j}^{\\ast}=\\!\\theta_{j}$ . Then the separation between covariance $S_{i}$ and $S_{j}$ and mean ${\\mathbfit{m}}_{i}$ and ${\\pmb{m}}_{j}$ is lower bounded by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq f(B,D)\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)+h(B,D,\\mu)\\Big(\\gamma_{i}^{2}+\\gamma_{j}^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,g(B)\\Big(|\\delta_{i}|\\operatorname*{min}\\big(|\\delta_{i}|,\\lambda_{m i n}(D)\\big)+|\\delta_{j}|\\operatorname*{min}\\big(|\\delta_{j}|,\\lambda_{m i n}(D)\\big)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where after intervention on node $k\\in\\{i,j\\}$ , $\\|c_{k}\\|$ is the norm of the perturbation (or change) in the $k^{t h}$ row of the adjacency matrix, $\\gamma_{k}$ is the perturbation in the mean and $|\\delta_{k}|$ is the perturbation in the variance of the noise distribution of node $k$ (as defined in the Atomic Intervention paragraph of $\\S3$ ). Also, $B=\\^{\\stackrel{.}{}}(I-A)^{-1}$ , and $D$ and $\\pmb{\\mu}$ are the covariance matrix and mean of the noise distribution in $\\mathcal{M}$ , respectively. Furthermore, \u201c ${\\bf\\nabla}^{f}{}^{,}\\,{\\bf\\nabla}^{\\langle\\epsilon}g^{,\\rangle}$ , and \u201c $h^{\\ast}$ are positive valued polynomial functions of $B$ , $\\pmb{\\mu}$ and smallest eigenvalue of $D$ (see the proof in $\\S A.2$ for the exact expressions). ", "page_idx": 5}, {"type": "text", "text": "Remark. If one of the distributions is observational, i.e., say $i=0$ , then the above bound holds with $\\|c_{i}\\|^{2}=0$ , $\\gamma_{i}=0$ , and $|\\delta_{i}|=0$ . ", "page_idx": 5}, {"type": "text", "text": "Remark. Different types of interventions will allow us to change certain parameters in the above bound. Let the exogenous noise variables $u_{i}\\sim\\mathcal N(\\mu_{i},\\sigma_{i})$ when not intervened. Now, if we intervene on node $v_{i}$ , then the new noise variable has distribution $u_{i}^{\\prime}\\sim\\mathcal{N}(\\mu_{i}+\\gamma_{i},\\sigma_{i}^{\\prime})$ given as follows for different intervention types: ", "page_idx": 5}, {"type": "text", "text": "$(I)$ do intervention: $\\|c_{i}\\|\\neq0$ if $v_{i}$ is not a root node, $|\\delta_{i}|\\neq0\\;i f\\sigma_{i}\\neq0_{\\l}$ , and $\\gamma_{i}\\neq0$ if the value of node $v_{i}$ is set to any value other than $\\mu_{i}$ .   \n(2) stochastic intervention: $\\|c_{i}\\|\\neq0$ if $v_{i}$ is not a root node.   \n(3) shift intervention: $\\gamma_{i}\\neq0$ .   \n(4) soft intervention is the most general case, and nothing is guaranteed to be non-zero. ", "page_idx": 5}, {"type": "text", "text": "Next, we restate a definition from Belkin and Sinha [2] that defines the radius of identifiability $({\\mathcal{R}}(\\theta))$ of a probability distribution. If $\\mathcal{R}(\\theta)>0$ , this implies that we can uniquely identify the distribution. ", "page_idx": 5}, {"type": "text", "text": "Definition 5.1. Let $\\mathbb{P}_{\\theta},\\theta\\in\\Theta_{\\mathrm{i}}$ , be a family of probability distributions. For each $\\theta$ we define the radius of identifiabiity ${\\mathcal{R}}(\\theta)$ as the supremum of the following set: ", "page_idx": 5}, {"type": "equation", "text": "$\\{r>0\\,|\\,\\forall\\theta_{1}\\neq\\theta_{2},(\\Vert\\theta_{1}-\\theta\\Vert<r,\\Vert\\theta_{2}-\\theta\\Vert<r)\\implies(\\mathbb{P}_{\\theta_{1}}\\neq\\mathbb{P}_{\\theta_{2}})\\}$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In other words, ${\\mathcal{R}}(\\theta)$ is the largest number, such that the open ball of radius ${\\mathcal{R}}(\\theta)$ around $\\theta$ intersected with $\\Theta$ is an identifiable $(s u b)$ family of the probability distribution. If no such ball exists, $\\mathcal{R}(\\theta)=0$ . ", "page_idx": 5}, {"type": "text", "text": "Next, we restate a result from [2] adapted to our setting, which shows that there exists an efficient algorithm for disentangling a mixture of Gaussians as long as the parameters are separated, which will ensure that the radius of identifiability $\\mathcal{R}(\\theta)>0$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.2 (Theorem 3.1 in Belkin and Sinha [2]). Let $\\mathbb{P}_{m i x}(V)$ be a mixture of Gaussians with parameters $\\boldsymbol{\\theta}=\\left\\{(\\pmb{m}_{1},S_{1},\\pi_{1}),\\dots,(\\pmb{m}_{|\\mathbb{Z}|},S_{|\\mathbb{Z}|},\\pi_{|\\mathbb{Z}|})\\right\\}\\in\\Theta$ where $\\Theta$ is the set of parameters within a ball of radius $Q$ . Then, there exists an algorithm which given $\\epsilon\\mathrm{~>~0~}$ and $0\\,<\\,\\delta\\,<\\,1$ and poly $\\big(n,\\operatorname*{max}(\\textstyle\\frac{1}{\\epsilon},\\frac{1}{\\mathcal{R}(\\Theta)}\\big),\\frac{1}{\\delta},Q\\big)$ samples from $\\mathbb{P}_{m i x}(V)$ , with probability greater than $(1-\\delta)$ outputs a parameter vector $\\hat{{\\boldsymbol\\theta}}=\\hat{{\\boldsymbol\\theta}}=(\\hat{\\pmb{m}}_{1},\\hat{S}_{1},\\hat{\\pi}_{1}),\\dots,(\\hat{\\pmb{m}}_{|\\mathcal{Z}|},\\hat{S}_{|\\mathcal{Z}|},\\hat{\\pi}_{|\\mathcal{Z}|})\\in\\Theta$ such that there exists a permutation $\\rho:\\{1,...\\,,|{\\mathcal{Z}}|\\}\\to\\{1,...\\,,|{\\mathcal{Z}}|\\}$ satisfying: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{i_{k}\\in\\mathbb{Z}}\\left(\\|m_{i_{k}}-\\hat{m}_{\\rho(i_{k})}\\|^{2}+\\|S_{i_{k}}-\\hat{S}_{\\rho(i_{k})}\\|^{2}+|\\pi_{i_{k}}-\\hat{\\pi}_{\\rho(i_{k})}|^{2}\\right)\\leq\\epsilon^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the radius of identifiability ${\\mathcal{R}}(\\theta)$ is lower bounded by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\big(\\mathcal{R}(\\theta)\\big)^{2}\\geq\\operatorname*{min}\\Big(\\frac{1}{4}\\operatorname*{min}_{i\\neq j}(\\|m_{i}-m_{j}\\|^{2}+\\|S_{i}-S_{j}\\|^{2}),\\operatorname*{min}_{i}\\pi_{i}\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "input :mixed dataset $(\\mathcal{D}_{\\operatorname*{mix}})$ , observational data $\\left(\\mathcal{D}_{\\mathrm{obs}}\\right)$ , number of nodes $(n)$ , cutoff ratio $(\\tau)$ output :Mixture Distribution Parameters $(\\theta)$ , Intervention Targets $(\\mathcal{T})$ , causal graph $(\\hat{\\mathcal{G}})$ ) ", "page_idx": 6}, {"type": "text", "text": "1. Esstimate $\\theta_{k}\\triangleq\\left\\{({\\hat{\\mu}}_{1},{\\hat{S}}_{1}),\\dots({\\hat{\\mu}}_{k},{\\hat{S}}_{k})\\right\\}=0$ aussianMixtureModel $\\left(\\mathcal{D}_{m i x},k\\right)$ for each possible number of component in the mixture i.e $k\\in[n+1]$ . Define $\\Theta\\triangleq\\{\\theta_{1},\\ldots,\\theta_{n+1}\\}$ be the set of estimated parameters and $\\mathcal{L}=\\{l_{1},\\ldots,l_{n+1}\\}$ be the log-likelihood of the mixture data corresponding to the models with a different number of components.   \n2. To estimate the number of components in the mixture $(k_{*})$ iterate over $k=(n+1)$ to 2: (a) stop where the relative change in the likelihood increases above a cutoff ratio i.e $\\frac{|l_{k}\\bar{-}l_{k-1}|}{l_{k}}>\\tau$ (b) $k_{*}=k$ if the stopping criteria is met otherwise $k_{*}=1$ .   \n3. I $\\bar{\\mathbf\\Xi},\\hat{\\mathcal{G}}=\\mathbf{UTIGSP}\\big(\\mathcal{D}_{\\mathrm{obs}},\\theta_{k_{*}}\\big)$ [26] ", "page_idx": 6}, {"type": "text", "text": "return \u0398, I, G\u02c6 ", "page_idx": 6}, {"type": "text", "text": "Our Theorem 4.1 along with Assumption 4.1 states that for every pair $i,j\\;\\in\\;([n]\\cup\\{0\\})^{\\otimes2}$ we have $\\|S_{i}-S_{j}\\|_{F}^{2}+\\|{\\bar{\\pmb{m}}}_{i}-\\pmb{m}_{j}\\|_{F}^{2}\\,\\stackrel{.}{>}\\,0$ . Also, by construction of the mixture of interventions (in Definition 4.1), we have $\\pi_{i}>\\^{\\cdot}0,\\forall i\\in[n]\\cup\\{0\\}$ . This implies that the radius of convergence $\\mathcal{R}(\\theta)>0$ and thus the parameters of the mixture of interventions $\\mathbb{P}(V)$ can be identified uniquely given samples from the mixture distribution with sample size inversely proportional to ${\\mathcal{R}}(\\theta)$ . ", "page_idx": 6}, {"type": "text", "text": "6 Empirical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Experiment on Simulated Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Proposition 4.1.1 establishes that given samples from the mixture distribution, one can identify the underlying causal graph up to its I-MEC. To learn the causal graph, we first disentangle the mixture. Theorem 4.1 and 5.2 show that the sample complexity of our mixture disentangling algorithm is inversely proportional to various parameters of the underlying system and the intervention parameters $(\\gamma,|\\delta|$ and $\\|c\\|,$ . Our simulation study further validates our theoretical results and characterizes the end-to-end performance of identifying the causal graph with such mixture data and its dependence on the above-mentioned parameters. ", "page_idx": 6}, {"type": "text", "text": "Simulation Setup We consider data generated from a Linear-SEM with additive Gaussian noise, $\\pmb{x}=(I{-}A)^{-1}\\pmb{\\eta}$ (see $\\S3.$ ), with n endogenous variables and corresponding exogenous (noise) variables. Here $\\pmb{\\eta}\\sim\\mathcal{N}(\\mathbf{0},D)$ , where the noise covariance matrix is diagonal with entries $D=d i a g(\\sigma^{2},\\dots,\\sigma^{2})$ and $\\sigma=1$ unless otherwise specified. $A$ is the (lower-triangular) weighted adjacency matrix whose weights are sampled in the range $[-1,-0.5]\\cup[0.5,1]$ bounded away from 0. Let $g^{*}$ denote the causal graph corresponding to this linear SEM with edge $i\\rightarrow j\\Leftrightarrow A_{j i}>0$ . By sampling from the resulting multivariate Gaussian distribution, we obtain observational data. Next, for each causal graph, we generate separate interventional data by intervening on a given set of nodes in the graph one at a time (atomic interventions), which is again a Gaussian distribution but with different parameters (see Atomic Intervention paragraph in $\\S3$ ). We experiment with two settings: ", "page_idx": 6}, {"type": "text", "text": "(1) all: where we perform an atomic intervention on all nodes in the graph. ", "page_idx": 6}, {"type": "text", "text": "(2) half: where we perform an atomic intervention on a randomly selected half of the nodes. Then, the mixed data is generated by pooling all the individual atomic interventions and observational data with equal proportions into a single dataset. The decision to use equal proportions of samples from all components is solely intended to simplify the design choices for the experiment setup. In our experiment, we vary the total number of samples in the mixed dataset as $N\\in\\{2^{10},2^{11},\\dots,2^{17}\\}$ . In particular, we perform two kinds of atomic interventions: \u201cdo\" and \u201cstochastic\" (see Interventions paragraph in $\\S3$ for a formal definition). The initial noise distribution for all the nodes is univariate Gaussian distribution ${\\mathcal{N}}(0,1)$ . In our experiments for $d o$ interventions, instead of setting the final variance of noise distribution to 0, we set it to a very small value of $10^{-9}$ for numerical stability. Unless otherwise specified, we perform 10 runs for each experimental setting and plot the 0.05 and 0.95 quantiles. See B for additional details on the experimental setup. ", "page_idx": 6}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/5bdcb4b22386d3707d452daec065879a4537c3badfc4bc753d748c53a08b4398.jpg", "img_caption": ["Figure 1: Performance of Alg. 1 as we vary sample size and number of nodes: The first row (a-c) shows the performance when the mixed data contains atomic intervention on all the nodes and observational data. The second row (d-f) shows the performance when the number of atomic interventions (chosen randomly) in mixed data is taken to be half of the number of nodes along with observational data. The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD. The symbols $(\\uparrow)$ represent higher is better, and $(\\uparrow)$ represents the opposite (see Evaluation metric paragraph in $\\S6$ ). In summary, performance improves for both cases as the number of samples increases. However, the graph with more nodes requires a larger sample to perform similarly. For a detailed discussion, see $\\S6.1$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Method Description and Evaluation Metrics Given the mixed data generated from the underlying true causal graph $\\mathcal{G}^{\\ast}$ , the goal is to estimate the underlying causal graph $\\hat{\\mathcal G}$ . We break down the task into two steps. First, we disentangle the mixture data and identify the parameters of the individual interventional and/or observational distributions. Our theoretical result (Theorem 4.1) uses [2] for identifiability of the parameters of a mixture of Gaussians. Since they only show the existence of such an algorithm, we use the standard sklearn python package [19] that implements an EM algorithm to estimate the parameters of the mixture. Importantly, our experiment doesn\u2019t require prior knowledge about the number of components (k) in the mixture. We train separate mixture models varying the number of components. Then we select the optimal number of components using the log-likelihood curve of the ftited Gaussian mixture model using a simple thresholding heuristic (see step 2 in Alg. 1). We leave the exploration of better model selection criteria for future work. For all our simulation experiments, unless otherwise specified, we use a cutoff threshold of 0.07, chosen arbitrarily. In $\\S B$ , we experiment with different values of this threshold and show that Mixture-UTIGSP is robust to this choice. The intervention targets present in the mixture are still unknown at this step. Next, we provide the estimated mixture parameters to an existing causal discovery algorithm with unknown intervention targets (UT-IGSP [26]), which internally estimates the unknown intervention targets and outputs one of the possible graphs from the estimated I-MEC. We assume that observational data is given as an input to the UT-IGSP algorithm. The proposed algorithm is provided in Alg. 1. See B for our hyperparameter choice and other experimental details. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics We evaluate the performance of Mixture-UTIGSP (Alg. 1) on three metrics: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Parameter Estimation Error: This metric measures the least absolute error between the estimated parameters (mean and covariance matrix defining each individual distribution) after the first step of Mixture-UTIGSP matched with the ground truth parameters considering all possible matchings between the components averaged over all runs. See $\\S B.5$ for details. ", "page_idx": 7}, {"type": "table", "img_path": "aC9mB1PqYJ/tmp/5eeee124a4f64b3ebd32f38d67f4704c834f6f1d376c41d73ceb1167a6b39801.jpg", "table_caption": ["Table 1: Performance of Alg. 1 on Protein Signalling Dataset [22]: We evaluate the performance of Mixture-UTIGSP as we vary the cutoff ratio to select the number of component in the mixture. The second column shows the number of estimated components where the actual number of components in the mixture is 6. The third and fourth columns show the Jaccard Similarity of the identified intervention target of Mixture-UTIGSP and oracle versions of the UT-IGSP algorithm. The fourth and last column shows the SHD between the estimated and true causal graphs for both methods respectively. Overall we observe that at a lower cutoff threshold Mixture-UTIGSP is able to perform as well as the oracle UT-IGSP algorithm on all the metrics. See $\\S B.2$ for detailed discussion. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Average Jaccard Similarity (JS): We measure the average Jaccard similarity between the estimated intervention target and the corresponding ground truth (atomic) intervention target. We use the matching between the estimated and ground truth components found while calculating the parameter estimation error averaged over all runs. See $\\S B.5$ for details. ", "page_idx": 8}, {"type": "text", "text": "Average Structural Hamming Distance (SHD): Given the estimated and ground truth graphs, we compute the SHD between the two graphs averaged over all runs. ", "page_idx": 8}, {"type": "text", "text": "Results Fig. 1 shows the performance of Alg. 1 in the two settings, \u201call\" in the first row and \u201chalf\" in the second row (see Simulation setup above). The first column shows the performance of the first step of Alg. 1 where the mixture parameters are identified. We observe that parameter estimation error decreases as the number of samples increases in both settings. As expected, larger graphs require a larger sample size to perform similarly to smaller-sized graphs within each setting. ", "page_idx": 8}, {"type": "text", "text": "Step 1 of our Alg. 1 only recovers the parameters $(\\{(m_{i},S_{i})\\}_{i=1}^{k})$ of the components present in the mixture distribution. In step 2 of our Alg. 1, we call UT-IGSP [26] that identifies the individual intervention targets from the estimated distribution parameters and also returns a causal graph from the estimated I-MEC. Fig. 1b and 1e show the average Jaccard Similarity between the ground truth and the estimated intervention targets. The colored lines denote experiments on graphs with different numbers of nodes. The corresponding dotted lines show the oracle performance of UT-IGSP when the separated ground truth mixture distributions were given as input. As expected, in both the settings (Fig. 1b and 1e), the oracle version performs much better compared to its non-oracle counterpart for small sample sizes $2^{10}$ to $2^{14}$ ) but performs similarly as sample size increases. ", "page_idx": 8}, {"type": "text", "text": "Finally, in the third column (Fig. 1c and 1f), we calculate the SHD between the estimated causal graph $\\hat{\\mathcal G}$ and the ground truth causal graph $g^{\\ast}$ . The SHD of the graph estimated by Mixture-UTIGSP and the oracle version are similar for different node settings and all sample sizes. This suggests that small errors in the estimation of the parameters of the mixture distribution don\u2019t affect the estimation of the underlying causal graph. ", "page_idx": 8}, {"type": "text", "text": "Additional Experiments: In $\\S B$ , we provide details on the experimental setup and additional results. In Fig. 2, we plot two additional metrics for the simulation experiments. The first metric is the number of estimated components in the mixture and the second metric is the the error in estimation of the mixing coefficient. In Fig. 3, we study the sensitivity of the cutoff ratio used by Mixture-UTIGSP to select the number of components in the mixture. Next, in Fig. 4, we evaluate the performance of our Alg. 1 as we vary the density, i.e., the expected number of edges in the graph. In Fig. 6, we show how the sparsity of the graph and other intervention parameters like the value of the new mean and variance of the noise distribution after intervention affects the performance of Alg. 1. ", "page_idx": 8}, {"type": "text", "text": "6.2 Experiment on Biological Dataset ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We evaluate our method on the Protein Signaling dataset [22] to demonstrate real-world applicability. The dataset is collected from flow cytometry measurement of 11 phosphorylated proteins and phospholipids and is widely used in causal discovery literature [31, 26]. The dataset consists of 5846 measurements with different experimental conditions and perturbations. Following Wang et al. [31], we define the subset of the dataset as observational, where only the receptor enzymes were perturbed in the experiment. Next, we select other 5 subsets of the dataset where a signaling protein is also perturbed in addition to the receptor enzyme. The observational dataset consists of 1755 samples, and the 5 interventional datasets have 911, 723, 810, 799, and 848 samples, respectively. The mixed dataset is created by merging all the observational and interventional datasets. ", "page_idx": 9}, {"type": "text", "text": "The total number of nodes in the underlying causal graph is 11. Thus, the maximum number of possible components in the mixture is 12 (11 single-node interventional distribution and one observational). In the mixture dataset described above, we have 6 components (1 observational and 5 interventional). The second column in Table 1 shows that Mixture-UTIGSP recovers 4 components close to the ground truth 6 when the cutoff ratio is 0.01 (step 2 of Alg. 1). Next, we give the disentangled dataset from the first step of our algorithm to identify the unknown target. Though the Jaccard similarity of the recovered target is not very high (average of 0.03 shown in the last row of the third column, where the maximum value is 1.0), it is similar to that of the oracle performance of UT-IGSP when the disentangled ground truth mixture distributions were given as input. This shows that it is difficult to identify the correct intervention targets even with correctly disentangled data. Also, the SHD between the recovered graph and the widely accepted ground truth graph for Mixture-UTIGSP (ours) and UT-IGSP (oracle) is very close. Overall, at a lower cutoff ratio, the performance of Mixture-UTIGSP is close to the Oracle UT-IGSP algorithm. Unlike the simulation case (see Fig. 3), Mixture-UTIGSP\u2019s performance is sensitive to the choice of the cutoff ratio on this dataset. In Fig. 5, we plot the ground truth graph curated by the domain expert alongside the estimated causal graph for visualization. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We studied the problem of learning the mixture distribution generated from observational and/or multiple unknown interventional distributions generated from the underlying causal graph. We show that the parameters of the mixture distribution can be uniquely identified under the mild assumption that ensures that any intervention changes the distribution of the observed variables. As a consequence of our identifiability result, under an interventional faithfulness assumption (Squires et al. [26]), we show that the underlying true causal graph can be identified up to its I-MEC based on a mixture of unknown interventions, thereby obtaining the same identifiability results as in the unmixed setting. Finally, we conduct a simulation study to validate our findings empirically. We demonstrate that as the sample size increases, we obtain parameter estimates of the mixture distribution that are closer to the ground truth and, as a result, we eventually recover the correct underlying causal graph. ", "page_idx": 9}, {"type": "text", "text": "8 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Since our work is the first to study the problem of a mixture of causal interventions without assuming knowledge of causal graphs, we have restricted our attention to one particular family of causal models\u2014Linear-SEM with additive Gaussian noise. In the future, it would be interesting to study this problem for a more general family of causal models. Further, our work uses Belkin and Sinha [2] for identifying the parameters of a mixture of Gaussians, which assumes that the number of components in the mixture is fixed. Recent progress in [7] gi\u221aves an efficient algorithm for recovering the parameters when the number of components is almost $\\sqrt{n}$ , where $n$ is the number of variables. However, they only work in perturbative settings, and proving the result for non-perturbative settings is out of the scope of the current paper. Finally, to identify the parameters of the mixture distribution in our empirical study, we use heuristics to estimate the number of components. It would be interesting to explore other methods to automatically select the number of components. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Joshua Angrist and Victor Lavy. Does teacher training affect pupil learning? evidence from matched comparisons in jerusalem public schools. Journal of Labor Economics, 19(2):343\u201369, 2001. URL https://EconPapers.repec.org/RePEc:ucp:jlabec:v:19:y:2001:i:2: p:343-69.   \n[2] Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 103\u2013112, 2010. URL https://api.semanticscholar.org/CorpusID:3089712.   \n[3] Daniel Eaton and Kevin Murphy. Exact bayesian structure learning from uncertain interventions. In Marina Meila and Xiaotong Shen, editors, Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, volume 2 of Proceedings of Machine Learning Research, pages 107\u2013114, San Juan, Puerto Rico, 21\u201324 Mar 2007. PMLR. URL https://proceedings.mlr.press/v2/eaton07a.html.   \n[4] Markus I. Eronen. Causal discovery and the problem of psychological interventions. New Ideas in Psychology, 59:100785, 2020. ISSN 0732-118X. doi:https://doi.org/10.1016/j.newideapsych.2020.100785. URL https://www. sciencedirect.com/science/article/pii/S0732118X19301436.   \n[5] Nir Friedman, Michal Linial, Iftach Nachman, and Dana Pe\u2019er. Using bayesian networks to analyze expression data. In Proceedings of the Fourth Annual International Conference on Computational Molecular Biology, RECOMB \u201900, page 127\u2013135, New York, NY, USA, 2000. Association for Computing Machinery. ISBN 1581131860. doi:10.1145/332306.332355. URL https://doi.org/10.1145/332306.332355.   \n[6] Yanfang Fu, Jennifer Foden, Cyd Khayter, Morgan Maeder, Deepak Reyon, J Joung, and Jeffry Sander. High-frequency off-target mutagenesis induced by crispr-cas nucleases in human cells. Nature biotechnology, 31, 06 2013. doi:10.1038/nbt.2623.   \n[7] Rong Ge, Qingqing Huang, and Sham M. Kakade. Learning mixtures of gaussians in high dimensions. In Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC \u201915, page 761\u2013770, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450335362. doi:10.1145/2746539.2746616. URL https://doi.org/ 10.1145/2746539.2746616.   \n[8] Lucas Girard and Yannick Guyonvarch. Bridging methodologies: Angrist and imbens\u2019 contributions to causal identification. 2024. URL https://api.semanticscholar.org/CorpusID: 267760356.   \n[9] Spencer L. Gordon, Erik Jahn, Bijan Mazaheri, Yuval Rabani, and Leonard J. Schulman. Identification of mixtures of discrete product distributions in near-optimal sample and time complexity, 2023.   \n[10] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft interventions with unknown targets: Characterization and learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9551\u20139561. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf.   \n[11] Adam Tauman Kalai, Alex Samorodnitsky, and Shang-Hua Teng. Learning and smoothed analysis. In 2009 50th Annual IEEE Symposium on Foundations of Computer Science, pages 395\u2013404, 2009. doi:10.1109/FOCS.2009.60.   \n[12] Jack Kelly, Carlo Berzuini, Bernard Keavney, Maciej Tomaszewski, and Hui Guo. A review of causal discovery methods for molecular network analysis. Molecular Genetics and Genomic Medicine, 10(10), October 2022. ISSN 2324-9269. doi:10.1002/mgg3.2055. Funding Information: This work was jointly supported by the British Heart Foundation and The Alan Turing Institute (which receives core funding under the EPSRC grant EP/N510129/1) as part of the Cardiovascular Data Science Awards (Round 2, SP/19/10/34813). Publisher Copyright: $\\copyright$ 2022 The Authors. Molecular Genetics and Genomic Medicine published by Wiley Periodicals LLC.   \n[13] K. S. Kendler and J. Campbell. Interventionist causal models in psychiatry: repositioning the mind\u2013body problem. Psychological Medicine, 39(6):881\u2013887, 2009. doi:10.1017/S0033291708004467.   \n[14] Abhinav Kumar and Gaurav Sinha. Disentangling mixtures of unknown causal interventions. In Cassio de Campos and Marloes H. Maathuis, editors, Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, volume 161 of Proceedings of Machine Learning Research, pages 2093\u20132102. PMLR, 27\u201330 Jul 2021. URL https://proceedings. mlr.press/v161/kumar21a.html.   \n[15] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93\u2013102, 2010. URL https://api.semanticscholar.org/CorpusID:3250359.   \n[16] Joris M. Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.   \n[17] Masashi Okamoto. Distinctness of the Eigenvalues of a Quadratic form in a Multivariate Sample. The Annals of Statistics, 1(4):763 \u2013 765, 1973. doi:10.1214/aos/1176342472. URL https://doi.org/10.1214/aos/1176342472.   \n[18] Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009. ISBN 052189560X.   \n[19] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[20] Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. Elements of Causal Inference: Foundations and Learning Algorithms. The MIT Press, 2017. ISBN 0262037319.   \n[21] Donald D. Price, Damien G. Finniss, and Fabrizio Benedetti. A comprehensive review of the placebo effect: recent advances and current thought. Annual review of psychology, 59:565\u201390, 2008. URL https://api.semanticscholar.org/CorpusID:11316014.   \n[22] Karen Sachs, Omar Perez, Dana Pe\u2019er, Douglas A Lauffenburger, and Garry P Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science (New York, N.Y.), 308(5721):523\u2014529, April 2005. ISSN 0036-8075. doi:10.1126/science.1105809. URL https://doi.org/10.1126/science.1105809.   \n[23] Basil Saeed, Snigdha Panigrahi, and Caroline Uhler. Causal structure discovery from distributions arising from mixtures of DAGs. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8336\u20138345. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/saeed20a.html.   \n[24] Ayush Sawarni, Rahul Madhavan, Gaurav Sinha, and Siddharth Barman. Learning good interventions in causal graphs via covering. In Robin J. Evans and Ilya Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 of Proceedings of Machine Learning Research, pages 1827\u20131836. PMLR, 31 Jul\u201304 Aug 2023. URL https://proceedings.mlr.press/v216/sawarni23a.html.   \n[25] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvarinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(72):2003\u20132030, 2006. URL http://jmlr.org/papers/v7/shimizu06a.html.   \n[26] Chandler Squires, Yuhao Wang, and Caroline Uhler. Permutation-based causal structure learning with unknown intervention targets. In Ryan P. Adams and Vibhav Gogate, editors, Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online, August 3-6, 2020, volume 124 of Proceedings of Machine Learning Research, pages 1039\u20131048. AUAI Press, 2020. URL http://proceedings.mlr.press/v124/squires20a.html.   \n[27] Eric V. Strobl. Causal discovery with a mixture of dags. Mach. Learn., 112(11):4201\u20134225, mar 2022. ISSN 0885-6125. doi:10.1007/s10994-022-06159-y. URL https://doi.org/10. 1007/s10994-022-06159-y.   \n[28] Bo Thiesson, Christopher Meek, David Maxwell Chickering, and David Heckerman. Learning mixtures of dag models. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201998, page 504\u2013513, San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc. ISBN 155860555X.   \n[29] Burak Varici, Dmitriy Katz, Dennis Wei, Prasanna Sattigeri, and Ali Tajer. Separability analysis for causal discovery in mixture of DAGs. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id $\\equiv$ ALRWXT1RLZ.   \n[30] Xiaoling Wang, Yebo Wang, Xiwei Wu, Jinhui Wang, Yingjia Wang, Zhaojun Qiu, Tammy Chang, He Huang, Ren-Jang Lin, and Jiing-Kuan Yee. Unbiased detection of off-target cleavage by crispr-cas9 and talens using integrase-defective lentiviral vectors. Nature biotechnology, 33, 01 2015. doi:10.1038/nbt.3127.   \n[31] Yuhao Wang, Liam Solus, Karren Dai Yang, and Caroline Uhler. Permutation-based causal inference algorithms with interventions. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 5824\u20135833, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.   \n[32] Jiaqi Zhang, Louis Cammarata, Chandler Squires, Themistoklis Sapsis, and Caroline Uhler. Active learning for optimal intervention design in causal models. Nature Machine Intelligence, 5:1\u201310, 10 2023. doi:10.1038/s42256-023-00719-0. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Missing Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 I-faithfulness in UT-IGSP algorithm. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we restate the assumption from Squires et al. [26] that is needed for consistency of the UT-IGSP algorithm: ", "page_idx": 13}, {"type": "text", "text": "Assumption A.1 $\\mathcal{T}$ -faithfulness assumption). Let $\\mathcal{T}$ be a list of intervention targets. The set of distributions $\\{f^{o b s}\\}\\cup\\{f^{I}\\}_{I_{\\mathcal{T}}}$ is $\\mathcal{T}$ -faithful with respect to a DAG $\\mathcal{G}$ if $f^{o b s}$ is faithful with respect to $\\mathcal{G}$ and for any $I^{k}\\in\\mathcal{T}$ and disjoint $A,C\\subseteq[p]$ , we have that $(A\\perp\\zeta_{k}|C\\cup\\zeta_{\\mathcal{T}\\backslash\\{k\\}})_{\\mathcal{G}^{\\mathbb{Z}}}$ if and only if $f^{k}(x_{A}|x_{C})=f^{o b s}(x_{A}|x_{C})$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 5.1. [Parameter Separation] Let $\\mathbb{P}_{0}(V)$ denote the observational distribution of a linear SEM with additive Gaussian noise \u201c $\\mathcal{M}^{\\ast}$ (see $\\S3,$ ) with $\\mathbf{\\nabla}^{*}\\mathbf{n}^{\\prime\\prime}$ endogenous variables. For some $i,j\\in$ $[n]\\cup\\{0\\}$ , let ${\\mathbb P}_{i}(\\pmb{V})=\\mathcal{N}(\\pmb{m}_{i},S_{i})$ and $\\mathbb{P}_{j}(\\pmb{V})=\\mathcal{N}(\\pmb{m}_{j},S_{j})$ be two interventional distributions (observational if one of \u201ci\u201d or ${\\bf\\ddot{\\omega}}_{j}^{\\ast}=\\!\\theta_{j}$ . Then the separation between covariance $S_{i}$ and $S_{j}$ and mean ${\\mathbfit{m}}_{i}$ and ${\\pmb{m}}_{j}$ is lower bounded by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq f(B,D)\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)+h(B,D,\\mu)\\Big(\\gamma_{i}^{2}+\\gamma_{j}^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,g(B)\\Big(|\\delta_{i}|\\operatorname*{min}\\big(|\\delta_{i}|,\\lambda_{m i n}(D)\\big)+|\\delta_{j}|\\operatorname*{min}\\big(|\\delta_{j}|,\\lambda_{m i n}(D)\\big)\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where after intervention on node $k\\in\\{i,j\\}$ , $\\|c_{k}\\|$ is the norm of the perturbation (or change) in the $k^{t h}$ row of the adjacency matrix, $\\gamma_{k}$ is the perturbation in the mean and $|\\delta_{k}|$ is the perturbation in the variance of the noise distribution of node $k$ (as defined in the Atomic Intervention paragraph of $\\S3$ ). Also, $B=\\^{\\stackrel{.}{}}(I-A)^{-1}$ , and $D$ and $\\pmb{\\mu}$ are the covariance matrix and mean of the noise distribution in $\\mathcal{M}$ , respectively. Furthermore, \u201c ${\\bf\\nabla}^{f}{}^{,}\\,{\\bf\\nabla}^{\\langle\\epsilon}g^{,\\rangle}$ , and \u201c $h'$ \u201d are positive valued polynomial functions of $B$ , $\\pmb{\\mu}$ and smallest eigenvalue of $D$ (see the proof in $\\S A.2$ for the exact expressions). ", "page_idx": 13}, {"type": "text", "text": "Proof. First, we state a lemma that will give us a lower bound on the separation between the covariance matrix of two intervention distributions (or one could be observational). The proof is given in $\\S\\mathrm{A}.3$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (Minimum Covariance Separation). Let $S_{i}$ and $S_{j}$ be the covariance matrix of the Gaussian distribution corresponding to intervention on node $V_{i}$ and $V_{j}$ in the causal graph. Let, without loss of generality, $V_{i}$ be topologically greater than $V_{j}$ in the causal graph. Let $B=(I\\!-\\!A)^{-1}$ , $D=d i a g(\\sigma_{1},\\ldots,\\sigma_{n})$ be the covariance matrix of the noise distribution, let $S=B D B^{T}$ be the covariance matrix of the observed distribution $P(V)$ , and let ${\\pmb{c}}_{i}={\\pmb{a}}_{i}-{\\pmb{a}}_{i}^{\\prime}$ be the soft intervention performed on node $V_{i}$ (see $\\S3$ for definitions). Then we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|S_{i}-S_{j}\\|_{F}^{2}\\geq f(B,D)\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad+\\,g(B)\\Big(|\\delta_{i}|\\operatorname*{min}\\left(|\\delta_{i}|,\\lambda_{m i n}(D)\\right)+|\\delta_{j}|\\operatorname*{min}\\left(|\\delta_{j}|,\\lambda_{m i n}(D)\\right)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If one of the covariance matrices is from the observational distribution, i.e., say $S_{j}=S$ then the above lower bounds still holds with $\\|c_{j}\\|=0$ and $\\delta_{j}=0$ . ", "page_idx": 13}, {"type": "text", "text": "Using the above Lemma A.1 and substituting the explicit form of $f(B,D)$ from Eq. 19 we obtain: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|S_{i}-S_{j}\\|_{F}^{2}\\geq f(B,D)\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{g(B)\\Big(|\\delta_{i}|\\operatorname*{min}\\big(|\\delta_{i}|,\\lambda_{m i n}(D)\\big)+|\\delta_{j}|\\operatorname*{min}\\big(|\\delta_{j}|,\\lambda_{m i n}(D)\\big)\\Big)}_{\\triangleq\\omega}}\\\\ &{\\qquad\\qquad\\geq\\underbrace{\\frac{f(B,D)}{2}\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)+\\omega}_{\\triangleq\\zeta}+\\frac{\\lambda_{m i n}^{2}(D)\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)}{8\\|B^{-1}\\|_{F}^{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, we state another lemma that will give us a lower bound on the separation of the mean of two interventional distributions (or one of them could be observational). The proof of the lemma below is given in $\\S\\mathrm{A}.5$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2 (Minimum Mean Separation). Let $\\mathbf{\\nabla}m_{i}$ and ${\\pmb{m}}_{j}$ denote the mean of the Gaussian distribution corresponding to the intervention on node $V_{i}$ and $V_{j}$ in the causal graph. Then we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\left\\{\\begin{array}{l l}{\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}}+\\frac{\\gamma_{j}^{2}}{4\\|B^{-1}\\|_{F}^{2}},}&{\\psi_{i}^{+},\\psi_{j}^{+}\\;a r e\\;a c t i\\nu e}\\\\ {\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}},}&{\\psi_{i}^{+},\\psi_{j}^{-}\\;a r e\\;a c t i\\nu e\\;a n d\\;\\frac{\\gamma_{j}^{2}}{4\\|B\\mu\\|^{2}}\\leq\\|c_{j}\\|^{2}}\\\\ {\\frac{\\gamma_{j}^{2}}{4\\|B^{-1}\\|_{F}^{2}},}&{\\psi_{i}^{-},\\psi_{j}^{+}\\;a r e\\;a c t i\\nu e\\;a n d\\;\\frac{\\gamma_{i}^{2}}{4\\|B\\mu\\|^{2}}\\leq\\|c_{i}\\|^{2}}\\\\ {0,}&{\\psi_{i}^{-},\\psi_{j}^{-}\\;a r e\\;a c t i\\nu e\\;a n d\\;\\frac{\\gamma_{i}^{2}}{4\\|B\\mu\\|^{2}}\\leq\\|c_{i}\\|^{2},\\frac{\\gamma_{j}^{2}}{4\\|B\\mu\\|^{2}}\\leq\\|c_{j}\\|^{2}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi_{i}^{+}\\triangleq\\left(|c_{i}^{T}B\\pmb{\\mu}|<\\frac{|\\gamma_{i}|}{2}\\,o r\\,|c_{i}^{T}B\\pmb{\\mu}|>\\frac{3|\\gamma_{i}|}{2}\\right)}\\end{array}$ , $\\begin{array}{r}{\\psi_{i}^{-}\\triangleq\\frac{|\\gamma_{i}|}{2}\\leq|c_{i}^{T}B\\pmb{\\mu}|\\leq\\frac{3|\\gamma_{i}|}{2}}\\end{array}$ and similarly for $\\psi_{j}^{+}$ and $\\boldsymbol{\\psi}_{j}^{-}$ . If one of the means say ${\\pmb{m}}_{j}$ is from the observational distribution, i.e., $\\begin{array}{r}{{\\pmb{m}}_{j}={\\pmb{m}}=B{\\pmb{\\mu}},}\\end{array}$ , then setting $\\gamma_{j}=0$ above will give the appropriate bounds and only case 2 and 4 are applicable. ", "page_idx": 14}, {"type": "text", "text": "Now, From Case 1 of the above Lemma A.2 (Eq. 42, $\\psi_{i}^{+},\\psi_{j}^{+}$ is active) and above equation we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\zeta+\\frac{\\lambda_{m i n}^{2}(D)\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)}{8\\|B^{-1}\\|_{F}^{4}}+\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}}+\\frac{\\gamma_{j}^{2}}{4\\|B^{-1}\\|_{F}^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Case 2 of the above Lemma A.2 (Eq. 42, $\\psi_{i}^{+},\\psi_{j}^{-}$ is active) and using $\\frac{\\gamma_{j}^{2}}{4\\|B\\pmb{\\mu}\\|^{2}}\\,\\leq\\,\\|\\pmb{c}_{j}\\|^{2}$ we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\zeta+\\frac{\\lambda_{m i n}^{2}(D)\\|c_{i}\\|^{2}}{8\\|B^{-1}\\|_{F}^{4}}+\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}}+\\frac{\\lambda_{m i n}^{2}(D)\\gamma_{j}^{2}}{32\\|B^{-1}\\|_{F}^{4}\\|B\\mu\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Case 3 of the above Lemma A.2 (Eq. 42, $\\psi_{i}^{-},\\psi_{j}^{+}$ is active) and using $\\frac{\\gamma_{i}^{2}}{4\\|B\\pmb{\\mu}\\|^{2}}\\,\\leq\\,\\|\\pmb{c}_{i}\\|^{2}$ we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\zeta+\\frac{\\lambda_{m i n}^{2}(D)\\|c_{j}\\|^{2}}{8\\|B^{-1}\\|_{F}^{4}}+\\frac{\\gamma_{j}^{2}}{4\\|B^{-1}\\|_{F}^{2}}+\\frac{\\lambda_{m i n}^{2}(D)\\gamma_{i}^{2}}{32\\|B^{-1}\\|_{F}^{4}\\|B\\mu\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, for Case 4 and using4\u2225B\u03b3i\u00b5\u22252 $\\frac{\\gamma_{i}^{2}}{4\\|B\\pmb{\\mu}\\|^{2}}\\leq\\|\\pmb{c}_{i}\\|^{2}$ \u2264\u2225ci\u22252 and 4\u2225Bj\u00b5\u22252 \u2264\u2225cj\u22252 we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\zeta+\\frac{\\lambda_{m i n}^{2}(D)\\gamma_{i}^{2}}{32\\|B^{-1}\\|_{F}^{4}\\|B\\mu\\|^{2}}+\\frac{\\lambda_{m i n}^{2}(D)\\gamma_{j}^{2}}{32\\|B^{-1}\\|_{F}^{4}\\|B\\mu\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, combining Eq. 6, 7, 8, 9 we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|S_{i}-S_{j}\\|_{F}^{2}+\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\zeta+\\frac{\\gamma_{i}^{2}+\\gamma_{j}^{2}}{\\operatorname*{max}\\Big(4\\|B^{-1}\\|_{F}^{2},\\frac{32\\|B^{-1}\\|_{F}^{4}\\|B\\mu\\|^{2}}{\\lambda_{m i n}^{2}(D)}\\Big)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\frac{f(B,D)}{2}\\Big(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\Big)+g(B,D)\\Big(|\\delta_{i}|+|\\delta_{j}|\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+h(B,D,\\mu)(\\gamma_{i}^{2}+\\gamma_{j}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the case when one of the distributions is observational, say w.l.o.g. $S_{j}=S$ and $m_{j}=m$ , then the same analysis holds since $c_{j}=0$ and $\\delta_{j}=0$ (from Lemma A.1) and only the analyses of case 2 and case 4 are applicable (from Lemma A.2), thereby completing the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Lemma A.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Without loss of generality, throughout our analysis, we will assume that the endogenous variables $V_{1},\\ldots,V_{n}$ are topologically ordered based on the underlying causal graph. Thus, the corresponding adjacency matrix $A$ is lower triangular. The value of $\\lVert S_{i}-S_{j}\\rVert_{F}^{2}$ will be unaffected by any permutation of the node order in the matrix $A$ as shown next. Let $\\tilde{A}=P A P^{T}$ be the adjacency matrix when the nodes are permuted by the permutation matrix $P$ where $P P^{T}\\,=\\,I$ . Also, let the corresponding permuted covariance matrix be $\\tilde{S}=\\tilde{B}\\tilde{D}\\tilde{B}^{T}$ where $\\tilde{B}=(I-\\tilde{A})^{-1}=P(I-$ $A)^{-1}P^{T}=P B P^{T}\\implies\\tilde{S}=P B P^{T}P D P^{T}P B^{T}P^{T}=P B D B^{T}P^{T}=P S P^{T}$ . Similarly, we have $\\tilde{S}_{i}=P S_{i}P^{T}$ and $\\tilde{S}_{j}=P S_{j}P^{T}$ . Now we have: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{S}_{i}-\\tilde{S}_{j}\\|_{F}^{2}=\\|P(S_{i}-S_{j})P^{T}\\|_{F}^{2}=\\|S_{i}-S_{j}\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since the permutation matrix only permutes the row and column in the above equation, the Frobenius norm remains the same. ", "page_idx": 15}, {"type": "text", "text": "First, we state a lemma that characterizes the covariance matrix $S_{i}$ of the interventional distribution.   \nThe proof of this lemma can be found in $\\S\\mathrm{A}.4$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 (Covariance Matrix Update). Let $\\mathbb{P}_{i}(\\pmb{V})=\\mathcal{N}(\\pmb{m}_{i},S_{i})$ be an interventional distribution and let the endogenous nodes $V_{1},\\ldots V_{n}$ be topologically ordered based on the underlying causal graph, then we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nS_{i}=\\left\\{\\begin{array}{l l}{\\displaystyle S-\\delta_{i}\\pmb{r}_{i}\\pmb{r}_{i}^{T},}&{f o r\\,r o o t\\,n o d e}\\\\ {\\displaystyle B_{i}D B_{i}^{T}-\\delta_{i}\\pmb{r}_{i}\\pmb{r}_{i}^{T},}&{o t h e r w i s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{r}_{i}=B e_{i}$ , $\\delta_{i}=\\sigma_{i}-\\sigma_{i}^{\\prime}$ , $B_{i}=B-B e_{i}c_{i}^{T}B_{i}$ , $B=(I-A)^{-1}$ , $S$ is the covariance matrix of the observational distribution and $D$ is the covariance matrix of the observational noise distribution (see Atomic Intervention paragraph in $\\S3.$ ). ", "page_idx": 15}, {"type": "text", "text": "Thus from the above Lemma A.3, we have $S_{i}=B_{i}D B_{i}^{T}-\\delta_{i}\\pmb{v}_{i}\\pmb{v}_{i}^{T}$ . Substituting the value of $B_{i}$ from the above lemma again we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{i}=B D B^{T}-B D u_{i}v_{i}^{T}-v_{i}u_{i}^{T}D B^{T}+\\eta_{i}v_{i}v_{i}^{T}-\\delta_{i}v_{i}v_{i}^{T}}\\\\ &{\\quad=S-B D B^{T}c_{i}e_{i}^{T}B^{T}-B e_{i}c_{i}^{T}B D B^{T}+\\eta_{i}B e_{i}e_{i}^{T}B^{T}-\\delta_{i}B e_{i}e_{i}^{T}B^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ${\\pmb u}_{i}=B^{T}{\\pmb c}_{i}$ , ${\\pmb v}_{i}=B e_{i}$ , $\\eta_{i}=\\pmb{u}_{i}^{T}D\\pmb{u}_{i}=\\pmb{c}_{i}^{T}S\\pmb{c}_{i}$ . Next, multiplying the LHS and RHS of the above equation with $B^{-1}$ and $B^{-T}$ we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\nB^{-1}(S_{i}-S)B^{-T}=\\eta_{i}e_{i}e_{i}^{T}-D B^{T}c_{i}e_{i}^{T}-e_{i}c_{i}^{T}B D-\\delta_{i}e_{i}e_{i}^{T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The absolute value of $(i,k)^{\\mathrm{th}}\\;(k\\in[n])$ entry of this matrix is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{i}^{T}B^{-1}(S_{i}-S)B^{-T}e_{k}=\\left\\{\\begin{array}{l l}{{-D_{k k}c_{i}^{T}B e_{k},}}&{{k\\neq i}}\\\\ {{\\eta_{i}-2D_{i i}c_{i}^{T}B\\overline{{{e_{i}^{*}}}}-\\delta_{i}=\\eta_{i}-\\delta_{i},}}&{{k=i}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the second case $\\pmb{c}_{i}^{T}B\\pmb{e}_{i}\\,=\\,0$ and $D_{k k}\\,=\\,\\sigma_{k}$ is the $k^{\\mathrm{th}}$ diagonal entry of the matrix $D$ . Similarly, the $(j,k)^{\\mathrm{th}}$ entry of this matrix is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{j}^{T}B^{-1}(S_{i}-S)B^{-T}e_{k}=\\left\\{0,\\begin{array}{l l}{{k\\ne i}}&{{k\\ne i}}\\\\ {{-D_{j j}e_{j}^{T}B^{T}c_{i}}}&{{k=i}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, the covariance matrix of node $V_{j}$ and $j\\neq i$ is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{S_{j}=S-B D B^{T}c_{j}e_{j}^{T}B^{T}-B e_{j}c_{j}^{T}B D B^{T}+\\eta_{j}B e_{j}e_{j}^{T}B^{T}-\\delta_{j}B e_{j}e_{j}^{T}B^{T}}}\\\\ {{B^{-1}(S_{j}-S)B^{-T}=\\eta_{j}e_{j}e_{j}^{T}-D B^{T}c_{j}e_{j}^{T}-e_{j}c_{j}^{T}B D-\\delta_{j}e_{j}e_{j}^{T}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus the absolute value of the $(i,k)^{\\mathrm{th}}$ entry of the above matrix is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n|e_{i}^{T}B^{-1}(S_{j}-S)B^{-T}e_{k}|=|D_{i i}e_{i}B^{T}c_{j}(e_{j}^{T}e_{k})|=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since we had assumed without loss of generality that $V_{j}\\,\\prec V_{i}\\implies e_{i}B^{T}c_{j}=0$ . Similarly, the $(j,k)^{\\mathrm{th}}$ entry of this matrix is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{j}^{T}B^{-1}(S_{j}-S)B^{-T}e_{k}=\\left\\{\\begin{array}{l l}{-D_{k k}c_{j}^{T}B e_{k},}&{k\\neq j}\\\\ {\\eta_{j}-2D_{j j}e_{j}^{T}B^{T}c_{j}-\\delta_{j}=\\eta_{j}-\\delta_{j}}&{k=j}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the second case $e_{j}^{T}B^{T}\\pmb{c}_{j}=0$ . Thus, taking the difference of the $i^{\\mathrm{th}}$ and $j^{\\mathrm{th}}$ row of both the matrix (from Eq. 14, 15, 17 and 18 and using the fact that $\\pmb{c}_{i}^{T}\\boldsymbol{B}\\pmb{e}_{i}=0$ , $c_{j}^{T}B e_{j}=0$ and $\\pmb{c}_{j}^{T}B\\pmb{e}_{i}=0$ since without loss of generality we have $V_{j}\\prec V_{i}$ ) we obtain the following lower bound: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\bar{B}^{\\prime\\prime}(\\bar{B}_{-}^{\\prime\\prime})\\bar{B}_{-}^{\\prime\\prime}\\|^{2}\\leq\\sum_{\\substack{u=\\bar{\\mathcal{A}}}}\\left(e_{u}^{u},\\bar{B}_{u}^{\\prime\\prime}-\\bar{B}_{u}^{\\prime\\prime}\\right)T^{-1}e_{u}^{u}+\\sum_{\\substack{u=\\bar{\\mathcal{A}}}}\\left(e_{u}^{u},\\bar{B}_{u}^{\\prime\\prime}-\\bar{B}_{u}^{\\prime}\\right)T^{-1}e_{u}^{u})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left[\\frac{1}{\\sqrt{2}}\\left(D_{u}\\epsilon_{u}^{u}T_{\\mathcal{A}}\\bar{B}_{u}^{\\prime}+(\\epsilon_{u}-A_{u}^{\\prime})T^{1}\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.+\\left[\\frac{1}{\\sqrt{2}}\\left(D_{u}\\epsilon_{u}^{u}T_{\\mathcal{A}}\\bar{B}_{u}^{\\prime}+(\\epsilon_{u}-A_{u}^{\\prime})T^{2}\\right.\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.\\left.-\\frac{1}{\\sqrt{2}}D_{u}\\epsilon_{u}^{u}T_{\\mathcal{A}}\\bar{B}_{u}^{\\prime}+(\\epsilon_{u}-A_{u}^{\\prime})T^{2}\\right.\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.\\left.+\\frac{1}{\\sqrt{2}}\\left(D_{u}\\epsilon_{u}^{u}T_{\\mathcal{A}}\\bar{B}_{u}^{\\prime}+(\\epsilon_{u}-A_{u}^{\\prime})T^{2}+(D_{u}\\epsilon_{u}^{u}T_{\\mathcal{B}}\\bar{B}_{u}^{\\prime})^{2}\\right.\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.\\left.+\\frac{1}{\\sqrt{2}}\\left(D_{u}\\epsilon_{u}^{u}T_{\\mathcal{B}}\\bar{B}_{u}^{\\prime}+(\\epsilon_{u}-A_{u}^{\\prime})T^{2}\\right.\\right.\\right.}\\\\ & \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "after substituting $\\eta_{i}~=~c_{i}^{T}S c_{i}~\\geq~\\lambda_{m i n}(S)\\|c_{i}\\|^{2}$ , $\\eta_{j}~=~c_{j}^{T}S c_{j}\\ \\geq\\lambda_{m i n}(S)\\|c_{j}\\|^{2}$ and using Lemma A.4 that gives us a lower bound for the second and third term. Thus, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|S_{i}-S_{j}\\|_{F}^{2}\\geq\\frac{\\lambda_{m i n}(D)\\left(\\lambda_{m i n}(S)\\left(\\|c_{i}\\|^{2}+\\|c_{j}\\|^{2}\\right)\\right)}{4\\|B^{-1}\\|_{F}^{4}}\\,+\\frac{\\left|\\delta_{i}\\right|}{4\\|B^{-1}\\|_{F}^{4}}\\operatorname*{min}\\left(\\left|\\delta_{i}\\right|,\\lambda_{m i n}(D)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{\\left|\\delta_{j}\\right|}{4\\|B^{-1}\\|_{F}^{4}}\\operatorname*{min}\\left(\\left|\\delta_{j}\\right|,\\lambda_{m i n}(D)\\right)}\\\\ &{\\geq\\frac{\\lambda_{m i n}^{2}(D)\\left(\\left\\|c_{i}\\right\\|^{2}+\\left\\|c_{j}\\right\\|^{2}\\right)}{4\\|B^{-1}\\|_{F}^{4}}\\,+\\frac{\\left|\\delta_{i}\\right|}{4\\|B^{-1}\\|_{F}^{4}}\\operatorname*{min}\\left(\\left|\\delta_{i}\\right|,\\lambda_{m i n}(D)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{\\left|\\delta_{j}\\right|}{4\\|B^{-1}\\|_{F}^{4}}\\operatorname*{min}\\left(\\left|\\delta_{j}\\right|,\\lambda_{m i n}(D)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\lambda_{m i n}(S)\\ \\geq\\ \\lambda_{m i n}(D)\\lambda_{m i n}^{2}(B)\\ =\\ \\lambda_{m i n}(D)$ (\u2235 $\\lambda_{m i n}(S T)~\\ge~\\lambda_{m i n}(S)\\lambda_{m i n}(T)$ and $\\lambda_{m i n}(B)\\,=\\,\\lambda_{m i n}(B^{T})\\,=\\,1$ since it is a lower triangular matrix where all diagonal entries are 1) and $\\|S T\\|_{F}\\leq\\|S\\|_{F}\\|T\\|_{F}$ . For the case when one of the covariance matrices is $S$ corresponding to no intervention and the other is $S_{i}$ corresponding to intervention on node $V_{i}$ , then from Eq. 14 and ", "page_idx": 16}, {"type": "text", "text": "the following similar analysis we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|B^{-1}(S_{i}-S)B^{-T}\\|_{F}^{2}\\geq\\Bigg[\\displaystyle\\sum_{k=1}^{n}(D_{k k}c_{i}^{T}B e_{k})^{2}+(\\eta_{i}-\\delta_{i})^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\lambda_{m i n}(D)\\displaystyle\\frac{\\eta_{i}}{4}+\\frac{|\\delta_{i}|}{4}\\operatorname*{min}\\Big(|\\delta_{i}|,\\lambda_{m i n}(D)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\lambda_{m i n}^{2}(D)\\|c_{i}\\|^{2}+|\\delta_{i}|\\operatorname*{min}\\Big(|\\delta_{i}|,\\lambda_{m i n}(D)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\|S_{i}-S\\|_{F}^{2}\\geq\\frac{\\lambda_{m i n}^{2}(D)\\|c_{i}\\|^{2}+|\\delta_{i}|}{4\\|B^{-1}\\|_{F}^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "thereby completing our proof of this lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. Given $\\lambda\\geq0$ and $\\eta\\geq0$ , the following inequality holds true: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda\\eta+(\\eta-\\delta)^{2}\\geq\\frac{\\lambda\\eta}{4}+\\frac{|\\delta|}{4}\\operatorname*{min}\\Big(|\\delta|,\\lambda\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Case $^{\\,I}$ : $\\left(\\delta<0\\right)$ Let $T\\triangleq\\lambda\\eta+(\\eta-\\delta)^{2}$ , then we have $T\\ge\\lambda\\eta+\\delta^{2}\\ge\\lambda\\eta+\\delta^{2}/4$ . Case 2: $\\bar{\\delta}>0$ and $[0\\leq\\eta<\\delta/2\\;\\mathrm{or}\\;\\eta>3\\delta/2]\\Bigl)$ Again in this case $T\\ge\\lambda\\eta+\\delta^{2}/4$ . Case $^3$ : $\\bar{\\delta}>0$ and $\\delta/2\\le\\eta\\le3\\delta/2\\AA)$ In this case, we have $T\\ge\\lambda\\eta$ . Also, we have $\\eta>\\delta/2\\implies$ $T\\geq\\lambda\\delta/2$ which together implies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T\\geq\\operatorname*{max}\\Big(\\lambda\\eta,\\lambda\\displaystyle\\frac{\\delta}{2}\\Big)}\\\\ &{\\geq\\displaystyle\\frac{\\lambda\\eta}{2}+\\frac{\\lambda\\delta}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, combining the above three cases we have the following lower bound on the value of $T$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T\\geq\\operatorname*{min}\\Big(\\lambda\\eta+\\displaystyle\\frac{\\delta^{2}}{4},\\displaystyle\\frac{\\lambda\\eta}{2}+\\frac{|\\delta|}{4}\\Big)}}\\\\ {{\\qquad\\geq\\displaystyle\\frac{\\lambda\\eta}{4}+\\operatorname*{min}\\Big(\\frac{\\delta^{2}}{4},\\displaystyle\\frac{\\lambda|\\delta|}{4}\\Big)}}\\\\ {{\\qquad=\\displaystyle\\frac{\\lambda\\eta}{4}+\\frac{|\\delta|}{4}\\operatorname*{min}\\Big(|\\delta|,\\lambda\\Big),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Lemma A.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. From Eq. 3, we have $S_{i}=B_{i}D_{i}B_{i}^{T}$ where $B_{i}=(I-A+e_{i}c_{i}^{T})^{-1}$ and $D_{i}=D-(\\sigma_{i}-$ $\\sigma_{i}^{'})e_{i}e_{i}^{T}\\triangleq D-\\delta_{i}e_{i}e_{i}^{T}$ . Since $e_{i}c_{i}^{T}$ is a rank-1 update to the (I-A) matrix, using the ShermanMorrison identity we obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{B_{i}=(I-A+e_{i}c_{i}^{T})^{-1}}}\\\\ {{\\ =(I-A)^{-1}-\\frac{(I-A)^{-1}e_{i}c_{i}^{T}(I-A)^{-1}}{1+c_{i}^{T}(I-A)^{-1}e_{i}}}}\\\\ {{\\ =B-\\frac{B e_{i}c_{i}^{T}B}{1+c_{i}^{T}B e_{i}}}}\\\\ {{\\ =B-B e_{i}c_{i}^{T}B}}\\\\ {{\\triangleq B-r_{i}q_{i}^{T},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{l}{r_{i}\\triangleq\\,\\frac{B e_{i}}{d_{i}}}\\end{array}$ , scalar $d_{i}\\triangleq1+\\pmb{c}_{i}^{T}\\pmb{B}\\pmb{e}_{i}=1$ and $\\pmb{q}_{i}\\triangleq B^{T}\\pmb{c}_{i}$ . The scalar $d_{i}=1$ since $\\pmb{q}_{i}^{T}\\pmb{e}_{i}=$ $\\pmb{c}_{i}^{T}\\boldsymbol{B}\\pmb{e}_{i}=0$ given by the following lemma: ", "page_idx": 17}, {"type": "text", "text": "Lemma A.5. Let A and B be a lower triangular matrix and $c_{i}$ be a vector such that $c_{i}=a_{i}\\!-\\!a_{i}^{'}$ and $[\\pmb{c}_{i}]_{t}=0,\\forall t\\geq i$ (see Atomic Interventions paragraph of $\\S3$ for definition), then $\\pmb{q}_{i}^{T}\\pmb{e}_{i}=\\pmb{c}_{i}^{T}\\pmb{B}\\pmb{e}_{i}=0$ . Also, if $c_{i}\\neq\\mathbf{0}$ , then $\\pmb q_{i}=B^{T}\\pmb c_{i}\\neq\\mathbf0$ . ", "page_idx": 18}, {"type": "text", "text": "The proof of the above lemma can be found below after the proof of the current lemma. Next, $S_{i}$ is given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{i}=B_{i}D_{i}B_{i}^{T}}\\\\ &{\\quad=B_{i}(D-\\delta_{i}e_{i}e_{i}^{T})B_{i}^{T}}\\\\ &{\\quad=\\underbrace{B_{i}D B_{i}^{T}}_{\\mathrm{term~l}}-\\delta_{i}\\underbrace{B_{i}e_{i}e_{i}^{T}B_{i}^{T}}_{\\mathrm{term~2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If the intervened node $V_{i}$ is one of root nodes of the underlying unknown causal graph, then $\\displaystyle{\\pmb{c}}_{i}=\\mathbf{0}\\implies{\\pmb q}_{i}=B^{T}{\\pmb{c}}_{i}=\\mathbf{0}\\implies B_{i}=B$ (from Eq. 25), and thus: ", "page_idx": 18}, {"type": "equation", "text": "$$\nB_{i}D B_{i}^{T}=B D B^{T}=S,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $S$ is the covariance matrix of the observational distribution. Otherwise, for non-root nodes, term $^{\\,l}$ in the above equation is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{i}D B_{i}=(B-r_{i}\\pmb{q}_{i}^{T})D(B-r_{i}\\pmb{q}_{i}^{T})^{T}}\\\\ &{\\ =B D B^{T}-B D\\pmb{q}_{i}r_{i}^{T}-(B D\\pmb{q}_{i}r_{i}^{T})^{T}+r_{i}\\pmb{q}_{i}^{T}D\\pmb{q}_{i}r_{i}^{T}}\\\\ &{\\ =S-\\pmb{w}_{i}r_{i}^{T}-r_{i}\\pmb{w}_{i}^{T}+\\eta_{i}r_{i}r_{i}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{w}_{i}=B D\\pmb{q}_{i}$ and scalar $\\eta_{i}=\\pmb q_{i}^{T}D\\pmb q_{i}$ . Next simplifying term 2 in Eq. 26 we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{i}e_{i}(B_{i}e_{i})^{T}=\\Big(B e_{i}-r_{i}q_{i}^{T}e_{i}\\Big)\\Big(B e_{i}-r_{i}q_{i}^{T}e_{i}\\Big)^{T}}\\\\ &{\\qquad\\qquad\\qquad=B e_{i}e_{i}^{T}B^{T}=r_{i}r_{i}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $\\pmb{q}_{i}^{T}\\pmb{e}_{i}=\\pmb{c}_{i}^{T}\\pmb{B}\\pmb{e}_{i}=0$ from Lemma A.5 and $r_{i}\\,\\triangleq\\,B e_{i}$ (from Eq. 25). Thus the covariance matrix of the endogenous variables in the intervened distribution is given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{i}=\\left\\{{\\begin{array}{l l}{S-\\delta_{i}{\\pmb r}_{i}{\\pmb r}_{i}^{T},}&{{\\mathrm{for~root~node}}}\\\\ {B_{i}D B_{i}^{T}-\\delta_{i}{\\pmb r}_{i}{\\pmb r}_{i}^{T},}&{{\\mathrm{otherwise}}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "thereby completing our proof. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.5. Showing ${\\bf{q}}_{i}\\neq{\\bf{0}}$ : We are given that $B$ is a lower triangular matrix with nonzero diagonal entries. Also, $\\pmb{c}_{i}=\\pmb{a}_{i}-\\pmb{a}_{i}^{'}\\neq0$ and $[\\pmb{c}_{i}]_{t}=0,\\forall t\\geq i\\implies\\exists t\\,s.t.\\,[\\pmb{c}_{i}]_{t}\\neq0$ and let $t^{*}$ be that last index where $c_{i}$ is non-zero. Now lets look at the $[B^{T}\\pmb{c}_{i}]_{t^{*}}=\\pmb{c}_{t^{*}}\\cdot[B^{T}]_{t^{*},t^{*}}\\neq0$ since $B^{T}$ is a upper triangular matrix and $[c_{i}]_{t^{*}}\\neq0$ . ", "page_idx": 18}, {"type": "text", "text": "Showing $\\pmb{q}_{i}^{T}\\pmb{e}_{i}=0$ : $B e_{i}$ is the $i^{t h}$ column of the lower triangular matrix $B\\implies[B e_{i}]_{t}=0,\\forall t<$ $i\\implies c_{i}^{T}B e_{i}=0$ since $[\\pmb{c}_{i}]_{t}=0,\\forall t\\geq i$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.5 Proof of Lemma A.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Similar to the proof of Lemma A.1, without loss of generality, we will assume that the endogenous variables are topologically ordered based on the underlying causal graph such that the adjacency matrix $A$ is lower triangular. The permutation matrix will only permute the rows of the mean vectors $\\mathbf{\\nabla}m_{i}$ and ${\\pmb{m}}_{j}$ ; hence, there will be no effect on the value of $\\lVert\\bar{\\boldsymbol{m}_{i}}-\\boldsymbol{m}_{j}\\rVert_{F}^{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Now, let the mean of the Gaussian distribution corresponding to intervention on node $V_{i}$ be given by (see Atomic Intervention paragraph in $\\S3$ for definition): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\pmb m}_{i}=B_{i}{\\pmb\\mu}_{i}=(B-B e_{i}c_{i}^{T}B)({\\pmb\\mu}+\\gamma_{i}{\\pmb e}_{i})}}\\\\ {{\\Longrightarrow\\ {\\pmb B}^{-1}{\\pmb m}_{i}=(I-e_{i}c_{i}^{T}B)({\\pmb\\mu}+\\gamma_{i}{\\pmb e}_{i}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $B_{i}=(I-A_{i})^{-1}=B-B e_{i}c_{i}^{T}B$ (from Lemma A.3), $\\pmb{\\mu}_{i}$ is the new mean vector for the noise distribution, $\\mu$ is the mean vector of the observational noise distribution and $\\gamma_{i}e_{i}$ is the update ", "page_idx": 18}, {"type": "text", "text": "to the mean of the noise distribution when intervening on node $V_{i}$ . Similarly, the mean corresponding to the intervened distribution on node $V_{j}$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{m}_{j}=B_{j}\\pmb{\\mu}_{j}=(B-B e_{j}\\pmb{c}_{j}^{T}B)(\\pmb{\\mu}+\\gamma_{j}\\pmb{e}_{j})}\\\\ &{\\Longrightarrow\\ \\pmb{B}^{-1}\\pmb{m}_{j}=(I-\\pmb{e}_{j}\\pmb{c}_{j}^{T}B)(\\pmb{\\mu}+\\gamma_{j}\\pmb{e}_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Looking at the $i^{\\mathrm{th}}$ entry of the vector $B^{-1}m_{i}$ , we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\ne_{i}^{T}B^{-1}{m_{i}}=e_{i}^{T}\\pmb{\\mu}-\\pmb{c}_{i}^{T}B\\pmb{\\mu}+\\gamma_{i}-\\pmb{c}_{i}^{T}B\\pmb{e}_{i}^{\\pmb{\\nu}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the $i^{\\mathrm{th}}$ entry of the vector $B^{-1}m_{j}$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{i}^{T}B^{-1}m_{j}=e_{i}^{T}\\pmb{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, the $j^{\\mathrm{th}}$ entry of the vector $B^{-1}m_{i}$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{j}^{T}B^{-1}m_{i}=e_{j}^{T}\\pmb{\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the $i^{\\mathrm{th}}$ entry of the vector $B^{-1}m_{j}$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\ne_{j}^{T}B^{-1}m_{j}=e_{j}^{T}\\pmb{\\mu}-\\pmb{c}_{j}^{T}B\\pmb{\\mu}+\\gamma_{j}-\\pmb{c}_{j}^{T}B\\pmb{\\6}_{j}^{\\pmb{\\nu}^{0}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, the difference in the mean of both distributions can be lower bounded by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|B^{-1}(\\pmb{m}_{i}-\\pmb{m}_{j})\\|_{F}^{2}\\geq(\\gamma_{i}-\\pmb{c}_{i}^{T}B\\pmb{\\mu})^{2}+(\\gamma_{j}-\\pmb{c}_{j}^{T}B\\pmb{\\mu})^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now based on the value of $|\\gamma|_{i}$ and $|c_{i}^{T}B\\pmb{\\mu}|$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\gamma_{i}-c_{i}^{T}B\\mu)^{2}\\geq\\left\\{\\frac{\\gamma_{i}^{2}}{4},\\quad\\psi_{i}^{+}\\triangleq|c_{i}^{T}B\\mu|<\\frac{|\\gamma_{i}|}{2}\\;\\mathrm{or}\\;|c_{i}^{T}B\\mu|>\\frac{3|\\gamma_{i}|}{2}\\right.}\\\\ {0,\\quad\\left.\\psi_{i}^{-}\\triangleq\\frac{|\\gamma_{i}|}{2}\\leq|c_{i}^{T}B\\mu|\\leq\\frac{3|\\gamma_{i}|}{2},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\psi_{i}^{+}$ and $\\psi_{i}^{-}$ are used to denote two different mutually exclusive different events for ease of exposition later. In the case when $\\psi_{i}^{-}$ is true, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\gamma_{i}^{2}}{4}\\leq(c_{i}^{T}B\\pmb{\\mu})^{2}\\leq\\|\\pmb{c}_{i}\\|^{2}\\|B\\pmb{\\mu}\\|^{2}}\\\\ {\\frac{\\gamma_{i}^{2}}{4\\|B\\pmb{\\mu}\\|^{2}}\\leq\\|\\pmb{c}_{i}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\gamma_{j}-c_{j}^{T}B\\mu)^{2}\\geq\\left\\{\\frac{\\gamma_{j}^{2}}{4},\\quad\\psi_{j}^{+}\\triangleq|c_{j}^{T}B\\mu|<\\frac{|\\gamma_{j}|}{2}\\,\\,\\mathrm{or}\\,\\,|c_{j}^{T}B\\mu|>\\frac{3|\\gamma_{j}|}{2}\\right.}\\\\ {0,\\quad\\left.\\psi_{j}^{-}\\triangleq\\frac{|\\gamma_{j}|}{2}\\leq|c_{j}^{T}B\\mu|\\leq\\frac{3|\\gamma_{j}|}{2},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and in the event when $\\psi_{j}^{-}$ is true, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\gamma_{j}^{2}}{4}\\leq(\\pmb{c}_{j}^{T}B\\pmb{\\mu})^{2}\\leq\\|\\pmb{c}_{j}\\|^{2}\\|B\\pmb{\\mu}\\|^{2}}\\\\ {\\frac{\\gamma_{j}^{2}}{4\\|B\\pmb{\\mu}\\|^{2}}\\leq\\|\\pmb{c}_{j}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining Eq. 38 and 40 and using $\\|B^{-1}(\\pmb{m}_{i}-\\pmb{m}_{j})\\|_{F}\\leq\\|B^{-1}\\|_{F}\\|\\pmb{m}_{i}-\\pmb{m}_{j}\\|_{F}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\left\\{\\begin{array}{l l}{\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}}+\\frac{\\gamma_{j}^{2}}{4\\|B^{-1}\\|_{F}^{2}},}&{\\psi_{i}^{+},\\psi_{j}^{+}\\mathrm{~are~active}}\\\\ {\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}},}&{\\psi_{i}^{+},\\psi_{j}^{-}\\mathrm{~are~active,}\\mathrm{Eq.41~holds}}\\\\ {\\frac{\\gamma_{j}^{2}}{4\\|B^{-1}\\|_{F}^{2}},}&{\\psi_{i}^{-},\\psi_{j}^{+}\\mathrm{~are~active,}\\mathrm{Eq.39~holds}}\\\\ {0,}&{\\psi_{i}^{-},\\psi_{j}^{-}\\mathrm{~are~active,}\\mathrm{Eq.39~and~41~holds}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the case when one of the mean say ${\\pmb{m}}_{j}$ is observational then: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{m_{j}=m=B\\mu}}\\\\ {{\\implies B^{-1}m_{j}=\\mu.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus the $i^{\\mathrm{th}}$ and $j^{\\mathrm{th}}$ entry of ${\\pmb{m}}_{j}$ is $e_{i}^{T}{\\boldsymbol{\\mu}}$ and $e_{j}^{T}\\boldsymbol{\\mu}$ , respectively. Thus using Eq. 33 and 35 we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|B^{-1}(\\pmb{m}_{i}-\\pmb{m}_{j})\\|_{F}^{2}\\geq(\\gamma_{i}-\\pmb{c}_{i}^{T}B\\pmb{\\mu})^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then again, following a similar analysis as in Eq. 38 and 39 we have the following cases: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|m_{i}-m_{j}\\|_{F}^{2}\\geq\\left\\{\\frac{\\gamma_{i}^{2}}{4\\|B^{-1}\\|_{F}^{2}},\\begin{array}{l}{\\psi_{i}^{+}\\mathrm{is~active}}\\\\ {\\psi_{i}^{-}\\mathrm{~is~active},\\,\\mathrm{Eq.~}39\\mathrm{~holds},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is equivalent to say that only case 2 and 4 are applicable in Eq. 42. This completes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B Setup and Additional Empirical Result ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Experimental Setup Discussion - Simulation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Random Graph Generation: As mentioned in the Simulation setup in $\\S6$ , we randomly generate the adjacency matrix $A$ of the causal graphs used to simulate the mixture distribution. All the weights in the adjacency matrix are sampled in the range $[-1,-0.5]\\cup[0.5,1]$ bounded away from zero. This gives us a complete graph of all nodes. Thus, to introduce sparsity in the graph, we only keep an edge with probability $\\zeta$ by setting the corresponding value in the adjacency matrix to zero if the edge is removed. Unless otherwise specified, we set $\\zeta=0.8$ for all our experiments. ", "page_idx": 20}, {"type": "text", "text": "Step 1 of Alg. 1: We use the GaussianMixture class from the scikit-learn Python package to disentangle the components of the mixture. For all our experiments, we use the default $\\bar{t}o l=\\bar{1}0^{-3}$ used by GaussianMixture to decide on convergence of the underlying EM algorithm. ", "page_idx": 20}, {"type": "text", "text": "Step 2 of Alg. 1: We run the UT-IGSP algorithm in Step 2 of our Alg. 1 with the standard parameter mentioned in the documentation. Specifically, we use $\\dot{\\alpha}=10^{-3}$ for both MemoizedCITester and MemoizedInvarianceTester functions used by UT-IGSP. ", "page_idx": 20}, {"type": "text", "text": "Specific to results in Appendix Unless otherwise specified, for all the results in the appendix we run all the experiments for 5 random settings. We ignore the error bars in the appendix for clarity in exposition. Also for all the experiments, the half-intervention setting i.e. where the mixture contains intervention on half of the randomly selected nodes is considered. ", "page_idx": 20}, {"type": "text", "text": "B.2 Experimental Setup Discussion - Biology Dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Dataset: In the interventional data the signaling protein is also perturbed along with the receptor enzymes. The different perturbed signaling proteins (along with the number of samples corresponding) are: Akt (911), PKC (723), PIP2 (810), Mek (799), PIP3 (848). The observational data contained 1755 samples so overall 5846 samples were used for our experiments. For details see Wang et al. [31] and Sachs et al. [22]. ", "page_idx": 20}, {"type": "text", "text": "B.3 Code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The source code to all the experiments can be found in the following GitHub repository: https://github.com/BigBang0072/mixture_mec ", "page_idx": 20}, {"type": "text", "text": "B.4 Computational Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We use an internal cluster of CPUs to run all our experiments. We run 10 random runs for each of the experimental configurations and report the mean (as points) and $5^{t h}$ and $95^{t h}$ quantiles as error bars for all our experiments in the main paper, and we report only mean the mean for the experiments in the appendix to declutter the figures. ", "page_idx": 20}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/9b1dadc1b72e43b150679272a9667033db28756ce1cf13dafb303c9d2e96ac61.jpg", "img_caption": ["(a) num interventions $=$ all "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/0cf5f0b34947b32aa49448517a5ffadfeeaa4a2f4065c22718de3dcd3a17411f.jpg", "img_caption": ["(c) num interventions $=$ half "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/452e462fa1cefcf3040912104a8bfcc289cc2bfa641dcb0ea2cedd43bcb99b78.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "", "img_caption": ["(d) num interventions $=$ half "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 2: Other Evaluations Metrics for the simulation experiments in Fig. 1: The top row denotes the corresponding metrics for all interventions in the mixture setting and the bottom row to the half setting. The first column shows the number of components estimated by our algorithm MixtureUTIGSP. For the all setting, the actual number of components corresponding to the system with nodes 4,6 and 8 are 5,7,9 respectively (one intervention on each node $^+$ one observational distribution). We observe that Mixture-UTIGSP is able to correctly estimate the number of components even with a small number of samples. Similarly, for half setting, the actual number of components corresponding to the system with nodes 4,6 and 8 are 3,4,5 respectively (intervention on half of node and one observational distribution). Even for this case Mixture-UTIGSP is able to correctly estimate the number of components. The second column shows the error in the estimation of the mixing coefficient $\\pi_{i}$ \u2019s, see Definition 4.1). For both cases, we observe that the error in the estimation of the mixing coefficient goes to zero as the sample size increases. ", "page_idx": 21}, {"type": "text", "text": "B.5 Evaluation Metric ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Parameter Estimation Error Given the mixture distribution, the first step of our Alg. 1 estimates the parameters of every interventional and observational distribution present in the mixture (mixing weight $\\pi_{i}$ , mean ${\\pmb{m}}_{i}$ and covariance matrix $S_{i}$ ). Alg. 1 returns the maximum number of possible components, i.e. $k=n+1$ , by default. For all our experiments, we used this default value and left searching over the number of components for future work. To calculate the parameter estimation error, we first find the best match of the estimated parameters with the ground truth parameters based on the minimum error between the mean and covariance matrix. More specifically, let $k^{*}$ be the ground truth number of components in the mixture. Then, we iterate over all possible $k^{*}$ sized subsets of the estimated parameters and choose the one with the smallest absolute error sum between the mean and covariance matrix. Formally: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Parameter~Estimation~Error~}\\triangleq\\operatorname*{min}\\left\\{\\sum_{i=1}^{k^{\\ast}}\\left(|m_{i}-\\hat{m}_{\\rho(i)}|+|S_{i}-\\hat{S}_{\\rho(i)}|\\right):\\rho\\in p e r m([n+1])\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/da46363f674780a59e6acf0858593408847f514faf9ea45ea3ff8a9b0a3ae581.jpg", "img_caption": ["Figure 3: Performance of Alg. 1 as we change the cutoff ratio used for automatic component selection: We consider graphs with 6 nodes in this experiment with half intervention setting. In step 2 of Mixture-UTIGSP, we select the number of components using the log-likelihood curve. We scan the curve starting from the mixture model with the largest number of components to the smallest and stop where the relative change in the likelihood increases above a cutoff ratio (to select the elbow point of the curve). The cutoff ratio in the algorithm is chosen to be an arbitrary number close to zero. Here we compare the performance of Mixture-UTIGSP on all three metrics for the half setting of Fig. 1 as we vary the cutoff ratio. We observe that for the cutoff ratio close to zero i.e. 0.01, 0.15,0.3 the performance remains similar showing that the model selection criteria are robust to the selected cutoff ratio. The number of nodes "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "where $p e r m([n+1])$ represents all possible permutations of the indices $[0,1,\\ldots,n]$ and $\\rho^{*}$ is the corresponding permutation with the minimum error. ", "page_idx": 22}, {"type": "text", "text": "Average Jaccard Similarity (JS) In step 2 of our Alg. 1, UT-IGSP estimates the unknown interventional targets for each of the individual components disentangled in Step 1. We use the same matching $(\\rho^{*})$ found in the parameter estimation error step (as mentioned above) to calculate the Jaccard similarity between the estimated and ground truth intervention target for that component. Formally: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Avg.~Jaccard~Similarity}\\triangleq\\frac{1}{k^{\\ast}}\\sum_{i=1}^{k^{\\ast}}J S(t_{i},\\hat{t}_{\\rho(i)})=\\frac{1}{k^{\\ast}}\\sum_{i=1}^{k^{\\ast}}\\frac{|t_{i}\\cap\\hat{t}_{\\rho^{\\ast}(i)}|}{|t_{i}\\cup\\hat{t}_{\\rho^{\\ast}(i)}|},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pmb{t}_{i}$ is the ground truth intervention target set and $\\hat{t}_{j}$ is the estimated target set. For the case when both $\\pmb{t}_{i}=\\hat{\\pmb{t}}_{i}=\\phi$ , then $J S(\\pmb{t}_{i},\\hat{\\pmb{t}}_{i})\\triangleq1$ . ", "page_idx": 22}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/84070edf70aa2a5b5c9bc304af059a17659f7b283eaedbcd9174287346951316.jpg", "img_caption": ["Figure 4: Performance of Alg. 1 as we change the density of the underlying true causal graph: The mixture data contains atomic interventions on all nodes as well as observational data (half setting as described in the results in $\\S6_{\\cdot}$ ). The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD (see Evaluation metric paragraph in $\\S6$ ). In this experiment, we vary the density of the underlying causal graph by keeping the edges in a fully connected graph with a fixed probability, labeled as density in the legend of the above plots (see random graph generation paragraph in $\\S B.1$ for details). The maximum possible density is 1, i.e., the probability of keeping an edge is 1, corresponding to a fully connected graph, and the lowest possible density is 0. We observe that as the density of the graph increases, we require more samples to achieve similar performance to less dense graphs on all three metrics. Our Theorem 4.1 shows that the sample complexity required for estimating the parameters of the mixture is proportional to the norm of the adjacency matrix $\\|A\\|$ and as the density of the graph increases $\\|A\\|$ increases. Thus, as the density increases, we require more samples to achieve a similar performance in estimating the parameters of the mixture, as seen in the parameter estimation error plot above. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/0f81ba64cb3b16b2997931fc81dbe1e01770782a31454349dd1f85392174caf1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 5: Ground truth and estimated causal graph for Protein signaling dataset [22]: Fig 5a is the graph created with the help of domain experts for this problem [31]. 5b shows the graph estimated by our Mixture-UTIGSP and 5c is the graph estimated by oracle UT-IGSP when they are given the ground truth disentangled mixture. The blue colored arrow in 1b and 1c shows the correctly recovered edges in the domain expert graph. Green shows the edges with the same skeleton in the domain expert graph but in a reversed direction. The red shows the edges that are incorrectly added in the estimated graph. We observe that Mixture-UTIGSP correctly identifies two more edges (PKA- $>$ ERK and PKA- $>$ Akt) as compared to an oracle which could be due to randomness in the UTIGSP algorithm. For this estimation, the best-performing cutoff of 0.01 was selected (see Table 1). ", "page_idx": 23}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/87a9639ca6741b6718c34ce12df7d49158208e9ed37859378f561ba481d5dc8d.jpg", "img_caption": ["(a) Varying the mean of the noise distribution $(|\\gamma_{i}|)$ : The initial mean of the noise distribution is 0.0 for all the nodes. Upon intervening on a node to generate the interventional distribution, we change the mean of the noise to a different value. The variance of the noise distribution of the intervened node is kept the same as the initial distribution i.e. 1.0. From Theorem 4.1, we expect that as the new mean increases further from the initial mean $=0.0$ , the parameter estimation error should be lower for a given sample size and lead to better performance in intervention target estimation and causal discovery. As expected, the setting with the smallest change in the mean of the noise distribution (blue curve) has the worst performance. The case when the new noise mean is 10.0 (orange curve) is unusual where we see an unexpected increase in parameter estimation error. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "aC9mB1PqYJ/tmp/11db53fa80a981655f1d028bff576a0c1463f218abc52f3438844db66ae549d2.jpg", "img_caption": ["(b) Varying the variance of the noise distribution $(|\\delta_{i}|)$ : The initial variance of the noise distribution is 1.0 for all nodes, and we change the variance of the new noise distribution upon intervention in this experiment. The mean of the noise distribution of the intervened distribution is kept the same as the initial distribution i.e. 0.0. From Theorem 4.1, we see that sample complexity to recover the parameters of the mixture distribution is inversely proportional to the change in the noise variance $\\left|\\delta_{i}\\right|$ . Thus, we expect that the performance of Alg. 1 should improve as the new noise variance moves away from the initial noise variance 1.0. We can see in the above plot that the performance of the green curve ${\\dot{\\delta}}_{i}=0$ ) is worst in terms of the Jaccard similarity and SHD of the recovered graph, validating our expectation. Parameter estimation cannot be directly compared since as the variance increases, the norm of the covariance matrix increases, and thus, the overall error in the estimation error increases. We observe that compared to changing the mean (Fig. 6a) increasing the variance gives slightly lower performance gains. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 6: Performance of Alg. 1 as we change different parameters of interventions: We consider graphs with 6 nodes in this experiment. The mean of all noise distributions without any intervention is 0.0, and the variance is 1.0. The mixture data contains atomic interventions on all nodes and observational data (half setting as described in results in $\\S6$ ). The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD (see evaluation metric paragraph in $\\S6$ ). From Theorem 4.1, we observe that the sample complexity for recovering the parameters of the mixture is inversely proportional to the change in the mean of the noise distribution $\\gamma_{i}^{2}$ and change in the variance of the noise distribution $\\lvert\\delta_{i}\\rvert$ . In this experiment, we vary these two parameters one at a time and empirically validate this observation. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have given all the proof for our theoretical result (\u00a75) and validated the theoretical claims with empirical results $\\S6$ . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See $\\S8$ . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided all the assumptions and proof of our theoretical result in $\\S4$ , 5 and A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See $\\S6$ and B for the full details of empirical results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See $\\S B$ for the GitHub link. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See $\\S6$ and B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We report the $5^{t h}$ and $95^{t h}$ quantile of all the metrics from the 10 different random runs in the main. In the appendix we don\u2019t plot the error bars for clarity of exposition. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See $\\S B$ . ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we do. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We theoretically study the problem in a simple setting that might not be directly applicable in real-world scenarios. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We don\u2019t release any data or models. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: To the best of our knowledge, we have cited all the prior work and any package or library used in our empirical study. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We are not releasing any new assets right now along with our paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work doesn\u2019t involve any human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work doesn\u2019t need this. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]