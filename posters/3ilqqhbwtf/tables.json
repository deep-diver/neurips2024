[{"figure_path": "3ilqQHBWTf/tables/tables_5_1.jpg", "caption": "Table 1: Quantitative comparison with state-of-the-art methods on both synthetic and real-world benchmarks. The best and second best results of each metric are highlighted in red and blue, respectively. To align the metric of SSIM, we re-evaluate the previous methods based on their pre-trained models to obtain SSIM*. ", "description": "This table presents a quantitative comparison of the proposed LaSe-E2V model against eight state-of-the-art event-to-video reconstruction methods across three benchmark datasets (ECD, MVSEC, HQF).  The metrics used for comparison are Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  The best and second-best results for each metric are highlighted to easily identify the superior performance of the models.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/tables/tables_8_1.jpg", "caption": "Table 2: Quantitative comparison on temporal consistency on ECD [43] based on VBench [26].", "description": "This table presents a quantitative comparison of the temporal consistency of different event-to-video (E2V) reconstruction methods on the Event Camera Dataset (ECD) using the VBench metric.  The metric assesses three aspects of temporal quality: Subject Consistency (how well the subject is consistently reconstructed across frames), Background Consistency (how well the background is consistently reconstructed across frames), and Motion Smoothness (how smooth the motion is in the reconstructed video).  The table shows the performance of various methods including E2VID, FireNet, SPADE-E2VID, SSL-E2VID, ET-Net, HyperE2VID, and the proposed LaSe-E2V method.  The results are expressed as percentages, with higher percentages indicating better performance.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/tables/tables_9_1.jpg", "caption": "Table 3: Ablation study on context conditions.", "description": "This ablation study investigates the impact of different combinations of event data, text descriptions, and previous frames on the performance of the LaSe-E2V model.  The results are quantified using MSE, SSIM, and LPIPS metrics, showing that the combination of all three provides the best reconstruction results.", "section": "4.4 Ablation Study"}, {"figure_path": "3ilqQHBWTf/tables/tables_9_2.jpg", "caption": "Table 4: Ablation study on key components. \"EML\" denotes the event-aware mask loss. \"EI\" denotes event-based initialization.", "description": "This table presents the results of ablation studies conducted on the LaSe-E2V framework.  It shows the impact of the Event-guided Spatiotemporal Attention (ESA) module, the event-aware mask loss (EML), and the event-based initialization (EI) strategy on the key metrics: Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  Each row represents a different combination of these components, allowing for a quantitative analysis of their individual and combined contributions to the overall performance.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/tables/tables_17_1.jpg", "caption": "Table 1: Quantitative comparison with state-of-the-art methods on both synthetic and real-world benchmarks. The best and second best results of each metric are highlighted in red and blue, respectively. To align the metric of SSIM, we re-evaluate the previous methods based on their pre-trained models to obtain SSIM*. ", "description": "This table presents a quantitative comparison of the proposed LaSe-E2V model with eight existing state-of-the-art event-to-video (E2V) reconstruction methods across three benchmark datasets (ECD, MVSEC, and HQF).  The metrics used for comparison include Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  Both synthetic and real-world datasets were used to evaluate performance.  The table highlights the best and second-best results for each metric and dataset to showcase the superior performance of LaSe-E2V. Note that SSIM* represents a reevaluation of previous methods using a unified metric to ensure fair comparison.", "section": "4 Experiments"}, {"figure_path": "3ilqQHBWTf/tables/tables_17_2.jpg", "caption": "Table 1: Quantitative comparison with state-of-the-art methods on both synthetic and real-world benchmarks. The best and second best results of each metric are highlighted in red and blue, respectively. To align the metric of SSIM, we re-evaluate the previous methods based on their pre-trained models to obtain SSIM*. ", "description": "This table presents a quantitative comparison of the proposed LaSe-E2V model against eight state-of-the-art event-to-video reconstruction methods.  The comparison is done across three different datasets (ECD, MVSEC, and HQF), using four evaluation metrics: Mean Squared Error (MSE), Structural Similarity Index (SSIM), LPIPS, and a recalculated SSIM* (to account for inconsistencies in previous SSIM calculations). The best and second-best results for each metric on each dataset are highlighted.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/tables/tables_18_1.jpg", "caption": "Table 7: Quantitative comparison of HS-ERGB [58]. Results are conducted on 3 sequences with a total of 497 frames.", "description": "This table presents a quantitative comparison of the proposed LaSe-E2V method against existing state-of-the-art Event-to-Video (E2V) reconstruction methods on the HS-ERGB dataset.  The comparison focuses on three metrics: Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). Lower MSE and LPIPS values, and higher SSIM values, indicate better reconstruction quality. The table shows that LaSe-E2V significantly outperforms existing methods on all three metrics, demonstrating its improved performance in reconstructing high-quality videos from event data.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "3ilqQHBWTf/tables/tables_18_2.jpg", "caption": "Table 8: Comparisons between different prompting models on boxes of HQF.", "description": "This table compares the performance of two different prompting models, RAM and BLIP, on the task of generating image captions for boxes from the High-Quality Frames (HQF) dataset.  The comparison is based on three metrics: Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). Lower MSE and LPIPS scores indicate better performance, while higher SSIM indicates better performance.", "section": "4.2 Comparison with State-of-the-Art Methods"}]