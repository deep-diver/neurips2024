[{"figure_path": "S1fc92uemC/tables/tables_4_1.jpg", "caption": "Table 1: The instruction template for Stage-II. It is worth noting that all the tasks can be unified in the (x, c, y) format, which is able to facilitate effective knowledge transfer across tasks.", "description": "This table presents the instruction template used in Stage-II of the RankRAG framework.  It shows how different tasks (context-rich QA, retrieval-augmented QA, context ranking, and retrieval-augmented ranking) are formatted for the instruction tuning process in a unified (x, c, y) structure. This structure allows for effective knowledge transfer across the different task types. Each row represents a task type, showing the input question (x), the context provided (c), and the expected output answer (y).", "section": "4 RankRAG"}, {"figure_path": "S1fc92uemC/tables/tables_6_1.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"-\". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents the performance comparison of RankRAG against various baselines across nine different datasets.  The results are categorized into models without retrieval-augmented generation (RAG) and those with RAG.  Zero-shot evaluation is used, meaning no additional demonstrations were provided during testing.  The table highlights the performance of RankRAG, especially its competitive results against strong LLMs like GPT-4 and GPT-4-turbo, as well as other state-of-the-art RAG models.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study of RankRAG. We use Llama3-8B as the backbone. Where 'RQA' and 'RAR' stands for retrieval-augmented QA and retrieval-augmented ranking data, respectively. For 'w/o reranking', we do not perform ranking in the inference stage.", "description": "This table presents the ablation study results for the RankRAG model using Llama3-8B as the backbone. It shows the impact of removing different components of the RankRAG framework on its performance across nine datasets.  Specifically, it analyzes the effects of removing the reranking step, the retrieval-augmented QA data, and the retrieval-augmented ranking data. Additionally, it compares RankRAG's performance against two baselines: one using only the initial supervised fine-tuning (SFT) stage and another incorporating the RAFT method from the related work. The results demonstrate the importance of each component for RankRAG's overall performance.", "section": "5.3 Ablation Studies"}, {"figure_path": "S1fc92uemC/tables/tables_7_2.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"-\". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents the zero-shot performance of RankRAG and several baseline models across nine knowledge-intensive NLP datasets.  The table compares the exact match (EM), accuracy (Acc.), or F1 scores achieved by each model on each dataset.  It highlights RankRAG's superior performance compared to other RAG models, particularly on challenging datasets such as PopQA and 2WikimQA.  The results showcase RankRAG's ability to generalize well without relying on additional demonstrations or fine-tuning.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_7_3.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"- \". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents the zero-shot performance of RankRAG and various baselines across nine knowledge-intensive NLP datasets.  The results showcase RankRAG's performance compared to other models, highlighting its effectiveness, especially when compared to models with significantly more parameters.  The table also notes limitations with GPT-4 and GPT-4-turbo in certain datasets, and highlights the use of the KILT benchmark for specific models and datasets.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_8_1.jpg", "caption": "Table 6: Ranking performance with different ranking models. Unless specified, all baselines are used to rank the top 100 retrieved passages. RankRAG achieves better performance despite using fewer ranking data. * NQ, TriviaQA and HotpotQA are used for training the BGE-Reranker model. \u2020: Our re-implementation. \u2020 We only rerank top-30 passages for GPT-4 due to budget constraint.", "description": "This table compares the recall performance (R@5, R@10, R@20) of different ranking models on five datasets (NQ, TriviaQA, PopQA, HotpotQA, Inscit).  The models include various baselines like RankBERT, monoT5, BGE-Reranker, RankLLaMA, and ChatQA-1.5.  It also includes OpenAI's GPT-3.5 and GPT-4 as off-the-shelf LLMs used for reranking. Finally, it presents the performance of RankRAG 8B and 70B, highlighting its data efficiency by achieving better results than models trained on significantly more data.", "section": "5.5 A Closer Look at the Ranking Module"}, {"figure_path": "S1fc92uemC/tables/tables_9_1.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"-\". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents the zero-shot performance of RankRAG and various baseline models across nine knowledge-intensive NLP datasets.  It compares the Exact Match (EM) or Accuracy scores depending on the specific dataset and includes notes on any limitations or caveats for certain models.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_20_1.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"- \". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents a comparison of RankRAG's performance against various baselines across nine different datasets.  The evaluation is zero-shot, meaning no additional examples or fine-tuning was used.  The table highlights the performance of different models, including those with and without retrieval-augmented generation (RAG), and specifically notes instances where GPT-4 and GPT-4-turbo models refused to answer due to insufficient information in retrieved passages.  The average performance across all datasets is also provided.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_21_1.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"-\". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents a comparison of RankRAG's performance against various baselines across nine datasets, encompassing zero-shot evaluations without additional demonstrations.  The table highlights the performance differences across different models, including those with and without retrieval-augmented generation (RAG), and notes some limitations of GPT-4 and GPT-4-turbo in handling cases where relevant information is absent from retrieved passages.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_22_1.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"-\". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents a comparison of RankRAG's performance against various baselines across nine knowledge-intensive NLP datasets.  The results are categorized by whether or not a retrieval-augmented generation (RAG) method was used.  The table includes metrics like Exact Match (EM), Accuracy (Acc.), and F1 score, showing RankRAG's superior performance, especially on more challenging datasets. Note that some models may refuse to answer when relevant information is absent, affecting the reported scores.", "section": "5.2 Main Experiments"}, {"figure_path": "S1fc92uemC/tables/tables_22_2.jpg", "caption": "Table 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under zero-shot evaluation without additional demonstrations. Results unavailable in public reports are marked as \"-\". We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct, Llama3-ChatQA-1.5, and Llama3-RankRAG. Note that: GPT-4 and GPT-4-turbo may refuse to answer the question when retrieved passages do not contain relevant information, thus the EM / accuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.", "description": "This table presents a comparison of RankRAG's performance against various baselines across nine different datasets.  The results show RankRAG's zero-shot performance (without additional demonstrations) and considers different model sizes.  The table highlights RankRAG's improvement over existing methods, especially on more challenging datasets. Note that GPT-4 and GPT-4 turbo models sometimes refuse to answer if relevant information is missing from the retrieved context, affecting their EM/Accuracy scores.", "section": "5.2 Main Experiments"}]