[{"figure_path": "IGn0ktYDwV/tables/tables_5_1.jpg", "caption": "Table 1: Test accuracies on CIFAR-10. SAMPa-0.2 outperforms SAM across all models with halved total temporal cost. \u201cTemporal cost\u201d represents the number of sequential gradient computations per update. SAMPa-0.2 with 400 epochs is included for comprehensive comparison with SGD and SAM.", "description": "This table presents the test accuracies achieved by different models (DenseNet-121, Resnet-56, VGG19-BN, WRN-28-2, WRN-28-10) on the CIFAR-10 dataset using various optimization methods: SGD, SAM, SAMPa-0, SAMPa-0.2 (with 200 epochs), and SAMPa-0.2 (with 400 epochs).  The \"Temporal cost\" column indicates the number of gradient computations per epoch, showcasing the efficiency gains of SAMPa.  The results demonstrate that SAMPa-0.2 achieves higher accuracy than SAM while using only half the computational cost (or the same cost when comparing 400 epochs).", "section": "5.1 Image classification"}, {"figure_path": "IGn0ktYDwV/tables/tables_5_2.jpg", "caption": "Table 2: Test accuracies on CIFAR-100. SAMPa-0.2 outperforms SAM across all models with halved total temporal cost. \"Temporal cost\" represents the number of sequential gradient computations per update. SAMPa-0.2 with 400 epochs is included for a comprehensive comparison.", "description": "This table presents the test accuracies achieved by different models (DenseNet-121, Resnet-56, VGG19-BN, WRN-28-2, WRN-28-10) on the CIFAR-100 dataset using various optimization methods: SGD, SAM, SAMPa-0, SAMPa-0.2 (with 200 epochs), and SAMPa-0.2 (with 400 epochs).  The \"Temporal cost\" column indicates the number of gradient computations per update.  The results highlight that SAMPa-0.2 consistently outperforms SAM while maintaining a significantly reduced computational cost (halved). The inclusion of SAMPa-0.2 with 400 epochs allows for a direct comparison with SGD and SAM, showcasing the performance gains.", "section": "5.1 Image classification"}, {"figure_path": "IGn0ktYDwV/tables/tables_6_1.jpg", "caption": "Table 3: Top1/Top5 maximum test accuracies on ImageNet-1K.", "description": "This table presents the Top1 and Top5 accuracies achieved by SAM and SAMPa-0.2 on the ImageNet-1K dataset.  The results demonstrate the performance of both algorithms in a large-scale image classification task.  Top1 accuracy refers to the percentage of images correctly classified into their top predicted class, while Top5 accuracy represents the percentage of images correctly classified into one of their top 5 predicted classes.  The values are presented as mean \u00b1 standard deviation, indicating the variability in the results across multiple independent experiments.", "section": "5.1 Image classification"}, {"figure_path": "IGn0ktYDwV/tables/tables_6_2.jpg", "caption": "Table 4: Efficient SAM variants. The best result is in bold and the second best is underlined.", "description": "This table compares the performance of SAMPa-0.2 with several other efficient variants of the SAM algorithm.  The comparison includes test accuracy and the time per epoch.  The results show that SAMPa-0.2 achieves high accuracy with a significantly reduced computation time compared to other methods.", "section": "5.2 Efficiency comparison with efficient SAM variants"}, {"figure_path": "IGn0ktYDwV/tables/tables_7_1.jpg", "caption": "Table 5: Image fine-tuning. C10 and C100 represent CIFAR-10 and CIFAR-100 respectively.", "description": "This table presents the results of image fine-tuning experiments using the pre-trained ViT-B/16 checkpoint.  The model was fine-tuned on the CIFAR-10 and CIFAR-100 datasets using AdamW optimizer.  The table shows the top-1 test accuracy achieved by SAM and SAMPa-0.2 on both datasets after 10 epochs of training. SAMPa-0.2 shows improved accuracy compared to SAM.", "section": "5.3 Transfer learning"}, {"figure_path": "IGn0ktYDwV/tables/tables_7_2.jpg", "caption": "Table 6: Test results of BERT-base fine-tuned on GLUE.", "description": "This table presents the results of fine-tuning a BERT-base model on the GLUE benchmark dataset using different optimization methods: AdamW, AdamW with SAM, AdamW with SAMPa-0, and AdamW with SAMPa-0.1.  The table shows the performance metrics (accuracy, MCC, F1-score, Pearson/Spearman correlation) achieved by each method on various GLUE tasks: CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and WNLI.  It demonstrates the effectiveness of SAMPa-0.1 in improving the performance of BERT-base for NLP tasks in the GLUE benchmark.", "section": "5.3 Transfer learning"}, {"figure_path": "IGn0ktYDwV/tables/tables_7_3.jpg", "caption": "Table 7: Test accuracies of ResNet-32 models trained on CIFAR-10 with label noise.", "description": "This table presents the test accuracies of ResNet-32 models trained on CIFAR-10 datasets with varying levels of label noise (0%, 20%, 40%, 60%, 80%). The results are compared across four different optimization methods: SGD, SAM, SAMPa-0, and SAMPa-0.2.  Each entry represents the average test accuracy \u00b1 standard deviation obtained over multiple independent runs.  The table demonstrates the robustness of SAMPa-0.2 against label noise, achieving consistently higher accuracies compared to SAM and SGD, especially at higher noise rates.", "section": "5.4 Noisy label task"}, {"figure_path": "IGn0ktYDwV/tables/tables_8_1.jpg", "caption": "Table 8: Incorporation with variants of SAM. SAMPa in the table denotes SAMPa-0.2. The incorporation of SAMPa with SAM variants enhances both accuracy and efficiency.", "description": "This table presents the results of integrating SAMPa with several other variants of SAM algorithms.  It shows the test accuracy achieved by each combination on the CIFAR-10 dataset using the ResNet-56 model. The results demonstrate that incorporating SAMPa consistently improves the accuracy, highlighting its potential as a beneficial addition to existing SAM methods.  SAMPa-0.2 is used in all combinations shown.", "section": "5.5 Incorporation with other SAM variants"}, {"figure_path": "IGn0ktYDwV/tables/tables_14_1.jpg", "caption": "Table 9: Test accuracies of OptSAM on CIFAR-10.", "description": "This table presents the test accuracies achieved by three different optimization methods: SGD (Stochastic Gradient Descent), SAM (Sharpness-Aware Minimization), and OptSAM (a naive attempt at parallelizing SAM) on the CIFAR-10 dataset.  It demonstrates that OptSAM, a proposed alternative to SAM, performs worse than SAM and even underperforms the standard SGD, highlighting the challenges involved in naively parallelizing SAM.", "section": "B Additional Experiments"}, {"figure_path": "IGn0ktYDwV/tables/tables_14_2.jpg", "caption": "Table 10: Two gradients in SAM computed on the same or different batch on CIFAR-10. SAM computes them on the same batch while SAM-db is on two different batches.", "description": "This table compares the test accuracy of three different methods (SGD, SAM, and SAM-db) on the CIFAR-10 dataset using three different models (Resnet-56, WRN-28-2, and VGG19-BN). SAM-db represents a variation of SAM where the two gradient computations are performed on different batches instead of the same batch. The results show that using the same batch for both gradient computations in SAM leads to slightly better performance compared to using different batches in SAM-db, and both methods generally outperform SGD.", "section": "B.2 SAM with stochasticity"}, {"figure_path": "IGn0ktYDwV/tables/tables_15_1.jpg", "caption": "Table 4: Efficient SAM variants. The best result is in bold and the second best is underlined.", "description": "This table compares the performance of SAMPa-0.2 against other efficient variants of SAM on the CIFAR-10 dataset using ResNet-56.  It presents the test accuracy and time per epoch for each method, highlighting the efficiency and improved generalization capabilities of SAMPa-0.2. The results show that SAMPa-0.2 achieves superior accuracy with significantly less computational time compared to most other SAM variants.", "section": "5.2 Efficiency comparison with efficient SAM variants"}, {"figure_path": "IGn0ktYDwV/tables/tables_16_1.jpg", "caption": "Table 1: Test accuracies on CIFAR-10. SAMPa-0.2 outperforms SAM across all models with halved total temporal cost. \u201cTemporal cost\u201d represents the number of sequential gradient computations per update. SAMPa-0.2 with 400 epochs is included for comprehensive comparison with SGD and SAM.", "description": "This table presents the test accuracies achieved on the CIFAR-10 dataset using various models and optimization methods.  It compares the performance of SGD, SAM (Sharpness-Aware Minimization), and different variants of SAMPa (a parallelized version of SAM) with different perturbation parameters. The \"Temporal cost\" column indicates the number of sequential gradient computations required per epoch, highlighting the computational efficiency gains of SAMPa.  The inclusion of SAMPa-0.2 with 400 epochs allows for a more direct comparison to the other methods, which used 200 epochs.", "section": "5.1 Image classification"}, {"figure_path": "IGn0ktYDwV/tables/tables_16_2.jpg", "caption": "Table 13: Test accuracy of the gradient penalization method.", "description": "This table compares the test accuracy of three different methods: SAM, SAMPa-0.2, and the gradient penalization method on the Resnet-56 model with a standard dataset and a dataset with 80% noisy labels.  It shows the performance of SAMPa-0.2 relative to the other two methods, highlighting its effectiveness even with noisy labels.", "section": "B Additional Experiments"}, {"figure_path": "IGn0ktYDwV/tables/tables_17_1.jpg", "caption": "Table 14: Memory usage on each GPU.", "description": "This table compares the GPU memory usage (in MiB) of various efficient SAM variants, including SAM, SAMPa-0.2, LookSAM, AE-SAM, SAF, MESA, and ESAM.  The values indicate the approximate memory consumption per GPU during training on CIFAR-10 with ResNet-56.", "section": "5.2 Efficiency comparison with efficient SAM variants"}]