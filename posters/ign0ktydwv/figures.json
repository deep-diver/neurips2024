[{"figure_path": "IGn0ktYDwV/figures/figures_2_1.jpg", "caption": "Figure 1: Comparison on f(x) = ||x||\u00b2.", "description": "This figure compares the convergence performance of different algorithms on a simple convex quadratic function f(x) = ||x||\u00b2.  It demonstrates that RandSAM and OptSAM, two naive attempts at parallelizing SAM, fail to converge, highlighting the challenges involved in parallelizing the SAM algorithm. In contrast, SAM and SAMPa-\u03bb (a variant of SAMPa with \u03bb=0 and \u03bb=0.5) converge successfully, showcasing the effectiveness of the proposed parallelization approach. The y-axis represents the squared norm of the gradient (||\u2207f(x)||\u00b2), indicating the proximity to a minimum.  The x-axis represents the number of iterations.", "section": "2.2 Naive attempts"}, {"figure_path": "IGn0ktYDwV/figures/figures_6_1.jpg", "caption": "Figure 2: Computational time comparison for efficient SAM variants. SAMPa-0.2 requires near-minimal computational time in both ideal and practical scenarios.", "description": "This figure compares the computational time of different efficient variants of the Sharpness-Aware Minimization (SAM) algorithm.  The x-axis represents the number of sequential gradient computations, reflecting the computational cost in an idealized scenario. The y-axis shows the actual runtime in seconds. The figure demonstrates that SAMPa-0.2 achieves the shortest runtime, significantly outperforming other methods such as LookSAM, AE-SAM, SAF, MESA, and ESAM, both in terms of the number of sequential gradient computations and in actual wall-clock time. This highlights the efficiency of SAMPa-0.2 in practical settings.", "section": "5.2 Efficiency comparison with efficient SAM variants"}, {"figure_path": "IGn0ktYDwV/figures/figures_6_2.jpg", "caption": "Figure 2: Computational time comparison for efficient SAM variants. SAMPa-0.2 requires near-minimal computational time in both ideal and practical scenarios.", "description": "This figure compares the computational time of various efficient SAM (Sharpness-Aware Minimization) variants.  The top panel (a) shows the number of sequential gradient computations, a theoretical measure of efficiency.  The bottom panel (b) displays the actual training time in seconds, which incorporates factors like forward/backward passes and communication overhead. SAMPa-0.2 consistently shows a significantly reduced computational time compared to other methods, achieving near-minimal runtime in both theoretical and practical settings.", "section": "5.2 Efficiency comparison with efficient SAM variants"}, {"figure_path": "IGn0ktYDwV/figures/figures_15_1.jpg", "caption": "Figure 2: Computational time comparison for efficient SAM variants. SAMPa-0.2 requires near-minimal computational time in both ideal and practical scenarios.", "description": "This figure compares the computational time of several efficient SAM variants.  The left subplot (a) shows the number of sequential gradient computations, a theoretical measure of computational cost. The right subplot (b) depicts the actual wall-clock training time. SAMPa-0.2 consistently demonstrates the lowest computational time, both theoretically and practically, outperforming other efficient SAM variants, highlighting its efficiency gains.", "section": "5.2 Efficiency comparison with efficient SAM variants"}, {"figure_path": "IGn0ktYDwV/figures/figures_16_1.jpg", "caption": "Figure 4: Difference between \u2207f(xt) and \u2207f(yt).", "description": "This figure shows the cosine similarity and Euclidean distance between the gradients \u2207f(xt) and \u2207f(yt) throughout the training process of ResNet-56 on CIFAR-10.  The cosine similarity remains consistently high (above 0.99), indicating a close approximation between the gradients. The Euclidean distance decreases over time and approaches zero at the end of training, further demonstrating the effectiveness of the approximation used in SAMPa.", "section": "C The choice of Yt+1"}, {"figure_path": "IGn0ktYDwV/figures/figures_16_2.jpg", "caption": "Figure 4: Difference between \u2207f(xt) and \u2207f(yt).", "description": "This figure shows the cosine similarity and Euclidean distance between the gradients \u2207f(xt) and \u2207f(yt) during the training process of ResNet-56 on CIFAR-10.  The cosine similarity remains consistently high (above 0.99), indicating that the gradients are very similar. The Euclidean distance decreases over epochs, approaching zero at the end of training.  This visual representation supports the claim that \u2207f(yt) serves as a good approximation for \u2207f(xt) in SAMPa, which is crucial for the algorithm's parallelization and convergence.", "section": "C The choice of Yt+1"}]