[{"figure_path": "9f5tOXKoMC/figures/figures_2_1.jpg", "caption": "Figure 1: Graphical model for BADS. Shaded nodes, representing curated (Dm) and uncurated (Dt) data, are evidence. Unshaded nodes, including model \u03b8 and instance weights w, are random variables.", "description": "This figure presents the graphical model for the Bayesian Approach to Data Selection (BADS) method proposed in the paper.  The model involves three main components: the instance-wise weights (w), which represent the importance of each data point in the uncurated training data (Dt), the model parameters (\u03b8) of the neural network, and the curated meta-dataset (Dm). The shaded nodes (Dt and Dm) represent observed data, acting as evidence. The unshaded nodes (w and \u03b8) are unobserved random variables, which the model infers.  The model assumes that the weights w influence the prior distribution of the model parameters \u03b8, given the uncurated data Dt.  Then, the model parameters \u03b8 determine the likelihood of generating the curated meta-dataset Dm.  The overall goal is to infer the posterior distribution p(\u03b8, w|Dt, Dm) by performing Bayesian inference.", "section": "2.2 (Our Approach) Bayesian Data Point Selection (BADS)"}, {"figure_path": "9f5tOXKoMC/figures/figures_5_1.jpg", "caption": "Figure 2: Proof-of-Concept experiment results. The top row displays the overall test performance across the three scenarios throughout the training phase, with x and y axis denote the training steps and the evaluation metrics, respectively. The bottom row visualizes the model-predicted weights of data points in each mini-batches in the final 2000 steps in WebNLG training (scenario 3). x and y axis show the training steps and average weights, respectively. Data points in blue color are expected to get higher weights compared to their counterparts (in red color).", "description": "The figure shows the experimental results of the proposed method (BADS) and several baselines on three different scenarios: Data Balancing (MNIST), Data Denoising (CIFAR-10), and Efficient Learning (WebNLG). The top row displays the overall test performance for each scenario, comparing BADS to various baselines including BLO, CDS, ClassAct, mixing, etc.  The bottom row visualizes the weights assigned by BADS to data points in the WebNLG scenario. The visualization shows that BADS assigns higher weights to specific data points, which is expected to improve performance.", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_5_2.jpg", "caption": "Figure 3: The MNIST test accuracy when trained with meta sets in varying sizes (x-aixs).", "description": "This figure shows the MNIST test accuracy with different sizes of meta sets. The x-axis represents the number of examples per class in the meta set, while the y-axis shows the test accuracy. Each line represents a different data selection method: mixing, random_select, meta_only, dup_meta, contrastive, BLO, and Bayesian.  The plot illustrates how the test accuracy changes as the size of the meta set increases for each method.  It highlights the performance differences between various data selection techniques when dealing with imbalanced datasets. Notably, the Bayesian approach consistently outperforms other methods, especially with limited meta data.", "section": "3.2 Scenario 1: Data Balancing (MNIST)"}, {"figure_path": "9f5tOXKoMC/figures/figures_6_1.jpg", "caption": "Figure 2: Proof-of-Concept experiment results. The top row displays the overall test performance across the three scenarios throughout the training phase, with x and y axis denote the training steps and the evaluation metrics, respectively. The bottom row visualizes the model-predicted weights of data points in each mini-batches in the final 2000 steps in WebNLG training (scenario 3). x and y axis show the training steps and average weights, respectively. Data points in blue color are expected to get higher weights compared to their counterparts (in red color).", "description": "The figure shows the test performance of different data selection methods across three scenarios (data balancing, denoising, and efficient learning) throughout the training process. The bottom part visualizes how the model assigns weights to data points in each minibatch for WebNLG training.  Points predicted to be more important are highlighted in blue, while those predicted to be less important are in red.", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_7_1.jpg", "caption": "Figure 10: Model\u2019s performance in the three proof-of-concept scenarios with different \u03b2 and \u03c3. ", "description": "This figure displays the test accuracy for three different scenarios (MNIST, CIFAR, and WebNLG) across various hyperparameter settings for \u03b2 (sparsity level) and \u03c3 (impact constant). It visualizes how these hyperparameters affect the model's performance and convergence speed across different DPS (Data Point Selection) approaches and compares them to non-DPS baselines. Each subplot represents a different scenario, with the x-axis showing training steps and the y-axis showing test accuracy. The results illustrate the impact of \u03b2 and \u03c3 on the models\u2019 performance, convergence rate, and overall ability to handle imbalanced, noisy, and limited data.", "section": "3. Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_15_1.jpg", "caption": "Figure 1: Graphical model for BADS. Shaded nodes, representing curated (Dm) and uncurated (Dt) data, are evidence. Unshaded nodes, including model \u03b8 and instance weights w, are random variables.", "description": "This figure shows the graphical model used in the Bayesian Approach to Data Point Selection (BADS) method.  The shaded nodes represent the curated meta dataset (Dm) and uncurated training dataset (Dt). These are considered as evidence in the Bayesian model. The unshaded nodes represent the model parameters (\u03b8) and instance-wise weights (w). These are random variables to be inferred.  The model illustrates how the instance weights (w), given the uncurated data (Dt),  influence the prior distribution of model parameters (\u03b8). In turn, the model parameters (\u03b8) influence the likelihood of generating the meta dataset (Dm). The goal is to infer the posterior distribution of both model parameters (\u03b8) and instance weights (w) given both the curated and uncurated datasets.", "section": "2.2 (Our Approach) Bayesian Data Point Selection (BADS)"}, {"figure_path": "9f5tOXKoMC/figures/figures_16_1.jpg", "caption": "Figure 7: An example of Natural Language Generation.", "description": "The figure shows an example of how the WebNLG task works.  Input tuples consist of multiple (argument, predicate, argument) triplets, which are then used to generate a natural language text description.  This example illustrates the process of transforming structured data into a coherent textual narrative.", "section": "3.4 Scenario 3: Efficient Learning (WebNLG)"}, {"figure_path": "9f5tOXKoMC/figures/figures_17_1.jpg", "caption": "Figure 10: Model's performance in the three proof-of-concept scenarios with different \u03b2 and \u03c3. ", "description": "This figure displays the test accuracy across three scenarios (MNIST, CIFAR, WebNLG) using different values of \u03b2 (sparsity level) and \u03c3 (impact constant) in the BADS method.  Each subplot shows the test accuracy curves for various values of \u03b2 or \u03c3, allowing for an analysis of their individual and combined influence on model performance across different tasks. The results reveal the sensitivity of the BADS method to these hyperparameters and how their optimization is crucial for achieving optimal model performance.  The consistent trends across all three scenarios help to support the robustness and general applicability of the BADS approach.", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_17_2.jpg", "caption": "Figure 10: Model's performance in the three proof-of-concept scenarios with different \u03b2 and \u03c3. ", "description": "The figure displays the performance of different models (BADS, BLO, CDS, and other baselines) across three scenarios: Data Balancing (MNIST), Data Denoising (CIFAR), and Efficient Learning (WebNLG).  Each scenario is tested with various hyperparameters (\u03b2 and \u03c3). The x-axis shows training steps, and the y-axis shows the evaluation metric (accuracy or BLEU score). The plots show how the hyperparameters affect the performance of the models. For example, the impact of varying the hyperparameters on model performance in the three proof-of-concept scenarios is illustrated.", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_18_1.jpg", "caption": "Figure 10: Model's performance in the three proof-of-concept scenarios with different \u03b2 and \u03c3. ", "description": "This figure presents the performance of the proposed method (BADS) and baselines across three scenarios (Data Balancing, Data Denoising, Efficient Learning) with varying hyperparameters \u03b2 (sparsity level) and \u03c3 (impact constant).  It shows the test accuracy/BLEU score over training steps for each scenario and different values of \u03b2 and \u03c3, illustrating the sensitivity of the model's performance to these hyperparameters and the relative effectiveness of BADS in comparison to the other approaches. The results are separated into plots showing the effect of changing \u03b2 while keeping \u03c3 constant and vice-versa. ", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_19_1.jpg", "caption": "Figure 10: Model's performance in the three proof-of-concept scenarios with different \u03b2 and \u03c3. ", "description": "This figure displays the model's performance across three scenarios (MNIST, CIFAR, and WebNLG) with varying \u03b2 (sparsity level) and \u03c3 (impact constant).  It visually compares the performance of the BADS model against multiple baselines under different hyperparameter settings. The trends shown illustrate the sensitivity of the model's performance to these hyperparameters and help to understand the effect of different data selection strategies.", "section": "3.2 Scenario 1: Data Balancing (MNIST)"}, {"figure_path": "9f5tOXKoMC/figures/figures_20_1.jpg", "caption": "Figure 10: Model's performance in the three proof-of-concept scenarios with different \u03b2 and \u03c3.", "description": "This figure shows the performance of the models in three scenarios (MNIST, CIFAR, WebNLG) with different values of \u03b2 (sparsity level) and \u03c3 (impact constant).  It illustrates how these hyperparameters affect the model's convergence speed and overall accuracy. The plots compare the model's test accuracy over the course of training, demonstrating the effect of the hyperparameters.", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_20_2.jpg", "caption": "Figure 13: Proof-of-Concept experiment supplementary results. All plots illustrate the average weights of data points within mini-batches during the last 2000 training steps, with the x-axis representing the training steps and the y-axis showing the average weights. Classes depicted in blue are expected to receive higher weights compared to those in red. The top row displays the MNIST experiments from scenario 1, the middle row shows the CIFAR experiments from scenario 2, and the bottom row features the WebNLG experiments from scenario 3. The left, middle, and right columns correspond to BADS, BLO, and CDS, respectively.", "description": "This figure provides supplementary results from three proof-of-concept experiments showing the average weights assigned to data points in mini-batches during the final stages of training.  The plots show, for each of three methods (BADS, BLO, and CDS), the weights assigned to data points for the MNIST (data balancing), CIFAR (data denoising), and WebNLG (efficient learning) tasks, highlighting the differences in how the three methods prioritize data points.", "section": "3 Experiments: Proof of Concept"}, {"figure_path": "9f5tOXKoMC/figures/figures_21_1.jpg", "caption": "Figure 13: Proof-of-Concept experiment supplementary results. All plots illustrate the average weights of data points within mini-batches during the last 2000 training steps, with the x-axis representing the training steps and the y-axis showing the average weights. Classes depicted in blue are expected to receive higher weights compared to those in red. The top row displays the MNIST experiments from scenario 1, the middle row shows the CIFAR experiments from scenario 2, and the bottom row features the WebNLG experiments from scenario 3. The left, middle, and right columns correspond to BADS, BLO, and CDS, respectively.", "description": "This figure provides supplementary results for the three proof-of-concept experiments (data balancing, data denoising, and efficient learning) described in the paper.  Each row shows the average weights assigned to different data points within mini-batches over the last 2000 training steps for one experiment. Blue data points are expected to have higher weights than red points. The three rows represent MNIST, CIFAR, and WebNLG experiments respectively; the columns represent BADS, BLO, and CDS algorithms.  The visualization helps understand how each algorithm assigns weights to different data points in the different scenarios.", "section": "3 Experiments: Proof of Concept"}]