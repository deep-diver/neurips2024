[{"figure_path": "vYUx8j5KK2/tables/tables_6_1.jpg", "caption": "Table 1: Average test accuracy (%) on four simulated noisy datasets with different noise levels. The test accuracy is averaged over the last ten epochs. The best and second-best results in each case are highlighted in bold and underline, respectively.", "description": "This table presents the average test accuracy achieved by different methods (Full-training, Linear probing, Rein, Co-teaching, JoCor, CoDis, and CUFIT) on four medical image datasets (HAM10000, APTOS-2019, BloodMnist, and OrgancMnist) under various simulated noise levels (0.1, 0.2, 0.4, and 0.6).  The results highlight the performance of CUFIT in comparison to other methods, showcasing its robustness to noisy labels.", "section": "5 Experiments"}, {"figure_path": "vYUx8j5KK2/tables/tables_6_2.jpg", "caption": "Table 2: Average test accuracy (%) on real-world noisy datasets (Kaggle-EyePACS for training). After the training is done, we evaluate the model on two datasets: APTOS-2019 and FGADR. The best result and second-best result in each case are highlighted in bold and underline, respectively.", "description": "This table presents the average test accuracy achieved by different methods on two real-world noisy datasets (APTOS-2019 and FGADR) after training on the Kaggle-EyePACS dataset.  The methods compared include various baseline approaches and CUFIT, the proposed method.  The best and second-best results are highlighted for each dataset.  The \"Total\" row combines the results across both datasets. ", "section": "5 Experiments"}, {"figure_path": "vYUx8j5KK2/tables/tables_8_1.jpg", "caption": "Table 1: Average test accuracy (%) on four simulated noisy datasets with different noise levels. The test accuracy is averaged over the last ten epochs. The best and second-best results in each case are highlighted in bold and underline, respectively.", "description": "This table presents the average test accuracy achieved by different methods (Full-training, Linear probing, Rein, Co-teaching, JoCor, CoDis, and CUFIT) on four medical image datasets (HAM10000, APTOS-2019, BloodMnist, and OrgancMnist) with varying levels of simulated label noise (0.1, 0.2, 0.4, and 0.6).  The results are averaged across the last ten epochs of training.  The best and second-best results for each dataset and noise level are highlighted.", "section": "5 Experiments"}, {"figure_path": "vYUx8j5KK2/tables/tables_9_1.jpg", "caption": "Table 4: Average test accuracy on the natural image dataset with simulated noisy labels (CIFAR, symmetric noise at 80%) and real-world noisy labels (ANIMAL10N [41], which has an estimated noise ratio of 8%). The test accuracy is averaged over the last ten epochs. We use DINOv2 with Rein adapter for the experiment. Bold values the best result.", "description": "This table presents the average test accuracy achieved by different methods on three datasets: CIFAR10, CIFAR100, and ANIMAL10N.  Each dataset has a different noise rate applied to its labels.  The methods compared include full training, linear probing, Rein (adapter-based fine-tuning), Co-teaching, JoCor, CoDis, and CUFIT (the authors' proposed method). The results show the performance of each method on each dataset, highlighting the superior performance of CUFIT, especially under high noise rates.", "section": "Performance on noisy natural image classification benchmark"}]