[{"figure_path": "vYUx8j5KK2/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of linear probing (a) and adapter usage (b). Specifically, the weights of the foundation model are frozen, while the fully connected layer or adapter weights (shown in orange) are updated during the training phase. In (c), a performance comparison using a simulated noisy dataset (HAM10000) is presented. It demonstrates that linear probing is more robust to noisy labels compared to the adapter, whereas the adapter outperforms linear probing when there are no noisy labels.", "description": "This figure illustrates the concepts of linear probing and adapter usage in the context of vision foundation models (VFMs) for medical image classification with noisy labels.  Panel (a) shows linear probing where only a fully connected layer is trained, leaving the VFM weights frozen. Panel (b) shows adapter usage, where trainable adapters are added to the VFM, while the VFM weights remain frozen. Panel (c) compares the performance of linear probing and adapter usage against the proposed CUFIT method on a simulated noisy dataset, highlighting the robustness of linear probing to noise and the superior performance of adapters in clean data scenarios. CUFIT aims to combine the benefits of both approaches.", "section": "1 Introduction"}, {"figure_path": "vYUx8j5KK2/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of our proposed training framework, CUFIT, which consists of a pre-trained VFM and three distinct modules: (a) the linear probing module (LPM), (b) the intermediate adapter module (IAM), and (c) the last adapter module (LAM). During the training stage, the LPM selects clean samples for the IAM based on the agreement criterion, and the IAM selects clean samples for the LPM. During the inference stage, only the LAM is used for prediction.", "description": "This figure illustrates the Curriculum Fine-tuning framework (CUFIT) proposed in the paper. CUFIT consists of three modules that are trained simultaneously: a linear probing module (LPM), an intermediate adapter module (IAM), and a last adapter module (LAM).  The LPM uses all samples and serves as a filter for the IAM. The IAM, trained on samples selected by the LPM based on a criterion, further filters the samples for the LAM. Finally, only the LAM is used for inference to make predictions.  This curriculum approach helps mitigate the negative effects of noisy labels by progressively refining the sample selection and adapter training.", "section": "4 Method"}, {"figure_path": "vYUx8j5KK2/figures/figures_7_1.jpg", "caption": "Figure 3: Illustration of label precision (a,d), label recall (b,e), and test accuracy (c,f) vs. epoch. The first row is for HAM10000 with 40% noise rate, and the second row is for APTOS-2019 with 40% noise rate.", "description": "This figure compares the label precision, label recall, and test accuracy of the linear probing module, intermediate adapter module, last adapter module, and CoDis method across 100 epochs for two datasets, HAM10000 and APTOS-2019. Both datasets have a 40% noise rate. The plots show the performance of each module over time.  The results highlight the effectiveness of the proposed curriculum training framework.", "section": "5.2 Simulated noisy medical image classification benchmark"}, {"figure_path": "vYUx8j5KK2/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of linear probing (a) and adapter usage (b). Specifically, the weights of the foundation model are frozen, while the fully connected layer or adapter weights (shown in orange) are updated during the training phase. In (c), a performance comparison using a simulated noisy dataset (HAM10000) is presented. It demonstrates that linear probing is more robust to noisy labels compared to the adapter, whereas the adapter outperforms linear probing when there are no noisy labels.", "description": "This figure illustrates the core idea of the proposed method, CUFIT. It compares linear probing and adapter usage for handling noisy labels in medical image classification. (a) shows linear probing, where only the fully-connected layer is trained while the VFM's weights are frozen. (b) shows adapter usage, where the weights of the adapters are trained to handle noisy labels. (c) shows the performance comparison with a simulated noisy dataset, illustrating that linear probing is more robust to noisy labels but has lower performance in the absence of noise. Conversely, adapter usage has lower robustness in the noisy setting but shows higher performance in the absence of noise. This motivates the need for curriculum fine-tuning.", "section": "1 Introduction"}]