{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a foundational large language model that serves as a basis for several experiments in the target paper, demonstrating the practical application of SS1."}, {"fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "publication_date": "2016-00-00", "reason": "This foundational paper explores deep neural network compression techniques, which are relevant to the efficiency gains presented in the target paper through SS1 and quantization combination."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-03-00", "reason": "This paper introduces the lottery ticket hypothesis, which is foundational to understanding the theoretical underpinnings of sparse model training, a concept relevant to SS1's efficient parameter sharing."}, {"fullname_first_author": "Aditya Desai", "paper_title": "Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST) Hashing", "publication_date": "2023-00-00", "reason": "This work directly relates to the target paper's focus on hardware-efficient model compression, providing a comparative analysis and context for SS1's performance."}, {"fullname_first_author": "Tri Dao", "paper_title": "Monarch: Expressive structured matrices for efficient and accurate training", "publication_date": "2022-00-00", "reason": "This paper introduces Monarch, a state-of-the-art structured sparsity method for efficient neural network training, which is directly compared against SS1 in the target paper to establish performance benchmarks."}]}