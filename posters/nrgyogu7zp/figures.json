[{"figure_path": "nrgyOGU7ZP/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of weight tying in SS1. Same colored weights imply that they are tied to the same parameter in memory", "description": "This figure illustrates the weight tying mechanism used in the Sketch Structured Transform (SS1) method.  The left panel shows how weights within a group are tied together using a hash function, resulting in a rotation of the weights. The right panel demonstrates the K-coalescing (sharing parameter chunks) and N-coalescing (sharing the same hash function) techniques used in SS1 to enhance GPU-friendliness.  This structured parameter sharing allows SS1 to reduce computations while maintaining expressiveness. Same-colored weights indicate that they are shared and point to the same parameter location in memory.", "section": "3 Sketch Structured Transform(SS1)"}, {"figure_path": "nrgyOGU7ZP/figures/figures_5_1.jpg", "caption": "Figure 2: Upper bound on variance of unit norm vectors", "description": "This figure shows the upper bound on the variance of inner products between unit norm vectors under three different scenarios: projection alone, quantization alone, and a combination of both.  The x-axis represents the compression ratio, and the y-axis represents the variance.  The graph demonstrates that combining projection and quantization can yield better results (lower variance) than using either technique alone, particularly in the high-compression regime.  This supports a key finding in the paper regarding the synergistic benefits of using both methods together.", "section": "Theoretical aspects of SS1"}, {"figure_path": "nrgyOGU7ZP/figures/figures_15_1.jpg", "caption": "Figure 3: Latency vs. Perplexity Plot for GPT-S", "description": "This figure shows the trade-off between latency (in milliseconds) and perplexity for GPT-S models using different linear transformation methods: SS1, Monarch, LowRank, and a baseline model.  Lower latency and lower perplexity are both desirable; the ideal point would be in the lower-left corner. The plot helps visualize the quality-efficiency tradeoffs of each method.", "section": "5.1 Accuracy vs. Latency evaluation of SS1"}, {"figure_path": "nrgyOGU7ZP/figures/figures_16_1.jpg", "caption": "Figure 4: Latency(ms) vs. Parameters for an instance of workload in GPT2", "description": "This figure shows the latency in milliseconds (ms) for different model sizes of GPT2 against the number of parameters and compression ratios.  Four different matrix multiplication methods are compared: nnLinear (standard linear layer), LowRankLinear (low-rank linear layer), Monarch (structured matrix), and SS1 (Sketch Structured Transform). The x-axis represents the compression factor (1x, 2x, 4x, 8x, 16x), and the y-axis represents latency in milliseconds.  Each subplot represents a different input size.", "section": "3 Sketch Structured Transform(SS1)"}]