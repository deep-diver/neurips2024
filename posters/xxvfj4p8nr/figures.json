[{"figure_path": "XXVfj4P8nr/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of VL-SAM. Without additional training, we connect the vision-language and segment-anything models with attention maps as the intermediate prompts.", "description": "The figure illustrates the VL-SAM framework, which combines a vision-language model (VLM) and a segment-anything model (SAM) without any additional training.  The VLM processes the image input and generates an attention map highlighting potential objects. This attention map serves as a prompt for the SAM, which then performs object detection and segmentation. An iterative refinement process further refines the results using the SAM's output to improve the attention map and subsequent segmentation.", "section": "1 Introduction"}, {"figure_path": "XXVfj4P8nr/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of VL-SAM framework. We first use VLM to describe the input image and generate all possible objects' names. Then, for each object name, we obtain the corresponding attention map with the attention map generation module. Finally, we sample point prompts from the attention map and send them to SAM to predict detection and segmentation results.", "description": "This figure provides a high-level overview of the VL-SAM framework. It shows how the Vision-Language Model (VLM) and Segment-Anything Model (SAM) are integrated.  First, VLM processes the image and generates a list of potential objects. Then, for each object, an attention map is generated by a dedicated module that aggregates information from multiple heads and layers of the VLM. These attention maps guide SAM, which uses point sampling and iterative refinement for more accurate object detection and segmentation.", "section": "3 Method"}, {"figure_path": "XXVfj4P8nr/figures/figures_4_1.jpg", "caption": "Figure 3: Head aggregation. We aggregate information from all attention heads with head weights.", "description": "This figure illustrates the head aggregation step in the VL-SAM framework.  The input is a stack of attention maps from multiple heads of the Vision-Language Model (VLM).  A MeanMax operation is applied to the attention head weights to generate a single set of weights (W). These weights are then used to weight the individual attention maps from each head before they are aggregated to produce a final attention map.", "section": "3.2 Attention Map Generation"}, {"figure_path": "XXVfj4P8nr/figures/figures_4_2.jpg", "caption": "Figure 4: Attention flow. We propagate attention from the first layer to last layer with attention flow.", "description": "This figure illustrates the attention flow mechanism used in VL-SAM.  Attention maps from each layer of the Vision-Language Model (VLM) are aggregated and propagated to the next layer using a process described in equation (3). This iterative refinement helps to produce a more comprehensive and accurate attention map that highlights regions of interest for object detection and segmentation.", "section": "3.2 Attention Map Generation"}, {"figure_path": "XXVfj4P8nr/figures/figures_4_3.jpg", "caption": "Figure 5: Illustration of attention collapse. For each column, from left to right, we show image inputs, attention flow (collapse), regularized attention flow, and generated answers from VLM.", "description": "This figure demonstrates the attention flow mechanism in VL-SAM and the problem of attention collapse. The leftmost column shows the input image. The middle column visualizes the attention flow without regularization, exhibiting the issue of attention collapse, where attention focuses on a few areas, even though it should cover the whole image. In the rightmost column, the regularized attention flow is presented; it effectively prevents the collapse and shows an improved distribution of attention, leading to a more complete and accurate description of the scene generated by VLM.  This directly impacts the quality of the prompts used for object segmentation.", "section": "3.2 Attention Map Generation"}]