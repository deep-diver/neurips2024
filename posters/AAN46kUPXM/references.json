{"references": [{"fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "publication_date": "2016-00-00", "reason": "This paper is foundational to the field of model compression, introducing a multi-pronged approach to significantly reduce model size and computational cost."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2019-00-00", "reason": "This paper introduced the Lottery Ticket Hypothesis, a seminal concept in neural network pruning that greatly influenced the direction of research on sparse model training."}, {"fullname_first_author": "Hao Li", "paper_title": "Pruning filters for efficient convnets", "publication_date": "2017-00-00", "reason": "This work introduced a filter-level pruning method that efficiently removes less important filters in convolutional neural networks, directly impacting the structure of the model."}, {"fullname_first_author": "Zhuang Liu", "paper_title": "Learning efficient convolutional networks through network slimming", "publication_date": "2017-00-00", "reason": "This paper introduced Network Slimming, a channel pruning method that uses the L1 norm to identify and remove less important channels in convolutional layers, improving computational efficiency."}, {"fullname_first_author": "Pavlo Molchanov", "paper_title": "Importance estimation for neural network pruning", "publication_date": "2019-00-00", "reason": "This paper proposed a novel method for estimating the importance of network weights using a Taylor expansion, improving the effectiveness of weight-based pruning methods."}]}