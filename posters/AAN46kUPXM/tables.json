[{"figure_path": "AAN46kUPXM/tables/tables_7_1.jpg", "caption": "Table 1: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using ResNet architectures [17] - ResNet-56 (left) and ResNet-110 (right).", "description": "This table presents a quantitative comparison of different model compression techniques on the CIFAR-10 dataset using ResNet-56 and ResNet-110 architectures.  It compares the performance (top-1 accuracy) and compression ratios (in terms of parameters and FLOPs) achieved by various importance-based pruning methods against the proposed Neural Expressiveness (NEXP) method.  The table highlights the gains in compression efficiency and often maintains comparable accuracy with NEXP compared to other state-of-the-art techniques.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/tables/tables_8_1.jpg", "caption": "Table 2: Analytical Comparison of Importance-based solutions and Expressiveness on ImageNet-1k using ResNet-50 [17].", "description": "This table presents a comparison of different model compression techniques on the ImageNet-1k dataset using the ResNet-50 architecture.  It shows the top-1 and top-5 accuracy, the change in accuracy compared to the baseline, and the compression ratio achieved in terms of the number of parameters and FLOPs for various methods.  The methods compared include several importance-based pruning techniques and the proposed Neural Expressiveness (NEXP) method.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/tables/tables_15_1.jpg", "caption": "Table 3: Sensitivity analysis of the input\u2019s sampling strategies after training (PaT) using various similarity metrics.", "description": "This table presents the results of a sensitivity analysis conducted to evaluate the impact of different input sampling strategies on the expressiveness scores after the model training is completed.  The analysis uses four different similarity metrics (Euclidean Distance, Cosine Similarity, Pearsonr Similarity, and ssim_index) to compare the expressiveness maps obtained using random sampling and k-means sampling against the true NEXP scores (non-approx).  The table shows that even with a limited dataset (60 samples), the results obtained are highly similar to those using the entire training set, validating the robustness of NEXP.", "section": "3.3 Dependency to Input Data"}, {"figure_path": "AAN46kUPXM/tables/tables_15_2.jpg", "caption": "Table 4: Sensitivity analysis of the input's sampling strategies at Initialization (PaI) using various similarity metrics.", "description": "This table presents the results of a sensitivity analysis conducted to evaluate how different input sampling strategies affect the expressiveness scores at initialization.  It compares the expressiveness scores obtained using random sampling and k-means sampling against the true NEXP scores (calculated using the entire dataset). Multiple similarity metrics (Euclidean Distance, Cosine Similarity, Pearson Similarity, and Structural Similarity Index) are used to assess the similarity between the different sampling strategies and the true NEXP scores.", "section": "3.3 Dependency to Input Data"}, {"figure_path": "AAN46kUPXM/tables/tables_15_3.jpg", "caption": "Table 3: Sensitivity analysis of the input's sampling strategies after training (PaT) using various similarity metrics.", "description": "This table presents the results of a sensitivity analysis performed to evaluate the impact of different input sampling strategies on the Neural Expressiveness (NEXP) estimations.  It compares the results obtained using random sampling and k-means sampling against the true NEXP values (calculated using the entire training dataset). Multiple similarity metrics (Euclidean Distance, Cosine Similarity, Pearsonr Similarity, and Structural Similarity Index Measure) are employed to assess the similarity between the obtained NEXP scores and the true NEXP values. The analysis shows that using a smaller mini-batch of samples provides sufficiently accurate approximations of the true NEXP scores.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/tables/tables_19_1.jpg", "caption": "Table 6: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using VGGNet architectures [44].", "description": "This table presents a comparison of different model compression methods on the CIFAR-10 dataset using VGG-16 and VGG-19 network architectures.  The methods compared include various importance-based pruning techniques (L1, GAL, HRank, SCP, DCP-Adapt) and the proposed Neural Expressiveness (NEXP) method. For each method, the table shows the top-1 accuracy (Base and \u0394), and the compression ratio achieved in terms of the number of parameters and FLOPs.  The table highlights the performance and compression efficiency of NEXP compared to existing state-of-the-art methods.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/tables/tables_19_2.jpg", "caption": "Table 7: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using GoogLeNet [45].", "description": "This table presents a comparison of different model compression methods on the CIFAR-10 dataset using the GoogLeNet architecture.  It compares the top-1 accuracy, the change in accuracy compared to the baseline, and the compression ratios achieved in terms of both the number of parameters and FLOPs. The methods compared include several importance-based pruning techniques (GAL, HRank, ABC) and the proposed NEXP method. The table highlights the compression efficiency and performance trade-offs of each method.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/tables/tables_19_3.jpg", "caption": "Table 8: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using DenseNet-40 [21].", "description": "This table presents a comparison of different model compression methods on the CIFAR-10 dataset using the DenseNet-40 architecture.  It shows the top-1 accuracy, the percentage change in accuracy compared to the baseline, and the compression ratios achieved in terms of the number of parameters and FLOPs for several methods.  The methods compared include GAL-0.5 [32], HRank [30], and the proposed NEXP method. The table highlights the performance and compression efficiency of each method.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/tables/tables_19_4.jpg", "caption": "Table 9: Performance Outcomes for MobileNet-v2 on the CIFAR-10 Dataset.", "description": "This table presents a comparison of the performance of the proposed Neural Expressiveness (NEXP) pruning method against the state-of-the-art method DCP [61] on the CIFAR-10 dataset using the MobileNet-v2 architecture. The comparison includes the base top-1 accuracy, the change in accuracy after pruning (\u25b3Acc), the parameter compression ratio (#Params \u2193), and the FLOPs compression ratio (#FLOPs \u2193). The results show that NEXP achieves a higher compression ratio while maintaining a similar level of accuracy compared to DCP.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}]