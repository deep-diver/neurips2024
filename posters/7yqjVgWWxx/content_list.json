[{"type": "text", "text": "Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Discrete diffusion models with absorbing processes have shown promise in lan  \n2 guage modeling. The key quantities to be estimated are the ratios between the   \n3 marginal probabilities of two transitive states at all timesteps, called the concrete   \n4 score. In this paper, we reveal that the concrete score in absorbing diffusion can be   \n5 expressed as conditional probabilities of clean data, multiplied by a time-dependent   \n6 scalar in an analytic form. Motivated by the finding, we propose reparameterized   \n7 absorbing discrete diffusion (RADD), a dedicated diffusion model that character  \n8 izes the time-independent conditional probabilities. Besides its simplicity, RADD   \n9 can reduce the number of function evaluations (NFEs) by caching the output of   \n10 the time-independent network when the noisy sample remains unchanged in a   \n11 sampling interval. Empirically, RADD is up to 3.5 times faster while consistently   \n12 achieving a better performance than the strongest baseline. Built upon the new   \n13 factorization of the concrete score, we further prove a surprising result that the   \n14 exact likelihood of absorbing diffusion can be rewritten to a simple form (named   \n15 denoise cross-entropy) and then estimated efficiently by the Monte Carlo method.   \n16 The resulting approach also applies to the original parameterization of the concrete   \n17 score. It significantly advances the state-of-the-art discrete diffusion on 5 zero-shot   \n18 language modeling benchmarks (measured by perplexity) at the GPT-2 scale. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Auto-regressive models [1, 2, 3] have dominated the area of language modeling for many years. In   \n21 particular, such models significantly benefit from large-scale transformers [4] and training data and   \n22 have achieved remarkable progress [5, 6, 7, 8]. From a probabilistic perspective, the sequential sam  \n23 pling process of auto-regressive models is inefficient and limits the reasoning ability in nonsequential   \n24 orders [9, 10]. Intrinsically, this is because such models characterize the joint distribution by the   \n25 chain rule of probability, motivating research on developing other types of generative models for text.   \n26 Diffusion models [11, 12, 13] generate data in a coarse-to-fine manner efficiently [14, 15, 16, 17, 18]   \n27 and all dimensions simultaneously, providing an appealing alternative to auto-regressive models.   \n28 Among other efforts [19, 20, 21, 22, 23, 24, 20, 25, 26, 27, 28, 29] (see Section 5 for a comprehensive   \n29 discussion), score entropy discrete diffusion (SEDD) [29] has shown promise in text generation. In   \n30 particular, SEDD has achieved comparable results to auto-regressive models on 5 zero-shot language   \n31 modeling benchmarks at the GPT-2 scale. Meanwhile, SEDD can reduce the number of function   \n32 evaluations (NFEs) in sampling and fulfill text conditioned on prompts at different positions.   \n33 Technically, SEDD employs a discrete-state (absorbing) Markov process that adds noises to data by   \n34 randomly replacing a token with a mask token [M] and then learns a reverse process to denoise from   \n35 an entirely masked sentence. The key quantities to be estimated in SEDD are the ratios between the   \n36 marginal probabilities of two transitive states at all timesteps, called the concrete score. SEDD also   \n37 proposes a \u201cscaling trick\u201d (see details in Section 3) that scales the output of the score estimation by a   \n38 factor. The trick has been proven very effective in practice yet not fully understood in theory [29].   \n39 One of our main contributions is to reveal that the concrete score in absorbing diffusion can be ex  \n40 pressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic   \n41 form. Our finding theoretically explains the benefits of the scaling trick as a reparameterization   \n42 for better optimization. Motivated by the finding, we propose reparameterized absorbing discrete   \n43 diffusion (RADD), a dedicated diffusion model that characterizes the time-independent conditional   \n44 probabilities by removing the time embedding from the score estimation in SEDD. Besides its   \n45 simplicity, RADD can significantly reduce the NFEs by caching the output of the time-independent   \n46 network when the noisy sample remains unchanged in a sampling interval (see Fig. 1).   \n47 Built upon the new factorization of the concrete score, we further prove a surprising result that the   \n48 exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoise cross  \n49 entropy, DCE) and then estimated efficiently by the Monte Carlo method. To establish the theory,   \n50 we apply a change of variable from the time $t$ to the probability that a single-dimensional token is   \n51 masked at time $t$ in the forward process. By integrating the probability variable analytically, we show   \n52 that DCE enumerates all orders to decompose the joint distribution auto-regressively and accumulates   \n53 log densities of all conditional distributions in every order, finishing the proof. Such theoretical   \n54 findings enable exact likelihood evaluation and optimization for both the original parameterization of   \n55 absorbing diffusion [29] and the proposed RADD.   \n56 Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than   \n57 the strongest baseline, i.e. SEDD with the scaling trick [29]. Further, the DCE loss applies to both   \n58 RADD and SEDD for precise likelihood evaluation. It significantly advances the state-of-the-art   \n59 discrete diffusion (i.e. SEDD [29]) on 5 zero-shot language modeling benchmarks (measured by   \n60 perplexity) at the GPT-2 scale. The empirical evidence validates our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "61 In summary, this paper has several contributions: ", "page_idx": 1}, {"type": "text", "text": "62 \u2022 Deeper understanding of discrete diffusion: Both the factorization form of the concrete   \n63 score and DCE loss for the exact likelihood computation reveal important yet overlooked   \n64 theoretical properties of absorbing discrete diffusion, which explain the mysterious scaling   \n65 trick, provide practice guidance, and may inspire future work.   \n66 \u2022 Simplification: By removing the time conditions, we reparameterize the model to focus on   \n67 a time-independent conditional probability, simplifying the existing model.   \n68 \u2022 Efficient sampling: Leveraging the reparameterized form, RADD with a caching strategy   \n69 is consistently faster while achieving a better performance than the strongest competitor.   \n70 \u2022 Improved likelihood evaluation: The exact likelihood evaluation approach significantly   \n71 advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks   \n72 (measured by perplexity) at the GPT-2 scale. ", "page_idx": 1}, {"type": "text", "text": "73 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "74 In this section, we present preliminaries on continuous-time discrete diffusion models. We start with   \n75 the one-dimensional case in Section 2.1, followed by the multi-dimensional case in Section 2.2. ", "page_idx": 1}, {"type": "text", "text": "76 2.1 Single dimension ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "77 Let $x$ denote a single dimensional sample with possible values in $\\{1,\\ldots,N\\}$ . A continuous-time   \n78 discrete Markov chain at time $t$ is characterized by a transition rate matrix $Q_{t}$ as follows ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{t+\\Delta t|t}(\\hat{x}|x)=\\left\\{\\!\\!\\begin{array}{l l}{\\!\\!Q_{t}(x,\\hat{x})\\Delta t+o(\\Delta t),}&{\\hat{x}\\neq x,}\\\\ {\\!\\!1+Q_{t}(x,x)\\Delta t+o(\\Delta t),}&{\\hat{x}=x,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "79 where $Q_{t}(x,\\hat{x})$ is the $(x,{\\hat{x}})$ element of transition rate matrix $Q_{t}$ , denoting the transition rate from   \n80 state $x$ to state $\\hat{x}$ at time $t$ . Equivalently, we can directly define $Q_{t}(x,\\hat{x})$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{t}(x,\\hat{x})=\\left\\{\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\frac{p_{t+\\Delta t|t}(\\hat{x}|x)}{\\Delta t},\\quad\\;\\hat{x}\\neq x,}\\\\ {\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\frac{p_{t+\\Delta t|t}(x|x)-1}{\\Delta t},\\quad\\hat{x}=x.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "81 Given the above definition, denote $P_{s\\to t}(x,\\hat{x}):=p_{t|s}(\\hat{x}|x)$ . The following Kolmogorov\u2019s forward   \n82 equation holds [26, 30]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d}{d t}P_{s\\to t}=P_{s\\to t}Q_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "83 In practice [26, 29], $Q_{t}$ is parameterized as $\\sigma(t)Q$ , where $\\sigma(t)$ is a scalar function and $Q$ is a   \n84 constant matrix. In this case, the solution to Eq. (2.3) can be solved analytically as $P_{s\\rightarrow t}\\;=\\;$   \n85 $\\exp\\left((\\bar{\\sigma}(t)-\\bar{\\sigma}(s))Q\\right)$ , where $\\begin{array}{r}{\\bar{\\sigma}(t)=\\int_{0}^{t}\\sigma(s)d s}\\end{array}$ and exp is the matrix exponential. Therefore, we   \n86 can directly sample $\\pmb{x}_{t}$ from $\\pmb{x}_{s}$ in one step for any $t>s$ .   \n87 Further, $Q$ is often designed to diffuse towards a uniform distribution or an absorbing state [M].   \n88 Recent work [20, 26] suggests that the absorbing matrix achieves better empirical performance.   \n89 Besides, as detailed in Section 3, the specific structure of the absorbing matrix can be leveraged   \n90 to improve performance and accelerate sampling. Therefore, we focus on the absorbing matrix as   \n91 follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nQ^{\\mathrm{absorb}}=\\left[\\begin{array}{c c c c c}{-1}&{0}&{\\cdot\\cdot\\cdot}&{0}&{1}\\\\ {0}&{-1}&{\\cdot\\cdot\\cdot}&{0}&{1}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdot\\cdot\\cdot}&{-1}&{1}\\\\ {0}&{0}&{\\cdot\\cdot\\cdot}&{0}&{0}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 The time reversal of the forward process is characterized by a reverse transition rate matrix ${\\tilde{Q}}_{t}$ [31, 32],   \n93 whose element from state $x$ to state $\\hat{x}$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{Q}_{t}(x,\\hat{x})=\\left\\{\\frac{p_{t}(\\hat{x})}{p_{t}(x)}Q_{t}(\\hat{x},x),\\quad\\quad\\hat{x}\\neq x,}\\\\ {-\\sum_{k\\neq x}\\tilde{Q}_{t}(x,k),\\,\\,\\,\\,\\hat{x}=x.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "94 Simulating the reverse process requires to learn the reverse transition rate $\\tilde{Q}_{t}(x,\\hat{x})$ . As $Q_{t}(x_{t},\\hat{x}_{t})$   \n95 is known, it is sufficient to estimate the concrete score $\\frac{p_{t}\\left({\\hat{x}}_{t}\\right)}{p_{t}\\left(x_{t}\\right)}$ by a score network $s_{\\theta}(x_{t},t)\\,\\approx$   \n96 [ pptt((xx\u02c6tt))]x\u02c6t\u2208X [28]. Denoising score entropy (DSE) [29] is an effective objective to train the score   \n97 network ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\mathbb{E}_{\\widetilde{x}\\sim p_{t\\mid0}(\\cdot\\vert x_{0})}\\sum_{y\\neq\\widetilde{x}}Q_{t}\\left(\\widetilde{x},y\\right)\\left(s_{\\theta}\\left(\\widetilde{x},t\\right)_{y}-\\frac{p_{t\\mid0}\\left(y\\mid x_{0}\\right)}{p_{t\\mid0}\\left(\\widetilde{x}\\mid x_{0}\\right)}\\log s_{\\theta}\\left(\\widetilde{x},t\\right)_{y}+K\\left(\\frac{p_{t\\mid0}\\left(y\\mid x_{0}\\right)}{p_{t\\mid0}\\left(\\widetilde{x}\\mid x_{0}\\right)}\\right)\\right)d t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 where $K(a):=a\\log a-a$ . In particular, the DSE loss in Eq. (2.6) is an evidence lower bound   \n99 (ELBO) of the negative log-likelihood with an unknown gap. Nevertheless, existing work [29] still   \n100 employs it for training and likelihood evaluation. ", "page_idx": 2}, {"type": "text", "text": "101 After training, sampling from the model can be understood as discretizing the following process ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d}{d t}P_{s\\rightarrow t}=P_{s\\rightarrow t}{\\tilde{Q}}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "102 where $d t$ is an infinitesimal negative timestep and the concrete score is replaced by the score network.   \n103 Existing samplers include the Euler method, Gillespie method, and Tweedie $\\tau$ -leaping, as detailed in   \n104 Appendix D. ", "page_idx": 2}, {"type": "text", "text": "105 2.2 Multi-dimension ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "106 The multi-dimensional cases consider a state space of size $d$ like $\\mathcal{X}^{d}=\\{1,\\dots,n\\}^{d}$ . We denote   \n107 the sample as a sequence of one-dimensional data, i.e. $\\pmb{x}\\;=\\;x^{1}\\ldots x^{d}$ . The transition matrix   \n108 $Q_{t}\\,\\in\\,\\Bar{\\mathbb{R}^{n}}^{d}{\\times n^{d}}$ has an exponential number of possible states, making it expensive to reverse. To   \n109 alleviate this issue, existing work [26, 29] assumes independence between dimensions and each   \n110 dimension is a one-dimensional diffusion process with the same transition rate matrix $Q_{t}^{\\mathrm{tok}}\\in\\mathbb{R}^{n\\times n}$ . ", "page_idx": 2}, {"type": "text", "text": "Under the independent assumption, $Q_{t}$ assigns zero values [26, 29] for all sequences with a Hamming distance larger than 1. According to Eq. (2.4), it is sufficient to model the concrete score between sequences that differ by a Hamming distance of 1, such as $\\widehat{\\pmb{x}}_{t}=x_{t}^{1}\\cdot\\cdot\\cdot\\widehat{x}_{t}^{i}\\cdot\\cdot\\cdot x_{t}^{d}$ given $\\pmb{x}_{t}=x_{t}^{1}\\cdot\\cdot\\cdot x_{t}^{d}$ . Therefore, the score network $\\pmb{s}_{\\theta}(\\cdot,t):\\{1,\\dots,n\\}^{d}\\rightarrow\\mathbb{R}^{d\\times n}$ is defined as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{\\theta}\\left(x_{t},t\\right)_{\\hat{x}_{t}}=s_{\\theta}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d},t\\right)\\left[i,\\widehat{x}_{t}^{i}\\right]\\approx\\frac{p_{t}\\left(x_{t}^{1}\\ldots\\widehat{x}_{t}^{i}\\ldots x_{t}^{d}\\right)}{p_{t}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d}\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "111 which leads to the following expression to estimate the reverse transition rate matrix ${\\tilde{Q}}_{t}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{Q}_{t}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d},x_{t}^{1}\\ldots\\widehat{x}_{t}^{i}\\ldots x_{t}^{d}\\right)=Q_{t}^{\\mathrm{tok}}\\left(\\widehat{x}_{t}^{i},x_{t}^{i}\\right)\\frac{p_{t}\\left(x_{t}^{1}\\ldots\\widehat{x}_{t}^{i}\\ldots x_{t}^{d}\\right)}{p_{t}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\approx Q_{t}^{\\mathrm{tok}}\\left(\\widehat{x}_{t}^{i},x_{t}^{i}\\right)s_{\\theta}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d},t\\right)[i,\\widehat{x}_{t}^{i}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "112 Existing samplers assume that each dimension is independent within a small interval $\\Delta t$ and update   \n113 each dimension in parallel for efficiency [29, 26]. ", "page_idx": 3}, {"type": "text", "text": "114 3 Reparameterized absorbing discrete diffusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "115 In Section 3.1, we reveal that the concrete score of absorbing discrete diffusion can be reparameterized   \n116 as conditional distributions of clean data, which enables efficient sampling by caching the output of   \n117 time-independent network (see Section 3.2) and exact likelihood computation (see Section 3.3) by   \n118 applying the change of variable from time to the probability of being masked in a single dimension. ", "page_idx": 3}, {"type": "text", "text": "119 3.1 Parameterizing the concrete score as conditional distributions of clean data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "120 A key observation is that only the transition from the masked token to an unmasked token is valid in   \n121 the reverse process of an absorbing discrete diffusion. In particular, according to the definition of   \n122 the transition matrix of the absorbing process (see Eq. (2.4)), we have $Q^{\\mathrm{absorb}}(\\hat{x}_{t}^{i},x_{t}^{i})=0$ for any   \n123 unmasked $x_{t}^{i}\\neq[\\mathbf{M}]$ and $\\hat{x}_{t}^{i}\\neq x_{t}^{i}$ . Therefore, the corresponding element in the transition matrix of   \n124 the reverse process ${\\tilde{Q}}_{t}$ (see Eq. (2.5)) equals zero. Namely, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{Q}_{t}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d},x_{t}^{1}\\ldots\\widehat{x}_{t}^{i}\\ldots x_{t}^{d}\\right)=\\sigma(t)Q^{\\mathrm{absob}}\\left(\\widehat{x}_{t}^{i},x_{t}^{i}\\right)\\frac{p_{t}\\left(x_{t}^{1}\\ldots\\widehat{x}_{t}^{i}\\ldots x_{t}^{d}\\right)}{p_{t}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d}\\right)}=0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "125 for any unmasked state $x_{t}^{i}\\neq[\\mathbf{M}]$ and $\\hat{x}_{t}^{i}\\neq x_{t}^{i}$ and it is unnecessary to model the corresponding   \n126 concrete score $\\frac{p_{t}\\left(x_{t}^{1}\\ldots{\\widehat{x}}_{t}^{i}\\ldots x_{t}^{d}\\right)}{p_{t}\\left(x_{t}^{1}\\ldots x_{t}^{i}\\ldots x_{t}^{d}\\right)}$ . Also, note that the concrete score always takes the value of one if \u00b7   \n127 $\\hat{x}_{t}^{i}=x_{t}^{i}$ . Therefore, we only need to characterize the concrete score for $\\boldsymbol x_{t}^{i}=[\\mathbf M]$ and $\\hat{x}_{t}^{i}\\neq[\\mathbf{M}]$ .   \n128 Interestingly, in this case, we discover that the concrete score has a simple analytic form w.r.t. to the   \n129 conditional distributions of clean data, as summarized in the following Theorem 1.   \n130 Theorem 1. (Analytic concrete score in absorbing case, proof in Appendix $B$ ) For $\\mathbf{\\boldsymbol{x}}_{t}=$   \n131 $\\boldsymbol x_{t}^{1}\\ldots\\boldsymbol x_{t}^{i}\\ldots\\boldsymbol x_{t}^{d}$ and $\\mathbf{\\widehat{x}}_{t}\\,=\\,x_{t}^{1}\\,.\\,.\\,.\\,\\widehat{x}_{t}^{i}\\,.\\,.\\,.\\,x_{t}^{d}$ , if $\\boldsymbol{x}_{t}^{i}=[M]$ and $\\hat{x}_{t}^{i}\\neq[M]$ , the concrete score at time   \n132 $t$ can be expressed as a time-in d ependent conditional distribution at time zero multiplied by an   \n133 analytic time-dependent term: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{p_{t}\\left(x_{t}^{1}\\cdot\\cdot\\cdot\\widehat{x}_{t}^{i}\\cdot\\cdot\\cdot x_{t}^{d}\\right)}{p_{t}\\left(x_{t}^{1}\\cdot\\cdot\\cdot x_{t}^{i}\\cdot\\cdot\\cdot x_{t}^{d}\\right)}=\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}p_{0}(\\widehat{x}_{t}^{i}|x_{t}^{U M}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 where xtUMis the vector consists of all unmasked tokens of xt. ", "page_idx": 3}, {"type": "text", "text": "135 One immediate implication of Theorem 1 is to theoretically explain the benefit of the \u201cscaling   \n136 trick\u201d in existing work [29] (see Appendix C.2 therein), which significantly improves the practical   \n137 performance of discrete diffusion (see Table 2) but has not been fully understood.   \n138 In particular, the scaling trick divides the output of the score network $\\scriptstyle{s_{\\theta}}$ by a factor of $e^{\\bar{\\sigma}(t)}-1$ .   \n139 Equivalently, it reparameterizes $\\pmb{s}_{\\theta}(\\pmb{x}_{t},t)$ as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{\\theta}(x_{t},t)=\\frac{1}{e^{\\bar{\\sigma}(t)}-1}\\tilde{s}_{\\theta}(x_{t},t)=\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\tilde{s}_{\\theta}(x_{t},t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "7yqjVgWWxx/tmp/a8257d9a0dd50963c2738fdb5af3d07f305618ae77c4028737a535be48080f84.jpg", "img_caption": ["Figure 1: Expected number of function evaluations (E-NFE) over a different number of sampling steps. E-NFE is measured by Tweedie $\\tau$ -leaping method with log-linear noise schedule. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "7yqjVgWWxx/tmp/97e87528ff90d7accd5cee0f07a000edc94a88dfad06c57210793cf63924439a.jpg", "img_caption": ["Figure 2: Sample quality measured by perplexity (\u2193). We compare SEDD with Euler and Tweedie $\\tau$ -leaping (abbr. $\\mathrm{T}{\\cdot}\\tau^{\\prime}$ ) samplers, and RADD with Euler sampler. We show E-NFE for RADD with caching and NEF otherwise. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "140 where the scaling factor coincides with the time-dependent term in Theorem 1. In the original   \n141 parameterization, the score network $s_{\\theta}$ must model the whole time-dependent concrete score. In   \n142 contrast, with the scaling trick, the reparameterized score $\\widetilde{\\pmb{s}}_{\\theta}(\\pmb{x}_{t},t)$ can focus on capturing the clean   \n143 data distribution $p_{0}(\\hat{x}^{i}|\\pmb{\\bar{x}}_{t}^{\\mathrm{UM}})$ and simplifies learning, according to Theorem 1.   \n144 Further, Theorem 1 suggests that it is unnecessary to incorporate the time $t$ in the reparameterized   \n145 score, and the reparameterized score $\\widetilde{\\pmb{s}}_{\\theta}(\\pmb{x}_{t},t)$ should output a valid probability distribution. Mo  \n146 tivated by the insights, we propose reparameterized absorbing discrete diffusion (RADD), which   \n147 employs a network $c_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t})$ that removes the time condition from the input and takes the softmax as   \n148 final nonlinearity. Formally, we can write our reparameterization as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l}{\\displaystyle\\mathbf{\\boldsymbol{s}}_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t)=\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}c_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "149 In practice, we make a minimal modification of the score network in SEDD [29] for simplicity and   \n150 fairness, detailed in Appendix F.1.   \n151 Moreover, RADD also enjoys a more efficient sampling process than SEDD [29] (with or without the   \n152 scaling trick) based on its simplified parameterization, as presented below. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "153 3.2 Efficient samplers to reduce NFE by caching $c_{\\theta}{\\left(x_{t}\\right)}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "154 For the reverse process of an absorbing discrete diffusion, once a token is generated from $[\\mathbf{M}]$ to   \n155 an unmasked token, it never transits to another token. Therefore, for a sequence $\\pmb{x}_{t}$ of length $d$ , $\\pmb{x}_{t}$   \n156 changes at most $d$ times, irrespective of the number of sampling steps $D$ . In the other steps, $\\pmb{x}_{t}$   \n157 remains in all $d$ dimensions. We highlight that we can cache $c_{\\theta}(x_{t})$ naturally without evaluating the   \n158 time-independent $c_{\\theta}$ to reduce the NFE compared to SEDD (see Appendix E for the pseudo-code, ).   \n159 As shown in Fig. 2, RADD with the caching strategy is more efficient than SEDD given any number   \n160 of sampling steps, especially given large sampling steps. This is as expected because the NFE is   \n161 limited within the generating sequence length.   \n162 Note that the NFEs with the caching strategy is a random variable. To quantify it, we calculate the   \n163 expected NFEs (abbr. E-NFEs) required in an analytic form, conditioned on the sampling method,   \n164 time steps, and noise schedule. Specifically, denote $l$ as the generating sequence length, which does   \n165 not equal $d$ generally. Given the sampling time steps $\\{t_{0}=0,\\cdot\\cdot\\cdot\\,,t_{n}=T\\}$ , let $N_{k}\\in\\{0,\\cdots,l\\}$   \n166 denote the number of changed dimensions of $\\textbf{\\em x}$ in $[t_{k-1},t_{k})$ . Since we perform function evaluation ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "167 in $[t_{k-1},t_{k})$ only when $\\textbf{\\em x}$ changes (i.e. $N_{k}\\neq0$ ), the NFEs and E-NFEs can expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\mathrm{NFEs}}(n)=\\displaystyle\\sum_{k=1}^{n}\\mathbb{I}(N_{k}\\neq0),}\\\\ {{\\mathrm{E-NFEs}}(n)=\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}[\\mathbb{I}(N_{k}\\neq0)]=\\displaystyle\\sum_{k=1}^{n}P(N_{k}\\neq0).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "168 For each dimension $i$ , let $r_{k}$ represent the probability that $\\boldsymbol{x}^{i}$ changes within the interval $[t_{k-1},t_{k})$ .   \n169 As the probability is independent in different dimensions (proof in Appendix D.3), $N_{k}$ follows a   \n170 binomial distribution with parameters $l$ and $r_{k}$ . Therefore, Eq. (3.4) can be further simplified as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{E-NFEs}(n)=\\sum_{k=1}^{n}P(N_{k}\\neq0)=\\sum_{k=1}^{n}(1-(1-r_{k})^{l}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "171 which applies to all samplers. Further, $r_{k}$ can be analytically expressed w.r.t. the time steps and   \n172 noise schedule for both Euler and Tweedie $\\tau$ -leaping samplers, as detailed in Appendix D.3. Taking   \n173 Tweedie $\\tau$ -leaping method with log-linear noise schedule [29] for example, its E-NFEs is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{E-NFEs}(n)=\\sum_{k=1}^{n}(1-(1-{\\frac{1}{n}})^{l})=n(1-(1-{\\frac{1}{n}})^{l}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "174 Appendix D.3 provides the proof. As shown in Fig. 1, we plot the curve of Eq. (3.6) in blue, which   \n175 agrees with our experiments (the red stars). ", "page_idx": 5}, {"type": "text", "text": "176 3.3 Denoise cross-entropy for exact likelihood evaluation and training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "177 As illustrated in Theorem 1, the concrete score can be understood as a rescaled conditional distribution   \n178 on clean data. From this perspective, it is natural to wonder: is it possible to evaluate and optimize   \n179 the exact likelihood of the model instead of the ELBO? Surprisingly, the answer is yes for both the   \n180 original parameterization [29] and our new parameterization.   \n181 Let $q_{\\theta}(x_{0})$ denote the model distribution at time zero defined by $s_{\\theta}$ , or our $c_{\\theta}$ , which approximates   \n182 the true distribution $p_{0}(\\mathbf{\\boldsymbol{x}}_{0})$ . Inspired by the cross-entropy loss in auto-regressive models, we define   \n183 the denoising cross-entropy loss $\\mathcal{L}_{\\mathrm{DCE}}^{T}(\\pmb{x}_{0})$ as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{DCE}}^{T}(x_{0}):=\\displaystyle\\int_{0}^{T}\\mathbb{E}_{\\tilde{x}\\sim p_{t\\mid0}(\\cdot\\vert x_{0})}\\sum_{y\\neq\\tilde{x}}Q_{t}\\left(\\tilde{x},y\\right)\\left(-\\frac{p_{t\\mid0}\\left(y\\mid x_{0}\\right)}{p_{t\\mid0}\\left(\\tilde{x}\\mid x_{0}\\right)}\\log s_{\\theta}\\left(\\tilde{x},t\\right)_{y}\\right)d t,}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{0}^{T}\\mathbb{E}_{\\tilde{x}\\sim p_{t\\mid0}(\\cdot\\vert x_{0})}\\sum_{y\\neq\\tilde{x}}Q_{t}\\left(\\tilde{x},y\\right)\\left(-\\frac{p_{t\\mid0}\\left(y\\mid x_{0}\\right)}{p_{t\\mid0}\\left(\\tilde{x}\\mid x_{0}\\right)}\\log\\left(\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}c_{\\theta}(\\tilde{x})_{y}\\right)\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "184 Compared with the DSE loss in Eq. (2.6), our DCE loss simply removed the terms $\\pmb{s}_{\\theta}\\left(\\tilde{\\pmb{x}},t\\right)_{y}$ and   \n185 $K\\left(\\frac{p_{t|0}(\\pmb{y}|\\pmb{x}_{0})}{p_{t|0}(\\tilde{\\pmb{x}}|\\pmb{x}_{0})}\\right)$ , however, it shows that DCE loss exactly equals the negative log-likelihood of $q_{\\theta}(x_{0})$   \n186 with a sufficiently long process in absorbing discrete diffusion.   \n187 Theorem 2. Suppose $\\left\\{X_{t}\\right\\}$ is a continuous time Markov chain with transition rate matrix $Q_{t}=$   \n188 $\\sigma(t)Q^{a b s o r b}$ . For a given data $\\pmb{x}_{0}$ , if $\\sigma(t)$ satisfies $\\begin{array}{r}{\\int_{0}^{\\infty}\\sigma(\\tau)d\\tau=\\infty}\\end{array}$ , then the denoising cross-entropy   \n189 loss defined in Eq. (3.8) with $T\\rightarrow\\infty$ exactly equals the negative log-likelihood of $\\pmb{x}_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{D C E}^{\\infty}(\\pmb{x}_{0})=-\\log q_{\\theta}(\\pmb{x}_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "190 The proof of Theorem 2 consists of three key steps, detailed in Appendix C.1, Appendix C.2   \n191 and Appendix C.3 respectively. In the first step, we apply a change of variable from $t$ to $\\lambda(t)=$   \n192 $1-e^{-\\bar{\\sigma}(t)}$ , which is the probability of a token is masked from 0 to $t$ in the forward process. Further,   \n193 inspired by the factorization form discovered in Theorem 1, the denoising cross-entropy loss for both   \n194 parameterizations can then be rewritten as an integral of $\\lambda$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DCE}}^{\\infty}(x_{0})=\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\tilde{x}\\sim p_{\\lambda}(\\tilde{x}|x_{0})}\\left[\\sum_{\\tilde{x}^{i}=[\\mathbf{M}]}-\\log q_{\\theta}(x_{0}^{i}|\\tilde{x}^{\\mathrm{UM}})\\right]d\\lambda,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "195 where $p_{\\lambda}(\\tilde{\\pmb{x}}|\\pmb{x}_{0})$ is the joint distribution induced by masking each dimension in $\\scriptstyle x_{0}$ independently   \n196 with a probability $\\lambda$ .   \n197 In the second step, we demonstrate that the integral w.r.t. $\\lambda$ in Eq. (3.10) can be integrated analytically,   \n198 and the DSE loss can be rewritten as expectations over the number and positions of masks as follows: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DCE}}^{\\infty}(x_{0})=d\\mathbb{E}_{k\\sim U(\\{1,\\cdots,d\\})}\\frac{1}{k}\\mathbb{E}_{\\tilde{\\mathbf{x}}\\sim U(\\tilde{x}_{k})}\\left[\\sum_{\\tilde{\\mathbf{x}}^{i}=\\left[\\mathbf{M}\\right]}-\\log q_{\\theta}(\\pmb{x}_{0}^{i}|\\tilde{\\mathbf{x}}^{\\mathrm{UM}})\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "199 where we denote $\\tilde{\\mathcal{X}}_{k}:=\\{\\tilde{\\pmb{x}}:\\tilde{\\pmb{x}}\\in\\tilde{\\mathcal{X}}$ and $\\tilde{\\pmb{x}}$ has exact $k$ dimensions masked by $\\left[\\mathbf{M}\\right]\\}$ and $U(\\cdot)$ as   \n200 uniform distribution.   \n201 Finally, in the third step, we prove that Eq. (3.11) enumerates all orders to decompose the joint   \n202 distribution auto-regressively and accumulates log densities of all conditional distributions in every   \n203 order. Therefore, it is equivalent to the negative log-likelihood of $q_{\\theta}$ : ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DCE}}^{\\infty}(\\pmb{x}_{0})=-\\log q_{\\theta}(\\pmb{x}_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "204 Theorem 2 enables exact likelihood computation for both the original model $\\scriptstyle{s_{\\theta}}$ and our $c_{\\theta}$ , providing   \n205 a more accurate measure of model performance. Take $c_{\\theta}$ for example, Eq. (3.8) can be rewritten as a   \n206 form of expectation on $t$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n^{\\mathrm{\\scriptscriptstyleT}}\\!\\!\\!\\!-\\!\\!\\!\\!\\operatorname{\\calD}\\!\\mathrm{t}_{\\l}(x_{0})=\\frac{1}{T}\\mathbb{E}_{t\\sim U([0,T])}\\mathbb{E}_{\\hat{\\pi}\\sim p_{t|0}(\\cdot\\,|x_{0})}\\sum_{y\\neq\\hat{x}}Q_{t}\\left(\\tilde{x},y\\right)\\left(-\\frac{p_{t|0}\\left(y\\mid x_{0}\\right)}{p_{t|0}\\left(\\tilde{x}\\mid x_{0}\\right)}\\log\\left(\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}c_{\\theta}(\\tilde{x})y\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "207 Naturally, we can take the Monte Carlo estimation of $\\mathcal{L}_{\\mathrm{DCE}}^{T}(x_{0})$ by sampling $t$ to approximate   \n208 $-\\log q_{\\theta}(x_{0})$ according to Eq. (3.13). In addition, it can be used as an efficient and valid training   \n209 target for discrete diffusion models, as an alternative to the ELBO (i.e. DSE loss). For pseudo-code   \n210 of training, see Appendix E. ", "page_idx": 6}, {"type": "text", "text": "211 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "212 We present the experimental setups in Section 4.1. We then evaluate the performance of accelerated   \n213 generation in Section 4.2 and zero-shot perplexity on various language datasets in Section 4.3. ", "page_idx": 6}, {"type": "text", "text": "214 4.1 Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "215 Model. We use RADD model $c_{\\theta}$ reparameterzied as described in Section 3.1. Compared with   \n216 SEDD small model, RADD model has 7M fewer parameters due to the removal of time-condition,   \n217 which equates to an $8\\%$ decrease from the original 90M non-embedding parameters. We trained   \n218 our RADD model $c_{\\theta}$ using denoising score entropy and denoising cross entropy, abbreviated as   \n219 RADD-DSE and RADD-DCE. For SEDD small model, we employed their pre-trained model.   \n220 Data. In line with the methodology outlined by SEDD, we trained on the OpenWebText [33]   \n221 dataset and tested on the LAMBADA, WikiText2, PTB, WikiText103, and One Billion Words   \n222 datasets [34, 35, 36]. For data splits and data processing, we adopted the same settings and techniques   \n223 as SEDD, which involves packing sentences to generate uniform-length blocks as model input.   \n224 Training setup. We used the same training setup for RADD and SEDD. Specifically, we used a   \n225 log-linear noise schedule where the expectation of the number of changed tokens at time $t$ is linear   \n226 with $t$ . For simplicity, we also used the same optimization configuration as SEDD, which can be   \n227 suboptimal for our RADD model and DCE loss. For more details see Appendix F.   \n228 Metric. Following previous work [29], we conduct experiments on unconditional generation and   \n229 language modeling tasks. For generation, we use perplexity (PPL) on unconditional samples measured   \n230 by an additional larger language model (i.e. GPT-2 large) to evaluate sample quality. To access   \n231 inference efficiency, we computed the inference time on a single NVIDIA 4090 GPU with a batch   \n232 size of 8 and averaged over 1024 samples. For language modeling tasks, we report the perplexity   \n233 calculated on the dataset with different models. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "7yqjVgWWxx/tmp/71351c8a7bbccb468688f9bd1e9c87a12466cbf5e1a52e34e42298ffb5bcd7b7.jpg", "table_caption": ["Table 1: Avarage inference time of a single sample with varying sampling steps. The table compares the average inference time (in seconds) for the SEDD small model using both Euler and Tweedie $\\tau$ -leaping (abbreviated as $\\mathrm{T}{\\cdot}\\tau\\mathrm{~.~}$ ) sampling methods, and the RADD small model using the Euler method with a caching strategy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "234 4.2 Efficient sampling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "235 We compare the sample quality measured by perplexity between SEDD and our RADD-DCE model,   \n236 as shown in Fig. 2. For a fixed NFE, RADD-DCE with the Euler sampler outperforms SEDD with   \n237 multiple samplers. It suggests that RADD with caching accelerates the sampling process and beneftis   \n238 sample quality at the same time. Besides, the acceleration by cache strategy is particularly significant   \n239 with large sampling steps, as analyzed in Section 3.2.   \n240 We further compare the running time for the methods in Table 1. Across all sampling steps, RADD   \n241 consistently requires the shortest sampling time and outperforms SEDD with different samplers.   \n242 Quantitatively, RADD achieves a speed-up of $2.5\\sim3.5$ times as shown in Table 1. These results   \n243 agree with the analysis of the E-NFEs in Fig. 1, validate the effectiveness of RADD and caching   \n244 strategy, and demonstrate the practical implications of our Theorem 1.   \n245 According to Eq. (3.11), we can also use RADD as an auto-regressive model to generate samples in   \n246 different orders, leading to worse performance as a discrete diffusion, as detailed in Appendix F.4.   \n247 We present more sampling details in Appendix F.3. and the generated samples in Appendix G.1. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "248 4.3 Improved zero-shot perplexity on language modeling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "249 Following SEDD, we present zero-shot perplexities on the LAMBADA, WikiText2, PTB, Wiki  \n250 Text103, and 1 Billion Words datasets [37] in Table 2 and compare the zero-shot perplexity of our   \n251 model with other baseline models [20, 38, 29].   \n252 Firstly, we conduct an ablation study of the scaling trick in the middle of the Table 2. With an   \n253 absorbing process, the perplexity of the scaled version of SEDD outperforms its unscaled version,   \n254 which matches our theoretical discovery in Theorem 1.   \n255 Secondly, without any modification of the model, we estimate the exact likelihood of the baseline   \n256 model SEDD [29] based on Theorem 2 in Table 2. We observe that perplexity is consistently better   \n257 than the ELBO of the strongest discrete diffusion models, which validates our Theorem 2.   \n258 Lastly, we report the maximum likelihood training results of RADD in the last row in Table 2. We   \n259 observed that RADD-DCE outperforms RADD-DSE, but their performances are slightly worse than   \n260 SEDD. This discrepancy could be because we did not search the hyperparameters and directly applied   \n261 identical optimization configures as SEDD, which may be suboptimal. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "262 5 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "263 Continouous-state diffusion models for text generation. Several works have been proposed to   \n264 apply continuous diffusion to text [19, 21, 22, 23]. Li et al. [19] use an embedding layer to map   \n265 discrete tokens to a latent space and learn a continuous-state diffusion on it. Bit Diffusion [22] learns a   \n266 continuous diffusion model to generate binary bits of discrete tokens. However, transforming between   \n267 these continuous representations and discrete tokens by thresholding may lose information. Bayesian   \n268 Flow Network [23] achieves competitive log-likelihood on character-level language modeling tasks   \n269 and is proven equivalent to continuous stochastic differential equations trained by denoising score   \n270 matching [24]. Such models underperform auto-regressive models on standard text generation tasks.   \n271 Discrete-state diffusion models for text generation. Several discrete-state diffusion models have   \n272 been proposed [11, 39, 20]. D3PM [20] proposed a diffusion framework based on any probability   \n273 transition matrix and trained with a lower bound of log-likelihood. DiffusionBERT [25] utilizes a   \n274 pre-trained BERT [40] as an initialization of diffusion. Furthermore, [26] generalizes the framework   \n275 to continuous time by introducing a rate matrix. It is difficult to apply the score matching in such   \n276 models because the gradient of the data distribution is undefined. Several works try to generalize the   \n277 score matching on discrete data [29, 28, 26, 27]. Meng et al. [28] introduce the concrete score and the   \n278 denoising concrete score matching loss. Furthermore, SEDD bridges the discrete state diffusion and   \n279 the concrete score by introducing a denoising score entropy loss [29]. By incorporating an absorbing   \n280 process, SEDD achieves competitive performance with the auto-regressive models, especially, GPT-2. ", "page_idx": 7}, {"type": "table", "img_path": "7yqjVgWWxx/tmp/7740540e63a3ef437161b63ab08b34194952453f0e1917c1c731affb77133e8e.jpg", "table_caption": ["Table 2: Zero-shot language modeling perplexity (\u2193) on five datasets. \u2020 labels the results based on ELBO which is taken from [20, 38, 29] and \u22c6labels the results based on the exact likelihood implemented by us. In this table, SEDD-U / SEDD-S refer to the unscaled and scaled absorbing models respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "281 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "282 We introduce RADD, a dedicated discrete diffusion model that characterizes the time-independent   \n283 conditional probabilities, built upon a new factorization form of the concrete score. RADD is much   \n284 more efficient by reducing the NFEs with a cache strategy while retaining a better performance   \n285 than strong baselines. Furthermore, we propose DCE loss and prove it is equivalent to the negative   \n286 log-likelihood of absorbing diffusion. When applied to SEDD, DCE significantly advances the   \n287 state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks at the GPT-2 scale.   \n288 Limitaition. Our model has been trained and evaluated primarily on the GPT-2 scale. For broader   \n289 applicability, it is essential to explore the effects of scaling on the performance [41], which is left as   \n290 future work. The success of diffusion transformers on images [42, 43, 44] and videos [45] suggests   \n291 that diffusion models can be scaled up by incorporating transformers.   \n292 Another limitation is that our model can only generate full-length outputs, unlike auto-regressive   \n293 models that can produce variable-length outputs. This restricts the flexibility of our model in certain   \n294 applications. We leave the investigation on this issue as future work.   \n295 Social impact. For the current theoretical and experimental scope of this paper, we have not found any   \n296 6 direct social impacts. However, considering future developments, the paper potentially contributes   \n297 to the next-generation large language models. In this context, this work could significantly reduce   \n298 the inference cost of language models but may also lead to hallucinations, amplify biases and   \n299 discrimination in the data, and pose risks of misuse. As with other generative models, addressing   \n300 these issues requires further advancements in the field. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "301 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "302 [1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language   \n303 understanding by generative pre-training. 2018.   \n304 [2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n305 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n306 [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n307 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n308 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n309 [4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n310 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n311 processing systems, 30, 2017.   \n312 [5] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. November 2022. URL   \n313 https://openai.com/blog/chatgpt/.   \n314 [6] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni   \n315 Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4   \n316 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n317 [7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n318 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,   \n319 Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation   \n320 language models, 2023.   \n321 [8] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,   \n322 Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.   \n323 arXiv preprint arXiv:2305.10403, 2023.   \n324 [9] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz   \n325 Korbak, and Owain Evans. The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a\".   \n326 arXiv preprint arXiv:2309.12288, 2023.   \n327 [10] Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan. Are we   \n328 falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse. arXiv   \n329 preprint arXiv:2311.07468, 2023.   \n330 [11] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper  \n331 vised learning using nonequilibrium thermodynamics. In International conference on machine   \n332 learning, pages 2256\u20132265. PMLR, 2015.   \n333 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances   \n334 in neural information processing systems, 33:6840\u20136851, 2020.   \n335 [13] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and   \n336 Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv   \n337 preprint arXiv:2011.13456, 2020.   \n338 [14] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv   \n339 preprint arXiv:2010.02502, 2020.   \n340 [15] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the   \n341 optimal reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503,   \n342 2022.   \n343 [16] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential   \n344 integrator. arXiv preprint arXiv:2204.13902, 2022.   \n345 [17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver:   \n346 A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in   \n347 Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n348 [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm  \n349 solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint   \n350 arXiv:2211.01095, 2022.   \n351 [19] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto.   \n352 Diffusion-lm improves controllable text generation, 2022.   \n353 [20] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Struc  \n354 tured denoising diffusion models in discrete state-spaces. In Advances in Neural Information   \n355 Processing Systems, 2021.   \n356 [21] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin,   \n357 Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis   \n358 Hawthorne, R\u00e9mi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categor  \n359 ical data, 2022.   \n360 [22] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using   \n361 diffusion models with self-conditioning, 2023.   \n362 [23] Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian   \n363 flow networks, 2024.   \n364 [24] Kaiwen Xue, Yuhao Zhou, Shen Nie, Xu Min, Xiaolu Zhang, Jun Zhou, and Chongxuan Li.   \n365 Unifying bayesian flow networks and diffusion models through stochastic differential equations,   \n366 2024.   \n367 [25] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusion  \n368 bert: Improving generative masked language models with diffusion models. arXiv preprint   \n369 arXiv:2211.15029, 2022.   \n370 [26] Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and   \n371 A. Doucet. A continuous time framework for discrete denoising models. In Advances in Neural   \n372 Information Processing Systems, 2022.   \n373 [27] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time   \n374 discrete diffusion models, 2023.   \n375 [28] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching:   \n376 Generalized score matching for discrete data, 2023.   \n377 [29] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the   \n378 ratios of the data distribution, 2024.   \n379 [30] William J Anderson. Continuous-time Markov chains: An applications-oriented approach.   \n380 Springer Science & Business Media, 2012.   \n381 [31] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time   \n382 discrete diffusion models. In The Eleventh International Conference on Learning Representa  \n383 tions, 2023.   \n384 [32] Frank Kelly. Reversibility and stochastic networks. 1980. URL https://api.   \n385 semanticscholar.org/CorpusID:125211322.   \n386 [33] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/   \n387 OpenWebTextCorpus, 2019.   \n388 [34] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,   \n389 Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA   \n390 dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual   \n391 Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages   \n392 1525\u20131534, Berlin, Germany, August 2016. Association for Computational Linguistics. URL   \n393 http://www.aclweb.org/anthology/P16-1144.   \n394 [35] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture   \n395 models, 2016.   \n396 [36] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and   \n397 Tony Robinson. One billion word benchmark for measuring progress in statistical language   \n398 modeling, 2014.   \n399 [37] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language   \n400 models are unsupervised multitask learners. 2019. URL https://api.semanticscholar.   \n401 org/CorpusID:160025533.   \n402 [38] Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. In   \n403 Advances in Neural Information Processing Systems, 2023.   \n404 [39] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax   \n405 flows and multinomial diffusion: Learning categorical distributions. Advances in Neural   \n406 Information Processing Systems, 34:12454\u201312465, 2021.   \n407 [40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of   \n408 deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,   \n409 2018.   \n410 [41] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza   \n411 Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom   \n412 Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia   \n413 Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre.   \n414 Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https:   \n415 //api.semanticscholar.org/CorpusID:247778764.   \n416 [42] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth   \n417 words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on   \n418 Computer Vision and Pattern Recognition, pages 22669\u201322679, 2023.   \n419 [43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings   \n420 of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n421 [44] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao,   \n422 Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale.   \n423 arXiv preprint arXiv:2303.06555, 2023.   \n424 [45] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min   \n425 Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled   \n426 text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.   \n427 [46] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,   \n428 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing   \n429 Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489.   \n430 [47] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of   \n431 deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer  \n432 ence of the North American Chapter of the Association for Computational Linguistics: Human   \n433 Language Technologies, Volume 1 (Long and Short Papers). Association for Computational   \n434 Linguistics, 2019.   \n435 [48] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. In Interna  \n436 tional Conference on Computer Vision, 2023.   \n437 [49] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer   \n438 with rotary position embedding. Neurocomputing, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "439 A Proof of Proposition 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "440 Since the different dimensions of the forward diffusion process are independent of each other, we   \n441 can first analyze the conditional distribution in one dimension. This can be derived directly from   \n442 Eq. (2.3), but for a better understanding, here we provide a more intuitive proof in the case when   \n443 Qt = Qabsorb.   \n444 Lemma 1. (Analytic conditional distribution for absorbing case) Suppose $\\left\\{X_{t}\\right\\}$ is a continuous   \n445 time Markov chain with transition rate matrix $Q_{t}=\\sigma(t)Q^{a\\bar{b}s o r b}$ , given the value $x$ at time zero , the   \n446 conditional distribution $p_{t|0}(x_{t}|x)$ has the following analytic form: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t|0}(x_{t}|x)=\\left\\{\\!\\!\\begin{array}{l l}{e^{-\\bar{\\sigma}(t)},}&{x_{t}=x,\\smallskip}\\\\ {1-e^{-\\bar{\\sigma}(t)},}&{x_{t}=[M],}\\\\ {0.}&{x_{t}\\neq[M]\\,a n d\\,x_{t}\\neq[M].}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "447 Proof. Given the initial value $x\\in\\mathcal{X}=\\{1,\\cdots,N\\}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\nx_{t}=\\left\\{{\\begin{array}{l l}{x,}&{t<T_{h},}\\\\ {\\left[\\mathbf{M}\\right],}&{t\\geq T_{h},}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "448 where $T_{h}$ is the holding time before the system transitions to the next state. ", "page_idx": 12}, {"type": "text", "text": "449 Based on the properties of the $\\boldsymbol{Q}^{\\mathrm{absorb}}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t+\\Delta t|t}(x|x)=1-(-\\sigma(t)Q^{\\mathrm{absorb}}(x,x))\\Delta t+o(\\Delta t).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "450 Partitioning the interval $[0,t]$ into $\\{s_{k}\\}_{k=0}^{n}$ , make use of Memoryless Property of Continuous-Time   \n451 Markov Chains: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle p_{t|0}(x|x)=\\prod_{k=1}^{n}p_{s_{k}|s_{k-1}}(x|x)}\\\\ {\\displaystyle}&{\\displaystyle=\\prod_{k=1}^{n}(1-(-\\sigma(t_{k})Q^{\\mathsf{a b o r b}}(x,x))(s_{k}-s_{k-1})+o\\big((s_{k}-s_{k-1})\\big))}\\\\ {\\displaystyle}&{\\displaystyle=\\exp(\\sum_{k=1}^{n}\\ln(1-(-\\sigma(t_{k})Q^{\\mathsf{a b o r b}}(x,x))(s_{k}-s_{k-1})+o((s_{k}-s_{k-1})))}\\\\ &{\\displaystyle=\\exp(\\sum_{k=1}^{n}-(-\\sigma(t_{k})Q^{\\mathsf{a b o r b}}(x,x))(s_{k}-s_{k-1})+o((s_{k}-s_{k-1}))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "452 Let $\\operatorname*{max}(s_{k}-s_{k-1})\\to0$ , we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t|0}(x|x)=\\exp(-\\int_{0}^{t}-\\sigma(s)Q^{\\mathrm{absorb}}(x,x)d s)=\\exp(-(-Q^{\\mathrm{absorb}}(x,x)\\bar{\\sigma}(t))).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "453 As $Q^{\\mathrm{absorb}}(x,x)=-1$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t|0}(x|x)=P(T_{h}>t)=e^{-\\bar{\\sigma}(t)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "454 ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t|0}([\\boldsymbol{\\mathbf{M}}||x)=P(T_{h}>t)=1-e^{-\\bar{\\sigma}(t)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "455 ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t|0}(k|x)=0\\quad{\\mathrm{if~}}k\\neq[\\mathbf{M}]{\\mathrm{~and~}}k\\neq x.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "456 ", "page_idx": 12}, {"type": "text", "text": "457 Proposition 1. (Analytic joint distribution for absorbing case) ", "page_idx": 12}, {"type": "text", "text": "458 Suppose $\\left\\{X_{t}\\right\\}$ is a continuous time Markov chain with transition rate matrix $Q_{t}=\\sigma(t)Q^{a b s o r b}$ . For   \n459 $\\pmb{x}_{t}=x_{t}^{1}\\cdots x_{t}^{d}$ with $N_{1}$ components as $[M]$ and $N_{2}=d-N_{1}$ components as specific value, $p_{t}(\\pmb{x}_{t})$   \n460 can be expressed as Eq. (A.11): ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{t}({\\pmb x}_{t})=[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}}[e^{-\\bar{\\sigma}(t)}]^{N_{2}}p_{0}({\\pmb x}_{t}^{U M}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "461 where $\\pmb{x}_{t}^{U M}:=\\{\\pmb{x}^{k}|\\pmb{x}^{k}\\neq[M]\\}$ represents unmasked part of $\\pmb{x}_{t}^{U M}$ . ", "page_idx": 12}, {"type": "text", "text": "462 Proposition 1 shows that the joint distribution $p_{t}(\\pmb{x}_{t})$ can be expressed as the multiplication of two   \n463 terms. One is an analytic term only depending on time, the other is a joint distribution of clean data   \n464 $p_{0}(\\pmb{x}_{t}^{\\mathrm{UM}})$ with $N_{2}$ dimensions independent of time.   \n465 Proof. Without loss of generality, let\u2019s assume that the preceding $N_{1}$ terms of $\\textbf{\\em x}$ are all $[\\mathbf{M}]$ , and the   \n466 remaining $N_{2}$ terms are fixed at specific values. That is, $\\pmb{x}_{t}=[\\mathbf{M}]\\cdot\\cdot\\cdot[\\mathbf{M}]x_{t}^{N_{1}+1}\\cdot\\cdot\\cdot x_{t}^{d}$ , and here $x^{k}$   \n467 is a fixed value in . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "468 Use the law of total probability and Lemma 1, along with independent property: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{P}(|\\mathbf{M}|)-\\sum_{i=1}^{M}\\sum_{l=1}^{n-1}\\cdots\\cdots\\ x_{i}^{d}}\\\\ &{=\\sum_{i=1}^{M}\\bigg(\\mathbf{M}\\Big(\\mathbf{M}\\Big)^{\\top}\\mathbf{A}_{i}^{\\top}(1\\!\\!-\\!\\!w_{i}^{\\top}\\!)^{N_{i}+1}\\cdots\\mathbf{A}_{i}^{\\top}\\mathbf{L}_{P}\\Big)|\\Phi_{0}(\\mathbf{x}_{i})}\\\\ &{=\\sum_{i,k=1}^{M}\\sum_{l=1}^{M}\\ _{P}[\\Phi_{i1}(\\mathbf{M})]\\cdots[\\mathbf{M}]\\mathbf{x}_{i}^{N+1}\\cdots\\mathbf{A}_{i}^{\\top}\\mathbf{L}_{P}\\Big|\\mathbf{y}_{0}(\\mathbf{x}_{i}^{\\top}\\cdots\\mathbf{x}_{i}^{d})|{\\Phi}_{0}(\\mathbf{x}_{i}^{\\top}\\cdots\\mathbf{x}_{i}^{d})}\\\\ &{\\quad_{x_{i}^{N}}\\cdots\\cdots\\ x_{i}^{d}\\,}\\\\ &{=\\sum_{i,k=1}^{M}\\sum_{l=1}^{M}\\bigg(\\mathbf{M}\\Big)_{i}^{\\top}\\bigg\\{\\sum_{k=1}^{d}\\sum_{i=1}^{d}\\sum_{l=1}^{M}\\sum_{\\sigma_{i}^{\\top}\\in\\mathcal{S}_{i}^{k}}\\Big(\\mathbf{J}_{i}^{\\top}\\mathbf{\\Phi}_{i2}^{\\top}\\mathbf{\\Phi}_{i1}^{\\top}\\Big)\\Phi_{0}(\\mathbf{x}_{i}^{\\top}\\cdots\\mathbf{x}_{i}^{d})}\\\\ &{\\quad_{P}(|\\mathbf{x}_{i}^{\\top}\\cdots\\mathbf{x}_{i}^{d}\\mathbf{c}_{i}^{\\top}\\mathbf{x}_{i}^{\\top}\\cdots\\mathbf{x}_{i}^{d})}\\\\ &{=\\sum_{i=1}^{M}\\sum_{l=1}^{N}\\ \\sum_{l=1}^{N_{i}}(\\mathbf{M}\\Big)_{i}^{\\top}\\mathbf{x}_{i}^{\\top}\\Big\\{\\Phi^{(d)}\\Big\\}^{N_{i}}p_{\\sigma_{i}}(\\mathbf{x}_{i}^{\\top}\\cdots\\mathbf{x}_{i}^{N_{I}}x_{i}^{N_{I}+1}\\cdots x_{i}^{d})}\\\\\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "469 In the general case, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{t}({\\pmb x}_{t})=[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}}[e^{-\\bar{\\sigma}(t)}]^{N_{2}}p_{0}({\\pmb x}_{t}^{\\mathrm{UM}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "470 which shows that the likelihood of noisy data $\\textbf{\\em x}$ at time $t$ equals the likelihood of unmasked part of $\\textbf{\\em x}$   \n471 at time 0 multiplied by a analytic time-dependent term. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "472 B Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "473 Theorem 1. (Analytic concrete score in absorbing case, proof in Appendix $B$ ) For $\\mathbf{\\mathcal{x}}_{t}=$   \n474 $\\boldsymbol x_{t}^{1}\\ldots\\boldsymbol x_{t}^{i}\\ldots\\boldsymbol x_{t}^{d}$ and $\\mathbf{\\dot{x}}_{t}\\,=\\,x_{t}^{1}\\,.\\,.\\,.\\,\\widehat{x}_{t}^{i}\\,.\\,.\\,.\\,x_{t}^{d}$ , if $\\boldsymbol{x}_{t}^{i}=[M]$ and $\\hat{x}_{t}^{i}\\neq[M]$ , the concrete score at time   \n475 $t$ can be expressed as a time-in d ependent conditional distribution at time zero multiplied by an   \n476 analytic time-dependent term: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{p_{t}\\left(x_{t}^{1}\\cdot\\cdot\\cdot\\widehat{x}_{t}^{i}\\cdot\\cdot\\cdot x_{t}^{d}\\right)}{p_{t}\\left(x_{t}^{1}\\cdot\\cdot\\cdot x_{t}^{i}\\cdot\\cdot\\cdot x_{t}^{d}\\right)}=\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}p_{0}(\\widehat{x}_{t}^{i}|x_{t}^{U M}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "477 where xtUMis the vector consists of all unmasked tokens of xt. ", "page_idx": 13}, {"type": "text", "text": "478 Proof. According to Proposition 1, if $\\boldsymbol x_{t}^{i}=[\\mathbf M]$ and $\\hat{x}_{t}^{i}\\neq[\\mathbf{M}]$ , x\u02c6tUM= (xtUM, x\u02c6it), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p_{t}(\\hat{\\pmb{x}}_{t})}{p_{t}(\\pmb{x}_{t})}\\,{=}\\frac{[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}-1}[e^{-\\bar{\\sigma}(t)}]^{N_{2}+1}p_{0}(\\hat{\\pmb{x}}_{t}^{\\mathrm{UM}})}{[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}}[e^{-\\bar{\\sigma}(t)}]^{N_{2}}p_{0}(\\pmb{x}_{t}^{\\mathrm{UM}})}}\\\\ &{\\quad\\quad\\quad\\ \\ {=}\\frac{[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}-1}[e^{-\\bar{\\sigma}(t)}]^{N_{2}+1}p_{0}(\\pmb{x}_{t}^{\\mathrm{UM}},\\hat{x}_{t}^{i})}{[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}}[e^{-\\bar{\\sigma}(t)}]^{N_{2}}p_{0}(\\pmb{x}_{t}^{\\mathrm{UM}})}}\\\\ &{\\quad\\quad\\quad\\ \\ {=}\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}p_{0}(\\hat{x}_{t}^{i}|\\pmb{x}_{t}^{\\mathrm{UM}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "479 ", "page_idx": 13}, {"type": "text", "text": "480 C Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "481 C.1 Denoising cross-entropy loss by $\\lambda$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "482 According to definition of $Q_{t}$ we can simplify Eq. (3.8) as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{XCE}}^{\\mathrm{ov}}(x_{0})}\\\\ &{=\\displaystyle\\int_{0}^{\\infty}\\mathbb{E}_{\\tilde{x}\\sim p_{t\\mid0}(\\tilde{x}|,0)}\\left[\\sum_{\\tilde{x}^{i}=[\\mathbf{M}],j\\neq[\\mathbf{M}]}\\sigma(t)\\left(-\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}I(x_{0}^{i}=j)\\log\\left(\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}c_{\\theta}(\\tilde{x})[i,j]\\right)\\right)\\right]}\\\\ &{=\\displaystyle\\int_{0}^{\\infty}\\mathbb{E}_{\\tilde{x}\\sim p_{t\\mid0}(\\tilde{x}|,0)}\\left[\\sum_{\\tilde{x}^{i}=[\\mathbf{M}]}\\sigma(t)\\left(-\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}\\log\\left(\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}c_{\\theta}(\\tilde{x})[i,x_{0}^{i}]\\right)\\right)\\right]d t}\\\\ &{=\\displaystyle\\int_{0}^{\\infty}\\mathbb{E}_{\\tilde{x}\\sim p_{t\\mid0}(\\tilde{x}|,0)}\\left[\\sum_{\\tilde{x}^{i}=[\\mathbf{M}]}\\sigma(t)\\left(-\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}\\log\\left(\\frac{e^{-\\tilde{\\sigma}(t)}}{1-e^{-\\tilde{\\sigma}(t)}}q_{\\theta}(x_{0}^{i}|\\tilde{x}^{\\mathrm{UM}})\\right)\\right)\\right]d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "483 Define $\\lambda(t)\\;=\\;1\\,-\\,e^{-\\bar{\\sigma}(t)}$ , $d\\lambda\\;=\\;\\sigma(t)e^{-\\bar{\\sigma}(t)}d t$ . As $\\begin{array}{r}{\\bar{\\boldsymbol{\\sigma}}(t)\\;=\\;\\int_{0}^{t}\\boldsymbol{\\sigma}(\\tau)d\\tau}\\end{array}$ , we have $\\lambda(0)~=~0$ ,   \n484 $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\lambda(t)\\;=\\;1}\\end{array}$ . By a change of variables for the integration variable from $t$ to $\\lambda$ , we can   \n485 rewrite the above equation as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\bar{\\alpha}\\sim p_{\\lambda}(\\bar{\\alpha}|\\mathbf{x}_{0})}\\left[\\sum_{\\bar{x}^{i}=[\\mathbf{M}]}\\left(-\\log(\\frac{1-\\lambda}{\\lambda}q_{\\theta}(x_{0}^{i}|\\tilde{\\mathbf{x}}^{\\perp\\mathbf{M}}))\\right)\\right]d\\lambda}\\\\ &{\\displaystyle=\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\bar{\\alpha}\\sim p_{\\lambda}(\\bar{\\alpha}|\\mathbf{x}_{0})}\\sum_{\\bar{x}^{i}=[\\mathbf{M}]}\\left(-\\log(q_{\\theta}(x_{0}^{i}|\\tilde{\\dot{\\mathbf{x}}}^{\\perp\\mathbf{M}}))\\right)d\\lambda+\\displaystyle\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\bar{\\alpha}\\sim p_{\\lambda}(\\bar{\\alpha}|\\mathbf{x}_{0})}\\sum_{\\bar{x}^{i}=[\\mathbf{M}]}\\left(-\\log(\\frac{1-\\lambda}{\\lambda})\\right)d\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "486 By independence of forward process and Lemma 1, $\\begin{array}{r}{p_{t|0}(\\tilde{\\pmb{x}}|\\pmb{x}_{0})=\\prod_{i=1}^{d}p_{t|0}(\\tilde{x}^{i}|x_{0}^{i})}\\end{array}$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t|0}(\\tilde{x}^{i}|x_{0}^{i})=\\left\\{\\!\\!\\begin{array}{l l}{1-e^{-\\bar{\\sigma}(t)}}&{\\tilde{x}^{i}=[\\mathbf{M}],}\\\\ {e^{-\\bar{\\sigma}(t)}}&{\\tilde{x}^{i}=x_{0}^{i},}\\\\ {0}&{\\mathrm{else}.}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "487 Therefore, $\\begin{array}{r}{p_{\\lambda}(\\tilde{\\pmb{x}}|\\pmb{x}_{0})=\\prod_{i=1}^{d}p_{\\lambda}(\\tilde{\\boldsymbol{x}}^{i}|\\boldsymbol{x}_{0}^{i})}\\end{array}$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\lambda}(\\tilde{x}^{i}|x_{0}^{i})=\\left\\{\\!\\!\\begin{array}{l l}{{\\lambda}}&{{\\tilde{x}^{i}=[\\![\\mathbf{M}]\\!],}}\\\\ {{1-\\lambda}}&{{\\tilde{x}^{i}=x_{0}^{i},}}\\\\ {{0}}&{{\\mathrm{else.}}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "488 Consider the second term, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\hat{x}\\sim p_{\\lambda}(\\bar{x}|x_{0})}\\left[\\displaystyle\\sum_{\\bar{x}:=|\\mathbf{M}|}\\left(-\\log(\\frac{1-\\lambda}{\\lambda})\\right)\\right]d\\lambda}\\\\ &{=\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\hat{x}\\sim p_{\\lambda}(\\bar{x}|x_{0})}\\left[\\displaystyle\\sum_{i=1}^{d}\\mathbb{I}(\\bar{x}^{i}=[\\mathbf{M}])\\left(-\\log(\\frac{1-\\lambda}{\\lambda})\\right)\\right]d\\lambda}\\\\ &{=\\displaystyle\\int_{0}^{1}\\frac{1}{\\lambda}\\left[\\displaystyle\\sum_{i=1}^{d}p_{\\lambda}(\\bar{x}^{i}=[\\mathbf{M}]|x_{0})\\left(-\\log(\\frac{1-\\lambda}{\\lambda})\\right)\\right]d\\lambda}\\\\ &{=d\\displaystyle\\int_{0}^{1}-\\log(\\frac{1-\\lambda}{\\lambda})d\\lambda}\\\\ &{=d(\\lambda\\log\\lambda+(1-\\lambda)\\log(1-\\lambda))\\left|\\mathbf{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\lambda\\to0}\\lambda\\log\\lambda=\\operatorname*{lim}_{\\lambda\\to1}(1-\\lambda)\\log(1-\\lambda)=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "489 therefore, $(\\lambda\\log\\lambda+(1-\\lambda)\\log(1-\\lambda))|_{0}^{1}=0.$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DCE}}^{\\infty}(x_{0})=\\int_{0}^{1}\\frac{1}{\\lambda}\\mathbb{E}_{\\tilde{x}\\sim p_{\\lambda}(\\tilde{x}|x_{0})}\\left[\\sum_{\\tilde{x}^{i}=[\\mathbf{M}]}\\left(-\\log(q_{\\theta}(x_{0}^{i}|\\tilde{\\boldsymbol{x}}^{\\mathrm{UM}}))\\right)\\right]d\\lambda.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "490 C.2 Denoising cross-entropy loss by $k$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "491 By Eq. (C.3), we can express the loss in terms of $\\lambda$ . Given $x_{0}$ , we denote $\\begin{array}{r l}{\\tilde{\\mathcal{X}}}&{{}:=}\\end{array}$   \n492 $\\{x_{0}^{1},[\\mathbf{M}]\\}\\ \\times\\ \\cdot\\cdot\\{x_{0}^{d},[\\mathbf{M}]\\}$ as the sample space of $\\tilde{\\pmb{x}}$ , and define $\\tilde{\\mathcal{X}}_{k}\\ :=\\ \\{\\tilde{{\\pmb{x}}}\\ :\\ \\tilde{{\\pmb{x}}}\\ \\in\\ \\tilde{\\mathcal{X}}\\ \\wedge$   \n493 $\\tilde{\\pmb{x}}$ has exact k dimensions with values $\\left[\\mathbf{M}\\right]\\}$ . Obviously, $|\\tilde{\\mathcal{X}}|=2^{d}$ and $|\\tilde{\\mathcal{X}}_{k}|=C_{d}^{k}$ . We have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{0}^{1}\\frac{1}{\\lambda}\\frac{\\log{a}_{2}}{\\mu\\lambda}\\log{\\left(\\sum_{s=0}^{N-1}e^{-\\lambda\\log{\\left(\\phi_{1}(\\mu)|\\lambda^{(B)}\\right)}}\\right)}\\,\\Delta x}\\\\ &{=\\displaystyle\\int_{0}^{1}\\frac{1}{\\lambda}\\sum_{\\ell=K^{2}\\ell}\\phi(k){\\log{\\left[\\sum_{s=0}^{N-1}e^{-\\lambda\\phi_{\\ell}}(-1\\phi_{\\ell}(\\mu)|\\lambda^{(B)})\\right]}}\\,\\Delta x}\\\\ &{=\\displaystyle\\int_{0}^{1}\\frac{1}{\\lambda}\\sum_{\\ell=K^{2}\\ell\\neq s}^{\\ell}\\lambda^{(1-\\lambda)^{s+1}}\\left[\\sum_{s=0}^{N-1}\\left(-\\log(\\phi_{1}(\\mu)|\\lambda^{(B)})\\right)\\right]\\,\\Delta x}\\\\ &{=\\displaystyle\\int_{0}^{1}\\sum_{\\ell=K^{2}\\ell\\neq s}^{\\ell}\\lambda^{(1-\\lambda)^{s+1}}(-\\lambda)^{s+1}\\left[\\sum_{s=0}^{N-1}\\left(-\\log(\\phi_{1}(\\mu)|\\lambda^{(B)})\\right)\\right]\\,\\Delta x}\\\\ &{=\\displaystyle\\int_{0}^{1}\\sum_{\\ell=K^{2}\\ell}\\sum_{s=1}^{N-1}(-\\lambda)^{s+1}\\alpha\\sum_{s=0}^{N-1}\\left[\\sum_{s=1}^{N}\\sum_{\\ell=K^{2}\\ell}\\left(-\\log(\\phi_{1}(\\mu)|\\lambda^{(B)})\\right)\\right]\\,\\Delta x}\\\\ &{=\\displaystyle\\sum_{s=1}^{N}\\sum_{\\ell=K^{2}\\ell}\\phi_{1}^{\\lambda-\\lambda\\ell}(-\\lambda)^{s+1}\\alpha\\sum_{s=0}^{N-1}\\sum_{\\ell=K^{2}\\ell}\\left[\\sum_{s=0}^{N-1}(-\\log(\\phi_{1}(\\mu)|\\lambda^{(B)}))\\right]}\\\\ &{=\\displaystyle\\sum_{s=1}^{N}\\frac{(k-1)^{s}(\\lambda^{(1-\\lambda)^{s}\\ell}-\\lambda^{(1-\\lambda)^{s}\\ell})}{\\mu\\lambda}\\sum_{\\ell=K^{2}\\ell}\\left[\\sum_{s=0}^{N}\\sum_{\\ell=K^{2}\\ell}\\left(-\\log(\\phi_{ \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "494 This can be reformulated in the form of expectation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{d}\\frac{1}{k C_{d}^{k}}\\sum_{\\tilde{\\alpha}\\in\\tilde{\\mathcal{X}}_{k}}\\left[\\sum_{\\tilde{x}^{i}=[\\mathsf{M}]}\\left(-\\log(q_{\\theta}(x_{0}^{i}|\\tilde{\\boldsymbol{x}}^{\\mathrm{UM}}))\\right)\\right]}\\\\ &{\\displaystyle=\\sum_{k=1}^{d}\\frac{1}{k}\\mathbb{E}_{\\tilde{x}\\sim U(\\tilde{x}_{k})}\\left[\\sum_{\\tilde{x}^{i}=[\\mathsf{M}]}\\left(-\\log(q_{\\theta}(x_{0}^{i}|\\tilde{\\boldsymbol{x}}^{\\mathrm{UM}}))\\right)\\right]}\\\\ &{\\displaystyle=d\\mathbb{E}_{k\\sim U(\\{1,\\cdots,d\\})}\\frac{1}{k}\\mathbb{E}_{\\tilde{x}\\sim U(\\tilde{x}_{k})}\\left[\\sum_{\\tilde{x}^{i}=[\\mathsf{M}]}\\left(-\\log(q_{\\theta}(x_{0}^{i}|\\tilde{\\boldsymbol{x}}^{\\mathrm{UM}}))\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "495 C.3 Exact negative likelihood ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "496 Let $S_{d}$ represent the set of all permutations of the integers $1,\\cdot\\cdot\\cdot,d_{!}$ , and let $\\pi\\in S_{d}$ be one of these   \n497 permutations. Then, we can express $\\log q_{\\theta}(x_{0})$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log q_{\\theta}(x_{0})=\\mathop{\\mathbb{E}_{\\pi\\sim\\mathcal{V}(x_{0})}}\\log{\\varphi_{0}(x_{0})}}\\\\ {=}&{\\mathop{\\mathbb{E}_{\\pi\\sim\\mathcal{V}(x_{0})}}\\displaystyle\\sum_{i=1}^{d}\\log{\\varphi_{0}(x_{0}^{\\pi}|)}\\underline{{\\pi}}_{0}^{\\pi(c<i)})}\\\\ {=}&{\\displaystyle\\sum_{i=1}^{d}\\mathop{\\mathbb{E}_{\\pi\\sim\\mathcal{V}(x_{0})}}\\log{\\varphi_{0}(x_{0}^{\\pi}|)}\\underline{{\\pi}}_{0}^{\\pi(c<i)})}\\\\ {=}&{\\displaystyle\\sum_{i=1}^{d}\\frac{1}{d-i-1}\\mathop{\\mathbb{E}_{\\pi\\sim\\mathcal{V}(x_{0})}}\\sum_{r=i}^{d}\\log{\\varphi_{0}(x_{0}^{\\pi})}|x_{0}^{\\pi(c<i)})}\\\\ {=}&{\\displaystyle\\sum_{k=1}^{d}\\frac{1}{k}\\mathop{\\mathbb{E}_{\\pi\\sim\\mathcal{V}(x_{0})}}\\displaystyle\\sum_{r=i-k+1}^{d}\\log{\\varphi_{0}(x_{0}^{\\pi})}|x_{0}^{\\pi(c<i-k+1)})}\\\\ {=}&{\\displaystyle\\sum_{k=1}^{d}\\frac{1}{k}\\mathop{\\mathbb{E}_{\\pi\\sim\\mathcal{V}(x_{0})}}\\sum_{r=i-k+1}^{d}\\log{\\varphi_{0}(x_{0}^{\\pi})}|x_{0}^{\\pi(c<i-k+1)})}\\\\ {=}&{d\\mathbb{E}_{k\\sim\\mathcal{V}(\\{1,\\cdots,d\\})}\\displaystyle\\sum_{k=1}^{d}\\underline{{\\pi}}_{s^{\\pi}\\sim\\mathcal{V}(x_{0})}\\displaystyle\\sum_{r=i-k+1}^{d}\\log{\\varphi_{0}(x_{0}^{\\pi})}|x_{0}^{\\pi(c<i-k+1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "498 In this context, for a fixed $k$ , the condition x0\u03c0(<d\u2212k+1)can be understood as the unmasked part of   \n499 noisy data $\\tilde{\\pmb{x}}^{\\mathrm{UM}}$ . For $r=d-k+1,\\cdot\\cdot\\cdot\\,,d,\\,x_{0}^{\\pi(r)}$ corresponds to the $k$ items of the masked part.   \n500 Therefore, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi\\sim U(S_{d})}\\sum_{r=d-k+1}^{d}\\log q_{\\theta}(x_{0}^{\\pi(r)}|x_{0}^{\\pi(<d-k+1)})=\\mathbb{E}_{\\tilde{x}\\sim U(\\tilde{x}_{k})}\\sum_{\\tilde{x}^{i}=\\left[\\!\\operatorname{M}\\right]}\\log q_{\\theta}(x_{0}^{i}|\\tilde{\\mathbf{x}}^{\\mathrm{UM}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "501 Thus, substituting back, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\log q_{\\theta}(\\pmb{x}_{0})=d\\mathbb{E}_{k\\sim U(\\{1,\\cdots,d\\})}\\frac{1}{k}\\mathbb{E}_{\\tilde{\\pmb{x}}\\sim U(\\tilde{x}_{k})}\\sum_{\\tilde{x}^{i}=[\\mathbf{M}]}-\\log q_{\\theta}(x_{0}^{i}|\\tilde{\\pmb{x}}^{\\mathrm{UM}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "502 which is exactly Eq. (C.13). ", "page_idx": 16}, {"type": "text", "text": "503 This concludes the proof of the exact negative likelihood, showing the equivalence between the   \n504 expected negative log-likelihood and the denoising cross-entropy formulation. ", "page_idx": 16}, {"type": "text", "text": "505 D Sampling methods of discrete diffusion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "506 D.1 Original form in discrete diffusion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "507 Euler discrete method According to the Eq. (2.7), take $t=s-\\Delta s$ and use the Euler method, we   \n508 can simulate the reverse process by iteratively taking small $\\Delta t$ Euler steps at time s, calculate the   \n509 reverse transition rate based on $\\pmb{s}_{\\theta}(\\pmb{x}_{s},s)$ , and randomly sampling $x_{s-\\Delta s}$ . ", "page_idx": 16}, {"type": "text", "text": "510 Left term: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d}{d t}P_{s\\rightarrow t}_{_{|t=s-\\Delta s}}\\approx\\frac{P_{s\\rightarrow s-\\Delta s}-P_{s\\rightarrow s}}{\\Delta s}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "511 Right term: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{s\\rightarrow t}\\tilde{Q}_{t|t=s-\\Delta s}\\approx P_{s\\rightarrow s}\\tilde{Q}_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "512 As $P_{s\\rightarrow s}=I$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{s\\rightarrow s-\\Delta s}\\approx I+\\tilde{Q}_{s}\\Delta s.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "513 Rewrite in $t$ and consider a specific input $x_{t}$ , $x_{t-\\Delta t}$ is sampled from the following transition   \n514 probabilities: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{t-\\Delta t|t}(x_{t-\\Delta t}|x_{t})\\approx\\delta_{x_{t}x_{t-\\Delta t}}+\\tilde{Q}_{t}(x_{t},x_{t-\\Delta t})\\Delta t+O(\\Delta t)}\\\\ &{\\qquad\\qquad\\qquad\\approx\\delta_{x_{t}x_{t-\\Delta t}}+\\tilde{Q}_{t}(x_{t},x_{t-\\Delta t})\\Delta t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "515 where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{Q}_{t}(x_{t},x_{t-\\Delta t})\\approx\\left\\{\\!\\!\\begin{array}{l l}{Q_{t}(x_{t-\\Delta t},x_{t})s_{\\theta}(x_{t},t)_{x_{t-\\Delta t}}}&{x_{t}\\neq x_{t-\\Delta t},}\\\\ {-\\sum_{k\\neq x_{t}}\\tilde{Q}_{t}(x_{t},k)}&{x_{t}=x_{t-\\Delta t}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "516 Tweedie $\\tau$ -leaping If we know the analytic form of $P_{s\\rightarrow t}$ , it is possible to get the closed form   \n517 of reverse probability $P_{t\\to s}$ for any $s<t$ . According to the conditional decomposition of total   \n518 probability, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{diag}\\left(P_{t}^{T}\\right)P_{t\\rightarrow s}=\\left(\\operatorname{diag}\\left(P_{s}^{T}\\right)P_{s\\rightarrow t}\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "519 As $P_{s}^{T}P_{s\\rightarrow t}=P_{t}^{T}$ , the following equation holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{t\\rightarrow s}=\\operatorname{diag}\\left(P_{t}^{T}\\right)^{-1}P_{s\\rightarrow t}^{T}\\mathrm{diag}\\left(P_{s}^{T}\\right)=\\operatorname{diag}\\left(P_{t}^{T}\\right)^{-1}P_{s\\rightarrow t}^{T}\\mathrm{diag}\\left(P_{t}^{T}P_{s\\rightarrow t}^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "520 Given $x_{t}$ , to get $p_{s|t}(x_{s}|x_{t})$ , we only need to calculate row $x_{t}$ of $P_{t\\rightarrow s}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{t\\rightarrow s}(x_{t},\\cdot)=\\cfrac{1}{p_{t}(x_{t})}P_{s\\rightarrow t}^{T}(x_{t},\\cdot)\\circledast(P_{t}^{T}P_{s\\rightarrow t}^{-1})}\\\\ &{\\qquad\\qquad=P_{s\\rightarrow t}^{T}(x_{t},\\cdot)\\circledast(\\cfrac{P_{t}^{T}}{p_{t}(x_{t})}P_{s\\rightarrow t}^{-1})\\approx P_{s\\rightarrow t}^{T}(x_{t},\\cdot)\\circledast(s_{\\theta}(x_{t},t)^{T}P_{s\\rightarrow t}^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "521 D.2 Simplified form in reparameterized absorbing discrete diffusion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "522 Euler discrete method For $\\begin{array}{r l r}{x_{t}}&{{}=}&{[\\mathbf{M}]}\\end{array}$ , given the value of $\\hat{x}_{t}$ , use the $\\begin{array}{r l}{Q_{t}(\\hat{x}_{t},x_{t})}&{{}=}\\end{array}$   \n523 $\\sigma(t)Q^{\\mathrm{absorb}}(\\hat{x}_{t},x_{t})$ and $\\begin{array}{r}{\\pmb{s}_{\\theta}(x_{t},t)_{\\hat{x}_{t}}=\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\pmb{c}_{\\theta}(x_{t})_{\\hat{x}_{t}}}\\end{array}$ . Eq. (D.5) can be simplified as: ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{t-\\Delta t|t}(\\hat{x}_{t}|[\\mathbf{M}])=\\left\\{\\begin{array}{l l}{\\sigma(t)\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\Delta t c_{\\theta}(x_{t})_{\\hat{x}_{t}}}&{\\mathrm{~if~}\\hat{x}_{t}\\neq[\\mathbf{M}],}\\\\ {1-\\sigma(t)\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\Delta t}&{\\mathrm{~if~}\\hat{x}_{t}=[\\mathbf{M}].}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "524 For multi-dimension cases, similar results can be obtained: ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{t-\\Delta t|t}(x_{t-\\Delta t}^{i}|x_{t})=\\left\\{\\!\\!\\begin{array}{l l}{\\sigma(t)\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\Delta t c_{\\theta}(x_{t})[i,x_{t-\\Delta t}^{i}]}&{\\mathrm{if~}x_{t-\\Delta t}^{i}\\neq[\\mathbf{M}],}\\\\ {1-\\sigma(t)\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\Delta t}&{\\mathrm{if~}x_{t-\\Delta t}^{i}=[\\mathbf{M}].}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "525 for all $\\boldsymbol x_{t}^{i}=[\\mathbf M]$ . ", "page_idx": 17}, {"type": "text", "text": "526 Tweedie $\\tau$ -leaping Suppose $\\pmb{x}_{t}~=~x_{t}^{1}\\cdot\\cdot\\cdot x_{t}^{d}$ has $N_{1}$ components as $[\\mathbf{M}]$ and $N_{2}~=~d\\,-\\,N_{1}$   \n527 components as specific values. Without loss of generality, let\u2019s assume that the preceding $N_{1}$ terms   \n528 of $\\pmb{x}_{t}$ are all $\\mathbf{[M]}$ , and the remaining $N_{2}$ terms are fixed at specific values. For $1\\leq i\\leq d$ , given the   \n529 value of $x_{t-\\Delta t}^{i}\\neq[\\mathbf{M}]$ , : ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{t-\\Delta t|t}(x_{t-\\Delta t}^{i}|\\pmb{x}_{t})=\\frac{p_{t,t-\\Delta t}(\\pmb{x}_{t},x_{t-\\Delta t}^{i})}{p_{t}(\\pmb{x}_{t})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "530 By Proposition 1: ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{t}(\\pmb{x}_{t})=[1-e^{-\\bar{\\sigma}(t)}]^{N_{1}}[e^{-\\bar{\\sigma}(t)}]^{N_{2}}p_{0}(x_{t}^{N_{1}+1}\\cdots x_{t}^{d}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "531 Similar to the proof in Proposition 1: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{p_{\\mu(x,\\cdot,\\Delta)}(x,x,\\cdot,\\Delta)}\\\\ &{=\\displaystyle\\sum_{s\\in\\mathcal{N}_{D}}p_{\\mu(t-\\Delta)}|\\partial(x,x,\\cdot,x_{-\\Delta}^{i}|\\cdot|x_{0})|_{\\mathcal{P}_{0}}(x_{0})}\\\\ &{=\\displaystyle\\sum_{s\\in\\mathcal{N}_{D}}p_{\\mu(t-\\Delta)}|\\partial(x,\\cdot,x_{0})|\\partial(\\mathbf{N})|^{s},}\\\\ &{=\\displaystyle\\sum_{s\\in\\mathcal{N}_{D}}p_{\\mu(t-\\Delta)}|\\partial(\\mathbf{N})|^{s},[\\mathbf{M}]_{\\mathcal{N}}^{\\nu,\\lambda_{1}+1}\\cdots\\mathbf{\\mathcal{M}}_{\\mathcal{N}}^{\\nu,\\lambda_{\\mu}}\\mathbf{\\mathcal{M}}_{\\mathcal{N}}^{\\nu,1}\\cdots\\mathbf{\\mathcal{M}}_{\\mathcal{P}}^{\\nu,\\lambda_{\\mu}}|\\partial(x_{0}^{t})\\cdots\\mathbf{\\mathcal{M}}_{\\mathcal{N}}^{\\nu,\\lambda_{\\mu}}|\\partial(x_{0}^{t})}\\\\ &{\\overset{a\\mathrm{dil}}{=}\\displaystyle\\sum_{s\\in\\mathcal{N}_{D}}p_{\\mu(t-\\Delta)}|\\partial(\\mathbf{M})|^{s},}\\\\ &{=\\displaystyle\\sum_{s\\in\\mathcal{N}_{D}}p_{\\mu(t-\\Delta)}|\\partial(\\mathbf{M})|^{s},\\frac{\\nu_{1}(t-s_{1}^{\\lambda}|\\cdot|x_{0})}{s\\in\\mathcal{N}_{D}^{\\lambda}}|\\nabla\\widehat{\\mathbf{I}}_{1}\\succeq\\mathbf{\\mathcal{W}}_{\\mathcal{N}}^{\\nu,\\lambda_{0}}|\\partial(\\mathbf{M})|^{s},~~\\displaystyle\\prod_{b=\\mathcal{N}_{D}+1+1}^{d}p_{\\mu(t)}|\\partial_{\\nu}^{\\lambda_{1}}|\\cdots\\mathbf{\\mathcal{A}}_{\\mathcal{N}}^{\\nu,d_{\\mu}}|}\\\\ &{=\\displaystyle\\sum_{s\\in\\mathcal{N}_{D}}p_{\\mu(t-\\Delta)}|\\partial(\\mathbf{M})|^{s},}\\\\ &{\\overset{p}{\\mu(s,t,s_{1}^{\\lambda},\\ldots,x_{1}^{i})}\\cdots\\partial(\\mathbf{\\mathcal{N}})|\\partial(\\mathbf{M})|^{s},}\\\\ &{p_{\\mu}(x_ \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "532 Note we used the fact that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{(t,t-\\Delta t)|0}([\\mathbf{M}],x_{t-\\Delta t}^{i}|x_{t-\\Delta t}^{i})=p_{t|t-\\Delta t}([\\mathbf{M}]|x_{t-\\Delta t}^{i})p_{t-\\Delta t|0}(x_{t-\\Delta t}^{i}|x_{t-\\Delta t}^{i})}\\\\ &{\\phantom{=}=\\bigl(1-e^{-(\\bar{\\sigma}(t)-\\bar{\\sigma}(t-\\Delta t))}\\bigr)e^{-\\bar{\\sigma}(t-\\Delta t)}}\\\\ &{\\phantom{=}=e^{-\\bar{\\sigma}(t-\\Delta t)}-e^{-\\bar{\\sigma}(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "533 ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{t|0}([\\mathbf{M}]|x_{0}^{k})=1-e^{-\\bar{\\sigma}(t)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "534 by dividing the two expressions, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{t-\\Delta t|t}(x_{t-\\Delta t}^{i}|\\pmb{x}_{t})=\\frac{e^{-\\bar{\\sigma}(t-\\Delta t)}-e^{-\\bar{\\sigma}(t)}}{1-e^{\\bar{\\sigma}(t)}}p_{0}(x_{t-\\Delta t}^{i}|x_{t}^{N_{1}+1}\\cdot\\cdot\\cdot x_{t}^{d})}\\\\ &{\\qquad\\qquad\\approx\\frac{e^{-\\bar{\\sigma}(t-\\Delta t)}-e^{-\\bar{\\sigma}(t)}}{1-e^{\\bar{\\sigma}(t)}}c_{\\theta}(x_{t})[i,x_{t-\\Delta t}^{i}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "535 In general, for $\\boldsymbol x_{t}^{i}=[\\mathbf M]$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t-\\Delta t|t}(x_{t-\\Delta t}^{i}|x_{t})\\left\\{\\approx\\frac{e^{-\\bar{\\sigma}(t-\\Delta t)}-e^{-\\bar{\\sigma}(t)}}{1-e^{\\bar{\\sigma}(t)}}c_{\\theta}(x_{t})[i,x_{t-\\Delta t}^{i}],\\right.\\ \\left.x_{t-\\Delta t}^{i}\\neq[\\bf M],\\right.}\\\\ {=\\frac{1-e^{-\\bar{\\sigma}(t-\\Delta t)}}{1-e^{-\\bar{\\sigma}(t)}},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x_{t-\\Delta t}^{i}=[\\bf M].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "536 D.3 Discuss on the expectation of NFE ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "537 As discussed in Section Appendix D.2, for both the Euler method and Tweedie $\\tau$ -leaping, the   \n538 probability $p_{t-\\Delta t|t}^{i}([\\mathbf{M}]|x_{t})$ is only a factor of time which is independent of the other dimensions   \n539 of $\\pmb{x}_{t}$ once given $\\boldsymbol x_{t}^{i}=[\\mathbf M]$ . By the Law of Total Probability, it is easy to find that $p_{t-\\Delta t|t}^{i}([\\mathbf{M}]|[\\mathbf{M}])$   \n540 is also only a factor of time. Thus, given a specific sampling method and a set of time steps   \n541 $\\{t_{0}=0,\\cdot\\cdot\\cdot\\,,t_{n}=T\\}$ , the NFE can be treated as a random variable with a calculable expected value. ", "page_idx": 18}, {"type": "text", "text": "542 Let $N_{k}$ denote the number of dimensions of $\\textbf{\\em x}$ which changed in $[t_{k-1},t_{k})$ , so we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathrm{NFEs}}(n)=\\sum_{k=1}^{n}\\mathbb{I}(N_{k}\\neq0),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "543 ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{E-NFEs}(n)=\\sum_{k=1}^{n}\\mathbb{E}[\\mathbb{I}(N_{k}\\neq0)]=\\sum_{k=1}^{n}P(N_{k}\\neq0).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "544 For each dimension $i$ , let $r_{k}$ represent the probability that $x^{i}$ changes within the interval $[t_{k-1},t_{k})$ .   \n545 Consequently, $N_{k}$ follows a binomial distribution with parameters $l$ and $r_{k}$ , denoted as $N_{k}~\\sim$   \n546 Binomial $(l,r_{k})$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{E-NFEs}(n)=\\sum_{k=1}^{n}P(N_{k}\\neq0)=\\sum_{k=1}^{n}(1-(1-r_{k})^{l}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "547 By definition of $r_{k}$ and property of absorbing diffusion: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{k}=P(X_{t_{k-1}}^{i}\\neq[\\mathbf{M}],X_{t_{k}}^{i}=[\\mathbf{M}]|X_{t_{n}}^{i}=[\\mathbf{M}])}\\\\ &{\\quad=P(X_{t_{k-1}}^{i}\\neq[\\mathbf{M}]|X_{t_{k}}^{i}=[\\mathbf{M}])\\displaystyle\\prod_{l=k+1}^{n}P(X_{t_{l-1}}^{i}=[\\mathbf{M}]|X_{t_{l}}^{i}=[\\mathbf{M}])}\\\\ &{\\quad=(1-P(X_{t_{k-1}}^{i}=[\\mathbf{M}]|X_{t_{k}}^{i}=[\\mathbf{M}]))\\displaystyle\\prod_{l=k+1}^{n}P(X_{t_{l-1}}^{i}=[\\mathbf{M}]|X_{t_{l}}^{i}=[\\mathbf{M}]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "548 Eq. (D.23) can be determined given the sampling method and noise schedule. ", "page_idx": 19}, {"type": "text", "text": "549 For the Euler method, based on Equation Eq. (D.12), we can derive that: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP(X_{t_{l-1}}^{i}=[\\boldsymbol{\\mathbf{M}}]|X_{t_{l}}^{i}=[\\boldsymbol{\\mathbf{M}}])=1-\\sigma(t_{l})\\frac{e^{-\\bar{\\sigma}(t_{l})}}{1-e^{-\\bar{\\sigma}(t_{l})}}(t_{l}-t_{l-1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "550 Therefore, we can express $r_{k}$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nr_{k}=\\left(\\sigma(t_{k})\\frac{e^{-\\bar{\\sigma}(t_{k})}}{1-e^{-\\bar{\\sigma}(t_{k})}}(t_{k}-t_{k-1})\\right)\\prod_{l=k+1}^{n}(1-\\sigma(t_{l})\\frac{e^{-\\bar{\\sigma}(t_{l})}}{1-e^{-\\bar{\\sigma}(t_{l})}}(t_{l}-t_{l-1})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "551 For Tweedie $\\tau$ -leaping, By Eq. (D.17), similarly we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP(X_{t_{l-1}}^{i}=[\\mathbf{M}]|X_{t_{l}}^{i}=[\\mathbf{M}])=\\frac{1-e^{-\\bar{\\sigma}(t_{l-1})}}{1-e^{-\\bar{\\sigma}(t_{l})}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nr_{k}=(\\frac{e^{-\\bar{\\sigma}(t_{k-1})}-e^{-\\bar{\\sigma}(t_{k})}}{1-e^{-\\bar{\\sigma}(t_{k})}})\\prod_{l=k+1}^{n}(1-\\frac{1-e^{-\\bar{\\sigma}(t_{l-1})}}{1-e^{-\\bar{\\sigma}(t_{l})}})=\\frac{e^{-\\bar{\\sigma}(t_{k-1})}-e^{-\\bar{\\sigma}(t_{k})}}{1-e^{-\\bar{\\sigma}(t_{n})}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "552 Specifically, if we adopt a log-linear noise schedule, which implies $\\bar{\\sigma}(t)=-\\log(1-(1-\\epsilon)t)$ and   \n553 $\\begin{array}{r}{t_{k}=\\frac{k}{n}}\\end{array}$ , Equation Eq. (D.27) can be simplified to $\\scriptstyle{\\frac{1}{n}}$ . Substituting this result into Equation Eq. (D.20),   \n554 we obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{E-NFEs}(n)=\\sum_{k=1}^{n}(1-(1-{\\frac{1}{n}})^{l})=n(1-(1-{\\frac{1}{n}})^{l}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "555 E Algorithms for training and inference ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "556 F Experimental details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "557 F.1 Model details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "558 We implemented our RADD model based on SEDD architecture, which is an encoder-only transformer   \n559 model [46, 47] incorporating time conditioning [48] and using rotary positional encoding [49]. The   \n560 only difference is that we removed all parts related to time conditioning (i.e. TimeEmbedding,   \n561 adaLN-zero block [48]) and added a softmax operation at the end of the neural network to ensure the   \n562 output was a valid conditional distribution. Compared with SEDD small model, this modification led   \n563 to a reduction of 7M parameters, equating to an $8\\%$ decrease from the original 90M non-embedding   \n564 parameters. ", "page_idx": 19}, {"type": "text", "text": "Require: Network $c_{\\theta}$ , noise schedule $\\sigma$ (total noise $\\bar{\\sigma}$ ), time range $[0,T]$ , step size $\\Delta t$   \n1: $t\\gets T$ , $\\begin{array}{r}{\\pmb{x}_{T}\\leftarrow[\\mathbf{M}]\\dots[\\mathbf{M}],}\\end{array}$ , ${\\mathbf{}}c_{c a c h e}\\gets{\\mathbf{}}c_{\\theta}({\\mathbf{}}x_{t})$ d\u00d7 [ M] ", "page_idx": 20}, {"type": "text", "text": "2: while $t>0$ do   \n3: if Use Euler then   \n4: Construct transition densities $p(x_{t-\\Delta t}^{i}|\\pmb{x}_{t})$ by Eq. (D.12) use ccache   \n5: end if   \n6: if Use Tweedie $\\tau$ -leaping then   \n7: Construct transition densities $p(x_{t-\\Delta t}^{i}|\\pmb{x}_{t})$ by Eq. (D.17) use ccache   \n8: end if   \n9: $x_{t-\\Delta t}^{i}\\sim\\mathrm{Cat}(p(x_{t-\\Delta t}^{i}|x_{t}))$ for all $x_{t}^{i}=[\\mathbf{M}],x_{t-\\Delta t}^{i}\\leftarrow x_{t}^{i}$ for all $x_{t}^{i}\\neq[\\mathbf{M}]$   \n10: if $\\mathbf{\\Delta}\\mathbf{x}_{t-\\Delta t}\\neq\\mathbf{x}_{t}$ then   \n11: ${\\mathbf{}}c_{c a c h e}\\gets{\\mathbf{}}c_{\\theta}\\big({\\mathbf{}}x_{t}\\big)$   \n12: end if   \n13: $t\\leftarrow t-\\Delta t$ ,   \n14: end while ", "page_idx": 20}, {"type": "text", "text": "Algorithm 2 Conditional Sampling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Require: Network $c_{\\theta}$ , noise schedule $\\sigma$ (total noise $\\bar{\\sigma}$ ), time range $[0,T]$ , step size $\\Delta t$ , Prompt   \nspaces $\\Omega$ and tokens $\\tau$ .   \n1: $t\\gets T$ , construct $x_{T}$ with $x_{T}^{\\Omega}=\\mathcal{T}$ and $\\mathbf{\\Delta}x_{T}^{\\bar{\\Omega}}=[\\mathbf{M}]$ , ${\\mathbf{}c}_{c a c h e}\\gets{\\mathbf{\\mathbf{\\&}}}_{\\theta}\\big({\\mathbf{}x}_{t}\\big)$   \n2: while $t>0$ do   \n3: if Use Euler then   \n4: Construct transition densities $p(x_{t-\\Delta t}^{i}|\\pmb{x}_{t})$ by Eq. (D.12) use ccache   \n5: end if   \n6: if Use Tweedie $\\tau$ -leaping then   \n7: Construct transition densities $p(x_{t-\\Delta t}^{i}|\\pmb{x}_{t})$ by Eq. (D.17) use ccache   \n8: end if   \n9: $x_{t-\\Delta t}^{i}\\sim\\mathrm{Cat}(p(x_{t-\\Delta t}^{i}|x_{t}))$ for all $\\boldsymbol x_{t}^{i}=[\\mathbf M]$ , $x_{t-\\Delta t}^{i}\\gets x_{t}^{i}$ for all $x_{t}^{i}\\neq[\\mathbf{M}]$   \n10: if $\\mathbf{\\Delta}\\mathbf{x}_{t-\\Delta t}\\neq\\mathbf{x}_{t}$ then   \n11: ${\\mathbf{}}c_{c a c h e}\\gets{\\mathbf{}}c_{\\theta}\\big({\\mathbf{}}x_{t}\\big)$   \n12: end if   \n13: $t\\leftarrow t-\\Delta t,$ ,   \n14: end while ", "page_idx": 20}, {"type": "text", "text": "Algorithm 3 Training ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Require: Network $c_{\\theta}$ , noise schedule $\\sigma$ (total noise $\\bar{\\sigma}$ ), time range $[0,T]$ , data distribution $p_{\\mathrm{data}}$   \n1: repeat   \n2: $x_{0}\\sim p_{\\mathrm{data}}$ , $t\\sim U([0,T])$ .   \n3: construct $\\pmb{x}_{t}$ by $Z^{i}\\sim B e r n o u l l i(e^{-\\bar{\\sigma}(t)})$ , $x_{t}^{i}=\\mathbb{I}(Z^{i}=1)x_{0}^{i}+\\mathbb{I}(Z^{i}=0)[\\mathbf{M}]$   \n4: Calculate $\\begin{array}{r}{L_{\\theta}(\\pmb{x}_{t},\\pmb{x}_{0})=\\sum_{x_{t}^{i}=[\\mathbf{M}]}-\\sigma(t)\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}\\log\\left(\\frac{e^{-\\bar{\\sigma}(t)}}{1-e^{-\\bar{\\sigma}(t)}}c_{\\theta}(\\pmb{x}_{t})[i,x_{0}^{i}]\\right)}\\end{array}$   \n5: Take gradient descent on $\\nabla_{\\theta}L(x_{t},x_{0})$   \n6: until converged ", "page_idx": 20}, {"type": "text", "text": "Table 3: Quality of unconditionally generated text evaluated by perplexity (\u2193). For a fixed model, the best perplexity is bolded. ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "7yqjVgWWxx/tmp/ecc1fc407fcd4374f59102f13c0d7caf25749a64860d5654e9bd7dadf2a03193.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "565 F.2 Training details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "566 Following the settings in [29], we trained our model with the following configuration: ", "page_idx": 21}, {"type": "text", "text": "567 \u2022 Batch Size:512   \n568 \u2022 Learning Rate: $3\\times10^{-4}$   \n569 \u2022 Exponential Moving Average (EMA):0.9999   \n570 \u2022 Gradient Clipping: Gradient norm clipped to 1   \n571 \u2022 Warmup Schedule: Applied for the first 2500 iterations   \n572 We utilized 16 V100 32G GPUs or 16 A100 40G GPUs for training. For the A100 40G GPUs, we   \n573 leveraged flash attention to accelerate the training process. For the V100 32G GPUs, which do not   \n574 support flash attention or bfloat16, we employed float16 precision and used the Memory-Efficient   \n575 Attention mechanism available in torch.nn.functional.scaled_dot_product_attention. Additionally, we   \n576 used gradient checkpointing technique to save memory. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "577 F.3 Unconditional generation details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "578 We used Tweedie $\\tau$ -leaping method, which has optimal results with fixed NFE. For SEDD small,   \n579 we directly used their result. For RADD small, we generated 1000 samples to get the average value   \n580 following [29]. ", "page_idx": 21}, {"type": "text", "text": "581 F.4 Further evaluation of generative perplexity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "582 As stated in Theorem 1, $c_{\\theta}$ can be interpreted as a conditional distribution over clean data. A natural   \n583 idea is to use it directly to generate samples, which is similar to auto-regressive models. However,   \n584 there are $d!$ kinds of decomposition from joint distribution to conditional distribution, in which we   \n585 only tested three representative cases: ", "page_idx": 21}, {"type": "text", "text": "586 ", "page_idx": 21}, {"type": "text", "text": "587 ", "page_idx": 21}, {"type": "text", "text": "588 ", "page_idx": 21}, {"type": "equation", "text": "$\\begin{array}{r}{\\pi\\sim U(S_{d}),\\,p(x^{1}\\cdot\\cdot\\cdot x^{d})=\\prod_{k=1}^{d}p(x^{\\pi(k)}|x^{\\pi(<k)})}\\end{array}$ ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "589 Results are shown in Table 3. The perplexity is calculated on average of 1024 samples. For the   \n590 random case, we calculate the average perplexity between different randomly generated $\\pi$ . Generally,   \n591 we find that the perplexity by directly sampling from the conditional distribution is higher than   \n592 that achieved by Tweedie $\\tau$ -leaping. Among the different decomposition orders, the random order   \n593 demonstrated the best performance. ", "page_idx": 21}, {"type": "text", "text": "594 G Additional experimental results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "595 G.1 Additional samples ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "596 In this section, we present the unconditionally and conditionally generated text of RADD-DSE in   \n597 Fig.3 and Fig.4, respectively. Similarly, the results of RADD-DCE are shown in Fig.?? and Fig.??,   \n598 respectively. ", "page_idx": 21}, {"type": "text", "text": "and human face. \u201cAnd pretty damn conventional mating, so didn\u2019t come to the table like that with me. (As a character), it would be a pretty solid case to have,\u201d Andra says. \u201cI saw the way he did it, and I think played a little bit with some of his fans. He wanted me to get cute a little bit too.\u201d   \nAdvertisement   \nAs in the parking lot, Andra was growing frustrated with the way the cars recently fit in nicely.   \nIt also makes me feel like some things haven\u2019t changed until around this period of time, in the future. It\u2019s definitely the future here now \u2014 and I\u2019m always dubious about thinking quite long before Toyota ever introduced a new car. \u201cI think something that perfectly well fits all the guys,\u201d Andra says. \u201cI therefore could fit more well.\u201d   \n\u2013 Follow Matt Dyckton on Twitter $@$ Mittington. $<$ |endoftext| $>$ Spanish star Christina Rene got away after a teenage girl told to leave India for an unfamiliar place.   \nThe Russian woman chose Chelsea to stay home, saying the alternative was to get rid of a condition and die after receiving a medical diagnosis and go back to learning as a nurse. Chelsea was sentenced to the first arrest for a serious mental health conviction in October, and was transferred to a man who had taken his place, a 16-year-old man. The bailee\u2019s Appeal Court had appealed to a court to hear the case she learned from the teenagers. The Russian Internal Revenue Service found the woman charged with administering an emergency ward, and although the girl was still waiting for a doctor there, she instead went to visit another clinic for the treatment of female swi-virus virus.   \nThe court had not ruled out an in-life medical professional. Chelsea never applied to be a pregnant mother; her formal application assumed she was pregnant, dating from the summer of 2014.   \nOut of sound doubts, when her co-boyfriend Chelsea received a proposal to stay abroad for summer work abroad. Chelsea then applied to win to work and have a place in Russia. The grant of nearly $\\mathbb{S}5000$ Chelsea invited Chelsea out to go see a Moscow clinic. It took two weeks to find a doctor. These sors\u2019 of Moscow\u2019s federal courts confiscated the grant. The 17-year-old did not need to go to the hospital where she says she has received everything she has had in China, of course. Chelsea\u2019s lawyers Andre James Irani gave the court the doctors he could plead without permission of the young teenager on a regular basis. The Argentine was put out on bail, prompting a sex psychologist to meet her when she signed her papers, but no family member was present.   \n\u201cFirst thing I wasn\u2019t going to ask three days, but I\u2019m thinking about this months already. \u201cI\u2019m glad to see that they are waiting for her with their services. She\u2019s already built a good life. She\u2019s interested in her studies. But feel like she is? I want to be the only person who has ever been my friend,\u201d she said.   \n\u201cI took a lot of act, but she is more than never.\u201d   \nTo this day Christina Rene McCourner told the court the situation is between Denmark and Stockholm syndrome. She says Chelsea has refused to come to term.   \n\u201cShe says that she is talented at medical school. But she\u2019s telling a different story, anyway. There\u2019s also a case when you just can\u2019t get the CNN-type cowardice yourself going to the doctor,\u201d she said.   \nA team of Barcelona has been conducting medical inquiries into both doctors who treat the sick and those not who use medical services. \u201cHow has she been following her visit to Russia United States and since arriving unable to do so?\u201d   \n\u201cEach often when she said \u201cAll the bills are what may I doctor,\u201d I\u2019ve usually never used my medicine again,\u201d Chelsea attorneys say.   \nLife in the wrong world   \nWhile far-so in the past two weeks though, nearly speaking only three pregnant women in Mexico and Europe have requested that Chelsea stop using medical services at them all. Though many of its applicants have receiving medical assistance in Qatar and other parts of the world, they believe it means they should go elsewhere, according to the health services agency.   \nChelsea\u2019s received from facility staff dealing with financial responsibility say it is those who are vulnerable have been out of money for healthcare their life, not who are suffering from a lack of access. \u201cYou might think after you tell that provider is not available, you should give the load to somebody in Russia or Sweden,\u201d she ", "page_idx": 22}, {"type": "text", "text": "Figure 3: Unconditionally generated text of RADD-SE. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Hi, my name is Shade-Rayhelynis-Neelsons. Interviewer: Two days ago, so let me speak to you in brief.Drake: Hi, are you studying for graduate school in late July, and somebody is interviewing you for you for his classes. My first personal quote is: so when they\u2019re doing class they\u2019re going to a shower, and they expect me to not be part of the shower any way. I have a take, I want to check when something\u2019s not right and make sure I spot it so that someone can get it happening. My hair is an important piece of me, and I can be the one complete human being that when I have a shower and say, \"Okay, I really like my hair, and this is the thing that I would like and I want to believe something, I should just have an attitude check.\" Do you think anyone else can have time with me? Do I say I don\u2019t?I mean I get to 7 right the time I go to class, I sit back there and I feel like no one knows what to do. So, in my case I am not anxious, I\u2019m just saying, \"I feel like I have my hair without letting go from day to day, I\u2019ve got to feel like I\u2019ve been like the thing they should all be excited by.\"So, the days follow me, you start looking at my pictures, and you realize how pretty you are. It was just so big this moment for that ofifce, because that was seeing positive things.I\u2019m starting to grow up and be beautiful too.I mean\u2013I mean it\u2019s kind of fun, now seeing beautiful girls, especially really in their 20s, come from unusual backgrounds. Back in the 50s I met people at one of the first places Woosz made mons, asked us to visit charity walks, and he would buy us a suit. And actually he really wanted to, so I know what the inspiration is. It\u2019s something that someone could have found out, and where they\u2019re from $-\\textbf{l}$ think it is like everyone is finding ways to relate.This is one of those things I remember the most about when I went to order the products!For example, it may be taken after the sporting event, and I don\u2019t know, but they asked us to choose color for our favorite parts in the group....they asked us to take their favorite color, then they made their eyes for each skin.The one that was the round head, like that one I chose so much, but it was really a really painful transition...and I don\u2019t know what touched my skin, so I don\u2019t know what I would do with it, I will tell you...that was not what I focused on, but I think I may have pretty much my original o-still darker hair, so I chose to go with the round head which I did really. Anyway, my job was to get the hair done, and I do most of my hair for school because I got married and had so many children while I was So. So, I\u2019m always thinking again not to be confused. I had just been doing niggly since I was a child, and there was no reason to do it like that. The career had got started, and therefore I was really going all over the world no less. So one of the things I did was taking of his shirts so I look at him all in one go, I surprise him.I\u2019ve got some of the most cool things going with those two of this. You see these all look alike, clean, and awesome\u2013and they used to have a shop in the....a shop like that, they never sells coats, so that was really helpful to those guys, I feel, guys going into shop with these guys, and those guys can have the right color, they can have it that right look.I love my hair, it\u2019s like when I was a year old when I see scent in your nursery, to get the grandest response, I keep using the smell. I bring it to its level, honestly, it makes the weight apply.rake: Probably my favorite word to explain, after you explain, it isn\u2019t hard\u2013in case that\u2019s your thing, I\u2019m already who kind of like going, and I\u2019m able to get access when I\u2019m trying. I\u2019ve been trying to finish coloring for people, for years, so I would like to work on it\u2019s own but the colors there to do color pattern are picked for another reason to put the final color of,f that\u2019s like the other one for giving glitter or outline.So I\u2019m just going to be looking side by side at coloring over and over again. It\u2019s my vision. Each color is my dream.As you know, a color is just simply something tucked into a dye and what makes perfect or tail end to a hair is...and that is why I always shampoo twice a day and shower three times a day. ", "page_idx": 23}, {"type": "text", "text": "Figure 4: Conditionally generated text of RADD-SE. Prompt tokens are highlighted in blue. ", "page_idx": 23}, {"type": "text", "text": "as well. However, she did not sign.   \nGov. Johnluaj said the Amal effectively took the case to the Supreme Court. \"I was wrong to say that they left the Constitution in place and this is basically unconstitutional,\u201d Jackson said. \u201cI don\u2019t think they\u2019re saying that. I\u2019m. That means on the one hand they\u2019re going to have to intervene, or on the side of the other, they\u2019ll have to intervene. I can\u2019t think of changing a constitutional decision with them.\"   \nWhen Gov. Barack Obama announced Tuesday \u2014 and a federal court is set to challenge the way residents of the states violated the law \u2014 Attorney General Eric Holder chose to hold his own hearing. The entire state had a deadline to field a recipient of the letter asking for comment.   \nBecause hearings were held before, Ohio and 17 states have each had such a case before. At last night\u2019s hearings, more than four separate arguments were heard by a 52-45 margin in order to pass the repeal bill by a vote of 63-2 49 to 45. The bill also included Obamacare legislation and was pushed through Congress after opposition from 24 states. Both brought in a new governor, popular Gov Sen. Phil Bryant, another Republican in the Senate. Neil LePage neared an attorney in both cases and refused to find a new insurance secretary. Both House Leader Mitch McConnell and Republicans said they would repeal the law entirely. The law would have been in the Oval Ofifce of the Logged since 2011.   \nFormer Judge Anthony Teague, the Ohio Chief Judge, found the overwhelming majority vote in favor of the legislation well in line and said it was a \"needed forward.\"   \n\u201cThe things citizenship issues should go from statutes states have to regulations,\u201d he said. \u201cThey\u2019ve got this idea that the courts are getting knocked to their own corner. And once they see it turn around them, sure as hell they\u2019ll have faith dogged by judges exercising constitutional rights.\u201d   \nAttorney Holder, the assistant secretary of state for policy at the Department of Justice, has discussed the idea that federal courts such as the U.S. Supreme Court should handling legal issues such as making tough immigration decisions for illegal immigrants.   \n\u201cThe thought process of helping illegal immigrants goes beyond the judicial process. What I understand. . . criminal immigration measures, gang activity, afifrmative action efforts, criminal status,\u201d Holder said. \u201cAnd things like that, we expect in the state to go to a long way. There clearly needs to be a criminal justice program on immigration and reform, and we need to recognize those efforts not to start.\u201d   \nHolder has also said it is a \u201creal issue\u201d for the constitution if the letter is signed of.f He said it was essentially a message, seeking to show the majority \u201cpower of respect and the power of institutions.\u201d   \nBut the attorney general said he hoped it would be a task to figure out how to start safeguarding each of their citizens\u2019 rights while restoring their constitutional trust. \u201cThe only thing I think we can really do is have the judges to understand the nature of the judiciary, how powerful it is to be involved and to interfere with the government with no accountability on what path they want to move down,\u201d he said.<|endoftext $>$ Ex76561 molds at the worst parts of the UK economy will tell in the future \u2013 most poorer areas of England have suffered more than any person during the first years of 2008, according to ex-Home Secretary Jeremy Foot.   \nA total of 23,000 people of disposable incomes who have five mortgages \u2013 more than 4,000 households \u2013 will be considered as home buyers, even though published figures will be different from September, say researchers   \nResidents of more than 3,000 homes will be the third most likely to die because of jobs which fell in average terms home ownership during 2008 until the end of this year. It\u2019s a surprising drop, according to the Wall Street Journal, which says the economy and the top 1   \nProfessor Jeremy Foot, Home Secretary and the Information Society, raised concerns about the collapse of the UK\u2019s housing market \u2013 down from 39   \nHe said: \"The measure of who \u2019invested\u2019 at that time, doesn\u2019t include the number of people or businesses with the assets. There were only three big cities, New York were the other three?   \n\"This year \u2013 it was announced that Royal Bank of Scotland would be the first to close in 20 years \u2013 it turned out then that the poorer areas, including by 2009 and 2010, were hammered hardest,\" Professor Foot said.   \nProfessor Foot\u2019s lecture, which was released today in ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "I have a ick of death \u2014 that\u2019s where I am. That\u2019s what I need. That need to take something else. But I don\u2019t have that in my family. And here\u2019s why you might let it go. You don\u2019t want to really say what I wrote in my story, open your mind and finish it with purposeful thought. But I can do it if I get cancer.\u201d ", "page_idx": 25}, {"type": "text", "text": "\u201cI can not help but accept that you brother-in-law was really on me and that that\u2019s not how I need to be,\u201d I said with my father\u2019s sad smile. He let it go a few days later, but it didn\u2019t prevent him from thinking about it. But I made sure he wouldn\u2019t let-in-law leave. ", "page_idx": 25}, {"type": "text", "text": "My mother saved my only father for the life. And he was a falsehood, but no doubt. He saved me. ", "page_idx": 25}, {"type": "text", "text": "\u201cMy mother thank god. But do you ask outside of me the questions, ask him. He\u2019s dealing with this and prepare for their perception of bad things he said. They should\u2019t forget him because the bad thoughts come with it.\u201d ", "page_idx": 25}, {"type": "text", "text": "I blackened my father and watched them walk back up to him in front of a good news line.   \nHe stood with us for two days; we stared on from spring to spring. ", "page_idx": 25}, {"type": "text", "text": "\u201cIt\u2019s OK.\u201d He said. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u201cI know what you want when you come here.\u201d ", "page_idx": 25}, {"type": "text", "text": "\u201cI\u2019ll accept that,\u201d he said. \u201cWhat I thought to look back was bad. I\u2019m going to let it go and make an exception in this case. I came here after sharing the story. I\u2019ve been looking to the world. And I don\u2019t care for anyone in this world, but I want to make it harder for other people to try to make this mistake.\u201d ", "page_idx": 25}, {"type": "text", "text": "He didn\u2019t justify what he meant. My mother drove us the way I trucked him back to his house, and he chuckled at me in simple words: ", "page_idx": 25}, {"type": "text", "text": "\u201cI said to him I need to still be here, but do you want me to have this for the whole day?\u201d \u201cThese are my expectations. If we smile and I\u2019m laughing thank you can I be so happy?\u201d ", "page_idx": 25}, {"type": "text", "text": "Outside of my family, there was joy, joy, sorrow. At youngest, most of all, my father also saw the world. He suffered from hunger, his family lost access to wheelchair and every room. They fought with him sometimes, and when they needed to rest, he stepped back. I can only imagine, deep down, his family grew enormously with him. ", "page_idx": 25}, {"type": "text", "text": "Our journey with my parents at times had been a combination of things. We were all in pairs, and my brothers were such. Repeatedly we asked them to tell me when we wouldn\u2019t last to get that end. And I started missing my trip \u2014 never visited neighbors before. And finally one day I lost my patience. I all wanted to wander down to my father\u2019s house in76561. On my own, I couldn\u2019t see my beautiful mother again. I had an ear tumor that was brainblown away and lost all the ways to make the most of the time again. I took the bed from the entire bear family along with his three children, and fixed him up on one side and led him home through the open of the front door with my honey brother. We were hearing all of the other things I had heard were outlandish stories. Finally, he remained so grateful for his admiration for my dad. ", "page_idx": 25}, {"type": "text", "text": "What was possible my father really didn\u2019t have? ", "page_idx": 25}, {"type": "text", "text": "On November 2, 2013, my mother still did not show any empathy for my father. The loyalty he had was the only expression he had. ", "page_idx": 25}, {"type": "text", "text": "On the night where he was arrested, other members of his family noticed that they couldn\u2019t bribe a co-worker to walk up from his job. That led to him thinking, \u201cI want you to not be a badexample in your family. You can\u2019t serve an individual who will make up his stupid demands for you in order to make a profit.\u201d For me, to fulfill the compassion of my own brother, I\u2019ve become an able example of that, that kids. ", "page_idx": 25}, {"type": "text", "text": "With this decision, it was freeing that I had allowed him to try to decide how to make a better life for his family. I had allowed the animals to have a day of.f This is something he and I can do. His son had thought they could, and it was relief he had been rearranging that reality. The time our family spends on the house was at seven times a day. ", "page_idx": 25}, {"type": "text", "text": "Figure 6: Conditionally generated text of RADD-CE. Prompt tokens are highlighted in blue. ", "page_idx": 25}, {"type": "text", "text": "599 H License ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "600 URL and license for existing assets we used are provided in Table 4. ", "page_idx": 25}, {"type": "table", "img_path": "7yqjVgWWxx/tmp/d8dafcc970a3af33f716d6b8701092ee3a108428ba95fe26592a31a88007e7e5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "601 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our paper\u2019s contributions and scope. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the limitations in the main text. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each leolc result, does the paper provide the full set of assumptions and   \na complete (and correct) proof?   \n2 Answer: [Yes]   \n3 Justification: For each theoretical result, the paper provides the full set of assumptions and a   \n54 complete (and correct) proof. Please see the Appendix for more details.   \n55 Guidelines:   \n6 \u2022 The answer NA means that the paper does not include theoretical results.   \n7 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n58 referenced.   \n59 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n0 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \nthey appear in the supplemental material, the authors are encouraged to provide a short   \n2 proof sketch to provide intuition.   \n63 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n64 by formal proofs provided in appendix or supplemental material.   \n5 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n6 4. Experimental Result Reproducibility   \n7 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n8 perimental results of the paper to the extent that it affects the main claims and/or conclusions ", "page_idx": 27}, {"type": "text", "text": "of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our paper fully discloses all the information needed to reproduce the main experiment. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 27}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 28}, {"type": "text", "text": "703 some way (e.g., to registered users), but it should be possible for other researchers   \n704 to have some path to reproducing or verifying the results.   \n705 5. Open access to data and code   \n706 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n707 tions to faithfully reproduce the main experimental results, as described in supplemental   \n708 material?   \n709 Answer: [Yes]   \n710 Justification: We provide open access to the data and code, with sufficient instructions to   \n711 faithfully reproduce the main experimental results, as described in supplemental materials.   \n712 Guidelines:   \n713 \u2022 The answer NA means that paper does not include experiments requiring code.   \n714 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n715 public/guides/CodeSubmissionPolicy) for more details.   \n716 \u2022 While we encourage the release of code and data, we understand that this might not be   \n717 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n718 including code, unless this is central to the contribution (e.g., for a new open-source   \n719 benchmark).   \n720 \u2022 The instructions should contain the exact command and environment needed to run to   \n721 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n722 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n723 \u2022 The authors should provide instructions on data access and preparation, including how   \n724 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n725 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n726 proposed method and baselines. If only a subset of experiments are reproducible, they   \n727 should state which ones are omitted from the script and why.   \n728 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n729 versions (if applicable).   \n730 \u2022 Providing as much information as possible in supplemental material (appended to the   \n731 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "32 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "33 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n34 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n35 results?   \n36 Answer: [Yes]   \n37 Justification: We specify all the training and test details necessary to understand the results   \n38 in Section 4.1   \n740 \u2022 The answer NA means that the paper does not include experiments.   \n741 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n742 that is necessary to appreciate the results and make sense of them.   \n743 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n744 material. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "745 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "749 Justification: Error bars are not reported because it would be too computationally expensive.   \n750 Guidelines:   \n751 \u2022 The answer NA means that the paper does not include experiments.   \n752 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n753 dence intervals, or statistical significance tests, at least for the experiments that support   \n754 the main claims of the paper.   \n755 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n756 example, train/test split, initialization, random drawing of some parameter, or overall   \n757 run with given experimental conditions).   \n758 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n759 call to a library function, bootstrap, etc.)   \n760 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n761 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n762 of the mean.   \n763 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n764 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n765 of Normality of errors is not verified.   \n766 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n767 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n768 error rates).   \n769 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n770 they were calculated and reference the corresponding figures or tables in the text.   \n771 8. Experiments Compute Resources   \n772 Question: For each experiment, does the paper provide sufficient information on the com  \n773 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n774 the experiments?   \n775 Answer: [Yes]   \n776 Justification: We provide sufficient information on the computer resources (type of compute   \n777 workers, memory, time of execution) needed to reproduce the experiments.   \n778 Guidelines:   \n779 \u2022 The answer NA means that the paper does not include experiments.   \n780 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n781 or cloud provider, including relevant memory and storage.   \n782 \u2022 The paper should provide the amount of compute required for each of the individual   \n783 experimental runs as well as estimate the total compute.   \n784 \u2022 The paper should disclose whether the full research project required more compute   \n785 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n786 didn\u2019t make it into the paper).   \n787 9. Code Of Ethics   \n788 Question: Does the research conducted in the paper conform, in every respect, with the   \n789 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n790 Answer: [Yes]   \n791 Justification: We conduct in the paper conform, in every respect, with the NeurIPS Code of   \n792 Ethics.   \n793 Guidelines:   \n794 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n795 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n796 deviation from the Code of Ethics.   \n797 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n798 eration due to laws or regulations in their jurisdiction).   \n799 10. Broader Impacts   \n800 Question: Does the paper discuss both potential positive societal impacts and negative   \n801 societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "802 Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "803 Justification: We discuss the potential positive societal impacts and negative societal impacts   \n804 in Section 6 of the main text.   \n805 Guidelines:   \n806 \u2022 The answer NA means that there is no societal impact of the work performed.   \n807 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n808 impact or why the paper does not address societal impact.   \n809 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n810 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n811 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n812 groups), privacy considerations, and security considerations.   \n813 \u2022 The conference expects that many papers will be foundational research and not tied   \n814 to particular applications, let alone deployments. However, if there is a direct path to   \n815 any negative applications, the authors should point it out. For example, it is legitimate   \n816 to point out that an improvement in the quality of generative models could be used to   \n817 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n818 that a generic algorithm for optimizing neural networks could enable people to train   \n819 models that generate Deepfakes faster.   \n820 \u2022 The authors should consider possible harms that could arise when the technology is   \n821 being used as intended and functioning correctly, harms that could arise when the   \n822 technology is being used as intended but gives incorrect results, and harms following   \n823 from (intentional or unintentional) misuse of the technology.   \n824 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n825 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n826 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n827 feedback over time, improving the efficiency and accessibility of ML).   \n828 11. Safeguards   \n829 Question: Does the paper describe safeguards that have been put in place for responsible   \n830 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n831 image generators, or scraped datasets)?   \n832 Answer: [NA]   \n833 Justification: This paper poses no such risks.   \n834 Guidelines:   \n835 \u2022 The answer NA means that the paper poses no such risks.   \n836 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n837 necessary safeguards to allow for controlled use of the model, for example by requiring   \n838 that users adhere to usage guidelines or restrictions to access the model or implementing   \n839 safety filters.   \n840 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n841 should describe how they avoided releasing unsafe images.   \n842 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n843 not require this, but we encourage authors to take this into account and make a best   \n844 faith effort.   \n845 12. Licenses for existing assets   \n846 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n847 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n848 properly respected?   \n849 Answer: [Yes]   \n850 Justification: URL and license for existing assets we used are provided in Appendix H.   \n851 Guidelines:   \n852 \u2022 The answer NA means that the paper does not use existing assets.   \n853 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n854 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n855 URL.   \n856 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n857 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n858 service of that source should be provided.   \n859 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n860 package should be provided. For popular datasets, paperswithcode.com/datasets   \n861 has curated licenses for some datasets. Their licensing guide can help determine the   \n862 license of a dataset.   \n863 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n864 the derived asset (if it has changed) should be provided.   \n865 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n866 the asset\u2019s creators.   \n867 13. New Assets   \n868 Question: Are new assets introduced in the paper well documented and is the documentation   \n869 provided alongside the assets?   \n870 Answer: [Yes]   \n871 Justification: new assets introduced in this paper are well documented and provided alongside   \n872 the assets.   \n873 Guidelines:   \n874 \u2022 The answer NA means that the paper does not release new assets.   \n875 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n876 submissions via structured templates. This includes details about training, license,   \n877 limitations, etc.   \n878 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n879 asset is used.   \n880 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n881 create an anonymized URL or include an anonymized zip file.   \n882 14. Crowdsourcing and Research with Human Subjects   \n883 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n884 include the full text of instructions given to participants and screenshots, if applicable, as   \n885 well as details about compensation (if any)?   \n886 Answer: [NA]   \n887 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n888 Guidelines:   \n889 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n890 human subjects.   \n891 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n892 tion of the paper involves human subjects, then as much detail as possible should be   \n893 included in the main paper.   \n894 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n895 or other labor should be paid at least the minimum wage in the country of the data   \n896 collector.   \n897 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n898 Subjects   \n899 Question: Does the paper describe potential risks incurred by study participants, whether   \n900 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n901 approvals (or an equivalent approval/review based on the requirements of your country or   \n902 institution) were obtained?   \n903 Answer: [NA]   \n904 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n905 Guidelines:   \n906 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n907 human subjects. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]