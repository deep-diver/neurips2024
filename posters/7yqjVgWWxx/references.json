{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Improving language understanding by generative pre-training", "publication_date": "2018-00-00", "reason": "This paper introduced the groundbreaking GPT model, which is foundational to many subsequent large language models and is directly referenced in the context of autoregressive models."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper demonstrated the capabilities of large language models to perform various tasks with minimal explicit training, significantly advancing the field and directly influencing the current work's focus on zero-shot language modeling."}, {"fullname_first_author": "Jascha Sohl-Dickstein", "paper_title": "Deep unsupervised learning using nonequilibrium thermodynamics", "publication_date": "2015-00-00", "reason": "This foundational paper introduced the concept of denoising diffusion probabilistic models (DDPMs), which form the theoretical underpinning for the absorbing discrete diffusion models discussed in the paper."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This highly influential paper provided a practical framework for training diffusion models, significantly advancing their applicability and forming the basis for many subsequent works, including the current one's focus on discrete diffusion models."}, {"fullname_first_author": "Aaron Lou", "paper_title": "Discrete diffusion modeling by estimating the ratios of the data distribution", "publication_date": "2024-00-00", "reason": "This paper is the most directly related work, introducing score entropy discrete diffusion (SEDD), which the current work builds upon, reparameterizes, and improves upon."}]}