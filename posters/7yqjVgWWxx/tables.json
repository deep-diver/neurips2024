[{"figure_path": "7yqjVgWWxx/tables/tables_7_1.jpg", "caption": "Table 1: Avarage inference time of a single sample with varying sampling steps. The table compares the average inference time (in seconds) for the SEDD small model using both Euler and Tweedie T-leaping (abbreviated as T-7) sampling methods, and the RADD small model using the Euler method with a caching strategy.", "description": "This table compares the average inference time in seconds for generating a single sample using different sampling methods and models.  The comparison includes the SEDD small model with Euler and Tweedie T-leaping samplers, and the RADD small model which uses the Euler method along with a caching strategy to improve efficiency. The number of sampling steps is varied (32, 64, 128, 256, 512, 1024, 2048, 4096) to assess the impact on inference time.  For each configuration, the table also provides the resulting perplexity (PPL), a measure of the model's performance. ", "section": "4.2 Efficient sampling"}, {"figure_path": "7yqjVgWWxx/tables/tables_8_1.jpg", "caption": "Table 2: Zero-shot language modeling perplexity (\u2193) on five datasets. \u2020 labels the results based on ELBO which is taken from [20, 38, 29] and * labels the results based on the exact likelihood implemented by us. In this table, SEDD-U / SEDD-S refer to the unscaled and scaled absorbing models respectively.", "description": "This table presents the zero-shot language modeling perplexity results on five datasets (LAMBADA, WikiText2, PTB, WikiText103, and 1 Billion Words) for various models.  The models include GPT-2 (a strong baseline), D3PM, PLAID, and SEDD (with and without scaling, denoted by -U and -S, respectively). The authors' models, RADD-DSE and RADD-DCE, are also included.  Perplexity is lower is better.  The \u2020 symbol indicates perplexity was calculated using the evidence lower bound (ELBO), while * indicates the perplexity was calculated using the exact likelihood.", "section": "4.3 Improved zero-shot perplexity on language modeling"}, {"figure_path": "7yqjVgWWxx/tables/tables_21_1.jpg", "caption": "Table 3: Quality of unconditionally generated text evaluated by perplexity (\u2193). For a fixed model, the best perplexity is bolded.", "description": "This table presents the perplexity scores for unconditionally generated text, comparing different sampling methods (forward, backward, and random) for two model versions (RADD-DSE and RADD-DCE).  Perplexity is a measure of how well a model predicts a sample; lower scores indicate better performance.  The bolded scores highlight the lowest perplexity achieved by each model among the three generation methods.", "section": "4.2 Efficient sampling"}, {"figure_path": "7yqjVgWWxx/tables/tables_26_1.jpg", "caption": "Table 2: Zero-shot language modeling perplexity (\u2193) on five datasets. \u2020 labels the results based on ELBO which is taken from [20, 38, 29] and * labels the results based on the exact likelihood implemented by us. In this table, SEDD-U / SEDD-S refer to the unscaled and scaled absorbing models respectively.", "description": "This table presents the zero-shot language modeling perplexity results on five datasets for various models.  It compares the performance of the proposed RADD model against SEDD (both unscaled and scaled versions), along with other baselines.  The perplexity scores are a measure of how well the models predict held-out text. The table highlights the improvement achieved using the exact likelihood calculation method (*) compared to the ELBO method (\u2020).", "section": "4.3 Improved zero-shot perplexity on language modeling"}]