[{"figure_path": "ZWNdgc13aw/tables/tables_24_1.jpg", "caption": "Table 1: Maximum information gain bounds for common choice of kernels.", "description": "This table shows the upper bounds of the maximum information gain (\u0393\u03c4 ) for three common kernels used in Gaussian processes: Linear, RBF (Radial Basis Function), and Mat\u00e9rn.  The maximum information gain is a measure of the complexity of learning a function from data, and these bounds indicate how quickly this complexity grows with the number of data points (T). The bounds depend on the dimensionality (d) and hyperparameters (\u03bd, l) of the kernel.  For example, the linear kernel's complexity grows logarithmically with T and linearly with the dimension d, while the RBF kernel's complexity grows polynomially with T and logarithmically with the dimension d.  The Mat\u00e9rn kernel's complexity is a more complex function of T, d, \u03bd and l.", "section": "3.1 Theoretical Results"}, {"figure_path": "ZWNdgc13aw/tables/tables_25_1.jpg", "caption": "Table 2: Hyperparameters for results in Section 4.", "description": "This table lists the hyperparameters used in the experiments presented in Section 4 of the paper.  For each environment (Pendulum-GP, Pendulum, MountainCar, Reacher, CartPole, Swimmer, SoftArm, RaceCar), it provides details on the iCEM parameters (number of samples, number of elites, number of optimizer steps, HMPC horizon, number of particles) and the model training parameters (number of ensembles, network architecture, learning rate, batch size, number of epochs, action repeat). The hyperparameters were tuned for each environment to optimize performance.", "section": "B Practical algorithm and Experimental Details"}, {"figure_path": "ZWNdgc13aw/tables/tables_25_2.jpg", "caption": "Table 2: Hyperparameters for results in Section 4.", "description": "This table lists the hyperparameters used for the experiments in Section 4 of the paper.  It includes settings for the iCEM optimizer (number of particles, elites, steps, repeat), model training (number of samples, network architecture, learning rate, batch size, number of epochs), and the horizon H for each environment.  The values are specific to each environment (Pendulum-GP, Pendulum, MountainCar, Reacher, CartPole, Swimmer, SoftArm, RaceCar).", "section": "B Practical algorithm and Experimental Details"}]