[{"figure_path": "Mwj57TcHWX/tables/tables_7_1.jpg", "caption": "Table 1: Success rates (\u2191) of DiffTORI, DP3 and Residual + DP3 on 22 MetaWorld tasks. DiffTORI consistenly achieves higher or on-par success rates on all 22 tasks.", "description": "This table presents the success rates of three different methods (DiffTORI, DP3, and Residual + DP3) on 22 MetaWorld tasks, categorized by difficulty level (Medium, Hard, and Very Hard).  The higher the success rate, the better the method's performance. DiffTORI consistently shows equal to or better performance compared to other methods across all tasks. The results showcase DiffTORI's effectiveness in robotic manipulation tasks.", "section": "5.2.1 Meta World"}, {"figure_path": "Mwj57TcHWX/tables/tables_7_2.jpg", "caption": "Table 2: Failure rates (\u2193) of all methods on the Robomimic tasks. DiffTORI achieves the lowest failure rates on all tasks with diffusion policy as the base policy.", "description": "This table compares the failure rates of different imitation learning methods on four Robomimic tasks: Square, Transport, ToolHang, and Push-T.  The failure rate is the percentage of trials where the robot fails to complete the task successfully.  The table shows that DiffTORI consistently outperforms other methods, achieving the lowest failure rates across all tasks when using a diffusion policy as the base policy.  Different variants of DiffTORI and other baselines (including IBC, BC-RNN, and residual methods with various base policies) are compared.  The results highlight DiffTORI's superior performance in robustly completing the tasks.", "section": "5.2 Imitation Learning"}, {"figure_path": "Mwj57TcHWX/tables/tables_8_1.jpg", "caption": "Table 3: On Maniskill tasks, DiffTORI consistently achieves higher success rates (\u2191) on all tasks.", "description": "This table presents the success rates achieved by different methods on various ManiSkill tasks.  The methods compared include a baseline Behavior Cloning (BC) approach, BC with residual learning, and DiffTORI combined with BC.  DiffTORI consistently outperforms the other methods across all tasks, highlighting its effectiveness in improving the success rates of a baseline policy.", "section": "5.2 Imitation Learning"}, {"figure_path": "Mwj57TcHWX/tables/tables_15_1.jpg", "caption": "Table 2: Failure rates (\u2193) of all methods on the Robomimic tasks. DiffTORI achieves the lowest failure rates on all tasks with diffusion policy as the base policy.", "description": "This table presents the failure rates of different imitation learning methods on the Robomimic benchmark.  The methods compared include IBC, BC-RNN, Residual + BC-RNN, DiffTORI + BC-RNN, IBC + Diffusion, Residual + Diffusion, DiffTORI + Diffusion, DiffTORI + zero init., and DiffTORI + random init.  The results show that DiffTORI consistently achieves the lowest failure rates across all tasks when using the Diffusion policy as the base policy.  The table also shows that initializing the DiffTORI model with zero or random actions significantly increases its failure rate.", "section": "5.2 Imitation Learning"}, {"figure_path": "Mwj57TcHWX/tables/tables_16_1.jpg", "caption": "Table 3: On Maniskill tasks, DiffTORI consistently achieves higher success rates (\u2191) on all tasks.", "description": "This table presents the success rates achieved by different methods on various tasks within the ManiSkill benchmark.  The methods compared include a baseline Behavior Cloning (BC) method, a BC approach with residual learning, and the proposed DiffTORI method with and without different initialization strategies for its action. DiffTORI consistently demonstrates superior performance.", "section": "5.2.3 ManiSkill"}, {"figure_path": "Mwj57TcHWX/tables/tables_16_2.jpg", "caption": "Table 6: Failure rates (\u2193) of DiffTORI and Diffusion Policy using Positional Controllers on Robomimic Tasks.", "description": "This table compares the failure rates of DiffTORI and Diffusion Policy when using positional controllers on three Robomimic tasks (Square, Transport, and ToolHang).  It shows the failure rates for both policies using proficient human demonstrations (ph) and mixed human demonstrations (mh). The lower the failure rate, the better the performance.  This table helps demonstrate DiffTORI's improved performance compared to the base Diffusion Policy, even when using the same positional controller.", "section": "5.2.2 Robomimic"}, {"figure_path": "Mwj57TcHWX/tables/tables_16_3.jpg", "caption": "Table 2: Failure rates (\u2193) of all methods on the Robomimic tasks. DiffTORI achieves the lowest failure rates on all tasks with diffusion policy as the base policy.", "description": "This table presents the failure rates of different imitation learning methods on the Robomimic benchmark.  The methods compared include IBC, BC-RNN, Residual + BC-RNN, Diffusion Policy, IBC + Diffusion, Residual + Diffusion, and two variants of DiffTORI (DiffTORI + BC-RNN and DiffTORI + Diffusion).  The results demonstrate that DiffTORI consistently achieves the lowest failure rates across all tasks when using the diffusion policy as the base policy. This highlights DiffTORI's effectiveness in improving upon baseline policies for imitation learning.", "section": "5.2 Imitation Learning"}, {"figure_path": "Mwj57TcHWX/tables/tables_17_1.jpg", "caption": "Table 8: Cost of different algorithms on the Pendulum swingup tasks from Amos et al. As in Amos et al., we test in two settings, pendulum without damping and with damping. Lower cost means the better performance. DiffTORI performs slightly worse in the no damping case but noticeably better in the damping case.", "description": "This table compares the performance of different algorithms on a pendulum swing-up task, a common benchmark in reinforcement learning.  The task is tested in two conditions: with and without damping. The table shows the cost achieved by each method, with lower cost indicating better performance.  The results demonstrate that the proposed method, DiffTORI, shows a performance improvement in the more challenging scenario with damping.", "section": "A.3 Comparison to prior work Amos et al. [3]"}, {"figure_path": "Mwj57TcHWX/tables/tables_19_1.jpg", "caption": "Table 9: Hyperparameters used in DiffTORI.", "description": "This table lists the hyperparameters used in the DiffTORI model for both model-based reinforcement learning and imitation learning experiments.  For model-based RL, many parameters are shared with or similar to those used in the TD-MPC baseline.  Imitation learning uses a different set of hyperparameters adjusted for that task.  The table specifies the value for each hyperparameter, with some values indicating a range or schedule of changes during training.", "section": "Implementation Details"}, {"figure_path": "Mwj57TcHWX/tables/tables_20_1.jpg", "caption": "Table 1: Success rates (\u2191) of DiffTORI, DP3 and Residual + DP3 on 22 MetaWorld tasks. DiffTORI consistenly achieves higher or on-par success rates on all 22 tasks.", "description": "This table presents the success rates of three different methods (DiffTORI, DP3, and Residual + DP3) on 22 tasks from the MetaWorld benchmark.  The success rate represents the percentage of successful task completions.  The table shows that DiffTORI consistently outperforms or matches the performance of the other two methods across all 22 tasks, highlighting its effectiveness in robotic manipulation tasks.", "section": "5.2.1 Meta World"}]