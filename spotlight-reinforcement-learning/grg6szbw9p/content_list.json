[{"type": "text", "text": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sriyash Poddar,\u2217 Yanming Wan\u2217, Hamish Ivison, Abhishek Gupta\u2020, Natasha Jaques\u2020 ", "page_idx": 0}, {"type": "text", "text": "Paul G. Allen School of Computer Science and Engineering University of Washington Seattle, WA 98195 <sriyash, ymwan, hamishiv, abhgupta, nj>@cs.washington.edu weirdlabuw.github.io/vpl ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for aligning foundation models to human values and preferences. However, current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional RLHF frameworks simply average over them, leading to inaccurate rewards and poor performance for individual subgroups. To address the need for pluralistic alignment, we develop a class of multimodal RLHF methods. Our proposed techniques are based on a latent variable formulation - inferring a novel user-specific latent and learning reward models and policies conditioned on this latent without additional user-specific data. While conceptually simple, we show that in practice, this reward modeling requires careful algorithmic considerations around model architecture and reward scaling. To empirically validate our proposed technique, we first show that it can provide a way to combat underspecification in simulated control problems, inferring and optimizing user-specific reward functions. Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy. We additionally show the benefits of this probabilistic framework in terms of measuring uncertainty, and actively learning user preferences. This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning from human feedback (RLHF) has become the predominant technique for aligning AI foundation models to human values. Across domains like natural language processing (NLP) [52] to robotics [62, 50], RLHF is an effective way to improve the performance, accuracy, and safety of AI models, by ensuring that they align with human preferences [45, 34, 29] The question then becomes: whose preferences? Current RLHF approaches rely on a prescriptive set of values curated by a small set of AI researchers [39, 60, 2]. Moreover, they typically assume that all end-users share the same set of values. Given the concerning lack of diversity in AI [22], it is clear that this approach cannot account for the range of social, moral, and political values that inform preferences in populations. Recent work has demonstrated that with current RLHF techniques if the majority population has a weak preference for an outcome that will severely disadvantage a minority group, the learned reward model will ignore the minority group\u2019s preferences [58]. These issues necessitate pluralistic alignment of models to human preferences [59, 44] (see Figure 1); ideally, we would democratize RLHF to account for a wider variety of human values to better serve a diverse population. ", "page_idx": 0}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/c672556a40bdb90b028680a7fd7ceeccf255de7ae9799f14bafc11634072d4aa.jpg", "img_caption": ["Figure 1: Current RLHF approaches [52] incorrectly assume a unimodal BTL reward model for a diverse population of users. In this example, users have diverging preferences over the level of detail provided in the responses from a large language model. Without additional context, the BTL model considers both responses to be equally likely. In contrast, our method, VPL, is a personalized approach to RLHF. Using a few samples from a particular user, we infer the distribution over their distinct preferences. Based on this distribution, we condition the reward model to more accurately predict rewards, and enable steering the resulting policy to personalize to the specific user. This enables accounting for and serving the preferences of under-represented groups which would otherwise be ignored by the standard BTL model [52]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Current approaches to RLHF [52] use the Bradley-Terry-Luce (BTL) [15] model to learn a reward model that explains the human preferences. While the BTL model accounts for noisy preferences, RLHF typically applies this model under the \u2018unimodal\u2019 assumption that all human preferences are derived from a single utility function. This fails to capture scenarios where preferences diverge\u2014 i.e. are multi-modal\u2014due to fundamentally different utilities. For example, Figure 1 shows a case where one group of users prefers detailed responses, while another prefers concise ones. By performing maximum likelihood estimation under the unimodal BTL model, current methods learn a reward function that averages these multi-modal preferences (akin to mode averaging in imitation learning [51]. As we show in our experimental results, this model misspecification leads to reward models that are inaccurate, and the policies optimized on these rewards fail to accomplish tasks per any of the distinct preferences (see Figures 6, 3). Thus, vanilla RLHF methods [52, 17] are insufficient for aligning AI systems to diverse human values. ", "page_idx": 1}, {"type": "text", "text": "In many applications, from large language models (LLMs) to assistive robotics [7], users are diverse, and AI systems must adapt the generated responses to user-specific preferences to successfully complete the task. Consider, for example, a robot assistant putting away dishes in a user\u2019s kitchen: each individual has unique preferences for how the dishes in their kitchen are organized, potentially diverging from others\u2019 preferences. In the context of LLMs, failing to adapt to user-specific preferences can make them unhelpful, unsafe, and vulnerable to jailbreak in the presence of conflicting objectives [2, 58]. To build safe and performant foundation models serving a diverse population, we need methods that can explicitly account for and adapt to the inherent plurality of human preferences. ", "page_idx": 1}, {"type": "text", "text": "These insights suggest that human preferences are not derived from a single utility function, but are affected by unobserved, hidden user context [58]. To accurately model individual utilities, RLHF should be able to efficiently infer and adapt to the user context. With this in mind, we formulate RLHF as a latent variable problem. Building on techniques from variational inference [11, 38], we propose a method\u2014Variational Preference Learning (VPL)\u2014for multi-modal reward modeling. Intuitively, given a few preference annotations from a particular user, our approach uses a variational encoder to infer a latent distribution over hidden context, and a latent conditional reward model to accurately recover the true multi-modal preference distribution. We derive an evidence lower bound (ELBO) ", "page_idx": 1}, {"type": "text", "text": "for latent-variable preference-based reward optimization. Our proposed algorithm, VPL, effectively learns a distribution of reward functions from large corpora of preferences from diverse users. ", "page_idx": 2}, {"type": "text", "text": "In developing practical training methods for such latent-conditioned reward models, we show that several complexities and technical considerations arise. A key problem is that binary comparisons inherently lack information regarding the scale of rewards. Under the BTL model (and correspondingly the VPL model), the preference label between two alternatives $A$ and $B$ can only provide information about $r_{A}-r_{B}$ . So, the learned reward function may have vastly varying reward scales across individual users that adversely affect the optimization landscape of multi-user reinforcement learning [31, 69] using these rewards. To mitigate this, we show how a simple pairwise classification scheme [61, 49] can appropriately bound and scale reward estimates within our latent variable framework, thereby enhancing the performance of downstream learned policies. The predicted latent distribution enables several additional capabilities. In Section 4 we use our approach to learn latent-conditioned policies that can personalize to particular users at test time. Additionally, the latent variable reward models can measure uncertainty in the reward distribution [56]. So, in Section 4.2, we use our approach to actively query [8, 10, 9] users to minimize the number of labels they need to provide before we can adapt to their distinct preferences. ", "page_idx": 2}, {"type": "text", "text": "Overall, our work introduces a latent variable framework for reward modeling that can encode and approximate the distribution of user preferences directly from binary comparisons, and steer the downstream policy to adapt to diverse user preferences. We conduct a broad range of experiments across three simulated robotics environments and two language tasks with confilcting user preferences. Our results show that in simulated domains, VPL accurately models rewards and improves task performance and personalization. We are able to scale this method to many users, and use active learning to adapt efficiently to particular users with significantly fewer queries at test time. In the language domain, we are able to train multiple LLM-based reward models that learn a separable embedding space that can distinguish between users with divergent preferences. The resulting models outperform existing by RLHF approaches $10\u201325\\%$ [52, 58] by more precisely predicting rewards that align with diverse users and objectives across multiple datasets. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning from Human Feedback (RLHF): We focus on the problem of reinforcement learning (RL) from binary human preferences using the Bradley-Terry-Luce (BTL) model [15]. This has a rich history in the field of RL and robotics, often referred to as Preference-based RL (PbRL) [65, 28, 1, 8, 10]. We specifically build on the framework outlined in Christiano et al. [17] and further expanded in recent works [52, 2, 74, 60, 37, 58]. This has seen a wide range of applications ranging from training robots [17, 10, 62] to finetuning language models for alignment [52, 74, 60]. Our work, enabling RLHF with diverse preferences, is easily applicable to any preference-based learning method, including recent techniques [55, 24] that circumvent reward modeling altogether. ", "page_idx": 2}, {"type": "text", "text": "RLHF under non-BTL models: Prior work has aimed to study non-BTL models of human behavior and preferences [12, 43, 42, 37, 61] to account for human irrationality and uncertainty in preferences, or intransitive preferences [49, 61, 64]. However, our key argument in this work is less about human irrationality (i.e. inconsistency), and more about the divergence between potentially rational preferences for different labeling users and end users. In this sense, our work is complementary in that the latent variable model can easily be adapted to non-BTL models as well. In fact, we incorporate the technique proposed in Swamy et al. [61] to improve reward learning for downstream applications. ", "page_idx": 2}, {"type": "text", "text": "Personalized RLHF: While some works [39, 40, 35] characterize similar challenges as VPL, they largely focus on exploring the societal issues underpinning the need for personalization and introducing datasets with diverse annotations. Conitzer et al. [19] argue Social Choice Theory could provide insights into how to aggregate diverse preferences. But these works do not propose a technical method to achieve modeling diverse preferences. ", "page_idx": 2}, {"type": "text", "text": "Several prior works have looked at trading off conflicting alignment objectives (such as remaining both helpful and harmless) through techniques like Pareto-optimal optimization [13, 16] or multiobjective RL [67, 21]. For example, Jang et al. [33] treats personalization as a multi-objective RL problem, requiring explicit decomposition into different sub-rewards and learning independent reward models for different users. Further, Dai et al. [21] introduces Safe RLHF, an approach that explicitly models the different objectives of helpfulness vs harmfulness. In contrast, our work does not aim to reconcile the diversity but rather solve the model misspecification and learn reward models that can infer the context and specialize to a particular user. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Some methods which deal with diverse underlying preferences (e.g. [14]) aim to model disagreement in the data but typically require more information such as annotator demographics [26, 4]. Li et al. [46] learns to map user information to a fixed set of representations for each user, and conditions the reward model on this embedding. Feng et al. [25] proposes a pluralistic alignment framework, using smaller LMs that are trained on community or user-specific data. In contrast, our method does not assume access to personal user information, but only a few preference annotations from each user. ", "page_idx": 3}, {"type": "text", "text": "Some works have explored personalization outside of the context of reward learning. Zhao et al. [72] uses in-context learning with few samples from a particular user group, to learn preferences over discrete answers to a question, but does not focus on reward learning with a BTL loss over long context prompt and responses, which is our focus. In addition, our method learns a latent distribution over user types to enable capabilities like active learning [71] and latent-conditioned policy learning. ", "page_idx": 3}, {"type": "text", "text": "Dumoulin et al. [23] explores annotator-model misspecification and uses synthetic examples to show how RLHF fails in the presence of two users with different preferences, but they do not propose a scalable approach for resolving this issue. The closest work to ours is Distributional Preference Learning (DPL) [58], which aims to account for hidden context in RLHF. While the motivation is similar, the techniques proposed in DPL include modeling only the mean and variance of the reward distribution across users, or a particular categorical approximation to the reward distribution. As such, DPL can capture uncertainty in the inferred reward distribution, but cannot accurately predict the reward for a particular user. In contrast, VPL explicitly models a user-specific latent variable $z$ , and learns $z$ -conditioned, user-specific reward models, enabling us to accurately reconstruct divergent reward functions for different users. As we will show in Section 6, this leads to significantly improved empirical reward prediction results. Unlike DPL, VPL takes a variational approach to user modeling and allows for reward interpretability, active learning, and model specialization to users at test time. ", "page_idx": 3}, {"type": "text", "text": "3 Technical Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we build on the framework of preference-based reward learning (often referred to as RLHF) [17, 1]. In particular, we will consider reward learning methods based on the BradleyTerry-Luce (BTL) choice model [15]. RLHF has two key phases: (1) inferring a reward function from human-provided labels of ordinal preferences; (2) using reinforcement learning (RL) to train a decision-making policy that maximizes the rewards inferred in step (1). We will instantiate this framework abstractly, which can then be specialized to both LLMs and robotics. ", "page_idx": 3}, {"type": "text", "text": "We define a Markov decision process (MDP) [53] $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{T},\\gamma,\\rho_{0})$ , with state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , transition dynamics $\\tau$ , discount factor $\\gamma$ and initial state distribution $\\rho_{0}$ . Notably, we do not have access to an underlying reward function, but instead have annotators who rank pairs of states $s_{A}$ and $s_{B}$ . We assume annotators have an unknown reward function $r(s)$ that informs their preference labels. More formally, an annotator $[h\\in\\mathrm{H})$ ) takes a pair of states $s_{A}$ and $s_{B}$ , and returns an ordinal preference i.e. $y=\\mathbb{1}(s_{A}\\succ s_{B})$ , according to $r(s)$ [15, 23]; where $\\mathrm{H}$ is the space of all possible annotators. Given a dataset of annotated preferences $\\mathcal{D}=\\{(s_{A}^{i},s_{B}^{i},y=\\mathbb{1}(s_{A}^{i}\\succ s_{B}^{i}))\\}_{i=1}^{N}$ , a typical RLHF procedure learns a reward function $r_{\\phi}(s)$ using a maximum likelihood objective (MLE) on the preferences, where the likelihood: $p_{\\phi}(y\\mid s_{A},\\dot{s}_{B})$ can be defined using the BTL model: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\phi}(y=1\\mid s_{A},s_{B})=1-p_{\\phi}(y=0\\mid s_{A},s_{B})=p_{\\phi}(s_{A}\\succ s_{B})=\\frac{e^{r_{\\phi}(s_{A})}}{e^{r_{\\phi}(s_{A})}+e^{r_{\\phi}(s_{B})}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note, that while in this case, the ordinal preferences are on states $s_{A},s_{B}$ , this can be generalized to trajectories or snippets [17, 52]. Finally, the recovered reward function can then be used to learn (or finetune) a policy $\\pi_{\\theta}(a|s)$ that can act to maximize the expected sum of approximated rewards in the environment using standard RL algorithms [57, 30, 41] i.e. $\\begin{array}{r}{\\pi_{\\theta}=\\arg\\operatorname*{max}_{\\theta}\\frac{\\mathbb{E}}{\\pi_{\\theta}}\\left[\\sum_{t}\\gamma^{t}r_{\\phi}(s_{t})\\right]}\\end{array}$ . We note that while the BTL model accounts for some IID noise in preferences through the probabilistic formulation [17, 3, 23], it does not account for hidden-context and divergent human preferences [58], and it does not allow the underlying reward models and policies to be personalized to specific users. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The standard BTL formulation described in Section 3 is based on the assumption that all annotators $h\\in\\mathrm{H}$ share a single underlying reward function $r_{\\phi}(s)$ . This does not hold in practice with a diverse range of annotators. To model diverse, pluralistic preferences, we frame multi-modal reward learning as a latent variable problem, where the latent variable $z$ represents the hidden context affecting the underlying reward function (and thereby the preferences) of an annotator $h\\in\\mathrm{H}$ ; for instance, it could be representative of their lived experience, or the style of response/policy that they would like the agent to perform. Following this, the latent-conditional reward $r_{\\phi}(s,z)$ is a function of latent $z$ along with state $s$ , and preference likelihoods can be expressed with a latent-conditional BTL model: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\phi}(y=1\\mid s_{A},s_{B},z)=p_{\\phi}(s_{A}\\succ s_{B}\\mid z)=\\frac{e^{r_{\\phi}(s_{A},z)}}{e^{r_{\\phi}(s_{A},z)}+e^{r_{\\phi}(s_{B},z)}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The maximum likelihood objective for this model from a dataset of preference labels (with multiple annotators), $\\operatorname*{max}_{\\phi\\ \\ s_{A},s_{B},y\\sim\\mathcal{D}}\\Bigl[\\log p_{\\phi}(y\\ |\\ s_{A},s_{B})\\Bigr]=\\operatorname*{lim}_{s_{A},s_{B},y\\sim\\mathcal{D}}\\left[\\log\\int p_{\\phi}(y\\ |\\ s_{A},s_{B},z)p(z)d z\\right]$ is intractable due to marginalization over the unobserved latent variable $z$ . To tackle this, we can introduce a variational posterior approximation $q_{\\psi}(z\\mid\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N})$ , conditional on multiple annotations $\\{(s_{A}^{i},s_{B}^{i},y^{i}=h(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}$ provided by the same annotator $h^{2}$ . This results in a corresponding evidence lower bound (ELBO), ${\\mathcal{L}}(\\phi,\\psi)$ , for the intractable marginal $\\log p_{\\phi}(y\\mid s_{A},s_{B})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\{s_{A}^{i},s_{B},y=h(s_{A},s_{B})\\}_{i=1}^{N}\\sim\\mathcal{D}}{\\mathbb{R}}\\left[\\underset{\\substack{s\\sim q_{\\psi}(z)\\,\\left(\\{s_{A}^{i},s_{B}^{i},y^{i}\\}\\right)_{i=1}^{N}}}{\\mathbb{R}}[\\log p_{\\phi}(y\\mid s_{A},s_{B},z)]-D_{\\mathrm{KL}}(q_{\\psi}(z\\mid\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N})\\mid p(z))\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This objective samples a user $h\\sim\\mathrm{H}$ from the given annotators and a set of annotations from this particular user $\\{(s_{A}^{i^{*}},s_{B}^{i},y^{i}=h(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}$ for inferring the latent variable $z$ through the posterior $q_{\\psi}(z~\\mid~\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N})$ l.i hoGoivde onb tjheect ipvoes nuvsoilnvge st hoe pctiomntiezixntug atl wBoT tLe rmmos:d eal $(\\log p_{\\phi}(y\\mid s_{\\bar{A}},s_{B},z))$   \nand a regularization term $(D_{\\mathrm{KL}}(q_{\\psi}(z\\mid\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N}))\\mid\\mid p(z)))$ against a prior $p(z)$ . Intuitively this objective encodes a set of user-provided annotations $\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N}$ into a latent distribution using the encoder $q_{\\psi}$ , and then learns a latent-conditional reward function $\\bigl\\gamma(s,z)$ that best explains the annotated preference data. As the variational encoder $q_{\\psi}$ generates a latent distribution, this formulation further enables uncertainty estimation and active learning, as shown in Section 4.2. We describe details further in Appendix B.3. ", "page_idx": 4}, {"type": "text", "text": "We emphasize that the only additional requirement of this objective, compared to standard RLHF [17], is a set of annotated pairs from the same annotator, and this information is easily available when annotators are queried in batches [10]. We refer to Algorithm 1 for further details on implementation. ", "page_idx": 4}, {"type": "text", "text": "Personalized, latent-conditioned policies. The learned reward models $r_{\\phi}(s,z)$ can be used to train personalized, user-specific policies. During training, we learn a latent-conditioned policy $\\pi_{\\boldsymbol{\\theta}}(\\cdot|\\bar{s},z)$ that maximizes the rewards $r_{\\phi}(s,z)$ for different values of $z$ . This allows the policy to adapt to diverse user preferences encoded in the latent space. We can use any standard (RL) algorithm [57, 30, 41] to optimize the latent-conditional reward maximization objective: $\\pi_{\\theta}\\,=$ arg max\u03c0, $\\mathbb{E}_{z\\in p(z)}\\left[\\sum_{t}\\gamma^{t}r_{\\phi}(s_{t},z)\\right]$ . For this, we sample $z$ from the prior $p(z)$ during training and learn the policy $\\pi_{\\boldsymbol{\\theta}}(\\cdot|s,z)$ , to maximize the corresponding latent-conditioned reward $r_{\\phi}(s,z)$ . ", "page_idx": 4}, {"type": "text", "text": "At test-time, we infer a specific user\u2019s $(h_{\\mathrm{test}})$ latent context $z$ by posterior inference using a set of labeled preference queries $\\{(s_{A}^{i},s_{B}^{i},y^{i}=h_{\\mathrm{test}}(s_{A}^{i},s_{B}^{i}))\\}_{i=1}^{N}$ i.e $\\dot{z^{\\prime}}\\!\\sim q_{\\psi}(z\\mid\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{\\overline{{{\\cal N}}}})$ . We then deploy the personalized policy $\\pi_{\\boldsymbol{\\theta}}(\\cdot|s,z)$ conditioned on the inferred $z$ for that user. We outline the complete algorithms for policy training and deployment in Appendix $\\mathbf{C}$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Scaled Rewards for Multi-Task Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practice, optimizing latent-conditioned reward functions learned with the VPL objective poses unique challenges. The pairwise preferences used to train the reward model in Section 4 do not have information about the scale of rewards, but only their relative ordering. As a simple illustration, if we have a pair of states $s_{A},s_{B}$ , where the users prefer $s_{A}$ i.e. $s_{A}\\succ s_{B}$ , two different reward functions: $r(s_{A})\\bar{=}100,r(s_{B})=50$ or $r(s_{A})=50$ , $r(s_{B})=0$ have the same likelihood under the BTL model. Empirically, we observe that this poses problems for optimizing Equation 3; different values of the latent variable $z$ generate reward functions of vastly different scales. This is an issue for several reasons: 1) varying reward scales adversely affect the landscape of multi-user policy optimization (often observed in multi-task RL) [31], and 2) it is challenging to identify states where user preferences diverge across the population as differently scaled rewards cannot be directly compared. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To address this issue, we experiment with several different techniques for scaling the learned reward functions (see Appendix A.2). Our key insight in solving this challenge lies in the observation that while raw rewards from BTL are not scaled equally across $z$ , probabilities from the preference likelihood model $p(y\\mid s_{A},s_{B},z)$ are appropriately scaled. This suggests that an effective solution to the reward scaling issue is to replace the raw rewards from the BTL model $(r(s,z))$ with likelihoods suggested by the pairwise preference likelihood model $p(y\\mid s_{A},s_{B},z)$ . In particular, a natural choice of scaled rewards for state $s_{A}$ is the expected likelihood that $s_{A}$ is \u201cpreferred\" to all other states (or a sampled set of states) $s_{B}$ observed in the data - $r_{\\phi}(s_{A},z)=\\underset{s_{B}\\in\\mathcal{S}}{\\mathbb{E}}\\big[p_{\\phi}(y=1\\mid s_{A},s_{B},z)\\big]$ . Since these are probabilities, normalized in the $[0,1]$ range, the scaling of rewards is consistent across latents $z$ . Note that these expected likelihood rewards can easily be obtained from the objective in Equation 3 since we train a latent-conditional preference classifier. While proposed from a very different perspective, we note the similarity of this reward scaling approach to recent work [61, 49], in particular, Self-Play Preference Optimization (SPO) [61], which was originally proposed to address the issue of intransitive preferences. Similar to [61] we assume that the oracle providing preference labels is Non-Markovian. Due to this similarity3, we use VPL-SPO to indicate this approach of preference-based reward scaling throughout our experiments (See Algorithm 3). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Active Learning of Preferences to Minimize Latent Uncertainty ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A natural question that arises for test time deployment of the latent-conditioned policies is how to obtain the set of state pairs $\\{(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}$ to be annotated with preference labels and used for posterior inference, $z\\sim q_{\\psi}(z\\mid\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N})$ . Not all query sets $\\{(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}$ are made equal; some are more informative than others. Certain states (where preferences vary across annotators) are particularly informative in accurately inferring the $z$ which should be used for policy deployment $\\bar{\\pi}(a|s,z)$ . In VPL, the probabilistic modeling of the variational encoder naturally allows for active selection of the most informative query set based on maximal information gain, following prior work [8, 10, 50]. Here the provision of preference labels $\\{y^{i}\\}_{i=1}^{N}$ will provide the maximum information about the latent distribution (and indirectly, the user preferences). This active query selection procedure can be expressed as the following optimization problem, maximizing the mutual information between the labels and the latent distribution. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}\\gets\\underset{\\{(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}}{\\arg\\operatorname*{max}}\\ Z\\left(z;\\{y^{i}\\}_{i=1}^{N}\\mid q_{\\psi},\\{(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The posterior $q_{\\psi}$ is a multivariate Gaussian, and assuming a uniform distribution over the set of labels, $q_{\\psi}(z\\mid\\{(s_{A}^{i},s_{B}^{i})\\}_{i=1}^{N})$ allows for closed form solution for mutual information $\\mathcal{T}$ . To solve the amcaroxissm sizaamtipolne s ofbrjoemct itvhee,  pwreef ecrheonscee  tdhaet aqsueetr. yF sineta $(s_{A}^{i},s_{B}^{i})_{i=1}^{N}$ uwsietrh  ltahbee lms aoxni tmhius mm ianxfiormmala tqiuoenr yg asient,, infer the latent, and condition the policy on this latent at deployment. ", "page_idx": 5}, {"type": "text", "text": "5 Scaling VPL for Reward Learning in Large Language Models (LLMs) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "VPL can be used to train pluralistic reward models for LLMs, accounting for diverse human preferences and values. Here we discuss the key details that are essential to scale our method to LLMs. The architecture of our LLM reward model is shown in Figure 2. Unlike prior work which attempted to insert summary embedding layers into LLMs (see e.g. [18]), we find that we can successfully compress user preference information into a concise, probabilistic embedding vector $z$ without sacrificing reward model performance. Further details and hyperparameters are discussed in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Prompt and Response Embeddings. Since using raw representations of the prompt and responses can increase the context length significantly, we use a pre-trained LLM to encode prompt and response pairs together [6] (to be consistent with previous notation, we assume a preferred state $s^{A}$ contains both a prompt and response $[x,r]$ , and $e^{A}=L L M(s^{A}))$ . For efficient training, we pre-compute and freeze the encoded embeddings. ", "page_idx": 6}, {"type": "text", "text": "Latent Encoder. Given a set of multiple encoded preference queries from the same user, $\\{(e_{i}^{A},e_{i}^{B},y^{i})\\}_{i=1}^{N}$ , we pass each through the same pair encoder to obtain $h_{i}=e n c(e_{i}^{A},e_{i}^{B})$ . The latent representation $z$ is generated using self-attention layer over the set of encoded pairs, $\\{h_{i}\\}_{i=1}^{N}$ . ", "page_idx": 6}, {"type": "text", "text": "Reward learning. Here, the representation $e^{A^{\\prime}}$ of a new state $s^{A^{\\prime}}$ is concatenated with a $z$ sampled from the posterior distribution which is passed into an MLP to predict the rewards. The LLM is fine-tuned using low-rank adaptation (LoRA) [32], and unlike typical RLHF settings, we find that we need to train the reward model for $\\geq1$ epochs to fit the encoder and the reward model. ", "page_idx": 6}, {"type": "text", "text": "Data augmentation. As we scaled VPL to larger datasets with more users, we found that augmenting the training dataset with multiple context samples from the same user for each new data point is essential to learning an effective encoder. Formally, at training time, given a prompt and response pair $s_{A}^{\\prime},s_{B}^{\\prime}$ from a particular user, we generate $\\dot{M}\\,\\in\\,\\{4,8\\}$ duplicates of this labeled response with different contexts i.e. annotated prompt and response pairs $(\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N})_{j=1}^{M}$ ; where each context $(\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N})_{j}$ is sampled from a user annotated subset of size $K$ $(K>N)$ . ", "page_idx": 6}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/f539a9572951a02a4187e73e8bb3ee80e70aaac581e701790e8961fe591d3492.jpg", "img_caption": ["Figure 2: VPL LLM architecture for reward learning. The left and right parts denote the encoder $q_{\\psi}$ and the reward model $r(s,z)$ respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6 Experimental Evaluation on Simulated Control Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our experiments, we answer the following questions: (1) Can VPL accurately learn a multi-modal distribution of reward functions from a preference dataset labeled by diverse users? (2) Do the inferred latent user vectors enable learning a multi-task personalized policy? (3) Can we leverage the posterior to actively query preferences for latent estimation? In this section, we show the beneftis of VPL in multiple simulated control tasks and demonstrate that VPL is able to capture multi-modality in the underlying reward functions, arising due to underspecification of annotator goals and preferences. In the following section, we ask whether VPL scales up to RLHF for LLMs. ", "page_idx": 6}, {"type": "text", "text": "Training and Evaluation Details: We test our hypothesis across evaluation domains in two steps: 1) We train a reward model on a dataset of preferences collected using diverse simulated humans; 2) We train a policy using RL to maximize the learned rewards. For these experiments, we use Implicit Q-Learning [41], an offline RL algorithm that achieves strong performance on offline RL benchmarks [27]. Using the learned reward function $r_{\\phi}(s,z)$ , and the prior $p(z)$ , we label the reward-free offline RL dataset $D=(s_{t},a_{t},s_{t+1})$ , by sampling a latent $z\\,\\in\\,p(z)$ , and setting the reward as $r_{t}=r_{\\phi}(s_{t})$ , or a one-step look-ahead method, where $r_{t}=r_{\\phi}(s_{t+1})$ (refer to Algorithm 2 for complete method) . We include the hyperparameters and the training details in the Appendix B ", "page_idx": 6}, {"type": "text", "text": "Baselines: We compare our method against multiple baselines: 1) Oracle [47]: This is a goalconditioned offilne RL method, that presents an oracle with access to the true reward functions for all annotators. 2) BTL [17]: This is the standard RLHF method from [17, 52] as a baseline, where the reward model is approximated using the unimodal $B T L$ function. 3) DPL [58]: Following the work on accounting for hidden context in RLHF, we train a distributional reward model, using both the mean-variance (MeanVar) and categorical (Categorical) approximation for the reward functions. 4) VPL (Ours): We denote two versions of our method, VPL and $\\mathbf{VPL}+\\mathbf{SPO}$ , discussed in Section 4. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.1 Tasks: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our approach on three diverse simulated control tasks (Figure 4) to demonstrate the effectiveness of VPL to learn latent conditioned policies. ", "page_idx": 6}, {"type": "text", "text": "Maze-Navigation is based on the D4RL benchmark [27]. Here, users guide a pointmass agent to goals marked with their preferred colors. The user\u2019s preferred color is underspecified, so the agent has to infer their preferences from a few annotated comparisons and navigate to one of two/ten locations. ", "page_idx": 7}, {"type": "text", "text": "Ravens-Manipulation requires the agent to rearrange an object on a table-top (akin to rearranging a dining table), based on user preferences. Built on the ravens benchmark [70], the agent controls a robot arm in a continuous action space to pick and place the box in one of the two preferred locations. ", "page_idx": 7}, {"type": "text", "text": "Habitat-Rearrange [68] tasks a mobile robot arm with relocating a bowl to a user-specified location among five candidates (e.g., desk, kitchen, table). The robot is equipped with motion primitives for navigation and manipulation, and the problem involves reasoning over these locations. This simplified one-step reasoning task highlights the need for personalization in assistive robotics, as each person has distinct preferences for robots in their homes. ", "page_idx": 7}, {"type": "text", "text": "Habitat-Tidy is a simulated environment similar to TidyBot [66], where the robot infers placement locations for objects in a kitchen based on user preferences. Users may prefer sorting or cleaning objects by different attributes (e.g., function or material), and the robot must adapt to these preferences. Like Habitat-Rearrange, the task involves reasoning over attributes for sorting based on existing preferences, using motion primitives for navigation and manipulation. ", "page_idx": 7}, {"type": "text", "text": "6.2 Can VPL capture multi-modal reward functions from a dataset of diverse preferences? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We generate preferences using multi-modal reward functions across multiple didactic and simulated discrete and continuous control experiments, as shown in Figures 6, 3, 4, 11 and 10. We see that the BTL baseline with an MLP model averages the different underlying rewards and learns an inaccurate reward model (Figure 3b). In the presence of a majority, BTL converges to the preferences ignoring the minority groups (See Figure 11). While DPL [58] can recover the uncertainty in the reward models due to underspecification, they have no mechanism to recover the individual reward functions. As a result, the DPL reward model estimates high variance rewards (see Figures 6, 10) for each particular user. In contrast, VPL infers the hidden user context using the latent variable formulation and accurately recovers the multi-modal reward distribution. ", "page_idx": 7}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/ee82315661c182d9307bf34c68026ce74de6a75456ece333faeae3285cb3fed5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Ground truth preferences (a) show that annotators prefer the robot navigate to two different goals. Unimodal BTL (b) averages over the two modes. VPL (c) accurately reconstructs diverse preferences, and learns $z$ conditioned policies to reach either goal. ", "page_idx": 7}, {"type": "text", "text": "6.3 Do distributional reward functions enable learning a steerable multi-task policy? ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/6548c44a79b387945481453242ee16c126388961c6df3a66aed2eb6fa6ae80c7.jpg", "img_caption": ["Figure 4: Performance of a downstream policy on diverse control and reasoning tasks, using the rewards trained using different baselines. We report the mean and standard error over five seeds. Note: Habitat envs have a one-step greedy policy so reward scaling and $\\mathrm{SPO}{+}\\mathrm{VPL}$ are not required. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "As discussed above, the baselines average the reward modes across the users and learn an inaccurate reward function. So, in Figure 4 the policy for the Maze-Navigation converges to a wrong goal, leading to poor performance and misalignment with all the underlying users. For the RavensManipulation experiments, the baselines randomly choose a goal location and do not adapt to the user preference at test time. Similarly, for the Habitat-Rearrange tasks the baselines are unable to capture the diversity in user preferences over the multiple locations and do not succeed in placing the bowl at the correct location. Finally, in the Habitat-Tidy task, we observe that the baselines converge to an accuracy achieved when they ignore the diverse preferences of sorting the objects by the users. ", "page_idx": 7}, {"type": "text", "text": "In contrast, the policies trained using VPL outperform all the baselines in terms of the task success rate, according to the user\u2019s underlying reward function. For the navigation task, VPL correctly infers the goal and the learned policy can navigate to the goals with a high success rate, and the performance is comparable to a goal-conditioned oracle. For Ravens-Manipulation, VPL infers the user latent at test time and accurately places the box at the right location. Finally, VPL is mostly able to correctly identify the preferred location for the bowl in the user\u2019s home and has a higher success rate than the baselines. We note that scaling the rewards via $\\mathrm{VPL}+\\mathrm{SPO}$ improves the performance of multi-task RL for optimizing diverse user preferences. In Habitat-Tidy, VPL is able to infer the user preferences and follow the preferred attributes while placing the objects. However, the accuracy of the robot is dependent on the context length and we discuss this in further detail in Appendix A.4. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.4 Can VPL enable active query selection for latent estimation? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 4.2, we present an objective to actively query users at test time to efficiently infer user preferences. Figure 5 shows that this technique leads to better performance of the learned policy across varying numbers of queries $\\left\\|N\\right\\|$ . This implies that the active learning objective 4 which maximizes information gain over the latent distribution generates queries that are more discriminative and provides a more informative posterior for user identification. This results in a more efficient adaptation of the downstream policy to the distinct user preferences, achieving the same performance with only half the queries. These methods can be potentially transferred to LLMs to query and identify user preferences with minimal questions. ", "page_idx": 8}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/d3280b44dfbe1ce861844d0e8cd8d1a88d2e43b7590ab663c5c8cac9e8b33d38.jpg", "img_caption": ["Figure 5: Active learning enables personalizing policies to user preferences with fewer queries. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 LLM Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we ask: can VPL scale up to the pluralistic alignment of LLM-based reward models? We compare the reward modeling performance of our method against two baselines: the vanilla BTL model and DPL [58]. We experiment with two LLMs: GPT2 [54] and Llama2-7B [63], and two pluralistic preference datasets. ", "page_idx": 8}, {"type": "text", "text": "7.1 Datasets for pluralistic LLM alignment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Prior RLHF works have focused mainly on unimodal BTL models, and as such there is a lack of publicly available datasets containing annotated preferences with divergent objectives. To evaluate our method on capturing multi-modality in preferences for LLMs, we consider two benchmarks. First, we introduce a synthetic dataset, Pets, that directly represents multimodal preferences, and second, we augment the publicly available UltraFeedback [20] dataset. ", "page_idx": 8}, {"type": "text", "text": "Pets. Here, the dataset is generated to reflect multi-modal user preferences, where each user has a preference ranking over four kinds of animals (in this case cats, dogs, birds, and rabbits). To simulate a setting where users agree on some comparisons and disagree on others, we consider two users who agree on the best and worst pet and disagree on the middle pair of rankings over pets. Preferences here are divergent in certain cases (middle pets), and agree in other instances (best and worst pets), requiring multimodal preference modeling. We evaluate our approach on two versions of the dataset: Pets (Full), and Pets (Divergent) which contains only those prompt and response pairs where the users are divergent (i.e. they have conflicting preferences). For the contexts $\\{(s_{A}^{i},\\stackrel{\\bullet}{s}_{B}^{i},y^{i})\\}_{i=1}^{N}$ , we randomly sample 1-4 other prompts and ranked responses from the same user. ", "page_idx": 8}, {"type": "text", "text": "UltraFeedback-P. To construct this dataset $U F{-}P$ (where $\\mathbf{P}$ stands for personalized), we use the finegrained scores over different attributes available in the UltraFeedback (UF) [20] dataset to construct different users - a similar approach to prior work [58]. We construct a dataset with two users, UF-P-2, who prefer either helpfulness or honesty (hidden attribute), i.e. they generate preferences using the scores for their chosen attributes. To test the ability of VPL to model more users than has been previously attempted in the literature, we create UF-P-4, which uses the fine-grained scores over all the four attributes in the UF dataset [20] to create a dataset with four different users. Here, the users are divergent because given two responses, users following different objectives can have opposite preferences - some users can prefer the helpful response, while others prefer the honest response. In some cases, these responses are at odds. To ensure that successfully modeling the data requires ftiting divergent preferences, we flitered out the responses where all users agree or are indecisive to primarily focus on multimodal preference modeling, and remove degenerate context queries that provide no information about the user distribution. However, in UF-P-4 the context can still contain queries where at least two users overlap. Thus, this provides a dataset to evaluate VPL in cases where different users agree on some responses, but not all of them. Finally, to generate the context $\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N}$ for inferring latent distributions, for each prompt and response pair, we sample different data points from a smaller subset of size $K$ from the dataset $\\;K\\,=\\,100$ for GPT2 and 16 for Llama2). For a deployed LLM system, this is analogous to having a known set of survey questions from which the user must answer a subset of 2-8 questions to personalize the model\u2019s behavior to their needs. ", "page_idx": 8}, {"type": "table", "img_path": "gRG6SzbW9p/tmp/5708183b4bdc8b0fd6b73b2f8de5f8c8c0b68124b55e1ce94e7cf78252ab5a79.jpg", "table_caption": ["Table 1: We compare the accuracy of different reward models trained on the two datasets. We report the mean and standard deviation of performance of GPT2-based models on three seeds, and one seed for Llama models. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7.2 Does VPL help to make LLM reward models more pluralistically aligned? ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Table 1, we see that VPL is able to learn a more accurate reward model across all the datasets, capturing the multi-modality in the language preference data. This indicates that VPL can infer the latent representation of the user\u2019s preferences $z$ from a few annotated samples, and successfully adapt the reward model. In contrast, the baselines\u2014including the BTL model typically used in widely deployed RLHF [52] models\u2014are unable to fti the datasets because they do not account for divergent preferences. Because the datasets are imbalanced, the baselines can sometimes perform better than random guessing by ftiting only the preferences of the majority group, and thus, the performance on Pets (Full) appears high, even though the baselines fail to adapt to the divergent preferences. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we presented VPL, a technique for pluralistic alignment of preference-based RLHF models via variational inference. We show that VPL can capture diverse preferences and be used for steerable personalized model learning while capturing uncertainty in preferences. We discussed practical considerations for scaling VPL for LLMs and policy learning and showed results across simulated control problems and LLM-based RLHF, significantly outperforming current RLHF techniques. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. A key limitation of this work is that as yet, realistic preference datasets containing the opinions of diverse users do not yet exist at scale. This limitation necessitated creating our synthetic preference datasets. Although this was also the approach taken in prior work on personalized RLHF (e.g. [58, 73]), an important direction for future work will be to apply VPL to more realistic preference data from diverse groups of users. Further, our current experiments on the UltraFeedback dataset assume that when adapting to a new user, we could ask them for preferences over samples from a fixed set of survey questions. In the future, it would be useful to relax this assumption and apply VPL to preferences obtained naturally during a conversation with the user. ", "page_idx": 9}, {"type": "text", "text": "We believe VPL could offer promising safety beneftis in addition to modeling diverse user preferences. Uncertainty detection may help prevent jailbreak attacks caused by conflicting rewards [58]. By capturing uncertainty in the distribution of user preferences, VPL could enhance safety by stopping or refusing to respond when uncertainty cannot be sufficiently reduced [36]. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank members of the WEIRD and Social RL for the valuable discussions and feedback on the project and manuscript. This project was partly supported by NSF grant no. 2212310. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] R. Akrour, M. Schoenauer, and M. Sebag. April: Active preference learning-based reinforcement learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23, pages 116\u2013131. Springer, 2012.   \n[2] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \n[3] A. Bajcsy, D. P. Losey, M. K. O\u2019Malley, and A. D. Dragan. Learning from physical human corrections, one feature at a time. In Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, pages 141\u2013149, 2018.   \n[4] C. Baumler, A. Sotnikova, and H. Daum\u00e9 III. Which examples should be multiply annotated? active learning when annotators may disagree. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 10352\u2013 10371, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-acl.658. URL https://aclanthology.org/2023.findings-acl.658.   \n[5] P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. LLM2Vec: Large language models are secretly powerful text encoders. arXiv preprint, 2024. URL https://arxiv.org/abs/2404.05961.   \n[6] K. Bhatia, A. Narayan, C. M. De Sa, and C. R\u00e9. Tart: A plug-and-play transformer module for task-agnostic reasoning. Advances in Neural Information Processing Systems, 36:9751\u20139788, 2023.   \n[7] T. Bhattacharjee, E. Gordon, et al. Is more autonomy always better? exploring preferences of users with mobility impairments in robot-assisted feeding. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction, 2020.   \n[8] E. Biyik and D. Sadigh. Batch active preference-based learning of reward functions. In 2nd Annual Conference on Robot Learning, CoRL 2018, Z\u00fcrich, Switzerland, 29-31 October 2018, Proceedings, volume 87 of Proceedings of Machine Learning Research, pages 519\u2013528. PMLR, 2018.   \n[9] E. B\u0131y\u0131k, M. Palan, N. C. Landolf,i D. P. Losey, and D. Sadigh. Asking easy questions: A user-friendly approach to active reward learning. arXiv preprint arXiv:1910.04365, 2019.   \n[10] E. B\u0131y\u0131k, D. P. Losey, M. Palan, N. C. Landolf,i G. Shevchuk, and D. Sadigh. Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences. The International Journal of Robotics Research, 41(1):45\u201367, 2022.   \n[11] D. Blei, A. Kucukelbir, and J. McAuliffe. Variational inference: A review for statisticians. arXiv preprint arXiv:1601.00670, 2016.   \n[12] A. Bobu, D. R. R. Scobee, J. F. Fisac, S. S. Sastry, and A. D. Dragan. LESS is more: Rethinking probabilistic models of human behavior. In T. Belpaeme, J. E. Young, H. Gunes, and L. D. Riek, editors, HRI \u201920: ACM/IEEE International Conference on Human-Robot Interaction, Cambridge, United Kingdom, March 23-26, 2020, pages 429\u2013437. ACM, 2020. doi: 10.1145/ 3319502.3374811. URL https://doi.org/10.1145/3319502.3374811.   \n[13] R. Boldi, L. Ding, L. Spector, and S. Niekum. Pareto-optimal learning from preferences with hidden context. ArXiv, abs/2406.15599, 2024. URL https://api.semanticscholar.org/ CorpusID:270703216.   \n[14] A. Bose, M. Curmei, D. L. Jiang, J. Morgenstern, S. Dean, L. J. Ratliff, and M. Fazel. Initializing services in interactive ml systems for diverse users, 2023. URL https://arxiv.org/abs/ 2312.11846.   \n[15] R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324, 1952. doi: 10.2307/2334029.   \n[16] S. Chakraborty, J. Qiu, H. Yuan, A. Koppel, F. Huang, D. Manocha, A. S. Bedi, and M. Wang. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. arXiv preprint arXiv:2402.08925, 2024.   \n[17] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Neural Information Processing Systems (NeurIPS), 2017.   \n[18] G. Cideron, S. Girgin, A. Raichuk, O. Pietquin, O. Bachem, and L. Hussenot. vec2text with round-trip translations. arXiv preprint arXiv:2209.06792, 2022.   \n[19] V. Conitzer, R. Freedman, J. Heitzig, W. H. Holliday, B. M. Jacobs, N. Lambert, M. Moss\u00e9, E. Pacuit, S. Russell, H. Schoelkopf, et al. Social choice for ai alignment: Dealing with diverse human feedback. arXiv preprint arXiv:2404.10271, 2024.   \n[20] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.   \n[21] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.   \n[22] H. Daume and K. Heller. Neurips 2018 demographics and inclusion survey: Summary of responses. Advances in Neural Information Processing Systems, 32, 2018.   \n[23] V. Dumoulin, D. D. Johnson, P. S. Castro, H. Larochelle, and Y. Dauphin. A density estimation perspective on learning from pairwise human preferences. ArXiv, abs/2311.14115, 2023. URL https://api.semanticscholar.org/CorpusID:265445749.   \n[24] K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   \n[25] S. Feng, T. Sorensen, Y. Liu, J. R. Fisher, C. Y. Park, Y. Choi, and Y. Tsvetkov. Modular pluralism: Pluralistic alignment via multi-llm collaboration. ArXiv, abs/2406.15951, 2024. URL https://api.semanticscholar.org/CorpusID:270703169.   \n[26] E. Fleisig, R. Abebe, and D. Klein. When the majority is wrong: Modeling annotator disagreement for subjective tasks. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6715\u20136726, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.415. URL https://aclanthology.org/2023.emnlp-main.415.   \n[27] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[28] J. F\u00fcrnkranz, E. H\u00fcllermeier, W. Cheng, and S.-H. Park. Preference-based reinforcement learning: a formal framework and a policy iteration algorithm. Machine learning, 89:123\u2013156, 2012.   \n[29] I. Gabriel. Artificial intelligence, values, and alignment. Minds and Machines, 30(3):411\u2013437, Sept. 2020. ISSN 1572-8641. doi: 10.1007/s11023-020-09539-2. URL http://dx.doi.org/ 10.1007/s11023-020-09539-2.   \n[30] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 1856\u20131865. PMLR, 2018. URL http://proceedings.mlr. press/v80/haarnoja18b.html.   \n[31] M. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and H. Van Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.   \n[32] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[33] J. Jang, S. Kim, B. Y. Lin, Y. Wang, J. Hessel, L. Zettlemoyer, H. Hajishirzi, Y. Choi, and P. Ammanabrolu. Personalized soups: Personalized large language model alignment via posthoc parameter merging. arXiv preprint arXiv:2310.11564, 2023.   \n[34] J. Ji, T. Qiu, B. Chen, B. Zhang, H. Lou, K. Wang, Y. Duan, Z. He, J. Zhou, Z. Zhang, F. Zeng, K. Y. Ng, J. Dai, X. Pan, A. O\u2019Gara, Y. Lei, H. Xu, B. Tse, J. Fu, S. McAleer, Y. Yang, Y. Wang, S.-C. Zhu, Y. Guo, and W. Gao. Ai alignment: A comprehensive survey, 2023.   \n[35] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun, Y. Wang, and Y. Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024.   \n[36] K. Kang, E. Wallace, C. Tomlin, A. Kumar, and S. Levine. Unfamiliar finetuning examples control how language models hallucinate. arXiv preprint arXiv:2403.05612, 2024.   \n[37] C. Kim, J. Park, J. Shin, H. Lee, P. Abbeel, and K. Lee. Preference transformer: Modeling human preferences using transformers for RL. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Peot1SFDX0.   \n[38] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.   \n[39] H. R. Kirk, B. Vidgen, P. R\u00f6ttger, and S. A. Hale. Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. arXiv preprint arXiv:2303.05453, 2023.   \n[40] H. R. Kirk, A. Whitefield, P. R\u00f6ttger, A. Bean, K. Margatina, J. Ciro, R. Mosquera, M. Bartolo, A. Williams, H. He, et al. The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. arXiv preprint arXiv:2404.16019, 2024.   \n[41] I. Kostrikov, A. Nair, and S. Levine. Offilne reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=68n2s9ZJWF8.   \n[42] C. Laidlaw and A. Dragan. The boltzmann policy distribution: Accounting for systematic suboptimality in human models. arXiv preprint arXiv:2204.10759, 2022.   \n[43] C. Laidlaw and S. Russell. Uncertain decisions facilitate better preference learning. Advances in Neural Information Processing Systems, 34:15070\u201315083, 2021.   \n[44] N. Lambert, T. K. Gilbert, and T. O. Zick. The history and risks of reinforcement learning and human feedback. 2023. URL https://api.semanticscholar.org/CorpusID:264405813.   \n[45] J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.   \n[46] X. Li, Z. C. Lipton, and L. Leqi. Personalized language modeling from personalized human feedback. ArXiv, abs/2402.05133, 2024. URL https://api.semanticscholar.org/CorpusID: 267547503.   \n[47] M. Liu, M. Zhu, and W. Zhang. Goal-conditioned reinforcement learning: Problems and solutions. arXiv preprint arXiv:2201.08299, 2022.   \n[48] N. Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022.   \n[49] R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, A. Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.   \n[50] V. Myers, E. B\u0131y\u0131k, and D. Sadigh. Active reward learning from online preferences, 2023.   \n[51] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters. An algorithmic perspective on imitation learning. Found. Trends Robotics, 7(1-2):1\u2013179, 2018. doi: 10.1561/ 2300000053. URL https://doi.org/10.1561/2300000053.   \n[52] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022.   \n[53] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.   \n[54] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. arXiv preprint, 2019.   \n[55] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[56] X. Ran, M. Xu, L. Mei, Q. Xu, and Q. Liu. Detecting out-of-distribution samples via variational auto-encoder with reliable uncertainty estimation. Neural Networks, 145:199\u2013208, 2022.   \n[57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.   \n[58] A. Siththaranjan, C. Laidlaw, and D. Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in rlhf. arXiv preprint arXiv:2312.08358, 2023.   \n[59] T. Sorensen, J. Moore, J. Fisher, M. Gordon, N. Mireshghallah, C. M. Rytting, A. Ye, L. Jiang, X. Lu, N. Dziri, et al. A roadmap to pluralistic alignment. arXiv preprint arXiv:2402.05070, 2024.   \n[60] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[61] G. Swamy, C. Dann, R. Kidambi, Z. S. Wu, and A. Agarwal. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.   \n[62] M. Torne, M. Balsells, Z. Wang, S. Desai, T. Chen, P. Agrawal, and A. Gupta. Breadcrumbs to the goal: Goal-conditioned exploration from human-in-the-loop feedback, 2023.   \n[63] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[64] Y. Wang, Q. Liu, and C. Jin. Is rlhf more difficult than standard rl? a theoretical perspective. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 76006\u201376032. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ efb9629755e598c4f261c44aeb6fde5e-Paper-Conference.pdf.   \n[65] A. Wilson, A. Fern, and P. Tadepalli. A bayesian approach for policy learning from trajectory preference queries. Advances in neural information processing systems, 25, 2012.   \n[66] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 2023.   \n[67] Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M. Ostendorf, and H. Hajishirzi. Fine-grained human feedback gives better rewards for language model training. ArXiv, abs/2306.01693, 2023. URL https://api.semanticscholar.org/CorpusID: 259064099.   \n[68] P. Xavi, U. Eric, A. Szot, M. D. Cote, R. Partsey, J. Yang, R. Desai, A. W. Clegg, M. Hlavac, T. Min, T. Gervet, V. Vondrus, V.-P. Berges, J. Turner, O. Maksymets, Z. Kira, M. Kalakrishnan, J. Malik, D. S. Chaplot, U. Jain, D. Batra, A. Rai, and R. Mottaghi. Habitat 3.0: A co-habitat for humans, avatars and robots, 2023.   \n[69] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824\u20135836, 2020.   \n[70] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages 726\u2013747. PMLR, 2021.   \n[71] S. Zhang, D. Yu, H. Sharma, Z. Yang, S. Wang, H. Hassan, and Z. Wang. Self-exploring language models: Active preference elicitation for online alignment. ArXiv, abs/2405.19332, 2024. URL https://api.semanticscholar.org/CorpusID:270095059.   \n[72] S. Zhao, J. Dang, and A. Grover. Group preference optimization: Few-shot alignment of large language models. arXiv preprint arXiv:2310.11523, 2023.   \n[73] Z. Zhou, J. Liu, C. Yang, J. Shao, Y. Liu, X. Yue, W. Ouyang, and Y. Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708, 2023.   \n[74] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Didactic example on a toy reward learning problem. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To more carefully understand the behavior of VPL empirically, we construct a didactic example [23] as shown in Figure 6. In this problem, let us consider a mixture of $M$ different annotators providing preferences, where each annotator $i$ has a reward function specified by $\\mathscr{N}(\\mu_{i},\\sigma_{i})_{i=1}^{M}$ that they use to assign binary preferences. Mathematically, we sample the preferences from a mixture of Gaussians: ", "page_idx": 14}, {"type": "equation", "text": "$$\np(s_{A}\\succ s_{B}\\mid i)=\\frac{e^{r_{i}(s_{A})}}{e^{r_{i}(s_{A})}+e^{r_{i}(s_{B})}};\\ \\mathrm{where}\\ e^{r_{i}}\\sim\\frac{1}{\\sigma_{i}\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_{i}}{\\sigma_{i}}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Multi-annotator preferences are simulated by sampling an annotator from this mixture distribution and then assigning binary preferences according to the chosen reward function. We train VPL as described in Section 4 to recover the underlying distribution over reward functions. As expected in Figure 6, standard RLHF [17] averages over the different modes since it can only represent a single reward function. While prior work in accounting for hidden context in RLHF (DPL [58]) can learn the uncertainty in the reward functions due to hidden context, it is not able to accurately disambiguate different modes. Meanwhile, VPL is able to infer the underlying context using the approximate posterior $q_{\\psi}$ and the recover the individual reward modes through the latent-conditional reward function $r(s,z)$ . ", "page_idx": 14}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/bd974323c354c00da05250eab217d1e3e62fe808c61be120b714580b74ce8381.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: Didactic experiments comparing standard BTL [17], DPL [58] and VPL (Ours). Four Gaussian reward functions generate different binary preference data. The traditional BTL approach [17] averages the different modes, and DPL [58] captures the uncertainty in the rewards due to the multi-modality but cannot accurately predict the true modes. VPL (ours) infers the hidden latent as described in Section 4 and recovers the individual distribution of reward functions. ", "page_idx": 14}, {"type": "text", "text": "A.2 Does scaling rewards help improve performance? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To avoid the problem of high variance rewards (Section 4), we compare the performance of $\\mathrm{\\DeltaVPL_{no-norm}}$ with $\\mathrm{VPL}+\\mathrm{SPO}$ . We further compare against two normalizing schemes: VPLbatchnorm where the rewards for each latent is normalized by the mean rewards across a set of state samples i.e. $\\begin{array}{r}{r^{\\prime}(s,z)\\,=\\,\\frac{r(s,z)}{\\frac{1}{M}\\sum_{s^{\\prime}\\in S}r(s^{\\prime},z)}}\\end{array}$ M1  s\u2032\u2208S r(s\u2032,z), and VPLmax-norm where all the rewards in the offline dataset are normalized by the maximum reward for any latent. ", "page_idx": 14}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/3dd7e0be236e41f5edac16bd837b57b3898de17e71cd11322360ed253abd322e.jpg", "img_caption": ["Figure 7: Comparing scaling methods on Maze-Navigation. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "In Section 4.1, we discuss the problem of generating scaled rewards from latent variable-based reward models and compare the performance across multiple baselines discussed above. As shown in Figure 7, the batch norm scaling generates highly biased estimates of the rewards, which is catastrophic for the method. However, VPL methods have decent performance at test-time, but are an unprincipled approach to the scaling problem. Our $\\mathrm{SPO}+\\mathrm{VPL}$ presents a general method for estimating normalized rewards. Thus, in Figure ", "page_idx": 14}, {"type": "text", "text": "7 we can see that our method outperforms the baseline approaches in terms of success rate. The baselines have an unscaled or a biased estimate of the multi-modal rewards leading to sub-optimal performance. For the ravens-manipulation environment, the dataset doesn\u2019t contain sub-optimal trajectories, VPL (with max norm) performs comparably. ", "page_idx": 14}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/f9b4df6418689a13e5006a4f7f28a23d53c3020b35a5f236b1572e60421c2552.jpg", "img_caption": ["well, where VPL $+{\\bf S}{\\bf P}{\\bf O}$ performs better than VPL. ", "Figure 8: Comparison of VPL\u2019s scalability on different tasks and user bases. ", "pabilities shown in Table 1 and Figure 4. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 Does VPL scale with the number of diverse users? ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In order to test the effectiveness of VPL in scaling to a control problem with larger modes of underlying preferences, we create a task with ten underlying locations that the users could prefer. So, the challenge here is to disambiguate the user preference among a larger space of possible goals and condition the policy to navigate successfully to the goal. Figure 8a shows that our method is able to navigate to the individual goals with a higher success rate, whereas the baseline DPL model [17] collapses to a single user mode. This demonstrates the beneftis of scaling VPL to a setting with a large population of diverse users. To test the method at a larger scale, we increase the number of users in the Habitat-Rearrange tasks to 100. It is a combinatorial problem as the users provide rankings over five locations, so all possible users/orderings are 5!. We observe in Figure 8b that VPL significantly outperforms the baselines in inferring the user preference, and steering the robot policy accordingly. ", "page_idx": 15}, {"type": "text", "text": "A.4 How does context length affect VPL? ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Habitat-TidyBot, the robot\u2019s task is to relocate an object in the kitchen (fork, knife, spoon, bowl, pitcher) to a specific location based on user preferences. The user can prefer to arrange objects according to the object function (i.e. kitchenware or tableware), or material (metal or plastic). The agent has to query the location of some of the objects, infer the user type, and arrange the requested object accordingly. Fig 4 shows that the baselines converge to the correct location for the objects agreed upon by the different users, but converge to one mode for the divergent choices. Meanwhile, VPL infers and adapts to user preferences and aligns with humans perfectly. However, one caveat to this is the context length i.e. the number of queries to each user. In Figure 9 we show that as the query length increases VPL can identify users with higher accuracy and achieve higher performance. This happens because certain queries are uninformative about the user preferences (such as things users agree on), and thus, it generates a high variance posterior. As a result, longer context length increases the probability of useful queries, which enables low variance posterior inference and improved alignment during decision-making. In Section 4.2, we also show how to use an active learning approach to generate queries based on a max information gain objective. For LLMs, in Figure 13, we show that a higher context length also provides robustness to noise in the annotated queries. ", "page_idx": 15}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/e16ef6e6d68e93385bda81d9df40da04871c864860e117ed4184d9f7ec879901.jpg", "img_caption": ["Figure 9: In the Habitat-TidyBot tasks, the agent has to relocate objects in the house based on the user\u2019s preferences. We show that the accuracy of user modeling and choosing the right location for the object increases with the number of queries the agent makes to infer the latent distribution. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.5 Visualizing Learned Rewards and Embeddings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We visualize the rewards generated by the baseline and the latent conditioned rewards models on the diverse domains described in Section 6. We observe that VPL reconstructs the multi-modal reward functions, based on the inferred latent distribution. Figure 3 shows that the BTL models averages the reward over the user-preferred goals, while VPL accurately reconstructs the individual user-specific rewards. Figure 10 shows that for optimal trajectories solving the task, VPL can accurately match the ground rewards for the two modes. At the same time, DPL [58] predicts high-variance rewards for both cases due to the inherent multi-modality. Finally, in Figure 11 we see that the majority of users consider the desk to be the preferred location of the bowl, and standard BTL models converge to the majority population. Meanwhile, VPL can generate user-specific rewards, satisfying all the user groups. ", "page_idx": 16}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/f6c44d3d0a6c4ad788ab494390368712ba36805101c50bb4b377e5c12db19427.jpg", "img_caption": ["Figure 10: In the Ravens-Manipulation task, we compare the predicted rewards for states $s_{t}$ along timesteps $t$ in oracle trajectories to either of the goals the user prefers. VPL (Ours) can learn the individual reward functions for the two different (closely matching the ground truth rewards for both users) leading to more performant policies (see Figure 4), while DPL [58] learns a high variance reward function due to the multi-modality. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/a766fe974be2976e90f0019cd7f5f4cf66cfe3dac741bc01ea4d8c9a9ce855b9.jpg", "img_caption": ["Figure 11: In the Habitat-Rearrange task, annotators have rankings, i.e. preferences over the different locations they want the robot to place their bowl in their home. Accordingly, (a) shows the rewards associated with a particular location (\"column\") for each annotator (\"row\"). We see that a majority of the users rank the desk as the most preferred location. Consequently, in (b), unimodal BTL converges to this majority preference ignoring other users. However, in (c) we see that VPL accurately reconstructs diverse preferences and aligns to all five users. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/41f6923d0163a8cf13a44ac8a41e860f50e9f3fb9d535dc974f20cbaa7ffc889.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 12: We train GPT2-based VPL, on the UltraFeedback-P dataset. In this plot, we visualize the T-SNE features of the latent distribution $z$ produced by the encoder $q_{\\psi}$ on a set of annotated prompts and responses $\\{s_{A}^{i},s_{B}^{i},y^{i}\\}_{i=1}^{N}$ from the two users in the dataset. We see that the encoder clusters the users in the latent space, allowing the decoder to personalize the reward models according to multiple objectives preferred by the diverse users belonging to a cluster. ", "page_idx": 17}, {"type": "text", "text": "In Figure 12, we show that the VPL-Encoder effectively learns a latent space with clusters that correspond to the user types in the dataset. Previous work has shown that attempting to compress information within an LLM into a single bottleneck embedding layer can hinder performance [18]. However, using the architectural design of VPL as well as user-context data augmentation, VPL is able to learn a compressed user representation that accurately separates users according to their preferences, from only a few preference labels. ", "page_idx": 17}, {"type": "text", "text": "A.7 Does VPL scale to real-world settings with larger and noisy users? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A key assumption in our approach is that context questions accurately represent individual users without noise in the underlying dataset. To test VPL\u2019s robustness to noisy context labels at test time, we injected noise by filpping the questions answered by each user and evaluated the trained model\u2019s accuracy in predicting rewards. This experiment can help us evaluate how well the model would generalize to new users that have similar preferences to those experienced during training, but may not answer questions in exactly the same way. Figure 13 illustrates that VPL is able to outperform prior work even when $25\\%$ of preference labels are flipped at test time. Notably, we observed that longer context lengths result in more accurate reward modeling, even with higher noise injection. This is because the encoder can generate more accurate inferences of the latent distribution when provided with more user information through a larger number of context questions and response labels. ", "page_idx": 17}, {"type": "image", "img_path": "gRG6SzbW9p/tmp/affda58f6ca0b3b97b869a616216694711e4feea6bb2905b7269e07d08f3eb5b.jpg", "img_caption": ["Figure 13: Reward accuracy with varying levels of noisy labels introduced at test time. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We note further that when $50\\%$ of the preference labels are flipped, the context preference queries provide no information about the user, and the performance of VPL is equivalent to the baseline BTL model. This mirrors the additional findings presented in Appendix A.8, which demonstrate that when VPL is trained on a unimodal preference language dataset, it gives equivalent performance to BTL. Essentially, when extra preference data is available for a user, VPL can personalize the reward model effectively and attain higher performance. But without that information, it performs just as well as the default BTL model. These findings, as shown in Table 1,Figure 13, and the Appendix, demonstrate VPL\u2019s effectiveness in handling multi-modality and noise in large-scale preference datasets, without compromising performance even when no additional preference information is available. This capability suggests the potential for VPL to contribute to the development of next-generation LLMs that are more personalized, inclusive, and efficient. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "A.8 Does VPL work under uni-modal settings? ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To test that the introduction of a variational framework does not decrease performance in settings where all users have single/aligned preferences, we run an experiment on the UF-P-4 dataset, where we considered the preferences of a single user (preferring the model to be \u201chonest\u201d over all other attributes) to analyze the single-modal case as suggested. The standard BTL model gives a $77.04\\%$ eval accuracy while our VPL model gives a $77.16\\%$ eval accuracy. Our model matches the baseline performance, indicating no drop in performance when using VPL compared to traditional RLHF over an unimodal dataset. ", "page_idx": 18}, {"type": "text", "text": "A.9 Social Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This work has a clear social impact when deployed on user-facing systems like LLMs or household robots. In pluralistic alignment, we assume that some differences in user preferences reflect divergent but equally valid perspectives on which moral or cultural values an LLM should align to; for example, individuals from one culture may hold collectivist moral values, while another culture is more individualist. Both value systems should be respected, and as such LLMs should be able to recognize and adapt to the values held by a particular user. However, the personalized model could potentially either be sycophantic or align with adversarial users, which is undesirable. This raises very interesting questions, such as: At what point should the LLM embrace a more universal set of values? How can we detect when such a point has occurred in a particular conversation? The probabilistic framework of the user distribution could allow us to identify low probability or rare behavior, and also the distributional nature of reward functions can help us point out responses where the users are divergent (maybe signifying disagreement). Additionally, a model could flexibly switch between adhering to the user\u2019s personal preferences and conforming to a more universal perspective on topics where it could be biased, or is sensitive to jailbreak [2]. Taking inspiration from Constitutional AI [2], we can allow a system designer to specify the topics for which the LLM should not engage in user personalization. Overall, this presents an exciting future research direction toward building safe personalized LLMs. ", "page_idx": 18}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Task Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We evaluate our methods on three simulated control environments. ", "page_idx": 18}, {"type": "text", "text": "Maze-Navigation. This task is adapted from the \"maze2d-medium-v2\" environment from the D4RL benchmark [27]. The observation space is the position and velocity of the robot $(p_{x},p_{y},p_{z})$ , and the pointmass is controlled using torque control. In this environment, point mass doesn\u2019t have access to a goal, and diverse users guide the agent to (two or ten) different locations in the maze, marked with their preferred colors. The users label the preferences over two states based on the shortest path to the goal from each state, i.e, the user prefers states closer to their preferred color location. The offilne dataset for IQL is collected using the waypoint controllers provided in the D4RL benchmark. For each episode, the agent is spawned at a random location in the maze, interacts with a random user, and navigates to the goal based on the learned reward model and corresponding policy trained on the offline dataset. The oracle reward function is the optimal Q-value of the state, generated using a dynamic programming solution, which is available in D4RL. ", "page_idx": 18}, {"type": "text", "text": "Ravens-Manipulation This task is adapted from the ravens benchmark [70]. The observation space is the 3-D position of the object, the 3-D position of the end effector, and the grasp state of the object, i.e., $(e e_{x},e e_{y},e e_{z},p_{x},p_{y},p_{z},g r a s p)$ . The agent commands absolute positions for the 3-DOF robot arm in end-effector space i.e. $(e e_{x}^{\\prime},e e_{y}^{\\prime},e e_{z}^{\\prime})$ . This setup resembles how a robot arm would infer user preferences to organize a dining table. The users prefer two different locations for the box spawned at a random location at the beginning of each episode. To collect offline data, we use a motion planning oracle with some added noise, which tries to pick the box and place it randomly at one of the two locations. The oracle reward function is as follows: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{reward}=\\frac{1}{100}\\left\\{\\begin{array}{l l}{100}\\\\ {5}\\\\ {2+\\mathrm{exp}(-\\mathrm{goal\\_dist})}\\\\ {\\mathrm{exp}(-\\mathrm{gripper\\_dist})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Habitat-Rearrange This is a task based on the Meta Habitat simulator [68]. Here, the objective for the Mobile Manipulator is to pick a bowl and place it at the user\u2019s preferred location in the home. However, the exact location is underspecified and needs to be inferred from the user-annotated preferences. The robot uses a motion primitive to navigate and place the object at five possible locations (\u2019desk\u2019, \u2019room\u2019, \u2019dining\u2019, \u2019coffee table\u2019, \u2019sofa\u2019). This problem is reduced to a discrete one-step problem, where the robot has to reason about the best possible location to put the bowl. For ranking the states, the users are generated by randomly choosing a five random orderings of the locations, each corresponding to an individual user. At test time, the agent is greedy and chooses the location with the maximum inferred reward from the learned reward model. ", "page_idx": 19}, {"type": "text", "text": "Habitat-TidyBot This task is based on Meta Habitat [68] and inspired from the TidyBot task [66]. In this environment, there are 5 objects in the kitchen (spoon, knife, plate, bowl, and spatula). Each user has their preferences for sorting the objects according to particular attributes (material such as metal, plastic or function i.e. tableware or cooking ware). The robot observes or queries the user for the existing location of a subset of objects, then rearranges the misplaced object according to the inferred user preference and greedily selects the goal with the higher reward. The baseline here would converge to sort the objects according to one attribute only, while VPL would infer the latent, and choose the correct location for the given object. ", "page_idx": 19}, {"type": "text", "text": "B.2 LLM Preference Learning Dataset Descriptions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Pets This dataset is generated synthetically to specifically study the ability of models to perform divergent preference modeling. The goal here is to choose between different types of pets. For each animal, including bird, cat, dog, and rabbit, we use GPT-4 to generate 100 sentences that describe these kinds of pets. Then we define two user groups based on their preference order over the pets. So as to have mix of contexts where users agree and disagree, we construct a preference ordering where both groups like birds the most and rabbits the least. One group of users prefers dogs to cats while another group disagrees and prefers cats to dogs. That is to say, among all 6 comparisons between two kinds of pets, only one pair (dogs versus cats) leads to divergent opinions, while the users agree on other comparisons (birds better than dogs, dogs better than rabbits and so on). This tests the ability of the preference models to capture multimodality, even when the users do agree on some set of preferences. ", "page_idx": 19}, {"type": "text", "text": "We then construct the Pets dataset by clustering the prompt and ranked responses according to the group of preferences that they align with. To generate the Pets dataset, we randomly sample a pair of different pets as well as two corresponding sentences, and then label them to be \u201cchosen\u201d and \u201crejected\u201d according to the preference of either \u201cdog group\u201d or \u201ccat group\u201d. The prompt is fixed to be \u201cHuman: Please talk about one kind of pet.\u201d After all the chosen/rejected pairs are generated, we randomly sample 1-4 pairs of responses from the same user that are comparing dogs and cats for use as the context to the variational encoder. These are informative since a user\u2019s choice over these contexts can clearly express the user\u2019s preference group. In this way, we can generate a \u201cfull\" Pets dataset, and based on that we fliter out a \u201cdivergent\" split that only contains controversial data points (comparing dogs and cats). This dataset is meant as a didactic test for language modeling capabilities, but scalability is further tested in the following section with the UltraFeedback-P dataset. ", "page_idx": 19}, {"type": "text", "text": "Here we show an example data point for Pets. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Prompt: \u201cHuman: Please talk about one kind of pets.\u201d \u2022 Response A: \u201cCats communicate through vocalizations.\u201d (Rejected) ", "page_idx": 19}, {"type": "text", "text": "\u2022 Contexts: [\"chosen\": \"Cats have a preference for certain types of litter.\",\"rejected\":\"Dogs enjoy exploring their surroundings.\"], [\"chosen\":\"Cats have a preferred scratching substrate.\",\"rejected\": \"Dogs have a unique personality.\"] ", "page_idx": 20}, {"type": "text", "text": "UltraFeedback-P The original UltraFeedback dataset [20] contains 64,000 pairs of responses, where each response is evaluated across four dimensions: helpfulness, honesty, instruction following, and truthfulness. Instead of using the averaged scores, we leverage the fine-grained scores to create a dataset with multi-modal, divergent user preferences. We assume that each user ranks responses based on the score for only one attribute. For example, one user might prioritize helpfulness, while another values honesty the most. Consequently, the same response pair can receive divergent labels from different users depending on their chosen attribute. Following prior work [2, 58], we only consider the first two attributes: helpfulness and honesty, to create the dataset UF-P-2. Next, we fliter out response pairs where the two users agree or are indecisive, since we care about multimodal reward modeling in this work, leaving approximately 4,000 prompt and response pairs labeled by each user. From this dataset, we randomly sample a smaller subset of K data points, which serves as our context sample set to help identify the latents corresponding to a particular user. For each prompt and pair of responses, we get 8 samples from the context set to characterize the particular user providing responses. These samples, along with the prompt, responses (with preferences over which response is chosen), form a complete data point (context $^+$ prompt $^+$ responses). This data point is then used to infer the latent distribution and personalized reward for that specific user. Additionally, we introduce a version of the dataset UF-P-4, where we consider all four attributes as distinct users. In the filtering step, we first filter out the response pairs where all of the four users agree on. Then within the data subset of each user, we filter out the response pairs where that particular user gives equal ratings to the two responses. It results in a dataset of approximately 7,500 prompt and response pairs labeled by each user. The way we generate the context is similar to UF-P-2, except that we randomly pick a number from 2 to 8 to be the number of samples to form the context. Note that in UF-P-4, it is still possible that a question from the context set cannot help with distinguish between two users, because we only fliter out the data points that are agreed by all users. This presents a large, diverse and challenging benchmark for pluralistic alignment, created from the existing available open-source datasets [20]. ", "page_idx": 20}, {"type": "text", "text": "B.3 Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Learned Prior. While the methods described in the methods section only learn the reward model and the latent encoder, we can also use a prior $p(z)$ as is common in variational inference methods. We assume that our prior is a multi-variate Gaussian with mean $\\mu$ and covariance $\\Sigma=\\mathrm{diag}(\\sigma\\sigma^{T})$ , where $\\mu,\\sigma\\in\\mathrm{R}^{d}$ ; where $d$ is the dimension of the latent embedding. In all experiments, they are initialized from a standard Gaussian. However, in our control experiments, we observed that using a learned Gaussian i.e. setting $\\mu$ and $\\sigma$ to learnable parameters under the ELBO objective in Eq. 3 improved performance and stability during training. ", "page_idx": 20}, {"type": "text", "text": "LLM Embeddings. In our experiments, we use the embedding from the last token as our encoding for the prompt+response input. However, we also tried llm2vec [5] and a weighted pooling mechanism [48]. However, we find that using the last token embedding as inputs to the encoder and for predicting the rewards performs the best. ", "page_idx": 20}, {"type": "text", "text": "iwnes psiaremdp lbey Active Learning Complexity. In our active inference technique, we use a sampling-based method [ 8q]u teor yg ebnatecrahtees  tohfe  saicztei , qwuehreirees ri st hneu mmobedre lo. f Gainvneont aat idoantsa speet ro bf aDtc qh uwerei egse $(s_{A}^{i},s_{B}^{i})_{i=1}^{|K|}$ $S$ $Q$ $Q$ (total possible combinations are ${}^{K}{C}_{N}^{\\phantom{\\dagger}}{\\phantom{\\dagger}}$ ). Here, $Q\\in[2,8]$ , so we need to perform $\\mathrm{O}(\\mathrm{S}^{\\ast}\\,\\mathrm{Q})$ passes over the model with batch size $2^{Q}\\sim[4,256]$ . Furthermore, this process only needs to be performed once after the model is trained to obtain the most discriminative set of queries for the given model. Finally, whenever a new user interacts with the system, we need to get labels on the actively inferred queries (usually 2-4) but do not require any additional passes over the query dataset. In our experiments (Figure 5), we show that using active learning allows the model to achieve comparable performance with fewer queries $(\\sim2)$ , as compared to randomly sampled larger $(\\sim8)$ queries. ", "page_idx": 20}, {"type": "text", "text": "B.4 Hyperparameters ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "gRG6SzbW9p/tmp/17cf5840e08e068f5d1639f3dd751479008393e15f3a20aa6a6e6ae67890b9e5.jpg", "table_caption": ["Table 2: Hyperparameters for learning reward models using VPL. We sweep over these values and report the best results on 5 seeds. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "gRG6SzbW9p/tmp/70056938f6a61ec275329c9a571ba882fd7d9ac76de4902a0fc16436e817634d.jpg", "table_caption": ["Table 3: Hyperparameters for IQL. We use the same parameters across all experiments "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "gRG6SzbW9p/tmp/f2bc90883e3945eba0fc1913bce66f784736250bb6b9cdafa18943f7daa50cee.jpg", "table_caption": ["Table 4: Hyperparameters for LLM experiments "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C Algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Algorithm 1 Learning Multimodal Reward Functions using VPL ", "page_idx": 22}, {"type": "text", "text": "Require: Preference Data $\\{(s_{A}^{i},s_{B}^{i},y^{i})\\}_{i=1}^{N}$   \nRequire: Encoder E, Reward Model R, prior $p(z)$   \n1: while not done do   \n2: Sample a batch $B\\sim D$   \n3: Compute $\\mu_{B},\\sigma_{B}=\\operatorname{E}(B)$   \n4: Sample $z\\sim\\mathcal N(\\mu_{B},\\sigma_{B})$   \n5: Append $z$ to $B$ : $\\{(s_{A},s_{B},y)\\}\\rightarrow\\{((s_{A}|z,s_{B}|z),y)\\}$   \n6: Compute rewards: $r_{s_{A}}=\\mathrm{R}(s_{A}|z)$ and $r_{s_{B}}=\\mathrm{R}(s_{B}|z)$   \n7: Compute reconstruction loss: $\\mathcal{L}_{\\mathrm{recon}}=\\mathrm{cross}$ entropy(\u03c3(rsA \u2212rsB), y)   \n8: Compute KL-loss: $\\mathcal{L}_{K L}=\\beta\\cdot D_{K L}(\\mathcal{N}(\\mu_{B},\\sum_{B})\\parallel p(z))$   \n9: Compute total loss: $\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{recon}}+\\mathcal{L}_{K L}$   \n10: Update $\\mathrm{E}$ and $\\mathrm{R}$ by optimizing $\\mathcal{L}_{\\mathrm{total}}$   \n11: end while ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Policy Optimization using IQL and VPL ", "page_idx": 22}, {"type": "text", "text": "Require: Offline Dataset $\\{\\tau_{1},\\tau_{2},\\dots\\}$   \nRequire: Reward Model $r_{\\phi}(s,z)$   \nRequire: Prior $p(z)$   \nRequire: Policy $\\pi(a|s,z)$   \n1: for each trajectory $\\tau_{i}=\\{(s_{t},a_{t},s_{t+1})\\}_{t=1}^{T}$ in $D$ do   \n2: Sample $z\\sim p(z)$   \n3: for each state $s_{t}$ in $\\tau_{i}$ do   \n4: Compute reward $r_{t}=r_{\\phi}{\\left(s_{t},z\\right)}$ # Alternatively, $r_{t}=r_{\\phi}\\big(s_{t+1},z\\big)$   \n5: Update dataset with $\\left(s_{t},r_{t},z\\right)$   \n6: end for   \n7: end for   \n8: Train policy $\\pi(a|s,z)$ using IQL ", "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We highlight the claims and contributions in the abstract and the introduction. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We highlight the limitations of our approach in the Conclusion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not claim any substantial theoretical results that requires a proof. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We describe all the algorithms, datasets, models and hyperparameters in detail in the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not provide immediate access to the data and code, but will do so in the future. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide all details in the main paper and the Appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We report standard error across multiple seeds for all results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We indicate the compute resources used in the Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We confirm to the code of ethics in every respect. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the impact of the work in the concluding section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 26}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We use publicly available datasets and models, and the synthetic toy dataset generated has no risk for misuse. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We cite all the datasets, models and other assets used in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We introduce a new dataset, and provide detailed explanation of the dataset in the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We dont use real humans or crowd sourcing as subjects for any of our experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our experiments do not require an IRB. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]