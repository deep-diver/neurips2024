[{"figure_path": "8KPyJm4gt5/figures/figures_1_1.jpg", "caption": "Figure 1: Suboptimality of a policy learned with log-loss behavior cloning (LogLossBC) as a function of the number of expert trajectories, for varying values of horizon H. In each environment, an imitator is trained according to LogLossBC and the regret with respect to the expert is reported, with reward normalized to be horizon-independent. (a) Continuous control with MuJoCo environment Walker2d-v4. (b) Discrete control with Atari environment BeamRiderNoFrameskip-v4. For both environments, we find that the regret is independent of horizon (or in the case of Atari, slightly improving with horizon), as predicted by our theoretical results. Full experimental details are provided in Appendix C.", "description": "The figure shows the suboptimality (regret) of a policy learned using LogLossBC plotted against the number of expert trajectories for different horizons (H).  Two environments are shown: MuJoCo (continuous control) and Atari (discrete control). The results indicate that the regret is largely independent of the horizon, supporting the paper's theoretical findings.", "section": "Experiments"}, {"figure_path": "8KPyJm4gt5/figures/figures_20_1.jpg", "caption": "Figure 2: Dependence of expected regret on the horizon for multiple choices for the number of imitator trajectories n. (a) Continuous control environment Walker2d-v4. (b) Discrete Atari environment BeamriderNoFrameskip-v4. For both environments, increasing the horizon does not lead to a significant increase in regret, as predicted by our theory.", "description": "This figure shows the dependence of expected regret on the horizon for different number of trajectories in two environments: MuJoCo Walker2d-v4 and Atari BeamRiderNoFrameskip-v4. The results indicate that increasing the horizon does not significantly affect the regret, which supports the theoretical findings of the paper.", "section": "Experiments"}, {"figure_path": "8KPyJm4gt5/figures/figures_21_1.jpg", "caption": "Figure 3: (a) Relationship between the number of expert trajectories and expected regret for the Dyck environment multiple choices of horizon H. The expert is trained to produce valid Dyck words of length H, and the imitator's ability to generate a valid word is evaluated. We find that regret increases as a function of H. (b) Logarithm of the product of weight matrix norms for the expert policy network as a function of H, for Dyck and Car environments. The log-product-norm acts as a proxy for complexity for the class II; we rescale such that log-product-norm at H = 10 is 1.0 for both domains. For Dyck, we find that as H increases, the complexity of II required to represent the expert policy (as measured by the log-product-norm) also increases, explaining the increasing regret in (a). However, the gain in log-product-norm for the Car domain is much lower, which is in line with the fact that the regret for the Car domain exhibits only mild scaling with horizon.", "description": "This figure shows the relationship between the number of expert trajectories, expected regret, and model complexity for the Dyck language generation task.  The left panel (a) displays how expected regret increases with horizon length (H) for different numbers of expert trajectories. The right panel (b) illustrates the growth in model complexity (measured by log product norm) as the horizon increases for Dyck and Car environments. The results suggest that increasing model complexity with increasing H contributes to higher regret in Dyck, while the Car environment exhibits much less sensitivity to horizon.", "section": "C Experiments"}, {"figure_path": "8KPyJm4gt5/figures/figures_22_1.jpg", "caption": "Figure 4: Dependence of expected regret on the number of expert trajectories for Car environment under varying values for horizon H for log-loss (a) and mean-squared loss (b). The expert policy network is trained on a set of 2 \u00d7 104 episodes generated by an optimal policy via behavior cloning. We use LogLossBC to train imitator policy for varying values of the horizon H and number of trajectories n. For both losses, we find that the expected regret goes down as the number of expert trajectories increases, but degrades slightly as a function of H.", "description": "This figure shows the relationship between expected regret and the number of expert trajectories in the Car environment for both log-loss and mean-squared loss functions.  The results are presented for various horizon values (H).  The key observation is that while regret decreases with increasing numbers of trajectories (as expected), a slight increase in regret is observed with increasing horizon values. This suggests a mild dependence of regret on the horizon.", "section": "Experiments"}, {"figure_path": "8KPyJm4gt5/figures/figures_23_1.jpg", "caption": "Figure 2: Dependence of expected regret on the horizon for multiple choices for the number of imitator trajectories n. (a) Continuous control environment Walker2d-v4. (b) Discrete Atari environment BeamriderNoFrameskip-v4. For both environments, increasing the horizon does not lead to a significant increase in regret, as predicted by our theory.", "description": "This figure displays the relationship between the expected regret and the horizon (H) for different numbers of trajectories (n) using two different environments: MuJoCo Walker2d (continuous control) and Atari BeamRider (discrete control). For both, the regret does not increase significantly with the horizon.  The results support the paper's theoretical findings that show the sample complexity for log-loss behavior cloning is horizon-independent.", "section": "Experiments"}, {"figure_path": "8KPyJm4gt5/figures/figures_24_1.jpg", "caption": "Figure 6: Evaluation of the quality of (i) Hellinger distance D\u00b2(P\u03c0*, P\u03c0), and (ii) validation loss as a proxy for rollout reward. We plot Hellinger distance and validation loss against mean reward for a over a single training run for Walker2d environment with H = 500 and n = 500. (a) Results for LogLossBC, where the validation loss and Hellinger distance D are highly correlated, and serve as good proxies for the expected reward of the policy. (b) Results for MSE loss, where the validation loss is less well correlated with the expected reward (note the cluster in the upper left hand corner), but the Hellinger distance D remains a good proxy.", "description": "This figure shows the relationship between the Hellinger distance to the expert policy, the validation loss, and the expected reward of the learned policy for two different loss functions: LogLossBC and MSE Loss. It demonstrates that Hellinger distance serves as a better proxy for rollout performance compared to validation loss, especially when using LogLossBC.", "section": "C Experiments"}, {"figure_path": "8KPyJm4gt5/figures/figures_24_2.jpg", "caption": "Figure 1: Suboptimality of a policy learned with log-loss behavior cloning (LogLossBC) as a function of the number of expert trajectories, for varying values of horizon H. In each environment, an imitator is trained according to LogLossBC and the regret with respect to the expert is reported, with reward normalized to be horizon-independent. (a) Continuous control with MuJoCo environment Walker2d-v4. (b) Discrete control with Atari environment BeamRiderNoFrameskip-v4. For both environments, we find that the regret is independent of horizon (or in the case of Atari, slightly improving with horizon), as predicted by our theoretical results. Full experimental details are provided in Appendix C.", "description": "The figure shows the suboptimality (regret) of a policy learned using log-loss behavior cloning as a function of the number of expert trajectories and the horizon (H).  Two plots are shown: one for a continuous control environment (MuJoCo Walker2d-v4) and another for a discrete control environment (Atari BeamRiderNoFrameskip-v4). The key finding is that the regret is largely independent of the horizon, which supports the paper's theoretical claims.", "section": "Experiments"}]