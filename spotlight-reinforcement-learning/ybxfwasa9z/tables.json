[{"figure_path": "YbxFwaSA9Z/tables/tables_18_1.jpg", "caption": "Table 1: Optimizer layer sizes for OPEN in the single-task and gridworld experiments.", "description": "This table shows the layer types and their dimensionalities for the OPEN optimizer used in the single-task and gridworld experiments.  The optimizer consists of a GRU followed by multiple fully connected layers with LayerNorm for stability.  The dimensionalities specify the number of input and output units for each layer.  This architectural configuration is crucial for the optimizer's ability to process various inputs and generate appropriate updates. ", "section": "B.4 Single-Task and Gridworld Optimizer Architecture Details"}, {"figure_path": "YbxFwaSA9Z/tables/tables_18_2.jpg", "caption": "Table 2: Optimizer layer sizes for OPEN in the multi-task experiment.", "description": "This table presents the architecture of the optimizer used in the multi-task experiment. It shows the different layers used in the optimizer, including GRU (Gated Recurrent Unit), fully connected layers, and LayerNorm layers. The dimensionality of each layer is also specified.  The architecture is designed to handle the complexities of multi-task learning by conditioning on multiple inputs and using a gated recurrent unit for capturing temporal dependencies. The use of small layer sizes is a design choice to limit memory usage and maintain computational efficiency.", "section": "B.5 Multi-Task Optimizer Architecture Details"}, {"figure_path": "YbxFwaSA9Z/tables/tables_19_1.jpg", "caption": "Table 3: PPO hyperparameters. All MinAtar environments used common PPO parameters, and are thus under one header.", "description": "This table lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in the experiments described in the paper.  It shows the settings used across different environments (MinAtar, Ant, Gridworld).  The hyperparameters control various aspects of the PPO algorithm, such as the number of training steps, batch sizes, learning rates, and other parameters that affect the training process. The consistency in hyperparameters across MinAtar environments highlights the authors' focus on controlling variables to isolate the effects of the optimization algorithm itself.", "section": "C Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_20_1.jpg", "caption": "Table 4: Optimization hyperparameters for MinAtar environments. \u2018Range\u2019 covers the range of values used in our hyperparameter tuning.", "description": "This table presents the hyperparameters used for Adam, RMSprop, Lion, and Optim4RL optimizers in the MinAtar environment. For each optimizer, it shows the learning rate (LR), beta1 (\u03b21), beta2 (\u03b22), decay (RMSprop only), and whether learning rate annealing was used. The \u2018Range\u2019 column specifies the range of values explored during hyperparameter tuning for each parameter.  The table details the values used for each environment (asterix, freeway, breakout, space invaders) during hyperparameter tuning, highlighting the different configurations explored for each algorithm.", "section": "C.2 Optimization Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_20_2.jpg", "caption": "Table 5: Optimization hyperparameters for Ant. \u2018Range\u2019 covers the range of values used in our hyperparameter tuning.", "description": "This table presents the hyperparameters used for the Adam, RMSprop, Lion, and Optim4RL optimizers when applied to the Ant environment.  For each optimizer, it lists the learning rate (LR), beta1 and beta2 parameters (for Adam, RMSprop, and Lion), the decay parameter (for RMSprop), and whether learning rate annealing was used (Anneal LR). The 'Range' column specifies the range of values explored during hyperparameter tuning for each optimizer in the Ant environment.", "section": "C.2 Optimization Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_20_3.jpg", "caption": "Table 6: Optimization hyperparameters for Gridworld. \u2018Range\u2019 covers the range of values used in our hyperparameter tuning.", "description": "This table shows the hyperparameters used for the Adam optimizer when training on the gridworld environment.  It lists the hyperparameters (learning rate (LR), beta1, beta2) and whether learning rate annealing was used.  The \"Range\" column indicates the range of values tested during hyperparameter tuning to find the optimal settings for each of these parameters.", "section": "C.2 Optimization Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_21_1.jpg", "caption": "Table 7: Hyperparameters for Craftax-Classic.", "description": "This table presents the hyperparameters used for training the PPO agent in the Craftax-Classic environment. It compares the hyperparameters used for the OPEN optimizer (0-shot), Adam optimizer (0-shot), and Adam optimizer (finetuned).  The table includes parameters such as the learning rate, beta1, beta2, whether or not learning rate annealing was used, the number of environments, environment steps, total timesteps, number of minibatches, number of epochs, discount factor, GAE lambda, PPO clip epsilon, value function coefficient, entropy coefficient, max gradient norm, layer width, number of hidden layers, and activation function.", "section": "C Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_21_2.jpg", "caption": "Table 3: PPO hyperparameters. All MinAtar environments used common PPO parameters, and are thus under one header.", "description": "This table shows the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in the experiments.  It lists hyperparameters such as the number of environments, number of environment steps, total timesteps, number of minibatches, number of epochs, discount factor, GAE lambda, PPO clip epsilon, value function coefficient, entropy coefficient, max gradient norm, layer width, number of hidden layers and activation function.  Note that the MinAtar environments all used the same PPO hyperparameters, hence they are grouped under a single header.", "section": "C.1 PPO Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_22_1.jpg", "caption": "Table 9: ES hyperparameters for meta-training. For MinAtar, the number of generations corresponds to {Asterix, Breakout, Freeway, SpaceInvaders}. We show the max generations for each optimizer (i.e., how long it was trained for), as well as the generation used (i.e., which training generation was used at inference time).", "description": "This table presents the hyperparameters used for training the optimizers with ES.  It shows the initial standard deviation (\u03c3init), decay rate (\u03c3decay), learning rate, learning rate decay, population size, number of rollouts, maximum number of generations, generation used for evaluation, and evaluation frequency for the different environments (MinAtar, Ant, Multi-Task, Gridworld).  The values reflect choices made to balance computational cost and performance, notably with differing maximum generation numbers for different optimizers and environments.", "section": "C.5 ES Hyperparameters"}, {"figure_path": "YbxFwaSA9Z/tables/tables_23_1.jpg", "caption": "Table 10: Distribution parameters for Gridworld. In training, the values for the experienced environment are sampled from the value range.", "description": "This table lists the parameters used to generate the gridworld environments for training the OPEN optimizer.  It shows the ranges from which the values for each parameter are randomly sampled during training. This randomization helps the optimizer generalize to a wider variety of gridworld environments.  Each parameter represents a characteristic of the gridworld environment such as the maximum number of steps in an episode, the range of rewards received from objects, and the probability of those objects terminating or reappearing. The number of walls and the size of the grid are also specified.", "section": "D Gridworld Details"}, {"figure_path": "YbxFwaSA9Z/tables/tables_23_2.jpg", "caption": "Table 11: Distribution parameters for Gridworld. Curly brackets denote a list of the true values, corresponding to each object, used in testing.", "description": "This table presents the parameters used to generate the gridworld environments for testing OPEN's generalization capabilities.  It lists the specific values used for three different gridworld configurations: `rand_dense`, `rand_sparse`, and `rand_long`. For each configuration, the table details the maximum steps allowed in an episode, the rewards associated with each object, the probabilities of object termination and respawning, the number of objects, the size of the grid, and the number of walls present. These parameters control the complexity and characteristics of the gridworld environments, allowing for a systematic evaluation of the algorithm's ability to generalize to unseen environments.", "section": "D Gridworld Details"}, {"figure_path": "YbxFwaSA9Z/tables/tables_24_1.jpg", "caption": "Table 12: Distribution parameters for Gridworld. At inference, true values are sampled from the given range, such that each seed is evaluated in a slightly different setting.", "description": "This table lists the parameters used to generate gridworld environments for testing.  The \"Name\" column specifies the type of gridworld (standard_maze, sixteen_rooms, labyrinth).  Each row then details the range of values sampled for each parameter (Max Steps in Episode, Object Rewards, Object p(terminate), Object p(respawn), Number of Objects, Grid Size) during testing. The ranges ensure variability across different test runs.", "section": "D Gridworld Details"}, {"figure_path": "YbxFwaSA9Z/tables/tables_34_1.jpg", "caption": "Table 13: Runtime for each optimizer. We evaluate each over 16 seeds, on 4 L40s GPUs. These take advantage of Jax's ability to parallelize over multiple GPUs [33].", "description": "This table shows the inference time in seconds for different optimizers across five different environments.  The experiments were run on four L40s GPUs, leveraging Jax's parallel processing capabilities. The table allows for a comparison of the computational efficiency of different optimizers, highlighting the trade-off between speed and performance.", "section": "J Experimental Compute"}, {"figure_path": "YbxFwaSA9Z/tables/tables_36_1.jpg", "caption": "Table 1: Optimizer layer sizes for OPEN in the single-task and gridworld experiments.", "description": "This table shows the layer sizes and dimensionality of the different layers in the OPEN optimizer for single-task and gridworld experiments.  It breaks down the architecture of the optimizer, specifying the type of layer (GRU, Fully Connected, Layernorm), and its dimensionality (number of nodes).  This is important for understanding the model's capacity and complexity.", "section": "B Optimizer Details"}]