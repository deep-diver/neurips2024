[{"figure_path": "YbxFwaSA9Z/figures/figures_1_1.jpg", "caption": "Figure 1: A visualization of OPEN. We train N agents, replacing the handcrafted optimizer of the RL loop with ones sampled from the meta-learner (i.e., evolution). Each optimizer conditions on gradient, momentum and additional inputs, detailed in Section 5.3, to calculate updates. The final returns from each loop are output to the meta learner, which improves the optimizer before repeating the process. A single inner loop step is described algorithmically in Appendix B.1.", "description": "This figure illustrates the OPEN algorithm's training process.  Multiple RL agents are trained concurrently.  Each agent uses a learned optimizer, sampled from a meta-learner, to update its parameters. The optimizers receive gradients, momentum, and additional handcrafted features as input (these features are detailed in Section 5.3 of the paper). The final return of each agent is then fed back to the meta-learner, allowing it to improve the learned optimizer over time. This process repeats, evolving the learned optimizer's ability to improve the RL agent's performance.", "section": "5 Method"}, {"figure_path": "YbxFwaSA9Z/figures/figures_6_1.jpg", "caption": "Figure 2: IQM of final returns for the five single-task training environments, evaluated over 16 random environment seeds. We plot 95% stratified bootstrap confidence intervals for each environment.", "description": "This figure presents the interquartile mean (IQM) of the final returns achieved by different optimizers (OPEN, No Features, Optim4RL, VeLO, Lion, RMSprop, Adam) across five single-task training environments (Freeway, Space Invaders, Breakout, Asterix, Ant).  The results are averaged over 16 different random seeds for each environment, showing the robustness of the findings.  95% stratified bootstrap confidence intervals are also displayed to indicate the uncertainty in the measurements. The figure highlights the performance of OPEN relative to other optimizers, particularly in three of the five environments (Freeway, Asterix, Breakout), showcasing its strong performance in single-task training.", "section": "Single-Task Training Results"}, {"figure_path": "YbxFwaSA9Z/figures/figures_6_2.jpg", "caption": "Figure 3: Mean, IQM and optimality gap (smaller = better), evaluated over 16 random seeds per environment for the aggregated, Adam-normalized final returns after multi-task training on MinAtar [62, 63]. We plot 95% stratified bootstrap confidence intervals for each metric.", "description": "This figure compares the performance of different optimizers in a multi-task setting where they were trained on multiple environments simultaneously.  The metrics shown are Mean, Interquartile Mean (IQM), and Optimality Gap.  OPEN significantly outperforms all other optimizers across all three metrics, highlighting its ability to adapt to diverse learning environments.", "section": "Multi-Task Training Results"}, {"figure_path": "YbxFwaSA9Z/figures/figures_7_1.jpg", "caption": "Figure 4: IQM of return, normalized by Adam, in seven gridworlds, with 95% stratified bootstrap confidence intervals for 64 random seeds. On the left, we show performance in the distribution OPEN and Adam were trained and tuned in. On the right, we show OOS performance: the top row shows gridworlds from Oh et al. [13], and the bottom row shows mazes from Chevalier-Boisvert et al. [67]. We mark Hidden Size = 16 as the in-distribution agent size for OPEN and Adam.", "description": "This figure shows the results of generalization experiments using the OPEN optimizer.  The left panel shows results for gridworlds from the training distribution, demonstrating in-distribution generalization. The right panel shows out-of-distribution (OOS) generalization results on gridworlds and mazes from different distributions than those used in training.  The results show OPEN's ability to generalize to unseen tasks, both within and outside of the training distribution, and to different agent network sizes.  The performance is normalized against Adam for comparison.  Error bars represent 95% stratified bootstrap confidence intervals calculated across 64 random seeds.", "section": "6.4 Generalization"}, {"figure_path": "YbxFwaSA9Z/figures/figures_7_2.jpg", "caption": "Figure 5: A comparison of OPEN and Adam with and without hyperparameter tuning in Craftax-Classic. We plot mean return over 32 seeds. Standard error is negligible (< 0.06).", "description": "This figure compares the performance of OPEN and Adam in the Craftax-Classic environment, both with and without hyperparameter tuning.  OPEN is evaluated in a zero-shot manner (i.e., without any prior training or tuning on Craftax-Classic), whereas Adam is tested both with and without tuning specifically for this environment.  The results demonstrate OPEN's ability to generalize effectively to a new environment, performing comparably to a finely-tuned Adam.", "section": "Results"}, {"figure_path": "YbxFwaSA9Z/figures/figures_8_1.jpg", "caption": "Figure 6: IQM of mean final return for 17 trained optimizers per ablation, evaluated on 64 random seeds each, alongside mean 7 = 0 dormancy for optimizers in the interquartile range. We show 95% stratified bootstrap confidence intervals.", "description": "This figure shows the results of an ablation study on the OPEN algorithm.  Seven different versions of the OPEN optimizer were created by removing one of its components (features or stochasticity).  Each version was trained and tested on 64 random seeds, allowing the study to assess the effects of each component on performance. The results are summarized using the Interquartile Mean (IQM) of the final return for each optimizer, as well as the mean dormancy (proportion of inactive neurons). Error bars represent 95% stratified bootstrap confidence intervals. The figure clearly illustrates the importance of each component of the algorithm, demonstrating that the full OPEN optimizer achieves significantly better results than any of its ablated variants.", "section": "7 Ablation Study"}, {"figure_path": "YbxFwaSA9Z/figures/figures_8_2.jpg", "caption": "Figure 7: IQM performance improvement of an optimizer with learnable stochasticity over one without. We plot 95% stratified bootstrap confidence intervals over 128 random seeds.", "description": "This figure shows the improvement in IQM (Interquartile Mean) performance when using an optimizer with learnable stochasticity compared to one without stochasticity. The experiment was conducted on the Deep Sea environment from the bsuite benchmark, varying the size of the environment (number of consecutive 'right' actions required to receive a reward).  The results are shown for different environment sizes, with error bars representing 95% stratified bootstrap confidence intervals over 128 random seeds.  The plot demonstrates the beneficial effect of learnable stochasticity, especially in larger environments, indicating its role in effective exploration.", "section": "7.2 Exploration"}, {"figure_path": "YbxFwaSA9Z/figures/figures_9_1.jpg", "caption": "Figure 8: IQM of final return for Adam and OPEN after training PQN in Asterix. We plot 95% stratified bootstrap confidence intervals over 64 seeds.", "description": "The figure shows the interquartile mean (IQM) of final returns for both Adam and OPEN optimizers when training a Proximal Q-Network (PQN) agent in the Asterix environment.  The results are based on 64 random seeds, providing robust statistical estimates of performance. Error bars indicate 95% stratified bootstrap confidence intervals.  This figure demonstrates that OPEN consistently outperforms Adam in this specific setting. It is a direct comparison evaluating how effectively each optimizer learns the update rules for the PQN algorithm,  illustrating OPEN's superior performance.", "section": "7.3 Alternative RL Algorithm"}, {"figure_path": "YbxFwaSA9Z/figures/figures_18_1.jpg", "caption": "Figure 1: A visualization of OPEN. We train N agents, replacing the handcrafted optimizer of the RL loop with ones sampled from the meta-learner (i.e., evolution). Each optimizer conditions on gradient, momentum and additional inputs, detailed in Section 5.3, to calculate updates. The final returns from each loop are output to the meta learner, which improves the optimizer before repeating the process. A single inner loop step is described algorithmically in Appendix B.1.", "description": "This figure illustrates the OPEN algorithm's meta-learning process. Multiple RL agents are trained, each using a different optimizer sampled from a meta-learner.  These optimizers receive gradient and momentum information, along with other context-specific features, to compute updates.  The performance of each agent, measured by its final returns, is fed back into the meta-learner to refine the optimizer's behavior over time.  This iterative process allows the meta-learner to evolve better optimizers specifically tailored for the challenges of RL.", "section": "5 Method"}, {"figure_path": "YbxFwaSA9Z/figures/figures_24_1.jpg", "caption": "Figure 10: IQM of return against size of environment in Deep Sea, with stratified bootstrap 95% confidence intervals for 128 random seeds. The only optimizer which is able to generalize has separate parameters between the actor and critic and incorporates learned stochasticity into its update.", "description": "This figure shows the results of an ablation study on the Deep Sea environment.  The study explores the impact of learned stochasticity and separate actor-critic parameters on the optimizer's ability to generalize across different environment sizes. The results demonstrate that only the optimizer with both features is capable of generalization.", "section": "I.2 Deep Sea Expanded Curve"}, {"figure_path": "YbxFwaSA9Z/figures/figures_25_1.jpg", "caption": "Figure 11: RL training curves comparing our learned optimizers and all other baselines, each trained or tuned on a single environment and evaluated on the same environment. We show mean return over training with standard error, evaluated over 16 seeds.", "description": "This figure compares the performance of different optimizers in reinforcement learning (RL) for five different single-task environments. The optimizers include OPEN (the proposed method), Adam, RMSprop, Lion, VeLO, Optim4RL, and No Features (a baseline). The curves show mean return with standard error over 16 random seeds, showcasing the progress of each optimizer over training time in each environment.  The results demonstrate OPEN's ability to learn highly performant update rules.", "section": "Single Environment Return Curves"}, {"figure_path": "YbxFwaSA9Z/figures/figures_26_1.jpg", "caption": "Figure 12: \ud835\udf49 = 0 dormancy curves for all optimizers on the four MinAtar environments, evaluated over 16 seeds each. We plot the mean dormancy with standard error.", "description": "This figure displays the dormancy (a measure of neuron inactivity) over training time for different optimizers in four MinAtar environments.  The plot shows the mean dormancy (with standard error bars) for each optimizer across 16 independent training runs. The purpose is to compare the plasticity (ability to retain activity in neurons) of different optimizers over the training process.", "section": "F.1 Dormancy"}, {"figure_path": "YbxFwaSA9Z/figures/figures_27_1.jpg", "caption": "Figure 13: Curves showing the cosine similarity between the updates from OPEN and \u2018No Features\u2019 with the gradient or momentum at different \u03b2 timescales. Each column corresponds to a different environment, and each row to a different timescale. In order, the rows show cosine similarity with gradient, followed by momentum at \u03b2 = [0.1, 0.5, 0.9, 0.99, 0.999, 0.9999]. We plot mean cosine similarities with standard error over 16 runs.", "description": "This figure shows the cosine similarity between updates generated by OPEN and a simpler optimizer ('No Features') against the gradient and momentum with different timescales (\u03b2 values).  The results are presented for each of the five MinAtar environments.  The figure aims to illustrate the relationship between OPEN's updates and traditional optimization components like gradient and momentum across various timescales.", "section": "F.2 Similarity Between Update and Gradient/Momentum"}, {"figure_path": "YbxFwaSA9Z/figures/figures_28_1.jpg", "caption": "Figure 13: Curves showing the cosine similarity between the updates from OPEN and \u2018No Features\u2019 with the gradient or momentum at different \u03b2 timescales. Each column corresponds to a different environment, and each row to a different timescale. In order, the rows show cosine similarity with gradient, followed by momentum at \u03b2 = [0.1, 0.5, 0.9, 0.99, 0.999, 0.9999]. We plot mean cosine similarities with standard error over 16 runs.", "description": "This figure shows the cosine similarity between updates generated by OPEN and a baseline method ('No Features') against the gradient and momentum at different timescales.  The analysis helps to understand how OPEN\u2019s updates relate to standard optimization methods (gradient and momentum) across various environments and timescales. The use of different beta values (\u03b2) for momentum calculation allows the examination of these relationships at various levels of smoothing.", "section": "F.2 Similarity Between Update and Gradient/Momentum"}, {"figure_path": "YbxFwaSA9Z/figures/figures_28_2.jpg", "caption": "Figure 13: Curves showing the cosine similarity between the updates from OPEN and \u2018No Features\u2019 with the gradient or momentum at different \u03b2 timescales. Each column corresponds to a different environment, and each row to a different timescale. In order, the rows show cosine similarity with gradient, followed by momentum at \u03b2 = [0.1, 0.5, 0.9, 0.99, 0.999, 0.9999]. We plot mean cosine similarities with standard error over 16 runs.", "description": "This figure shows the cosine similarity between updates generated by OPEN and a simpler model, \u2018No Features\u2019, against gradients and momentum with different beta values. The analysis shows that both optimizers align with momentum at similar timescales to those tuned in Adam, while the additional elements of OPEN cause less similarity to \u2018No Features\u2019.", "section": "F Analysis"}, {"figure_path": "YbxFwaSA9Z/figures/figures_29_1.jpg", "caption": "Figure 15: Plots of the normalized stochasticity weight with respect to time. We plot mean values with standard error over 16 seeds.", "description": "This figure visualizes how the weight of the learned stochasticity (factor) changes over time during training.  The stochasticity is incorporated into the actor's update rule as follows:  \u00fbactor = \u00fbactor + \u03b13\u03b4actor\u03b5, where \u03b5 is sampled from a standard normal distribution N(0,1).  The y-axis shows the normalized stochasticity weight (randomness/p), calculated as |\u00fbactor\u03b5|/p, where p is the parameter value. The x-axis represents the number of updates. The plot includes separate lines for five different environments (freeway, asterix, ant, breakout, spaceinvaders), each showing the mean normalized stochasticity weight with standard error calculated across 16 random seeds.  The plot shows that the stochasticity weight generally decreases over time, suggesting that the model prioritizes exploration early in training and exploitation later, which is expected.", "section": "F.4 Stochasticity"}, {"figure_path": "YbxFwaSA9Z/figures/figures_30_1.jpg", "caption": "Figure 11: RL training curves comparing our learned optimizers and all other baselines, each trained or tuned on a single environment and evaluated on the same environment. We show mean return over training with standard error, evaluated over 16 seeds.", "description": "This figure compares the performance of different optimizers in reinforcement learning (RL) during training on five different single-task environments (Freeway, Asterix, Breakout, SpaceInvaders, Ant).  The curves show the average return obtained by each algorithm over many training iterations (frames).  The results illustrate the learning speed and final performance of the proposed Learned Optimization for Plasticity, Exploration, and Non-stationarity (OPEN) method compared to traditional optimizers such as Adam and RMSprop, and other learned optimizers.", "section": "Single Environment Return Curves"}, {"figure_path": "YbxFwaSA9Z/figures/figures_31_1.jpg", "caption": "Figure 4: IQM of return, normalized by Adam, in seven gridworlds, with 95% stratified bootstrap confidence intervals for 64 random seeds. On the left, we show performance in the distribution OPEN and Adam were trained and tuned in. On the right, we show OOS performance: the top row shows gridworlds from Oh et al. [13], and the bottom row shows mazes from Chevalier-Boisvert et al. [67]. We mark Hidden Size = 16 as the in-distribution agent size for OPEN and Adam.", "description": "This figure demonstrates the generalization capabilities of OPEN across different gridworld environments. The left side shows results for environments within the training distribution, highlighting OPEN's consistent outperformance of Adam.  The right side assesses out-of-distribution generalization, revealing that OPEN maintains superior performance across varied gridworld types and agent network sizes. This underscores OPEN's ability to adapt and generalize beyond its training data.", "section": "6.4 Generalization"}, {"figure_path": "YbxFwaSA9Z/figures/figures_32_1.jpg", "caption": "Figure 19: IQM of return against size of environment in Deep Sea, with stratified bootstrap 95% confidence intervals for 128 random seeds. The only optimizer which is able to generalize has separate parameters between the actor and critic and incorporates learned stochasticity into its update.", "description": "This figure shows the results of an ablation study on the Deep Sea environment, which tests the ability of different optimizers to generalize to different problem sizes. The results show that the optimizer with separate parameters for the actor and critic, and with learned stochasticity, is the only one that generalizes well across a wide range of problem sizes. The other optimizers either perform poorly in larger environments or do not generalize at all.", "section": "I.2 Deep Sea Expanded Curve"}, {"figure_path": "YbxFwaSA9Z/figures/figures_33_1.jpg", "caption": "Figure 20: Training curves for PQN when optimised by OPEN and Adam in asterix. We show mean return with standard error, over 64 seeds.", "description": "This figure shows the training curves for the PQN algorithm in the Asterix environment, comparing the performance of OPEN and Adam as optimizers.  The x-axis represents the number of frames (training steps), and the y-axis represents the mean return achieved by each optimizer. Error bars indicate standard error calculated over 64 independent runs.  The plot illustrates that OPEN initially achieves lower returns than Adam at the beginning of training but eventually surpasses Adam's performance in the later stages.", "section": "I.3 PQN Return Curve"}]