{"references": [{"fullname_first_author": "Mandi Zhao", "paper_title": "On the effectiveness of fine-tuning versus meta-reinforcement learning", "publication_date": "2022-12-01", "reason": "This paper provides a comprehensive comparison of fine-tuning and meta-reinforcement learning, which is directly relevant to the core theme of offline meta-reinforcement learning (OMRL) explored in the target paper."}, {"fullname_first_author": "Jacob Beck", "paper_title": "A survey of meta-reinforcement learning", "publication_date": "2023-01-18", "reason": "This survey paper offers a broad overview of meta-reinforcement learning, providing valuable context and background information for understanding the advancements made in offline meta-reinforcement learning."}, {"fullname_first_author": "Tianhe Yu", "paper_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper introduces Meta-World, a benchmark used in the target paper, providing a common ground for evaluating and comparing various offline meta-reinforcement learning algorithms."}, {"fullname_first_author": "Chelsea Finn", "paper_title": "Model-agnostic meta-learning for fast adaptation of deep networks", "publication_date": "2017-07-01", "reason": "This paper introduces Model-Agnostic Meta-Learning (MAML), a foundational method in meta-learning which is inherently connected to OMRL.  Understanding MAML is crucial for comprehending the advancements in offline meta-learning."}, {"fullname_first_author": "Kate Rakelly", "paper_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables", "publication_date": "2019-06-01", "reason": "This paper addresses the efficiency challenge in offline meta-reinforcement learning by introducing probabilistic context variables, which is directly relevant to the context-based approach discussed in the target paper."}]}