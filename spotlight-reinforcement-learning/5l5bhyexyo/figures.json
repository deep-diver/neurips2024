[{"figure_path": "5l5bhYexYO/figures/figures_3_1.jpg", "caption": "Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \u2202RTG/\u2202a, but local policy improvement requires the opposite, i.e., \u2202RTG/\u2202a. Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.", "description": "This figure illustrates the core idea of the paper.  On the left, it shows how the Online Decision Transformer (ODT) struggles when pretrained with low-reward offline data. The ODT's policy gradient (\u2202RTG/\u2202a) is in the opposite direction needed for improvement in this scenario. The unreachable target RTG makes it difficult to recover.  The right side of the figure demonstrates the proposed solution which adds reinforcement learning (RL) gradients (TD3). This allows for local policy improvements by providing the needed \u2202RTG/\u2202a for improvement, leading to better overall performance.", "section": "3 Method"}, {"figure_path": "5l5bhYexYO/figures/figures_3_2.jpg", "caption": "Figure 2: An illustration of a simple MDP, showing how RL can infer the direction for improvement, while online DT fails. Panels (a) and (b) show, DDPG and ODT+DDPG manage to maximize reward and find the correct optimal action quickly, while ODT fails to do so. Panel (c) shows how a DDPG/ODT+DDPG critic (from light blue/orange to dark blue/red) manages to fit ground truth reward (green curve). Panel (d) shows that the ODT policy (changing from light gray to dark) fails to discover the hidden reward peak near 0 between two low-reward areas (near -1 and 1 respectively) contained in the offline data. Meanwhile, ODT+DDPG succeeds in finding the reward peak.", "description": "This figure demonstrates the effectiveness of RL gradients in online finetuning of decision transformers by comparing DDPG, ODT, and ODT+DDPG on a simple MDP.  It shows that DDPG and ODT+DDPG quickly learn the optimal action and maximize reward, unlike ODT. The critic plots demonstrate that DDPG and ODT+DDPG accurately learn the reward function, while ODT fails to identify a hidden reward peak.", "section": "3.1 Why RL Gradients?"}, {"figure_path": "5l5bhYexYO/figures/figures_7_1.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure shows the performance comparison of different methods on four Adroit environments (Pen, Hammer, Door, Relocate) with three different datasets (expert, cloned, human). The results indicate that TD3+ODT consistently outperforms other baselines, including ODT, PDT, TD3, DDPG+ODT, and TD3+BC.  While TD3, IQL, and TD3+ODT show decent initial performance, TD3 shows instability during online finetuning, whereas TD3+ODT demonstrates significant improvement compared to IQL.  The figure highlights the superiority of TD3+ODT, particularly when pretrained with low-reward offline data.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_7_2.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure shows the performance comparison of different methods on four robotic manipulation tasks from the Adroit environment.  Each task uses three different datasets: expert, cloned, and human. The results indicate that the proposed method (TD3+ODT) outperforms other baselines, especially during online fine-tuning.  While TD3, IQL, and TD3+ODT show decent initial performance, TD3 struggles later, whereas TD3+ODT significantly surpasses IQL's improvement.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_8_1.jpg", "caption": "Figure 5: Panel (a) shows ablations on RL coefficient \u03b1. While higher \u03b1 aids exploration as shown in the halfcheetah-medium-replay-v2 case, it may sometimes introduce instability, which is shown in the hammer-human-v1 case. Panel (b) shows ablations on Teval. Teval balances training stability and more information for decision-making.", "description": "This figure presents ablation studies on two hyperparameters: the RL coefficient \u03b1 and the evaluation context length Teval.  Panel (a) shows how varying \u03b1 affects the performance on two different environments, highlighting the trade-off between improved exploration and potential instability. Panel (b) demonstrates the impact of Teval on performance, illustrating the balance between utilizing sufficient contextual information and maintaining stable training.  The results suggest that carefully tuning these hyperparameters is crucial for optimal performance.", "section": "3.3 Why Does ODT Fail to Improve the Policy?"}, {"figure_path": "5l5bhYexYO/figures/figures_15_1.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure presents the results of the proposed method (TD3+ODT) and several baseline methods on four Adroit robotic manipulation tasks. Each task involves three different datasets: expert, cloned, and human. The plots display the average normalized rewards over time for each method.  The results show that the TD3+ODT method consistently outperforms the baseline methods, especially in the online finetuning phase (when the policy is updated using online data collected from the environment). While methods like TD3 and IQL achieve decent performance initially, they struggle to consistently improve during online finetuning.  In contrast, TD3+ODT shows greater and more consistent gains in performance, indicating the effectiveness of adding TD3 gradients to the online finetuning of decision transformers.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_16_1.jpg", "caption": "Figure 7: Our main final results re-evaluated using the rliable library with 10000 bootstrap replications. The x-axes are normalized scores (optimality gap is for \u222b0x Pr(reward \u2264 x)dx). Our method indeed outperforms all baselines on Adroit, MuJoCo and antmaze (umaze and medium).", "description": "This figure displays the results of the experiments using the rliable library, which provides more robust statistical analysis compared to simply using the average reward.  The results are presented for four environments (Adroit, MuJoCo, Antmaze umaze, and Antmaze medium), each showing median, interquartile mean (IQM), mean, and optimality gap across multiple runs, highlighting the improved performance of TD3+ODT over other baselines.", "section": "4 Experiments"}, {"figure_path": "5l5bhYexYO/figures/figures_17_1.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure shows the performance of different algorithms on four robotic manipulation tasks from the Adroit environment.  The x-axis represents the number of online transitions (interactions with the environment during fine-tuning), and the y-axis shows the normalized average reward.  Seven methods are compared: TD3+BC, IQL, ODT, PDT, TD3, DDPG+ODT, and TD3+ODT (the proposed method).  The figure shows that TD3+ODT consistently outperforms the baselines, especially when pre-training data has low reward.  While TD3, IQL, and TD3+ODT all perform reasonably well initially, TD3\u2019s performance degrades during online finetuning, whereas TD3+ODT significantly improves.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_17_2.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure displays the performance of different algorithms on four robotic manipulation tasks from the Adroit environment.  Three datasets are used for each task: expert (optimal performance), cloned (imitating expert), and human.  The results show that the proposed method (TD3+ODT) significantly outperforms baseline methods like ODT and TD3+BC, particularly when starting from lower-quality cloned or human datasets.  While TD3, IQL, and TD3+ODT initially show decent results, the TD3 baseline struggles significantly during online finetuning, highlighting the effectiveness of the proposed approach (TD3+ODT).", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_18_1.jpg", "caption": "Figure 10: An illustration of the training instability and the corresponding performance of a PPO-finetuned decision transformer. The x axis is 300\u00d7 the number of gradient steps.", "description": "This figure shows the training instability and the performance of a decision transformer finetuned with PPO. The left subplot shows the reward curve, indicating significant instability and poor performance. The right subplot displays the ratio of the current policy to the old policy, again revealing instability. This illustrates the difficulty of using PPO to finetune a decision transformer.", "section": "3.1 Why RL Gradients?"}, {"figure_path": "5l5bhYexYO/figures/figures_18_2.jpg", "caption": "Figure 6: Results on MuJoCo [58] Environments. The TD3 gradient significantly improves the overall performance of the decision transformer; autoregressive algorithms, such as ODT and PDT, fails to improve policy in most cases (especially on random dataset), while TD3+BC and IQL's improvement during finetuning is generally limited.", "description": "This figure shows the results of the experiments conducted on four MuJoCo environments (Hopper, HalfCheetah, Walker2d, and Ant) using different datasets (medium, medium-replay, and random).  The performance of six different methods are compared: TD3+BC, IQL, ODT, PDT, TD3, DDPG+ODT, and TD3+ODT (the proposed method). The key observation is that adding TD3 gradients significantly enhances the performance, especially when pretrained with low-reward data. In contrast, autoregressive methods like ODT and PDT struggle, particularly with random datasets, highlighting the benefit of incorporating reinforcement learning gradients in online finetuning of decision transformers.", "section": "4 Experiments"}, {"figure_path": "5l5bhYexYO/figures/figures_19_1.jpg", "caption": "Figure 12: An illustration of the training context length T2 during training. Ttrain is the context length of a2 upon sampling and evaluation. It is easy to see that T2 is randomized during training due to the left endpoint of the sampled trajectory segment. b) shows the distribution U' of T2; while T2 for step j \u2265 Ttrain is uniformly sampled between 1 and Ttrain because the start of the segment is uniformly sampled, T2 for step i < Ttrain will be capped at the start of the trajectory. Thus U' is not exactly uniform.", "description": "This figure illustrates how the context length used during training and evaluation of the decision transformer model varies.  Panel (a) shows the overall architecture, highlighting how the context length (T2) during training differs from the evaluation context length (Teval). Panel (b) focuses on the distribution of context lengths (T2) during training, demonstrating that it is not perfectly uniform due to the way the model samples trajectory segments.", "section": "3 Method"}, {"figure_path": "5l5bhYexYO/figures/figures_19_2.jpg", "caption": "Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \u2202RTG/\u2202a, but local policy improvement requires the opposite, i.e., \u2202RTG/\u2202a. Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.", "description": "This figure illustrates the limitations of Online Decision Transformers (ODT) when pretrained with low-reward offline data.  ODT, using a supervised learning approach, struggles to improve because the gradient it produces (\u2202RTG/\u2202a) points in the opposite direction needed for local policy improvement (\u2202RTG/\u2202a). The figure shows how the target return-to-go (RTG) is far from the actual return, leading to poor performance. The solution proposed in the paper is to add reinforcement learning (RL) gradients (like those from TD3) to provide the necessary local improvement signal and thus improve the policy.", "section": "3 Method"}, {"figure_path": "5l5bhYexYO/figures/figures_23_1.jpg", "caption": "Figure 14: An illustration of how Lipschitzness on the distribution of p\u03b2(RTG|s) could link the bound between Pr\u03b2(RTG > V\u03b2(s) + c|s) and p\u03b2(RTG|s). Note we do not take the left-hand side probability mass of p0 into account because the triangle of probability mass could be truncated by V\u03b2(s).", "description": "This figure demonstrates the relationship between the probability density function p\u03b2(RTG|s) and the cumulative probability Pr\u03b2(RTG > V\u03b2(s) + c|s) under the assumption that p\u03b2(RTG|s) is Lipschitz continuous.  It highlights how the Lipschitz condition, implying a bounded rate of change in the density function, constrains the cumulative probability. The shaded area represents a region of probability mass, and the figure uses this to visually illustrate how a bound on the rate of change in p\u03b2(RTG|s) translates to a bound on the cumulative probability. The exclusion of the left-hand shaded area signifies that the probability mass in that region is not being considered because it could be truncated by V\u03b2(s).", "section": "E Mathematical Proofs"}, {"figure_path": "5l5bhYexYO/figures/figures_23_2.jpg", "caption": "Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \u2202RTG/\u2202a, but local policy improvement requires the opposite, i.e., \u2202RTG/\u2202a. Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.", "description": "This figure illustrates the core idea of the paper.  Online Decision Transformers (ODT) struggle when pretrained with low-reward data because the gradient of the return-to-go (RTG) with respect to the action, produced by the transformer, points in the opposite direction needed for improvement.  Adding reinforcement learning (RL) gradients, such as those from TD3, provides a local improvement signal which addresses this limitation and leads to better online finetuning performance.", "section": "3 Method"}, {"figure_path": "5l5bhYexYO/figures/figures_24_1.jpg", "caption": "Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \\(\\frac{\\partial \\text{RTG}}{\\partial a}\\), but local policy improvement requires the opposite, i.e., \\(\\frac{\\partial \\text{RTG}}{\\partial a}\\). Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.", "description": "This figure illustrates the core idea of the paper.  It shows how the Online Decision Transformer (ODT) fails to improve when pretrained with low-reward offline data, highlighting the contrast between the gradient provided by the decision transformer (\u2202RTG/\u2202a) and what is needed for local policy improvement (\u2202RTG/\u2202a). The figure suggests that by adding RL gradients, the agent can improve locally and achieve better performance. The left panel shows how ODT struggles with an unreachable target RTG (Returns-To-Go), while the right panel illustrates that incorporating TD3 (Twin Delayed Deep Deterministic Policy Gradient) gradients enables local improvement by using a Q-function.", "section": "3 Method"}, {"figure_path": "5l5bhYexYO/figures/figures_25_1.jpg", "caption": "Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \u2202RTG/\u2202a, but local policy improvement requires the opposite, i.e., \u2202RTG/\u2202a. Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.", "description": "This figure illustrates the core idea of the paper.  Online Decision Transformers (ODTs) struggle to improve when pretrained with low-reward data because the gradient they produce (\u2202RTG/\u2202a) works against the direction needed for local policy improvement (\u2202RTG/\u2202a).  Adding RL gradients (such as TD3) allows the agent to improve locally, even if the target RTG is far from the pretrained policy and out of distribution, leading to better overall performance.", "section": "3 Method"}, {"figure_path": "5l5bhYexYO/figures/figures_28_1.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure shows the performance comparison of different methods on four robotic manipulation tasks from the Adroit environment.  The x-axis represents the number of online transitions, and the y-axis represents the normalized average reward.  The results demonstrate that the proposed TD3+ODT method outperforms several baselines, including the state-of-the-art Online Decision Transformer (ODT). Notably, while TD3, IQL, and TD3+ODT exhibit decent performance initially, TD3 struggles during online finetuning, whereas TD3+ODT significantly surpasses IQL in terms of performance improvement.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_29_1.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure displays the performance comparison of different methods on four Adroit robotic manipulation tasks: Pen, Hammer, Door, and Relocate.  Each task is tested with three datasets representing different data quality: expert, cloned, and human.  The results demonstrate that adding TD3 gradients to the ODT (Online Decision Transformer) significantly boosts online finetuning performance, particularly when pretrained on low-reward offline data.  The figure showcases the average normalized reward curves over 5 different seeds for each method and dataset. Notably, while TD3, IQL, and the proposed TD3+ODT perform well initially, TD3 degrades over time, whereas TD3+ODT consistently outperforms others, highlighting the effectiveness of the proposed approach.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_29_2.jpg", "caption": "Figure 20: The reward curves on hammer-cloned-v1 with different Ttrain and Teval = 1. While longer Ttrain leads to faster convergence in this environment, runs with too long Ttrain are also unstable.", "description": "This figure shows the effect of training context length (T_train) on the performance of the online finetuning of a decision transformer on the Hammer-cloned-v1 environment. Different curves represent different values of T_train, showing how the length of the training context affects the learning process.  The figure demonstrates that while increasing T_train initially improves the speed of convergence, excessively long context lengths (T_train) lead to instability and fluctuations in the learning process.", "section": "G.3 Ablations on training context length T_train"}, {"figure_path": "5l5bhYexYO/figures/figures_30_1.jpg", "caption": "Figure 21: The reward curves of our method when finetuned for more steps (we only report curves until the black line in Fig. 6). It is clearly shown that our method has greater potential for improvement when finetuned for more steps.", "description": "The figure shows the reward curves for three MuJoCo environments (Hopper, Walker2d, and Ant) with random datasets. The x-axis represents the number of online transitions, and the y-axis represents the normalized reward. The red line shows the performance of TD3+ODT, our proposed method. The dashed line indicates the point where the main paper stopped the experiment (500K steps).  The figure demonstrates that the TD3+ODT method continues to improve even beyond the 500K step mark, suggesting significant potential for improvement with extended finetuning. The shaded area indicates the standard deviation across 5 random seeds.", "section": "G More Results"}, {"figure_path": "5l5bhYexYO/figures/figures_30_2.jpg", "caption": "Figure 2: An illustration of a simple MDP, showing how RL can infer the direction for improvement, while online DT fails. Panels (a) and (b) show, DDPG and ODT+DDPG manage to maximize reward and find the correct optimal action quickly, while ODT fails to do so. Panel (c) shows how a DDPG/ODT+DDPG critic (from light blue/orange to dark blue/red) manages to fit ground truth reward (green curve). Panel (d) shows that the ODT policy (changing from light gray to dark) fails to discover the hidden reward peak near 0 between two low-reward areas (near -1 and 1 respectively) contained in the offline data. Meanwhile, ODT+DDPG succeeds in finding the reward peak.", "description": "This figure demonstrates the limitations of online decision transformers (ODT) when pretrained with low-reward data and how reinforcement learning (RL) gradients can improve performance.  It uses a simple MDP with a single state and continuous action space to illustrate how RL algorithms (DDPG, and DDPG combined with ODT) quickly learn to maximize reward and find optimal actions unlike the ODT that struggles. The figure also shows the learned critic function for the RL algorithms correctly approximate the reward function, while the ODT policy fails to identify a high-reward region, showcasing the benefit of incorporating RL gradients for online finetuning.", "section": "3.1 Why RL Gradients?"}, {"figure_path": "5l5bhYexYO/figures/figures_30_3.jpg", "caption": "Figure 20: The reward curves on hammer-cloned-v1 with different Ttrain and Teval = 1. While longer Ttrain leads to faster convergence in this environment, runs with too long Ttrain are also unstable.", "description": "This figure shows ablation studies on the effect of different training context lengths (T_train) on the performance of the TD3+ODT algorithm in the Hammer-cloned-v1 environment.  The results reveal that a longer training context length (T_train) leads to faster convergence during the initial phase of online finetuning. However, excessively long T_train values result in training instability and performance fluctuations. The optimal T_train value appears to be a balance between capturing sufficient context for accurate decision-making and preventing instability.  The shaded regions illustrate confidence intervals across multiple experimental runs.", "section": "G.3 Ablations on training context length Ttrain"}, {"figure_path": "5l5bhYexYO/figures/figures_31_1.jpg", "caption": "Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.", "description": "This figure compares the performance of different methods for online finetuning on the Adroit robotic manipulation tasks.  It shows the normalized average reward over online transitions for several methods, including the proposed TD3+ODT, as well as baselines like ODT, PDT, TD3, TD3+BC, IQL, and DDPG+ODT. The results demonstrate that TD3+ODT significantly outperforms the baselines, especially in scenarios where the offline data has low rewards.  While some other methods perform decently at the start of online finetuning, TD3+ODT shows considerably better improvement over time.", "section": "4.1 Adroit Environments"}, {"figure_path": "5l5bhYexYO/figures/figures_32_1.jpg", "caption": "Figure 25: The result of ODT and TD3+BC ablations (TD3+RVS, DDPG+ODT, TD3+BC with our architecture and curriculum RTG for ODT) on Adroit environments. The result shows that only TD3+BC with our architecture works. However, it remains worse than our method.", "description": "This figure presents an ablation study comparing several methods for online finetuning of decision transformers on the Adroit environment.  The goal is to determine the impact of different components on performance.  The methods compared include the original Online Decision Transformer (ODT), TD3+BC (a baseline combining TD3 and behavior cloning), TD3+RVS (using a supervised learning approach similar to the original Decision Transformer), TD3+BC with the transformer architecture from the proposed method (TD3+ODT), and the complete proposed method (TD3+ODT). The results demonstrate that only using the proposed method's architecture with TD3+BC leads to improved performance, although still below the full method.", "section": "G.9 Ablations on the Architecture"}, {"figure_path": "5l5bhYexYO/figures/figures_32_2.jpg", "caption": "Figure 25: The result of ODT and TD3+BC ablations (TD3+RVS, DDPG+ODT, TD3+BC with our architecture and curriculum RTG for ODT) on Adroit environments. The result shows that only TD3+BC with our architecture works. However, it remains worse than our method.", "description": "This figure presents ablation studies to isolate the impact of the transformer architecture and the RL-via-supervised-learning (RvS) method on the overall performance. Four methods were compared: ODT (baseline), TD3+BC, TD3+RvS (both using the original TD3+BC architecture), and TD3+BC using the transformer architecture from the proposed method.  The results across four Adroit environments highlight that only the combination of TD3+BC with the transformer architecture shows some improvement, demonstrating the importance of the chosen architecture in achieving better performance than the baseline ODT.  However, even this modified TD3+BC architecture did not achieve the same results as the proposed method (TD3+ODT), further emphasizing the synergistic effect of combining both the RL gradient and the transformer.", "section": "G.9 Ablations on the Architecture"}]