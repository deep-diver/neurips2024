[{"figure_path": "Y0EfJJeb4V/figures/figures_3_1.jpg", "caption": "Fig. 1: A: A schematic view of how a goal-reducer can be integrated with a policy network to generate actions. B: 3 types of subgoal sampling strategies. Red triangles: current states, green squares: goals, light green squares: subgoals sampled. C: An diagram of filtration radius changes trajectory connectivity. D: An example of proper filtered trajectory (black) compared with original random walk (gray) in 3D space.", "description": "This figure illustrates the core ideas behind the paper's proposed goal reduction method. Panel A shows how a goal-reducer module can be integrated with a policy network to generate actions by reducing complex goals into simpler subgoals. Panel B presents three different subgoal sampling strategies: random sampling, trajectory sampling, and loop-removal sampling. The loop-removal sampling strategy is the paper's main contribution, which aims to improve subgoal quality by removing loops or redundancies from the sampled trajectories. Panel C demonstrates how the filtration radius impacts trajectory connectivity, and Panel D provides an example of a proper filtered trajectory compared to an original random walk in a 3D space, illustrating the effectiveness of the loop-removal sampling method.", "section": "3 Methods"}, {"figure_path": "Y0EfJJeb4V/figures/figures_4_1.jpg", "caption": "Fig. 2: goal-reducer training results of geometric random graph (top) and the four-room gridworld task (bottom) with different strategies. A: Environment examples. B: Left: training loss, middle: training Optimality, right: training Equidex. C: Left: Optimality change when applying a trained goal-reducer recursively, right: same, but for Equidex.", "description": "This figure shows the training results of a goal-reducer using three different sampling strategies: random sampling, trajectory sampling, and loop-removal sampling.  The results are shown for two different environments: a geometric random graph and a four-room gridworld. The plots show the training loss, Optimality, and Equidex metrics over time.  Optimality and Equidex measure the quality of the subgoals generated by the goal-reducer.  The figure also shows how Optimality and Equidex change when the goal-reducer is applied recursively. Loop-removal sampling consistently outperforms the other two methods.", "section": "3.2 Effective goal-reducer through Loop-removal sampling"}, {"figure_path": "Y0EfJJeb4V/figures/figures_6_1.jpg", "caption": "Fig. 3: goal-reducer accelerates standard RL. A: An example input in the four-room navigation task (left) and performance comparison (right). B: Robot arm reach task (left) and performance comparison (right). C: How a goal-reducer agent works with only a local policy (left) and performance comparison of 3 algorithms (right).", "description": "This figure presents the results of experiments demonstrating the effectiveness of the goal-reducer in accelerating standard reinforcement learning (RL) algorithms across different tasks. Panel A shows a comparison of the performance of the standard Deep Q-learning (DRL) algorithm and a DRL algorithm augmented with the goal-reducer (DRL+GR) on a four-room navigation task. Panel B shows a similar comparison for a robotic arm reaching task, using the Soft Actor-Critic (SAC) algorithm as the baseline. Panel C illustrates the performance of a standalone goal-reducer with a local policy in contrast to the performance of the DRL and DRL+GR algorithms.", "section": "4 Results"}, {"figure_path": "Y0EfJJeb4V/figures/figures_7_1.jpg", "caption": "Fig. 4: goal-reducer in the treasure hunting task. A: The treasure hunting task description. B: Two configurations of maps used in the task. C: Population z-maps of S, G, and I in vmPFC. D: The relative representation z-value distribution for the three centers marked as S, G, and I in vmPFC. E: Population z-maps of S, Int. S, and I in bilateral putamen. F: Same as D, but for bilateral putamen.", "description": "This figure displays the results of comparing the goal-reducer model's performance with human brain activity during a treasure-hunting task. Panel A illustrates the task design, while panels B show the different map configurations. Panels C and E present activation maps in the vmPFC and putamen regions, respectively, showing the model's components' similarity to human brain activity.  Panels D and F present bar graphs summarizing the statistical results of the activation map comparisons.", "section": "4.3 goal-reducer in the Brain"}, {"figure_path": "Y0EfJJeb4V/figures/figures_13_1.jpg", "caption": "Fig. S. 1: Performance comparison between DRL+GR and DRL in the four-room navigation task with different sizes.", "description": "This figure shows the performance comparison between DRL (Deep Reinforcement Learning) and DRL+GR (DRL augmented with Goal Reducer) in a four-room navigation task with varying maze sizes (13x13, 15x15, 17x17, 19x19, 21x21).  The x-axis represents the number of transition steps, and the y-axis shows the correct rate. The shaded areas represent the standard deviation across multiple runs.  The results consistently demonstrate that DRL+GR outperforms DRL across all maze sizes, indicating the effectiveness of the goal-reducer in accelerating learning, even as task complexity increases.", "section": "A.3 goal-reducer accelerates DRL"}, {"figure_path": "Y0EfJJeb4V/figures/figures_14_1.jpg", "caption": "Fig.S. 2: An example of the goal-reducer planning process. Red dots show the agent's location, dark green dots (upper left) show the goal, and shadowed green circles show subgoals generated by goal-reducer over time. Darker green indicates more subgoals at the same location.", "description": "This figure shows an example of the goal-reducer planning process. The red dot represents the agent's current location. The dark green dot in the upper left corner represents the goal. The shadowed green circles represent the subgoals generated by the goal-reducer over time. The darker the green, the more subgoals are generated at that location. The figure shows the planning process at three different time steps: t=1, t=12, and t=26. At t=1, the agent is far from the goal and many subgoals are generated. At t=12, the agent is closer to the goal and fewer subgoals are generated. At t=26, the agent is very close to the goal and only a few subgoals are generated. This figure illustrates how the goal-reducer can effectively reduce the distance between the agent and the goal over time.", "section": "A.4 goal-reducer surpasses DRL+GR training details"}, {"figure_path": "Y0EfJJeb4V/figures/figures_15_1.jpg", "caption": "Fig. 1: A: A schematic view of how a goal-reducer can be integrated with a policy network to generate actions. B: 3 types of subgoal sampling strategies. Red triangles: current states, green squares: goals, light green squares: subgoals sampled. C: An diagram of filtration radius changes trajectory connectivity. D: An example of proper filtered trajectory (black) compared with original random walk (gray) in 3D space.", "description": "This figure demonstrates how a goal-reducer is integrated into a policy network (A). It also illustrates three types of subgoal sampling strategies (B): random sampling, trajectory sampling, and loop-removal sampling. Panel C shows how the filtration radius affects trajectory connectivity in loop-removal sampling, and panel D provides an example of a filtered trajectory compared to an original random walk in 3D space.", "section": "3 Methods"}]