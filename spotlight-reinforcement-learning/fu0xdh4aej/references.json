{"references": [{"fullname_first_author": "Haarnoja", "paper_title": "Soft actor-critic algorithms and applications", "publication_date": "2018-12-05", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, which is the basis for the BRO algorithm presented in this work."}, {"fullname_first_author": "Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-07-01", "reason": "This paper addresses the function approximation error in actor-critic methods, a key challenge in reinforcement learning that is also tackled by BRO."}, {"fullname_first_author": "Moskovitz", "paper_title": "Tactical optimism and pessimism for deep reinforcement learning", "publication_date": "2021-07-01", "reason": "This paper introduces optimistic exploration techniques, a core component of BRO\u2019s design to improve sample efficiency."}, {"fullname_first_author": "D'Oro", "paper_title": "Sample-efficient reinforcement learning by breaking the replay ratio barrier", "publication_date": "2022-07-01", "reason": "This paper investigates the impact of replay ratio on sample efficiency, a factor considered in BRO's design."}, {"fullname_first_author": "Bjorck", "paper_title": "Towards deeper deep reinforcement learning with spectral normalization", "publication_date": "2021-07-01", "reason": "This paper explores the use of spectral normalization for improving deep reinforcement learning, which is relevant to BRO\u2019s network architecture design."}]}