[{"figure_path": "fu0xdh4aEJ/figures/figures_0_1.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the BRO algorithm against other model-free (MF) and model-based (MB) reinforcement learning algorithms across four benchmark suites: DeepMind Control, MetaWorld, MyoSuite, and Dog & Humanoid.  BRO significantly outperforms all other algorithms across a total of 40 complex continuous control tasks.  The y-axis represents the interquartile mean performance, normalized to a maximum of 1.0, based on 10 independent runs with different random seeds.  All algorithms were trained for 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_1_1.jpg", "caption": "Figure 2: We report sample efficiency (left) and wallclock time (right) for BRO and BRO (Fast) (BRO with reduced replay ratio for increased compute efficiency), as well as baseline algorithms averaged over 40 tasks listed in Table 4. BRO achieves the best sample efficiency, whereas BRO (Fast) matches the sample efficiency of model-based TD-MPC2. In terms of wall clock efficiency, BRO runs approximately 25% faster than TD-MPC2. Remarkably, BRO (Fast) matches the wallclock efficiency of a standard SAC agent while achieving 400% better performance. The Y-axis reports the interquartile mean, with 1.0 representing the maximal possible performance.", "description": "This figure shows a comparison of sample efficiency and wall-clock time for the proposed BRO algorithm and its faster variant (BRO-Fast), along with several baseline reinforcement learning algorithms. The results are averaged across 40 benchmark tasks.  BRO demonstrates superior sample efficiency, while BRO-Fast achieves comparable efficiency to the model-based TD-MPC2. Notably, BRO-Fast matches SAC's wall-clock efficiency despite achieving significantly higher performance.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/figures/figures_2_1.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the BRO algorithm's performance against other model-free and model-based algorithms across three benchmark suites (DeepMind Control, MetaWorld, and MyoSuite) containing a total of 40 complex tasks.  The y-axis represents the normalized interquartile mean performance (1.0 being optimal), averaged across 10 random seeds, after 1 million environment steps.  The results visually demonstrate that BRO significantly outperforms the other algorithms in all three benchmark suites.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_3_1.jpg", "caption": "Figure 4: Scaling the critic parameter count for vanilla dense (Fujimoto et al., 2018), spectral normalization ResNet (Bjorck et al., 2021), and our BroNet for BRO (left), and SAC (right). We conclude that to achieve the best performance, we need both the right architecture (BroNet) and the correct algorithmic enhancements encapsulated in BRO. We report interquartile mean performance after 1M environment steps in tasks listed in Table 3, with error bars indicating 95% CI from 10 seeds. On the X-axis, we report the approximate parameter count of each configuration.", "description": "This figure shows the impact of scaling the critic network's parameter count on the performance of different algorithms (BRO and SAC) using three different architectures: vanilla dense, spectral normalization ResNet, and BroNet.  The left panel shows the results for BRO, while the right panel presents the results for SAC.  The x-axis represents the approximate number of parameters in millions (M), and the y-axis shows the interquartile mean (IQM) performance. The figure demonstrates that BroNet, combined with the BRO algorithm, achieves the best performance across different parameter counts. It highlights the importance of both appropriate architecture and algorithmic enhancements for achieving superior performance in continuous control tasks.", "section": "Scaling critic network and BroNet architecture"}, {"figure_path": "fu0xdh4aEJ/figures/figures_3_2.jpg", "caption": "Figure 5: BroNet architecture employed for actor and critic. Each fully connected layer is augmented with Layer Norm, which is essential to unlocking scaling. We use \u2248 5M parameters and N = 2 in the default setting.", "description": "This figure shows the architecture of BroNet, a neural network used in the BRO algorithm for both the actor and critic.  The key feature is the use of Layer Normalization after each fully connected layer.  This design is crucial for enabling effective scaling of the critic network which is a major part of the BRO algorithm's success in improving sample efficiency. The figure details the structure, showing the dense layers, Layer Normalization, ReLU activation function, and residual connections (indicated by N x). The default number of parameters is approximately 5 million, and N (the number of residual blocks) is set to 2. ", "section": "Scaling critic network and BroNet architecture"}, {"figure_path": "fu0xdh4aEJ/figures/figures_4_1.jpg", "caption": "Figure 6: To account for sample efficiency, we report the performance averaged at 250k, 500k, 750k, and 1M environment steps across different 5 replay ratios and 5 critic model sizes. All agents were evaluated in tasks listed in Table 3, and 10 random seeds per variant. The left figure shows performance scaling with increasing replay ratios (shapes) and model sizes (colors). The right figure examines the tradeoff between performance and computational cost when scaling replay ratios versus critic model sizes. Increasing model size leads to substantial performance improvements at lower compute costs compared to increasing the replay ratio. We present more scaling results in Appendix E, including a description of model sizes in Table 7.", "description": "This figure shows the results of an experiment investigating the impact of scaling replay ratio and critic model size on the performance of reinforcement learning agents.  The left panel shows how performance increases with both larger models and higher replay ratios. The right panel illustrates the trade-off between performance gains and computational cost when scaling using either method; increasing model size is more computationally efficient than increasing the replay ratio for achieving similar performance improvements.", "section": "Scaling replay ratio and relation to model scaling"}, {"figure_path": "fu0xdh4aEJ/figures/figures_5_1.jpg", "caption": "Figure 7: Impact of removing various BRO components on its performance. We report the percentage of the final performance for BRO (left) and BRO (Fast) (right). The y-axis shows the components that are ablated: -Scale denotes using a standard-sized network, +CDQ denotes using pessimistic Clipped Double Q-learning (which is removed by default in BRO), +RR=1 uses the standard replay ratio, -Dual \u03c0 removes optimistic exploration, and -Quantile and -WD stand for removing quantile Q-values and weight decay, respectively. We report the interquartile mean and 95% CIs for tasks in Table 3, with 10 random seeds. The results indicate that the Scale, CDQ, and RR=1 components are the most impactful for BRO. Since BRO (Fast) has RR=2 by default, reducing it to one does not significantly affect its performance.", "description": "This ablation study shows the impact of each BRO component on the overall performance. Removing the critic scaling (-Scale), using pessimistic Q-learning (+CDQ), and using the standard replay ratio (+RR=1) significantly reduces the performance. Other components, such as optimistic exploration (-Dual \u03c0), quantile Q-values (-Quantile), and weight decay (-WD) show smaller, but still noticeable impact.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/figures/figures_6_1.jpg", "caption": "Figure 8: IQM return learning curves for four Dog and three Humanoid environments from the DMC benchmark, plotted against the number of environment steps. Notably, the model-based approach (TD-MPC2) requires approximately 2.5 times more steps to match BRO performance.", "description": "This figure shows the learning curves for several reinforcement learning algorithms on seven locomotion tasks from the DeepMind Control suite (DMC). The y-axis represents the interquartile mean (IQM) return, a measure of the average reward obtained by the agents.  The x-axis represents the number of environment steps.  BRO significantly outperforms all other algorithms, reaching near-optimal performance much faster.  Specifically, the model-based algorithm TD-MPC2 takes about 2.5 times more steps to achieve similar results as BRO.", "section": "3 Analysis"}, {"figure_path": "fu0xdh4aEJ/figures/figures_7_1.jpg", "caption": "Figure 9: (Left) We analyze the importance of BRO components dependent on the critic model size. Interestingly, most components become less important as the critic capacity grows. (Right) We report the performance of BRO variants with and without a target network. All algorithm variants are run with 10 random seeds.", "description": "The left plot shows how the impact of several BRO design choices varies depending on the size of the critic network. The choices examined are disabling clipped double Q-learning (+CDQ), reducing the replay ratio (+RR=1), removing full-parameter resets (-Reset), removing the optimistic exploration policy (-Scaled \u03c0), removing quantile Q-value approximation (-Quantile), and reducing the batch size (+Batch). The right plot shows the impact of using target networks, comparing results of using target networks to results without them.", "section": "Analysis"}, {"figure_path": "fu0xdh4aEJ/figures/figures_7_2.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the proposed BRO algorithm against other state-of-the-art model-free and model-based reinforcement learning algorithms across three benchmark suites: DeepMind Control, MetaWorld, and MyoSuite.  The BRO algorithm achieves state-of-the-art results, significantly outperforming the other algorithms on 40 complex continuous control tasks. The y-axis represents the interquartile mean performance across 10 random seeds, with 1.0 indicating optimal performance.  The results show that BRO is highly sample-efficient, reaching near-optimal performance within 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_8_1.jpg", "caption": "Figure 11: We test three scenarios: offline (comparing vanilla BC to BroNet-based BC), offline fine-tuning (comparing vanilla IQL to BroNet-based IQL), and online with offline data (comparing vanilla SAC to BroNet-based SAC). The solid line represents BRO-based and the dashed line represents vanilla variants. Negative values on the X-axis refer to offline training. 10 seeds per task.", "description": "This figure compares the performance of three offline reinforcement learning algorithms (BC, IQL, SAC) with and without the BroNet architecture across three different scenarios: pure offline learning, offline fine-tuning, and online learning with offline data.  The BroNet architecture consistently improves performance across all three algorithms and scenarios, demonstrating its broad applicability and effectiveness.", "section": "4 Related Work"}, {"figure_path": "fu0xdh4aEJ/figures/figures_20_1.jpg", "caption": "Figure 2: We report sample efficiency (left) and wallclock time (right) for BRO and BRO (Fast) (BRO with reduced replay ratio for increased compute efficiency), as well as baseline algorithms averaged over 40 tasks listed in Table 4. BRO achieves the best sample efficiency, whereas BRO (Fast) matches the sample efficiency of model-based TD-MPC2. In terms of wall clock efficiency, BRO runs approximately 25% faster than TD-MPC2. Remarkably, BRO (Fast) matches the wallclock efficiency of a standard SAC agent while achieving 400% better performance. The Y-axis reports the interquartile mean, with 1.0 representing the maximal possible performance.", "description": "This figure compares the sample efficiency and wall-clock time of the proposed BRO algorithm and its faster variant (BRO-Fast) against several baseline reinforcement learning algorithms across 40 tasks.  The left panel shows BRO's superior sample efficiency, while the right panel demonstrates its faster wall-clock time, especially when compared to a model-based method (TD-MPC2).  BRO-Fast achieves similar sample efficiency to TD-MPC2 but with significantly improved performance compared to a standard SAC agent.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/figures/figures_21_1.jpg", "caption": "Figure 4: Scaling the critic parameter count for vanilla dense (Fujimoto et al., 2018), spectral normalization ResNet (Bjorck et al., 2021), and our BroNet for BRO (left), and SAC (right). We conclude that to achieve the best performance, we need both the right architecture (BroNet) and the correct algorithmic enhancements encapsulated in BRO. We report interquartile mean performance after 1M environment steps in tasks listed in Table 3, with error bars indicating 95% CI from 10 seeds. On the X-axis, we report the approximate parameter count of each configuration.", "description": "This figure compares the performance of different critic network architectures (vanilla dense, spectral normalization ResNet, and BroNet) when used with BRO and SAC algorithms. It shows that increasing the critic's parameter count leads to performance gains, but only with the right architecture (BroNet) and algorithm (BRO).  The plot demonstrates the interplay between architecture, algorithm, and model size in achieving optimal performance. Error bars represent 95% confidence intervals across 10 random seeds.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/figures/figures_21_2.jpg", "caption": "Figure 14: Comparison of five architecture designs across different environments: The top plot shows results on 5 DMC and 5 MetaWorld environments, the middle plot focuses on the 5 DMC environments, and the bottom plot highlights the Dog Trot environment. BRO and Spectral architectures each consist of 2 residual blocks. (Dense + LN) x 5 represents standard MLP networks with 5 hidden layers, each incorporating Layer Norm before activation. Lastly, BRO wo first LN refers to the BRO architecture without Layer Norm in the first Dense block, before the residual connection.", "description": "This figure compares the performance of five different network architectures across various continuous control tasks. The architectures tested are BRO, Spectral, BRO without the first Layer Normalization, and a simple 5-layer MLP with Layer Normalization.  The results show that BRO outperforms other architectures, particularly on challenging tasks like the Dog Trot environment.  This suggests that BRO's architecture, specifically the use of residual blocks and Layer Normalization, is crucial for achieving good performance on complex continuous control problems.", "section": "E Additional Experiments"}, {"figure_path": "fu0xdh4aEJ/figures/figures_21_3.jpg", "caption": "Figure 4: Scaling the critic parameter count for vanilla dense (Fujimoto et al., 2018), spectral normalization ResNet (Bjorck et al., 2021), and our BroNet for BRO (left), and SAC (right). We conclude that to achieve the best performance, we need both the right architecture (BroNet) and the correct algorithmic enhancements encapsulated in BRO. We report interquartile mean performance after 1M environment steps in tasks listed in Table 3, with error bars indicating 95% CI from 10 seeds. On the X-axis, we report the approximate parameter count of each configuration.", "description": "This figure compares the performance of different critic network architectures (vanilla dense, spectral normalization ResNet, and BroNet) when combined with BRO and SAC algorithms.  The x-axis shows the number of parameters in the critic network, and the y-axis represents the average performance across multiple tasks. The results demonstrate that BroNet, combined with BRO, achieves the best performance, highlighting the importance of both architectural design and algorithmic enhancements for optimal scaling.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/figures/figures_22_1.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the proposed BRO algorithm against other state-of-the-art model-free and model-based reinforcement learning algorithms across four benchmark suites (DeepMind Control, MetaWorld, MyoSuite, and Dog & Humanoid).  The bar chart shows the interquartile mean performance across 40 different tasks, with 1.0 indicating optimal performance.  BRO consistently outperforms other algorithms, demonstrating its effectiveness.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_22_2.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the BRO algorithm against other model-free and model-based algorithms across three benchmark suites: DeepMind Control, MetaWorld, and MyoSuite.  Each suite contains multiple tasks, and the graph shows that BRO significantly outperforms other algorithms across all 40 tasks. The Y-axis represents the interquartile mean performance (averaged across 10 random seeds), normalized to 1.0 for optimal performance on each benchmark. The experiment used 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_22_3.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure presents a bar chart comparing the performance of BRO against other model-free and model-based reinforcement learning algorithms across four benchmark suites: DeepMind Control, MetaWorld, MyoSuite, and Dog & Humanoid.  BRO consistently outperforms the other algorithms, achieving state-of-the-art results.  The y-axis shows the interquartile mean performance, normalized to 1.0 for optimal performance. The results are averaged over 10 random seeds and are based on 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_23_1.jpg", "caption": "Figure 19: We compare BRO (Fast) with SAC on the multi-task benchmark. 3 seeds", "description": "This figure compares the performance of BRO (Fast) and SAC algorithms on the MetaWorld MT50 benchmark.  The x-axis represents the number of environment steps (in thousands), and the y-axis shows the interquartile mean (IQM) of the performance. The shaded regions around the lines represent the 95% confidence intervals calculated from 3 random seeds.  This comparison highlights the superior performance of BRO (Fast) compared to SAC across the 50 different tasks within the MetaWorld MT50 benchmark.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/figures/figures_23_2.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the proposed BRO algorithm against other state-of-the-art model-free and model-based reinforcement learning algorithms across four benchmark suites (DeepMind Control, MetaWorld, MyoSuite, and Dog and Humanoid).  The results are shown as interquartile means across ten random seeds, normalized to the best possible performance (1.0). BRO consistently outperforms other methods across all benchmarks, demonstrating its superior performance. All agents were trained for 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_24_1.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of BRO against other state-of-the-art model-free and model-based reinforcement learning algorithms across four benchmark suites (DeepMind Control, MetaWorld, MyoSuite, Dog & Humanoid).  The results show that BRO significantly outperforms all other algorithms across all 40 tasks, achieving near-optimal performance on the challenging Dog & Humanoid tasks.  The y-axis represents the interquartile mean performance, normalized such that 1.0 indicates the best possible performance for each benchmark. The experiment is run for 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_24_2.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the proposed BRO algorithm against other state-of-the-art model-free and model-based reinforcement learning algorithms across four benchmark suites: DeepMind Control, MetaWorld, MyoSuite, and Dog & Humanoid.  The y-axis represents the interquartile mean performance, normalized such that 1.0 is the best possible score for each benchmark. The results show BRO significantly outperforming the other algorithms across all benchmark suites. This demonstrates the effectiveness of the BRO approach in scaling model capacity and achieving sample-efficient continuous control.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_25_1.jpg", "caption": "Figure 1: BRO sets new state-of-the-art outperforming model-free (MF) and model-based (MB) algorithms on 40 complex tasks covering 3 benchmark suites. Y-axes report interquartile mean calculated on 10 random seeds, with 1.0 representing the best possible performance in a given benchmark. We use 1M environment steps.", "description": "This figure compares the performance of the proposed BRO algorithm against other state-of-the-art model-free and model-based reinforcement learning algorithms across four benchmark suites (DeepMind Control, MetaWorld, MyoSuite, and Dog & Humanoid) with a total of 40 tasks.  The y-axis represents the interquartile mean performance, normalized to 1.0 (best possible performance), calculated across 10 random seeds. The x-axis represents the benchmark suite and individual tasks within each suite. The results show that BRO significantly outperforms all other algorithms across all benchmarks. Note that these results are obtained using 1 million environment steps.", "section": "1 Introduction"}, {"figure_path": "fu0xdh4aEJ/figures/figures_26_1.jpg", "caption": "Figure 2: We report sample efficiency (left) and wallclock time (right) for BRO and BRO (Fast) (BRO with reduced replay ratio for increased compute efficiency), as well as baseline algorithms averaged over 40 tasks listed in Table 4. BRO achieves the best sample efficiency, whereas BRO (Fast) matches the sample efficiency of model-based TD-MPC2. In terms of wall clock efficiency, BRO runs approximately 25% faster than TD-MPC2. Remarkably, BRO (Fast) matches the wallclock efficiency of a standard SAC agent while achieving 400% better performance. The Y-axis reports the interquartile mean, with 1.0 representing the maximal possible performance.", "description": "This figure compares the sample efficiency and wall-clock time of the proposed BRO algorithm (and its faster variant, BRO (Fast)) against several baseline reinforcement learning algorithms across 40 benchmark tasks.  The left panel shows BRO achieving superior sample efficiency, while the right panel demonstrates that BRO (Fast) achieves comparable wall-clock time to TD-MPC2 (a model-based method), despite significantly outperforming it in terms of final performance.  BRO (Fast) is even faster than a standard SAC agent while still achieving substantially better performance. The results are presented as interquartile means, normalized to the maximum possible performance (1.0).", "section": "Bigger, Regularized, Optimistic (BRO) algorithm"}]