{"references": [{"fullname_first_author": "Marc G Bellemare", "paper_title": "The arcade learning environment: An evaluation platform for general agents", "publication_date": "2013-01-01", "reason": "This paper introduces a widely used benchmark environment for evaluating reinforcement learning agents, providing a common testing ground for many exploration methods."}, {"fullname_first_author": "Yuri Burda", "paper_title": "Exploration by random network distillation", "publication_date": "2018-01-01", "reason": "This paper proposes a novel exploration method based on random network distillation, which has significantly influenced the development of intrinsic reward methods in reinforcement learning."}, {"fullname_first_author": "Marc Bellemare", "paper_title": "Unifying count-based exploration and intrinsic motivation", "publication_date": "2016-01-01", "reason": "This foundational paper unifies count-based exploration and intrinsic motivation, providing a theoretical framework for understanding and improving exploration strategies."}, {"fullname_first_author": "Volodymyr Mnih", "paper_title": "Human-level control through deep reinforcement learning", "publication_date": "2015-02-26", "reason": "This seminal work demonstrates the remarkable success of deep reinforcement learning in achieving human-level performance in Atari games, highlighting the potential of RL in complex domains."}, {"fullname_first_author": "Roberta Raileanu", "paper_title": "RIDE: rewarding impact-driven exploration for procedurally-generated environments", "publication_date": "2020-01-01", "reason": "This paper introduces RIDE, a state-of-the-art exploration method that leverages the impact of actions on the environment, which is directly compared with the proposed method in this paper."}]}