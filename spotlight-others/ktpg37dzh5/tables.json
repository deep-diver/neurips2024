[{"figure_path": "ktpG37Dzh5/tables/tables_8_1.jpg", "caption": "Table 1: Parameter compression % and accuracy for different baseline methods and settings of the proposed method. Standard deviations over ten runs are included. Best accuracy for the compression methods is given in bold.", "description": "This table presents a comparison of the performance of different pruning methods across various datasets and neural network architectures.  The methods compared include L2, E[\u03b8], SNR, BMRSN, and two variants of BMRSu. For each method and dataset, the table reports the compression rate achieved (%) and the corresponding accuracy.  The best accuracy for each compression rate is highlighted in bold.  The results showcase the effectiveness of the proposed BMRS method in achieving high compression rates while maintaining competitive accuracy compared to other state-of-the-art pruning methods.", "section": "5 Experiments"}, {"figure_path": "ktpG37Dzh5/tables/tables_9_1.jpg", "caption": "Table 2: Parameter compression % and accuracy for different baseline methods and settings of the proposed method. Standard deviations over three runs are included. Best accuracy for the compression methods is given in bold.", "description": "This table presents the results of experiments on CIFAR10 and TinyImagenet datasets using different pruning methods. The table shows the compression percentage and accuracy achieved by each method, including baselines like L2,  E[\u03b8], SNR, and the proposed BMRS and its variants (BMRSN, BMRSU-8, BMRSU-4).  The best accuracy for each compression rate is highlighted in bold.  This table allows for a comparison of the performance-efficiency trade-off of various pruning methods.", "section": "Experiments"}, {"figure_path": "ktpG37Dzh5/tables/tables_15_1.jpg", "caption": "Table 1: Parameter compression % and accuracy for different baseline methods and settings of the proposed method. Standard deviations over ten runs are included. Best accuracy for the compression methods is given in bold.", "description": "This table presents the performance comparison of different pruning methods on three datasets (MNIST, Fashion-MNIST, CIFAR10) using two neural network architectures (MLP and Lenet5).  The table shows the compression rate achieved by each method and the corresponding accuracy.  The best accuracy for each compression level is highlighted in bold.  The methods compared include L2 norm pruning, pruning based on the expected value of noise variables (E[\u03b8]), pruning based on signal-to-noise ratio (SNR), and the two proposed methods: BMRSN and BMRSu (with two different levels of precision).", "section": "5 Experiments"}, {"figure_path": "ktpG37Dzh5/tables/tables_16_1.jpg", "caption": "Table 2: Parameter compression % and accuracy for different baseline methods and settings of the proposed method. Standard deviations over three runs are included. Best accuracy for the compression methods is given in bold.", "description": "This table presents the compression rates and accuracies achieved by different pruning methods on the CIFAR10 and TinyImagenet datasets using two different model architectures: Resnet50-pretrained and Vision Transformer.  The methods compared include baseline methods (None, L2, E[\u03b8], SNR) and the proposed BMRS methods (BMRSN, BMRSU-8, BMRSU-4).  The best accuracy for each compression rate is highlighted in bold. This allows for a direct comparison of the performance and efficiency trade-offs of different pruning techniques. ", "section": "5 Experiments"}]