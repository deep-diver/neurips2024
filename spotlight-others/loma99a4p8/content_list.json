[{"type": "text", "text": "Diffusion Models With Learned Adaptive Noise ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subham Sekhar Sahoo Aaron Gokaslan Chris De Sa Cornell Tech, NYC, USA. Cornell Tech, NYC, USA. Cornell University, Ithaca, USA. ssahoo@cs.cornell.edu akg87@cs.cornell.edu cdesa@cs.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Volodymyr Kuleshov Cornell Tech, NYC, USA. kuleshov@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have gained traction as powerful algorithms for synthesizing highquality images. Central to these algorithms is the diffusion process, a set of equations which maps data to noise in a way that can significantly affect performance. In this paper, we explore whether the diffusion process can be learned from data. Our work is grounded in Bayesian inference and seeks to improve log-likelihood estimation by casting the learned diffusion process as an approximate variational posterior that yields a tighter lower bound (ELBO) on the likelihood. A widely held assumption is that the ELBO is invariant to the noise process: our work dispels this assumption and proposes multivariate learned adaptive noise (MULAN), a learned diffusion process that applies noise at different rates across an image. Specifically, our method relies on a multivariate noise schedule that is a function of the data to ensure that the ELBO is no longer invariant to the choice of the noise schedule as in previous works. Empirically, MULAN sets a new state-of-the-art in density estimation on CIFAR-10 and ImageNet and reduces the number of training steps by $50\\%$ . We provide the code1, along with a blog post and video tutorial on the project page: ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models, inspired by the physics of heat diffusion, have gained traction as powerful tools for generative modeling, capable of synthesizing realistic, high-quality images [51, 16, 43, 14]. Central to these algorithms is the diffusion process, a gradual mapping of clean images into white noise. The reverse of this mapping defines the data-generating process we seek to learn\u2014hence, its choice can significantly impact performance [22]. The conventional approach involves adopting a diffusion process derived from the laws of thermodynamics, which, albeit simple and principled, may be suboptimal due to its lack of adaptability to the dataset. ", "page_idx": 0}, {"type": "text", "text": "In this study, we investigate whether the notion of diffusion can be instead learned from data. Our motivating goal is to perform accurate log-likelihood estimation and probabilistic modelling, and we take an approach grounded in Bayesian inference [23]. We view the diffusion process as an approximate variational posterior: learning this process induces a tighter lower bound (ELBO) on the marginal likelihood of the data. Although previous work argued that the ELBO objective of a diffusion model is invariant to the choice of diffusion process [20, 22], we show that this claim is only true for the simplest types of univariate Gaussian noise: we identify a broader class of noising processes whose optimization yields significant performance gains. ", "page_idx": 0}, {"type": "image", "img_path": "loMa99A4p8/tmp/a40e97b68c80800f96915403ee11b171b4396316715bad4f47d45ac1eac50e7d.jpg", "img_caption": ["Figure 1: (Left) Comparison of noise schedule properties: Multivariate Learned Adaptive Noise schedule (MULAN) (ours) versus a typical scalar noise schedule. Unlike scalar noise schedules, MULAN\u2019s multivariate and input-adaptive properties improve likelihood. (Right) Likelihood in bits-per-dimension (BPD) on CIFAR-10 without data augmentation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Specifically, we propose a new diffusion process, multivariate learned adaptive noise (MuLAN), which augments classical diffusion models [51, 20] with three innovations: a per-pixel polynomial noise schedule, an adaptive input-conditional noising process, and auxiliary latent variables. In practice, this method learns the schedule by which Gaussian noise is applied to different parts of an image, and allows tuning this noise schedule to the each image instance. ", "page_idx": 1}, {"type": "text", "text": "Our learned diffusion process yields improved log-likelihood estimates on two standard image datasets, CIFAR10 and ImageNet. Remarkably, we achieve state-of-the-art performance with less than half of the training time of previous methods. Our method also does not require any modifications to the underlying UNet architecture, which makes it compatible with most existing diffusion algorithms. ", "page_idx": 1}, {"type": "text", "text": "Contributions In summary, our paper makes the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We demonstrate that the ELBO of a diffusion model is not invariant to the choice of noise process for many types of noise, thus dispelling a common assumption in the field.   \n2. We introduce MULAN, a learned noise process that adaptively adds multivariate Gaussian noise at different rates across an image in a way that is conditioned on arbitrary context (including the image itself).   \n3. We empirically demonstrate that learning the diffusion process speeds up training and matches the previous state-of-the-art models using $\\mathbf{2x}$ less compute, and also achieves a new state-of-the-art in density estimation on CIFAR-10 and ImageNet ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A diffusion process $q$ transforms an input datapoint denoted by $\\mathbf{x}_{\\mathrm{0}}$ and sampled from a distribution $q(\\mathbf{x}_{0})$ into a sequence of noisy latent variables $\\mathbf{x}_{t}$ for $t\\in[0,1]$ by progressively adding Gaussian noise of increasing magnitude [51, 16, 53]. The marginal distribution of each latent is defined by $q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\alpha_{t}\\mathbf{x}_{0},\\sigma_{t}I)$ where the diffusion parameters $\\alpha_{t},\\sigma_{t}\\in\\mathbb{R}^{+}$ implicitly define a noise schedule as a function of $t$ , such that $\\nu(t)=\\alpha_{t}^{2}/\\sigma_{t}^{2}$ is a monotonically decreasing function in $t$ . Given any discretization of time into $T$ timesteps of width $1/T$ , we define $\\dot{t(i)}=i/T$ and $s(i)=(i-1)/T$ and we use $\\mathbf{x}_{0:1}$ to denote the subset of variables associated with these timesteps; the forward process $q$ can be shown to factorize into a Markov chain $\\begin{array}{r}{q(\\mathbf{x}_{0:1})=q(\\mathbf{x}_{0})\\prod_{i=1}^{T}q(\\mathbf{x}_{t(i)}\\bar{\\vert\\mathbf{x}}_{s(i)})}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "The diffusion model $p_{\\theta}$ is defined by a neural network (with parameters $\\theta$ ) used to denoise the forward process $q$ . Given a discretization of time into $T$ steps, $p$ factorizes as $p_{\\theta}(\\mathbf{x}_{0:1})\\;=$ $\\begin{array}{r}{p_{\\theta}(\\mathbf{x}_{1})\\prod_{i=1}^{T}p_{\\theta}(\\mathbf{x}_{s(i)}\\vert\\mathbf{x}_{t(i)})}\\end{array}$ . We treat the $\\mathbf{x}_{t}$ for $t>0$ as latent variables and fit $p_{\\theta}$ by maximizing the evi dence lower bound (ELBO) on the marginal log-likelihood given by: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p_{\\theta}(\\mathbf{x}_{0})=\\operatorname{ELBO}(p_{\\theta},q)+\\operatorname{D}_{\\mathrm{KL}}\\big[q\\big(\\mathbf{x}_{t(1):t(T)}|\\mathbf{x}_{0}\\big)\\big]\\|p_{\\theta}\\big(\\mathbf{x}_{t(1):t(T)}|\\mathbf{x}_{0}\\big)\\big]\\geq\\operatorname{ELBO}(p_{\\theta},q)}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In most works, the noise schedule, as defined by $\\nu(t)$ , is either fixed or treated as a hyperparameter [16, 3, 18]. Chen [3], Hoogeboom et al. [18] show that the noise schedule can have a significant impact on sample quality. Kingma et al. [20] consider learning $\\nu(t)$ , but argue that the KL divergence terms in the ELBO are invariant to the choice of function $\\nu$ , except for the initial values $\\nu(0),\\nu(1)$ , and they set these values to hand-specified constants in their experiments. They only consider learning $\\nu$ for the purpose of minimizing the variance of the gradient of the ELBO. In this work, we show that the ELBO is not invariant to more complex forward processes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Diffusion Models With Multivariate Learned Adaptive Noise ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here, we introduce a new diffusion process, multivariate learned adaptive noise (MuLAN), which introduces three innovations: a per-pixel polynomial noise schedule, a conditional noising process, and auxiliary-variable reverse diffusion. We describe these below. ", "page_idx": 2}, {"type": "text", "text": "3.1 Why Learned Diffusion? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our goal is to perform accurate density estimation and probabilistic modelling, and we take an approach grounded in Bayesian inference [23]. Notice that the gap between the evidence lower bound $\\mathtt{E L B O}(p,q)$ and the marginal log-likelihood (MLL) in Eq. 1 is precisely the KL divergence $\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}\\big[q\\big(\\mathbf{x}_{t(1):t(T)}|\\mathbf{x}_{0}\\big)\\big|\\big|p_{\\theta}\\big(\\mathbf{x}_{t(1):t(T)}|\\mathbf{x}_{0}\\big)\\big]}\\end{array}$ between the diffusion process $q$ over the latents $\\mathbf{x}_{t}$ and the true posterior of the diffusion model. The diffusion process plays the role of a variational posterior $q$ in $\\mathtt{E L B O}(p,q)$ ; optimizing $q$ thus tightens the gap (MLL \u2212ELBO). ", "page_idx": 2}, {"type": "text", "text": "This observation suggests that the ELBO can be made tighter by choosing a diffusion processes $q$ that is closer to the true posterior $p_{\\theta}\\big(\\mathbf{x}_{t(1):t(T)}\\big|\\mathbf{x}_{0}\\big)$ . In fact, the key idea of variational inference is to optimize $\\operatorname*{max}_{q\\in\\mathcal{Q}}$ $\\mathrm{ELBO}(p,q)$ over a family of approximate posteriors $\\mathcal{Q}$ to induce a tighter ELBO [23]. Most diffusion algorithms, however optimize $\\operatorname*{max}_{p\\in\\mathcal{P}}$ $\\mathrm{ELBO}(p,q)$ within some family $\\mathcal{P}$ with a fixed $q$ . Our work seeks to jointly optimize $\\operatorname*{max}_{p\\in\\mathcal{P},q\\in\\mathcal{Q}}$ $\\mathrm{ELBO}(p,q);$ ; we will show in our experiments that this improves the likelihood estimation. ", "page_idx": 2}, {"type": "text", "text": "The task of log-likelihood estimation is directly motivated by applied problems such as data compression [31]. In that domain, arithmetic coding techniques can take a generative model and produce a compression algorithm that provably achieves a compression rate (in bits per dimension) that equals the model\u2019s log-likelihood [4]. Other applications of log-likelihood estimation include adversarial example detection [52], semi-supervised learning [5], and others. ", "page_idx": 2}, {"type": "text", "text": "Note that our primary focus is density estimation and probabilistic modeling rather than sample quality. The visual appeal of generated images (as measured by e.g., FID) correlates imperfectly with log-likelihood. We focus here on pushing the state-of-the-art in log-likelihood estimation, and while we report FID for completeness, we defer sample quality optimization to future work. ", "page_idx": 2}, {"type": "text", "text": "3.2 A Forward Diffusion Process With Multivariate Adaptive Noise ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Next, our plan is to define a family of approximate posteriors $\\mathcal{Q}$ , as well as a family suitably matching reverse processes $\\mathcal{P}$ , such that the optimization problem $\\scriptstyle\\operatorname*{max}_{p\\in{\\mathcal{P}},q\\in{\\mathcal{Q}}}\\mathrm{ELBO}(p,q)$ is tractable and does not suffer from the aforementioned invariance to the choice of $q$ . This subsection focuses on defining $\\mathcal{Q}$ ; the next sections will show how to parameterize and train a reverse model $p\\in\\mathcal{P}$ . ", "page_idx": 2}, {"type": "text", "text": "Notation. Given two vectors a and b, we use the notation ab to represent the Hadamard product (element-wise multiplication). Additionally, we denote element-wise division of a by $\\mathbf{b}$ as a / b. We denote the mapping diag(.) that takes a vector as input and produces a diagonal matrix as output. ", "page_idx": 2}, {"type": "text", "text": "3.2.1 Multivariate Gaussian Noise Schedule ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Intuitively, a multivariate noise schedule injects noise at different rates for each pixel of an input image. This enables adapting the diffusion process to spatial variations within the image. We will also see that this change is sufficient to make the ELBO no longer invariant in $q$ . ", "page_idx": 2}, {"type": "text", "text": "Formally, we define a forward diffusion process with a multivariate noise schedule $q$ via the marginal for each latent noise variable $\\mathbf{x}_{t}$ for $t\\in[0,1]$ , where the marginal is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\boldsymbol{\\alpha}_{t}\\mathbf{x}_{0},\\mathrm{diag}(\\boldsymbol{\\sigma}_{t}^{2})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{t},\\mathbf{x}_{0}\\in\\mathbb{R}^{d},\\,\\alpha_{t},\\pmb{\\sigma}_{t}\\in\\mathbb{R}_{+}^{d}$ and $d$ is the dimensionality of the input data. The $\\alpha_{t},\\sigma_{t}$ denote varying amounts of signal associated with each component (i.e., each pixel) of $\\mathbf{x}_{\\mathrm{0}}$ as a function of ", "page_idx": 2}, {"type": "text", "text": "time $t(i)$ . We define the multivariate signal-to-noise ratio as $\\pmb{\\nu}(t)=\\pmb{\\alpha}_{t}^{2}/\\pmb{\\sigma}_{t}^{2}$ and choose $\\alpha_{t},\\sigma_{t}$ so that $\\nu(t)$ decreases monotonically in $t$ along all dimensions and is differentiable in $t\\in[0,1]$ . Let $\\alpha_{t|s}=\\alpha_{t}/\\alpha_{s}$ and $\\pmb{\\sigma}_{t|s}^{2}=\\pmb{\\sigma}_{t}^{2}-\\pmb{\\dot{\\alpha}}_{t|s}^{2}/\\pmb{\\sigma}_{s}^{2}$ with all operations applied elementwise. ", "page_idx": 3}, {"type": "text", "text": "These marginals induce transition kernels between steps $s<t$ given by (Suppl. 19): ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathcal{N}\\left(\\mathbf{x}_{s};\\mu_{q}=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{0},\\;\\Sigma_{q}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Sec. 3.5, we argue that this class of diffusion process $\\mathcal{Q}$ induces an ELBO that is not invariant to $q\\in\\mathcal{Q}$ . The ELBO consists of a line integral along the diffusion trajectory specified by $\\nu(t)$ . A line integrand is almost always path-dependent, unless its integral corresponds to a conservative force field, which is rarely the case for a diffusion process [55]. See Sec. 3.5 for details. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Adaptive Noise Schedule Conditioned On Context ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Next, we extend the diffusion process to support context-adaptive noise. This enables injecting noise in a way that is dependent on the features of an image. Formally, suppose we have access to a context variable $\\mathbf{c}\\in\\mathbb{R}^{m}$ which encapsulates high-level information regarding $\\mathbf{x}_{\\mathrm{0}}$ . Examples of c could be a class label, a vector of attributes (e.g., features characterizing a human face), or even the input $\\mathbf{x}_{\\mathrm{0}}$ itself. We define the marginal of the latent $\\mathbf{x}_{t}$ in the forward process as $q(\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{c})=\\bar{\\mathcal{N}}(\\mathbf{x}_{t};\\alpha_{t}(\\mathbf{c})\\mathbf{x}_{0},\\pmb{\\sigma}_{t}^{2}(\\mathbf{c}))$ ; the reverse process can be similarly derived (Suppl. 19) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0},\\mathbf{c}\\rangle=\\mathcal{N}\\left(\\mu_{q}=\\frac{\\alpha_{t|s}(\\mathbf{c})\\sigma_{s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}(\\mathbf{c})\\alpha_{s}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{0},\\ \\Sigma_{q}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}(\\mathbf{c})\\sigma_{t|s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the diffusion parameters $\\alpha_{t},\\,\\sigma_{t}$ are now conditioned on c via a neural network. ", "page_idx": 3}, {"type": "text", "text": "Specifically, we parameterize the diffusion parameters $\\alpha_{t}(\\mathbf{c}),\\pmb{\\sigma}_{t}(\\mathbf{c}),\\pmb{\\nu}(t,\\mathbf{c})$ as $\\begin{array}{r l}{\\alpha_{t}^{2}(\\mathbf{c})}&{{}=}\\end{array}$ sigmoid $\\bar{(-\\gamma_{\\phi}(\\mathbf{c},t))}$ , $\\pmb{\\sigma}_{t}^{2}(\\mathbf{c})=\\mathrm{sigmoid}(\\gamma_{\\phi}(\\mathbf{c},t))$ , and $\\pmb{\\nu}(\\mathbf{c},t)=\\exp\\left(-\\gamma_{\\phi}(\\mathbf{c},t)\\right)$ . Here, $\\gamma_{\\phi}(\\mathbf{c},t):$ $\\mathbb{R}^{m}\\times[0,1]\\rightarrow[\\gamma_{\\operatorname*{min}},\\gamma_{\\operatorname*{max}}]^{d}$ is a neural network with the property that $\\gamma_{\\phi}(\\mathbf{c},t)$ is monotonic in $t$ . Following Kingma et al. [20], Zheng et al. [65], we set $\\gamma_{\\mathrm{min}}=-13.30$ , $\\gamma_{\\mathrm{max}}=5.0$ . ", "page_idx": 3}, {"type": "text", "text": "We explore various parameterizations for $\\gamma_{\\phi}(\\mathbf{c},t)$ . These schedules are designed in a manner that guarantees $\\gamma_{\\phi}(\\mathbf{c},0)=\\gamma_{\\mathrm{min}}\\mathbf{1_{d}}$ and $\\gamma_{\\phi}(\\mathbf{c},1)=\\gamma_{\\operatorname*{max}}\\mathbf{1}_{\\mathbf{d}}$ , Below, we list these parameterizations. The polynomial parameterization is novel to our work and yields significant performance gains. ", "page_idx": 3}, {"type": "text", "text": "Monotonic Neural Network [20]. We use the monotonic neural network $\\gamma_{\\mathrm{vdm}}(t)$ , proposed in VDM to express $\\gamma$ as a function of $t$ such that $\\gamma_{\\mathrm{vdm}}(t):[0,1]\\to[\\gamma_{\\mathrm{min}},\\gamma_{\\mathrm{max}}]d$ . Then we use FiLM conditioning [38] in the intermediate layers of this network via a neural network that maps $\\mathbf{z}$ . The activations of the FiLM layer are constrained to be positive. ", "page_idx": 3}, {"type": "text", "text": "Polynomial. (Ours) We express $\\gamma_{\\phi}(\\mathbf{c},t)$ as a monotonic degree 5 polynomial in $t$ . Details about the exact functional form of this polynomial and its implementation can be found in Suppl. E.2. ", "page_idx": 3}, {"type": "text", "text": "3.3 Auxiliary-Variable Reverse Diffusion Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In principle, we can fti a normal diffusion model in conjunction with our proposed forward diffusion process. However, variational inference suggests that the variational and the true posterior ought to have the same dependency structure: that is the only way for the KL divergence between these two distributions to be zero. Thus, we introduce a class of approximate reverse processes $\\mathcal{P}$ that match the structure of $\\mathcal{Q}$ and that are naturally suitable for joint optimization $\\begin{array}{r}{\\operatorname*{max}_{p\\in\\mathcal{P},q\\in\\mathcal{Q}}\\mathrm{ELBO}(p,q)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Formally, we define a diffusion model where the reverse diffusion process is conditioned on the context c. Specifically, given any discretization of $t\\,\\in\\,[0,1]$ into $T$ time steps as in Sec. 2, we introduce a context-conditional diffusion model $p_{\\theta}(\\mathbf{x}_{0:1}|\\mathbf{c})$ that factorizes as the Markov chain ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{0:1}|\\mathbf{c})=p_{\\theta}(\\mathbf{x}_{1}|\\mathbf{c})\\prod_{i=1}^{T}p_{\\theta}(\\mathbf{x}_{s(i)}|\\mathbf{x}_{t(i)},\\mathbf{c}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given that the true reverse process is a Gaussian as specified in Eq. 4, the ideal $p_{\\theta}$ matches this parameterization (the proof mirrors that of regular diffusion models; Suppl. D), which yields ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{\\mathbf{(x}_{s}|\\mathbf{c},\\mathbf{x}_{t})}=\\mathcal{N}\\left(\\mu_{p}=\\frac{\\alpha_{t|s}(\\mathbf{c})\\sigma_{s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}(\\mathbf{c})\\alpha_{s}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t),\\boldsymbol{\\Sigma}_{p}=\\mathrm{diag}\\left(\\sigma_{s}^{2}(\\mathbf{c})\\sigma_{t|s}^{2}(\\mathbf{c})/\\sigma_{t}^{2}(\\mathbf{c})\\right)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ , is a neural network that approximates $\\mathbf{x}_{\\mathrm{0}}$ . Instead of parameterizing $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ directly using a neural network, we consider two other parameterizations. One is the noise parameterization [16] where $\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)$ is the denoising model which is parameterized as $\\pmb{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\,=$ $(\\mathbf{x}_{t}-\\alpha_{t}(\\mathbf{c})\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{c}))/\\bar{\\sigma}_{t}(\\mathbf{c})$ ; see Suppl. E.1.1 and the other is v-parameterization [45] where $\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)$ is a neural network that models $\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)\\,=\\,(\\alpha_{t}(\\mathbf{c})\\mathbf{x}_{t}\\,-\\,\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t))/\\sigma_{t}(\\mathbf{c})$ ; see Suppl. E.1.2. ", "page_idx": 4}, {"type": "text", "text": "3.3.1 Challenges in Conditioning on Context ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Note that the model $p_{\\theta}(\\mathbf{x}_{0:1}|\\mathbf{c})$ implicitly assumes the availability of c at generation time. Sometimes, this context may be available, such as when we condition on a label. We may then fit a conditional diffusion process with a standard diffusion objective $\\mathbb{E}_{\\mathbf{x}_{0},c}[\\mathrm{ELBO}(\\mathbf{x}_{0},p_{\\theta}(\\mathbf{\\dot{x}}_{0:1}|\\mathbf{c}),q_{\\phi}(\\mathbf{x}_{0:1}|\\mathbf{c})]$ , in which both the forward and the backward processes are conditioned on c (see Sec. 3.4). ", "page_idx": 4}, {"type": "text", "text": "When c is not known at generation time, we may fti a model $p_{\\theta}$ that does not condition on c. Unfortunately, this also forces us to define $p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t})\\overset{.}{=}\\mathcal{N}(\\pmb{\\mu}_{p}(\\mathbf{x}_{t},t),\\pmb{\\Sigma}_{p}(\\mathbf{x}_{t},t))$ where $\\mu_{p}(\\mathbf{x}_{t},t),\\boldsymbol{\\Sigma}_{p}(\\mathbf{x}_{t},t)$ is parameterized directly by a neural network. We can no longer use a noise parameterization $\\pmb{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\,=\\,(\\mathbf{x}_{t}-\\alpha_{t}(\\mathbf{c})\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{c}))/\\pmb{\\sigma}_{t}(\\mathbf{c})$ because it requires us to compute $\\alpha_{t}(\\mathbf{c})$ and ${\\pmb\\sigma}_{t}({\\bf c})$ , which we do not know. Since noise parameterization plays a key role in the sample quality of diffusion models [16], this approach limits performance. ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Conditioning Noise on an Auxiliary Latent Variable ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose an alternative strategy for learning conditional forward and reverse processes $p,q$ that feature the same structure and hence support efficient noise parameterization. Our approach is based on the introduction of auxiliary variables [60], which lift the distribution $p_{\\theta}$ into an augmented latent space. Experiments (Suppl. D.3) and theory (Suppl. D) confirm that this approach performs better than parameterizing c using a neural network, $\\mathbf{c}_{\\theta}(\\mathbf{x}_{t},t)$ . ", "page_idx": 4}, {"type": "text", "text": "Specifically, we introduce an auxiliary latent variable $\\textbf{z}\\in\\mathbb{R}^{m}$ and define a lifted $p_{\\theta}(\\mathbf{x},\\mathbf{z})~=$ $p_{\\theta}(\\mathbf{x}|\\mathbf{z})p_{\\theta}(\\mathbf{z})$ , where $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$ is the conditional diffusion model from Eq. 5 (with context c set to $\\mathbf{z},$ and $p_{\\theta}(\\mathbf{z})$ is a simple prior (e.g., unit Gaussian or fully factored Bernoulli). The latents $\\mathbf{z}$ can be interpreted as a high-level semantic representation of $\\mathbf{x}$ that conditions both the forward and the reverse processes. Unlike $\\mathbf{x}_{0:1}$ , the $\\mathbf{z}$ are not constrained to have a particular dimension and can be a low-dimensional vector of latent factors of variation. They can be continuous or discrete. The learning objective for the lifted $p_{\\theta}$ is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p_{\\theta}(\\mathbf{x}_{0})\\geq\\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})}[\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{z})]-\\mathbf{D}_{\\mathrm{KL}}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})\\|p_{\\theta}(\\mathbf{z}))}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})}\\mathrm{ELBO}(p_{\\theta}(\\mathbf{x}_{0:1}|\\mathbf{z}),q_{\\phi}(\\mathbf{x}_{0:1}|\\mathbf{z}))-\\mathbf{D}_{\\mathrm{KL}}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})\\|p_{\\theta}(\\mathbf{z})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{ELBO}(p_{\\theta}(\\mathbf{x}_{0:1}|\\mathbf{z}),q_{\\phi}(\\mathbf{x}_{0:1}|\\mathbf{z}))$ denotes the variational lower bound (VLB) of a diffusion model (defined in Eq. 1) with a forward process $q_{\\phi}(\\mathbf{x}_{0:1}|\\mathbf{z})$ (defined in Eq. 4 and Sec. 3.2.2) and and an approximate reverse process $p_{\\theta}(\\mathbf{x}_{0:1}|\\mathbf{z})$ (defined in Eq. 5), both conditioned on $\\mathbf{z}$ . The distribution $q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})$ is an approximate posterior for $\\mathbf{z}$ parameterized by a neural network with parameters $\\phi$ . ", "page_idx": 4}, {"type": "text", "text": "Crucially, note that in the learning objective (Eq. 8), the context, which in this case is ${\\bf z}$ , is available at training time in both the forward and reverse processes. At generation time, we can still obtain a valid context vector by sampling an auxiliary latent from $p_{\\theta}(\\mathbf{z})$ . Thus, this approach addresses the aforementioned challenges and enables us to use the noise parameterization in Eq. 6. ", "page_idx": 4}, {"type": "text", "text": "Although we apply Jensen\u2019s inequality twice to get (8), this also enables us to learn the noise process, which significantly offsets any potential increase in ELBO gap reduction and improves $\\begin{array}{r}{\\mathrm{ELBO}(p_{\\boldsymbol{\\theta}}(\\mathbf{x}_{0:1}|\\mathbf{z}),q_{\\boldsymbol{\\phi}}(\\mathbf{x}_{0:1}|\\mathbf{\\dot{z}}))}\\end{array}$ by optimizing over a more expressive class of posteriors. This claim is empirically validated in Table 2. ", "page_idx": 4}, {"type": "text", "text": "3.4 Variational Lower Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we derive a precise formula for the learning objective (8) of the auxiliary-variable diffusion model. Using the objective of a diffusion model in (1) we can write (8) as the sum of four terms: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p_{\\theta}(\\mathbf{x}_{0})\\geq\\mathbb{E}_{q_{\\phi}}[\\mathcal{L}_{\\mathrm{recons}}+\\mathcal{L}_{\\mathrm{diffusion}}+\\mathcal{L}_{\\mathrm{prior}}+\\mathcal{L}_{\\mathrm{latent}}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The reconstruction loss, $\\mathcal{L}_{\\mathrm{recons}}$ , can be (stochastically and differentiably) estimated using standard techniques; see [23], $\\mathcal{L}_{\\mathrm{prior}}\\;=\\;-\\mathrm{D}_{\\mathrm{KL}}\\big[q_{\\phi}(\\mathbf{x}_{1}|\\mathbf{x}_{0},\\mathbf{z})\\|p_{\\theta}(\\mathbf{x}_{1})\\big]$ is the diffusion prior term, $\\mathcal{L}_{\\mathrm{latent}}\\,=$ $-\\mathrm{D}_{\\mathrm{KL}}\\big[\\mathbf{\\bar{q}}_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})\\|p_{\\theta}(\\mathbf{z})\\big]$ is the latent prior term, and $\\mathcal{L}_{\\mathrm{diffusion}}$ is the diffusion loss term, which we examine below. The complete derivation is given in Suppl. E.3. ", "page_idx": 5}, {"type": "text", "text": "3.4.1 Diffusion Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Discrete-Time Diffusion. We start by defining $p_{\\theta}$ in discrete time, and as in Sec. 2, we let $T>0$ be the number of total time steps and define $t(i)=i/T$ and $s(i)=(i-1)/T$ as indexing variables over the time steps. We also use $\\mathbf{x}_{0:1}$ to denote the subset of variables associated with these timesteps. Starting with the expression in Eq. 1 and following the steps in Suppl. E, we can write $\\mathcal{L}_{\\mathrm{diffusion}}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{\\mathrm{diffusion}}=-\\sum_{i=2}^{T}\\mathrm{D}_{\\mathrm{KL}}\\big[q_{\\phi}\\big(\\mathbf{x}_{s\\,(i)}|\\mathbf{x}_{t(i)},\\mathbf{x}_{0},\\mathbf{z}\\big)\\big|\\big|p_{\\theta}\\big(\\mathbf{x}_{s\\,(i)}|\\mathbf{x}_{t(i)},\\mathbf{z}\\big)\\big]}\\\\ {\\displaystyle=\\frac{1}{2}\\sum_{i=2}^{T}[\\big(\\epsilon_{t}-\\epsilon_{\\theta}\\big(\\mathbf{x}_{t},\\mathbf{z},t(i)\\big)\\big)^{\\top}\\mathrm{diag}\\left(\\gamma(\\mathbf{z},s(i)\\big)-\\gamma(\\mathbf{z},t(i))\\right)\\big(\\epsilon_{t}-\\epsilon_{\\theta}\\big(\\mathbf{x}_{t},\\mathbf{z},t(i)\\big)\\big)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Continuous-Time Diffusion. We can also consider the limit of the above objective as we take an infinitesimally small partition of $t\\in[0,1]$ , which corresponds to the limit when $T\\rightarrow\\infty$ . In Suppl. E we show that taking this limit of Eq. 10 yields the continuous-time diffusion loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{diffusion}}=-\\frac{1}{2}\\mathbb{E}_{t\\sim[0,1]}[(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t))^{\\top}{\\mathrm{diag}}\\left(\\nabla_{t}\\gamma(\\mathbf{z},t)\\right)(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t))]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\nabla_{t}\\gamma(\\mathbf{z},t)\\in\\mathbb{R}^{d}$ denotes the Jacobian of $\\gamma(\\mathbf{z},t)$ with respect to the scalar $t$ . We observe that the limit of $T\\rightarrow\\infty$ yields improved performance, matching the existing theoretical argument by Kingma et al. [20]. ", "page_idx": 5}, {"type": "text", "text": "3.4.2 Auxiliary latent loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We try two different kinds of priors for $p_{\\theta}(\\mathbf{z})$ : discrete $(\\mathbf{z}\\in\\{0,1\\}^{m})$ and continuous $\\mathbf{\\alpha}(\\mathbf{z}\\in\\mathbb{R}^{m})$ ). ", "page_idx": 5}, {"type": "text", "text": "Continuous Auxiliary Latents. In the case where ${\\bf z}$ is continuous, we select $p_{\\theta}(\\mathbf{z})$ as $\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{m})$ . This leads to the following $\\mathrm{KL}$ loss term: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})\\|p_{\\theta}(\\mathbf{z}))=\\frac{\\daleth}{2}(\\mu^{\\top}(\\mathbf{x}_{0})\\mu(\\mathbf{x}_{0}))+\\mathrm{tr}(\\Sigma^{2}(\\mathbf{x}_{0})-\\mathbf{I}_{m})-\\log|\\Sigma^{2}(\\mathbf{x}_{0})|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Discrete Auxiliary Latents. In the case where $\\mathbf{z}$ is discrete, we select $p_{\\theta}(\\mathbf{z})$ as a uniform distribution. Let $\\mathbf{z}~\\in~\\{0,1\\}^{m}$ be a $k$ -hot vector sampled from a discrete Exponential Family distribution $p_{\\theta}(\\mathbf{z};\\theta)$ with logits $\\theta$ . Niepert et al. [34] show that $\\mathbf{z}\\ \\sim\\ p_{\\boldsymbol{\\theta}}(\\mathbf{z};\\boldsymbol{\\theta})$ is equivalent to $\\mathbf{z}=\\arg\\operatorname*{max}_{y\\in Y}\\langle\\theta+\\epsilon_{g},y\\rangle$ where $\\epsilon_{g}$ denotes the sum of gamma distribution Suppl. F, $Y$ denotes the set of all $k$ -hot vectors of some fixed length $m$ . For $k>1$ , To differentiate through the arg max we use a relaxed estimator, Identity, as proposed by Sahoo et al. [44]. This leads to the following KL loss term: $\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}\\bigl(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})\\bigr|\\bigl|p_{\\theta}(\\mathbf{z})\\bigr)\\stackrel{!}{=}-\\sum_{i=1}^{m}q_{\\phi}(\\mathbf{z}\\bigl|\\mathbf{x}_{0}\\bigr)_{i}\\bigl(\\log q_{\\phi}(\\mathbf{z}\\bigl|\\mathbf{x}_{0}\\bigr)_{i}+\\log m\\bigr)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "3.5 The Variational Lower Bound as a Line Integral Over The Noise Schedule ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having defined our loss, we now return to the question of whether it is invariant to the choice of diffusion process. Notice that we may rewrite Eq. 11 in the following vectorized form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{diffusion}}=-\\frac{1}{2}\\int_{0}^{1}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t))^{2}\\cdot\\nabla_{t}\\nu(\\mathbf{z},t)\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the square is applied elementwise. We seek to rewrite (12) as a line integral $\\begin{array}{r}{\\int_{a}^{b}\\mathbf{f}(\\mathbf{r}(t))\\cdot\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbf{r}(t)\\mathrm{d}t}\\end{array}$ for some vector field f and trajectory $\\mathbf{r}(t)$ . Recall that $\\nu(\\mathbf{z},t)$ is monotonically decreasing in each coordinate as a function of $t$ ; hence, it is invertible on its image, and we can write $t=\\nu_{\\mathbf{z}}^{-1}(\\nu(\\mathbf{z},t))$ for some $\\nu_{z}^{-1}$ . Let $\\bar{\\mathbf{x}}_{\\theta}\\big(\\mathbf{x}_{\\nu(\\mathbf{z},t)},\\mathbf{z},\\nu(\\mathbf{z},t)\\big)\\equiv\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{\\nu_{z}^{-1}(\\nu(\\mathbf{z},t))},\\bar{\\mathbf{z}},\\nu_{\\mathbf{z}}^{-1}(\\nu(\\mathbf{z},t))\\big)$ and note that for all $t$ , we can write $\\mathbf{x}_{t}$ as $\\mathbf{x}_{\\pmb{\\nu}(\\mathbf{z},t)}$ ; see Eq. 30, and have $\\bar{\\mathbf{x}}_{\\theta}\\big(\\mathbf{x}_{\\nu(\\mathbf{z},t)},\\mathbf{z},\\nu(\\mathbf{z},t)\\big)\\equiv\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t},\\mathbf{z},t\\big)$ . We can then write the integral in (12) as $\\begin{array}{r}{\\int_{0}^{1}(\\mathbf{x}_{0}-\\bar{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu(\\mathbf{z},t)},\\mathbf{z},\\nu(\\mathbf{z},t)))^{2}\\cdot\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(\\mathbf{z},t)\\rangle\\mathrm{d}t}\\end{array}$ , which is a line integral with $\\mathbf{f}(\\mathbf{r}(t))\\equiv(\\mathbf{x}_{0}-\\bar{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu(\\mathbf{z},t)},\\mathbf{z},\\nu(\\mathbf{z},t)))^{2}$ and $\\mathbf{r}(t)\\equiv\\pmb{\\nu}(\\mathbf{z},t)$ . ", "page_idx": 6}, {"type": "text", "text": "Intuitive explanation. Imagine piloting a plane across a region with cyclones and strong winds, as shown in Fig. 5. Plotting a direct, straight-line course through these adverse weather conditions requires more fuel and effort due to increased resistance. By navigating around the cyclones and winds, however, the plane reaches its destination with less energy, even if the route is longer. ", "page_idx": 6}, {"type": "text", "text": "This intuition translates into mathematical and physical terms. The plane\u2019s trajectory is denoted by $\\mathbf{r}(t)\\in\\mathbb{R}_{+}^{n}$ , while the forces acting on it are represented by $\\mathbf{f}(\\mathbf{r}(t))\\,\\in\\,\\mathbb{R}^{n}$ . The work required to navigate is given by $\\begin{array}{r}{\\int_{0}^{1}\\mathbf{f}(\\mathbf{r}(t))\\cdot\\frac{d}{d t}\\mathbf{r}(t),d t}\\end{array}$ . Here, the work depends on the trajectory because $\\mathbf{f}(\\mathbf{r}(t))$ is not a conservative field. ", "page_idx": 6}, {"type": "text", "text": "This concept also applies to the diffusion NELBO. From Eq. 12, it\u2019s clear that the trajectory $\\mathbf{r}(t)$ is parameterized by the noise schedule $\\nu(\\mathbf{z},t)$ , which is influenced by complex forces, f (analogous to weather patterns), represented by the dimension-wise reconstruction error of the denoising model, $(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}^{}(\\mathbf{x}_{t},\\mathbf{z},t))^{2}$ . Thus, the diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}$ , can be interpreted as the work done along the trajectory $\\nu(\\mathbf{z},t)$ in the presence of these vector field forces f. By learning the noise schedule, we can avoid \u201chigh-resistance\u201d paths (those where the loss accumulates rapidly), thereby minimizing the overall \u201cenergy\u201d expended, as measured by the NELBO. Since the diffusion process corresponds to non-conservative force fields, as noted in Spinney & Ford [55], different noise schedules should yield different NELBOs\u2014a result supported by our empirical findings. In Suppl. E.5, we show that variational diffusion models are limited to linear trajectories $\\nu(t)$ , rendering their objective invariant to the noise schedule. In contrast, our approach learns a multivariate $\\pmb{\\nu}$ , enabling paths that achieve a better ELBO. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section reports experiments on the CIFAR-10 [25] and ImageNet-32 [58] datasets. We don\u2019t employ data augmentation and we use the same architecture and settings as in the VDM model [20]. The encoder, $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ , is modeled using a sequence of 4 ResNet blocks which is much smaller than the denoising network that uses 32 such blocks (i.e., we increase parameter count by only about $10\\%$ ); the noise schedule $\\gamma_{\\phi}$ is modeled using a two-layer MLP. In all our experiments, we use discrete auxiliary latents with $m=50$ and $k=15$ . A detailed description can be found in Suppl. G. ", "page_idx": 6}, {"type": "text", "text": "4.1 Training Speed ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In these experiments, we replace VDM\u2019s noise process with MULAN. On CIFAR-10, MULAN attains VDM\u2019s likelihood score of 2.65 in just 2M steps, compared to VDM\u2019s 10M steps 1). When trained on 4 V100 GPUs, VDM achieves a training rate of 2.6 steps/second, while MULAN trains slightly slower at 2.24 steps/second due to the inclusion of an additional encoder network. However, despite this slower training pace, VDM requires 30 days to reach a BPD of 2.65, whereas Mulan achieves the same BPD within a significantly shorter timeframe of 10 days. On ImageNet-32, VDM integrated with MULAN reaches a likelihood of 3.71 in half the time, achieving this score in 1M steps versus the 2M steps required by VDM. ", "page_idx": 6}, {"type": "text", "text": "4.2 Likelihood Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Table 2, we also compare MULAN with other recent methods on CIFAR-10 and ImageNet32. MULAN was trained using v-parameterization for 8M steps on CIFAR-10 and 2M steps on Imagenet-32. During inference, we extract the underlying probability flow ODE and use it to estimate the log-likelihood; see Suppl. I.2. Our algorithm establishes a new state-of-the-art in density estimation on both ImageNet-32 and CIFAR-10. In Table 8, we also compute variational lower bounds (VLBs) of $\\leq\\!2.59$ and $\\leq\\!3.71$ on CIFAR-10 and ImageNet, respectively. Each bound improves over published results (Table 2); our true NLLs (via flow ODEs) are even lower. ", "page_idx": 6}, {"type": "table", "img_path": "loMa99A4p8/tmp/2cc44386544ce2c5e46804b9fcf972330fcad6620ad305334eb9454029cc8e43.jpg", "table_caption": ["Table 1: Likelihood in bits per dimension (BPD) based on the Variational Lower Bound (VLB) estimate (Suppl. I.1), sample quality (FID scores) and number of function evaluations (NFE) on CIFAR-10, for vanilla VDM and VDM when endowed with MULAN. FID and NFE were computed for $10\\mathbf{k}$ samples generated using an adaptive-step ODE solver. Both methods use noise parameterization (Suppl. E.1.1). "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "loMa99A4p8/tmp/0100a7b3e2cd905d6c55f86343ba205eb2d1dcf19c4d79348f8bd1dc9bde1ae4.jpg", "table_caption": ["Table 2: Likelihood in bits per dimension (BPD) on the test set of CIFAR-10 and ImageNet. Results with $\\bullet\\prime\\prime$ means they are not reported in the original papers. Model types are autoregressive (AR), normalizing flows (Flow), diffusion models (Diff). We only compare with results achieved without data augmentation. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Alternative Learned Diffusion Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Concurrent work that seeks to improve log-likelihood estimation by learning the forward diffusion process includes Neural Diffusion Models (NDMs) [1] and DiffEnc [33]. In NDMs, the noise schedule is fixed, but the mean of each marginal $q(\\mathbf{x}_{t}|x_{0})$ is learned, while DiffEnc adds a correction term to $q$ . Diffusion normalizing flows (DNFs) represent an earlier effort where $q$ is a normalizing flow trained by backpropagating through sampling. In Table 3, we compare against NDMs, DiffEnc, and DNFs on the CIFAR-10 dataset, using the authors\u2019 published results; note that their published ImageNet numbers are either not available or are reported on a different dataset version that is not comparable. Our approach to learned diffusion outperforms previous and concurrent work. ", "page_idx": 7}, {"type": "table", "img_path": "loMa99A4p8/tmp/2617bdfc48351dd0e9cdeb5b469d0a8a3fde8013682c9827464b5133d85b2c5d.jpg", "table_caption": ["Table 3: Likelihood in bits per dimension (bpd) on CIFAR-10 for learned diffusion methods. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Analysis And Additional Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Due to the expensive cost of training, we only performed ablation studies on CIFAR-10 with a reduced batch size of 64 and trained the model for $2.5\\mathrm{M}$ training steps. In Fig. 2a we ablate each component of MULAN: when we remove the conditioning on an auxiliary latent space from MULAN so that we have a multivariate noise schedule that is solely conditioned on time $t$ , our performance becomes comparable to that of VDM, on which our model is based. Changing to a scalar noise schedule based on latent variable $\\mathbf{z}$ initially underperforms compared to VDM. This drop aligns with our likelihood formula (Eq. 6) which includes $\\mathrm{D}_{\\mathrm{KL}}\\big(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0}^{-})|p_{\\theta}(\\mathbf{z})\\big)$ , an extra term not in VDM. The input-conditioned scalar schedule doesn\u2019t offer any advantage over the scalar schedule used in VDM. This is due to the reasons outlined in Sec. 3.5. ", "page_idx": 7}, {"type": "image", "img_path": "loMa99A4p8/tmp/ac43bbb17ad11ee167e2e20e42675709c8ecea2d4882c85a72a6f3e1b942631c.jpg", "img_caption": ["(a) In MULAN w/o aux. latent, the noise isn\u2019t conditioned on a latent. MULAN w/o multivariate uses a scalar noise schedule. MULAN w/o adaptivity has a linear schedule and no auxiliary latents. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "loMa99A4p8/tmp/6550b358340c8a5fd309db3230af66f9da93c352dd4f332dbbb46faa3bd29136.jpg", "img_caption": ["(b) MULAN with different noise schedule parameterizations: polynomial, monotonic neural network, and linear. Our proposed polynomial parameterization performs the best. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Ablating components of MULAN on CIFAR-10 over 2.5M steps with batch size of 64. ", "page_idx": 8}, {"type": "text", "text": "Perceptual Quality While perceptual quality is not the focus of this work, we report FID numbers for the VDM model and MuLAN (Table 1). We use RK45 ODE solver to generate samples by solving the reverse time Flow ODE (Eq. 76). We observe that MuLAN does not degrade FIDs, while improving log-likelihood estimation. Note that MuLAN does not incorporate many tricks that improve FID such as exponential moving averages, truncations, specialized learning schedules, etc.; our FID numbers can be improved in future work using these techniques. ", "page_idx": 8}, {"type": "text", "text": "Loss curves for different noise schedules. We investigate different parameterizations of the noise schedule in Fig. 2b. Among polynomial, linear, and monotonic neural network, we find that the polynomial parameterization yields the best performance. The polynomial noise schedule is a novel component introduced in our work. The reason why a polynomial function works better than a linear or a monotonic neural network as proposed by VDM is rooted in Occam\u2019s razor. In Suppl. E.2, we show that a degree 5 polynomial is the simplest polynomial that satisfies several desirable properties, including monotonicity and having a derivative that equals zero exactly twice. More expressive models (e.g., monotonic 3-layer MLPs) are more difficult to optimize. ", "page_idx": 8}, {"type": "text", "text": "Examining the noise schedule. Since the noise schedule, $\\gamma_{\\phi}(\\mathbf{z},t)$ is multivariate, we expect to learn different noise schedules for different input dimensions and different inputs $\\mathbf{z}\\,\\sim\\,p_{\\theta}(\\mathbf{z})$ . In Fig. 3, we take our best trained model on CIFAR-10 and visualize the variance of the noise schedule at each point in time for different pixels, where the variance is taken on 128 samples $\\mathbf{z}\\sim p_{\\theta}(\\mathbf{z})$ . ", "page_idx": 8}, {"type": "text", "text": "We note an increased variation in the early portions of the noise schedule. However, on an absolute scale, the variance of this noise is smaller than we expected. We also tried to visualize noise schedules across different dataset images and across different areas of the same image; refer to Fig. 13. We also generated synthetic datasets in which each datapoint contained only high frequencies or only low frequencies, and with random masking ap", "page_idx": 8}, {"type": "image", "img_path": "loMa99A4p8/tmp/82048ba32a09f40145e3886a4cc509cad3f78db7db555ae75b90cac973b18f5d.jpg", "img_caption": ["", "Figure 3: Noise schedule visualizations for MULAN on CIFAR-10. In this figure, we plot the variance of $\\nu_{\\phi}(\\mathbf{z},t)$ across different $\\mathbf{z}\\sim p_{\\theta}(\\mathbf{z})$ where each curve represents the SNR corresponding to an input dimension. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "plied to parts of the data points; see Suppl. H. Surprisingly, none of these experiments revealed human-interpretable patterns in the learned schedule, although we did observe clear differences in likelihood estimation. We hypothesize that other architectures and other forms of conditioning may reveal interpretable patterns of variation; however, we leave this exploration to future work. ", "page_idx": 8}, {"type": "text", "text": "Replacing the noise schedules in a trained denoising model. We also confirm experimentally our claim that the learning objective is not invariant to the multivariate noise schedule. We replace the noise schedule in the trained denoising model with two alternatives: MULAN with scalar noise schedule, and a linear noise schedule: $\\gamma_{\\phi}(\\mathbf{z},t)=\\gamma_{\\operatorname*{min}}+t(\\gamma_{\\operatorname*{max}}-\\gamma_{\\operatorname*{min}})\\mathbf{1}_{\\mathbf{d}}$ ; see Kingma et al. [20]. For both the noise schedules the likelihood reduces to the same value as that of the VDM: 2.65. ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Diffusion models have emerged in recent years as powerful tools for modeling complex distributions [51, 16], extending flow-based methods [53, 24, 48, 49] The noise schedule, which determines the amount and type of noise added at each step, plays a critical role in diffusion models. Chen [3] empirically demonstrate that different noise schedules can significantly impact the generated image quality using various handcrafted noise schedules. Kingma et al. [20] showed that the likelihood of a diffusion model remains invariant to the noise schedule with a scalar noise schedule. In this work we show that the ELBO is no longer invariant to multivariate noise schedules. ", "page_idx": 9}, {"type": "text", "text": "Recent works explored multivariate noise schedules (including blurring, masking, etc.) [17, 42, 36, 12], yet none have delved into learning the noise schedule conditioned on the input data itself. Likewise, conditional noise processes are typically not learned [26, 39, 62] and their conditioner (e.g., a prompt) is always available. Auxiliary variable models [63, 60] add semantic latents in $p$ , but not in $q$ , and they don\u2019t condition or learn $q$ . In contrast, we learn multivariate noise conditioned on latent context. ", "page_idx": 9}, {"type": "text", "text": "Diffusion normalizing flows (DNFs) [64] learn a $q$ parameterized by a normalizing flow; however, such $q$ do not admit tractable marginals and require sampling full data-to-noise trajectories from $q$ , which is expensive. Concurrent work on neural diffusion models (NDMs) and DiffEnc admits tractable marginals $q$ with learned means and univariate schedules; this yields more expressive $q$ than ours but requires computing losses in a modified space that precludes using a noise parameterization and certain sampling strategies. Empirically, MuLAN performs better with fewer parameters (Suppl. A). ", "page_idx": 9}, {"type": "text", "text": "Optimal transport techniques seek to learn a noise process that minimizes the transport cost from data to noise, which in practice produces smoother diffusion trajectories that facilitate sampling. Schrondinger bridges [47, 6, 59, 37] learn expressive $q$ do not admit analytical marginals, require computing full data-to-noise trajectories and involve iterative optimization (e.g., sinkhorn), which can be slow. Rectification [27] seeks diffusion paths that are close to linear; this improves sampling, while our method chooses paths that improve log-likelihood. See Suppl. A for more detailed comparisons. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced MULAN, a context-adaptive noise process that applies Gaussian noise at varying rates across input data. Our theory challenges the prevailing notion that the likelihood of diffusion models is independent of the noise schedule: this independence only holds true for univariate schedules. Our evaluation of MULAN spans multiple image datasets, where it outperforms state-of-the-art generative diffusion models. We hope our work will motivate further research into the design of noise schedules, not only for improving likelihood estimation but also to improve image quality generation [35, 53]. A stronger fit to the data distribution also holds promise for improving downstream applications of generative modeling, e.g., compression or decision-making [32, 9, 8, 41]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially funded by Volodymyr Kuleshov\u2019s the National Science Foundation under awards DGE-1922551, CAREER awards 2046760 and 2145577, and the National Institute of Health under award MIRA R35GM151243, and by Christopher De Sa\u2019s NSF RI-CAREER award 2046760. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Grigory Bartosh, Dmitry Vetrov, and Christian A Naesseth. Neural diffusion models. arXiv preprint arXiv:2310.08337, 2023. ", "page_idx": 9}, {"type": "text", "text": "[2] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[3] Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023.   \n[4] Thomas M Cover and Joy A Thomas. Data compression. Elements of Information Theory, pp. 103\u2013158, 2005.   \n[5] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Russ R Salakhutdinov. Good semi-supervised learning that requires a bad gan. Advances in neural information processing systems, 30, 2017.   \n[6] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.   \n[8] Shachi Deshpande and Volodymyr Kuleshov. Calibrated uncertainty estimation improves bayesian optimization, 2023.   \n[9] Shachi Deshpande, Kaiwen Wang, Dhruv Sreenivas, Zheng Li, and Volodymyr Kuleshov. Deep multi-modal structural equations for causal effect estimation with unstructured proxies. Advances in Neural Information Processing Systems, 35:10931\u201310944, 2022.   \n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \n[11] J.R. Dormand and P.J. Prince. A family of embedded runge-kutta formulae. Journal of Computational and Applied Mathematics, 6(1):19\u201326, 1980. ISSN 0377-0427. doi: https://doi. org/10.1016/0771-050X(80)90013-3. URL https://www.sciencedirect.com/science/ article/pii/0771050X80900133.   \n[12] Weitao Du, He Zhang, Tao Yang, and Yuanqi Du. A flexible diffusion model. In International Conference on Machine Learning, pp. 8678\u20138696. PMLR, 2023.   \n[13] Pavlos S. Efraimidis and Paul G. Spirakis. Weighted random sampling with a reservoir. Information Processing Letters, 97(5):181\u2013185, 2006. ISSN 0020-0190. doi: https://doi.org/ 10.1016/j.ipl.2005.11.003. URL https://www.sciencedirect.com/science/article/ pii/S002001900500298X.   \n[14] Aaron Gokaslan, A Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion models trained on creative-commons images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8250\u20138260, 2024.   \n[15] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[17] Emiel Hoogeboom and Tim Salimans. Blurring diffusion models. arXiv preprint arXiv:2209.05557, 2022.   \n[18] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093, 2023.   \n[19] Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989.   \n[20] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[22] Diederik P Kingma and Ruiqi Gao. Understanding the diffusion objective as a weighted integral of elbos. arXiv preprint arXiv:2303.00848, 2023.   \n[23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[24] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.   \n[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[26] Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, and Tie-Yan Liu. Priorgrad: Improving conditional denoising diffusion models with data-dependent adaptive prior. arXiv preprint arXiv:2106.06406, 2021.   \n[27] Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based generative models. In International Conference on Machine Learning, pp. 18957\u201318973. PMLR, 2023.   \n[28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[30] Aaron Lou and Stefano Ermon. Reflected diffusion models. In International Conference on Machine Learning, pp. 22675\u201322701. PMLR, 2023.   \n[31] David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.   \n[32] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 16569\u201316594. PMLR, 2022. URL https://proceedings. mlr.press/v162/nguyen22b.html.   \n[33] Beatrix MG Nielsen, Anders Christensen, Andrea Dittadi, and Ole Winther. Diffenc: Variational diffusion with a learned encoder. arXiv preprint arXiv:2310.19789, 2023.   \n[34] Mathias Niepert, Pasquale Minervini, and Luca Franceschi. Implicit MLE: backpropagating through discrete exponential family distributions. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 14567\u201314579, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 7a430339c10c642c4b2251756fd1b484-Abstract.html.   \n[35] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 4055\u20134064. PMLR, 2018.   \n[36] Naama Pearl, Yaron Brodsky, Dana Berman, Assaf Zomet, Alex Rav Acha, Daniel Cohen-Or, and Dani Lischinski. Svnr: Spatially-variant noise removal with denoising diffusion. arXiv preprint arXiv:2306.16052, 2023.   \n[37] Stefano Peluchetti. Diffusion bridge mixture transports, schr\u00f6dinger bridge problems and generative modeling. Journal of Machine Learning Research, 24(374):1\u201351, 2023.   \n[38] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[39] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Gradtts: A diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pp. 8599\u20138608. PMLR, 2021.   \n[40] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[41] Richa Rastogi, Yair Schiff, Alon Hacohen, Zhaozhi Li, Ian Lee, Yuntian Deng, Mert R. Sabuncu, and Volodymyr Kuleshov. Semi-parametric inducing point networks and neural processes. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $\\equiv$ FE99-fDrWd5.   \n[42] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. arXiv preprint arXiv:2206.13397, 2022.   \n[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.   \n[44] Subham Sekhar Sahoo, Anselm Paulus, Marin Vlastelica, V\u00edt Musil, Volodymyr Kuleshov, and Georg Martius. Backpropagation through combinatorial algorithms: Identity with projection works. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ JZMR727O29.   \n[45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \n[46] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn $^{++}$ : Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.   \n[47] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] Phillip Si, Allan Bishop, and Volodymyr Kuleshov. Autoregressive quantile flows for predictive uncertainty estimation. In International Conference on Learning Representations, 2022.   \n[49] Phillip Si, Zeyi Chen, Subham Sekhar Sahoo, Yair Schiff, and Volodymyr Kuleshov. Semiautoregressive energy flows: exploring likelihood-free training of normalizing flows. In International Conference on Machine Learning, pp. 31732\u201331753. PMLR, 2023.   \n[50] John Skilling. The eigenvalues of mega-dimensional matrices. 1989. URL https://api. semanticscholar.org/CorpusID:117844915.   \n[51] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015.   \n[52] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017.   \n[53] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[54] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34: 1415\u20131428, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[55] Richard E. Spinney and Ian J. Ford. Fluctuation relations: a pedagogical overview, 2012. ", "page_idx": 13}, {"type": "text", "text": "[56] Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive density-estimator. Advances in Neural Information Processing Systems, 26, 2013.   \n[57] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016.   \n[58] A\u00e4ron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pp. 1747\u20131756. PMLR, 2016.   \n[59] Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schr\u00f6dinger bridge. In International conference on machine learning, pp. 10794\u201310804. PMLR, 2021.   \n[60] Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. Infodiffusion: Representation learning using information maximizing diffusion models. In International Conference on Machine Learning, pp. xxxx\u2013xxxx. PMLR, 2023.   \n[61] Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous relaxations. arXiv preprint arXiv:1901.10517, 2019.   \n[62] Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, and Bin Cui. Cross-modal contextualized diffusion models for text-guided visual generation and editing. arXiv preprint arXiv:2402.16627, 2024.   \n[63] Ruihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models, 2023.   \n[64] Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. Advances in Neural Information Processing Systems, 34:16280\u201316291, 2021.   \n[65] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood estimation for diffusion odes. arXiv preprint arXiv:2305.03935, 2023. ", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 14}, {"type": "text", "text": "2 Background 2 ", "page_idx": 14}, {"type": "text", "text": "3 Diffusion Models With Multivariate Learned Adaptive Noise 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3.1 Why Learned Diffusion? 3   \n3.2 A Forward Diffusion Process With Multivariate Adaptive Noise 3   \n3.3 Auxiliary-Variable Reverse Diffusion Processes 4   \n3.4 Variational Lower Bound . 6   \n3.5 The Variational Lower Bound as a Line Integral Over The Noise Schedule 6   \n4.1 Training Speed 7   \n4.2 Likelihood Estimation 7   \n4.3 Alternative Learned Diffusion Methods 8   \n4.4 Ablation Analysis And Additional Experiments 8 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "5 Related Work 10 ", "page_idx": 14}, {"type": "text", "text": "6 Conclusion 10 ", "page_idx": 14}, {"type": "text", "text": "Appendices 16 ", "page_idx": 14}, {"type": "text", "text": "Appendix A Comparing to Previous Work 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Diffusion Models with Custom Noise 17   \nA.2 Advanced Diffusion Models 17   \nA.3 Learned Diffusion . 18   \nA.4 Optimal Transport . 19 ", "page_idx": 14}, {"type": "text", "text": "Appendix B Standard Diffusion Models 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Forward Process . 20   \nB.2 Reverse Process 20   \nB.3 Variational Lower Bound 21   \nB.4 Diffusion Loss . . 21 ", "page_idx": 14}, {"type": "text", "text": "Appendix C Multivariate noise schedule 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Forward Process . 22   \nC.2 Reverse Process 23   \nC.3 Diffusion Loss . 23   \nC.4 Vectorized Representation of the diffusion loss . 24   \nC.5 Log likelihood and Noise Schedules: A Thermodynamics perspective 24   \nD.1 context is available during the inference time. 25   \nD.2 context isn\u2019t available during the inference time. 25   \nD.3 Experimental results 28 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix E MULAN: MUltivariate Latent Auxiliary variable Noise Schedule 29 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Parameterization in the reverse process 29   \nE.2 Polynomial Noise Schedule . 30   \nE.3 Variational Lower Bound 30   \nE.4 Diffusion Loss . 31   \nE.5 Recovering VDM from the Vectorized Representation of the diffusion loss 32 ", "page_idx": 15}, {"type": "text", "text": "Appendix F Subset Sampling ", "page_idx": 15}, {"type": "text", "text": "Appendix G Experiment Details 34 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1 Model Architecture 34   \nG.2 Hardware. 34   \nG.3 Hyperparameters 34 ", "page_idx": 15}, {"type": "text", "text": "Appendix H Datasets and Visualizations 34 ", "page_idx": 15}, {"type": "text", "text": "H.1 CIFAR-10 35   \nH.2 ImageNet-32 35   \nH.3 Frequency 37   \nH.4 Frequency-2 . 38   \nH.5 CIFAR-10: Intensity 38   \nH.6 Mask 39   \nI.1 VLB Estimate 42   \nI.2 Exact likelihood computation using Probability Flow ODE 42 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Appendix A Comparing to Previous Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "MULAN is the first method to introduce a learned adaptive noise process. A widely held assumption is that the ELBO objective of a diffusion model is invariant to the noise process [20]. We dispel this assumption: we show that when input-conditioned noise is combined with (a) multivariate noise, (b) a novel polynomial parameterization, and (c) auxiliary variables, a learned noise process yields an improved variational posterior and a tighter ELBO. This approach sets a new state-of-the-art in density estimation. While (a), (c) were proposed in other contexts, we leverage them as subcomponents of a novel algorithm. We elaborate further on this below. ", "page_idx": 15}, {"type": "text", "text": "The noise schedule, which determines the amount and type of noise added at each step, plays a critical role in diffusion models. Chen [3] empirically demonstrate that different noise schedules can significantly impact the generated image quality using various handcrafted noise schedules. Kingma et al. [20] showed that the likelihood of a diffusion model remains invariant to the noise schedule with a scalar noise schedule. In this work we show that the ELBO is no longer invariant to multivariate noise schedules. ", "page_idx": 16}, {"type": "text", "text": "Recent works, including Hoogeboom & Salimans [17], Rissanen et al. [42], Pearl et al. [36], have explored per-pixel noise schedules (including blurring and other types of noising), yet none have delved into learning or conditioning the noise schedule on the input data itself. The shared components among these models are summarized and compared in Table 4. ", "page_idx": 16}, {"type": "text", "text": "A.2 Advanced Diffusion Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Yang et al. [62] proposes noise processes that are conditioned on an external context (e.g., text). We also propose context-conditioned noise processes; however, their setting is that of conditional generation, where the context is always available at training and inference time, and the context represents external data. Our paper instead looks at unconditional generation, and we condition the noising process on the image itself that we want to generate (via latent variable) and learn how to apply noise across an image as a function of the image. ", "page_idx": 16}, {"type": "text", "text": "Lee et al. [26], Popov et al. [39] proposed using a data-dependent prior: however, they do not learn q and their noise process is not adaptive to the input $\\mathbf{x}_{\\mathrm{0}}$ . Thus they propose a fairly different set of methods from what we introduce. ", "page_idx": 16}, {"type": "text", "text": "Yang & Mandt [63], Wang et al. [60] have explored diffusion models with an auxiliary latent space, where the denoising network is conditioned on a latent distribution. Our paper also incorporate auxiliary latents, but unlike previous works, we add them to both $p$ and $q$ and we also also focus on learning the process $q$ (as opposed to doing representation learning using the auxiliary learned space). Lastly, our algorithm relies on many other components including a custom noise schedule, multivariate noise, etc. The shared components among these models are summarized and compared in Table 4. ", "page_idx": 16}, {"type": "table", "img_path": "loMa99A4p8/tmp/0826a0da53e62f13320e4935fdc04191e7ded74e66aeaa0b6cd734045c1b8ddb.jpg", "table_caption": ["Table 4: MULAN is a noise schedule that can be integrated into any diffusion model such as VDM [20] or InfoDiffusion [60]. The shared components between MULAN and these models are summarized and compared in this table. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.3 Learned Diffusion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Diffusion Normalizing Flow (DNF) uses the following forward process: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{x}_{t}=\\mathbf{f}_{\\theta}(\\mathbf{x}_{t},t)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the drift term $\\mathbf{f}_{\\theta}:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}^{d}$ is parameterized by a neural network with parameters $\\theta$ and the diffusion term $g(t)\\in\\mathbb{R}^{+}$ is a scalar constant and w is the standard Brownian motion. However, in MuLAN, the forward process is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{x}_{t}=\\mathbf{f}_{\\theta}(\\mathbf{z},t)\\odot\\mathbf{x}_{t}\\mathrm{d}t+\\mathbf{g}_{\\theta}(\\mathbf{z},t)\\odot\\mathrm{d}\\mathbf{w};\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{z}\\,\\in\\,\\{0,1\\}^{m}$ is the auxiliary latent vector, $\\mathbf{f}_{\\theta}:\\mathbb{R}^{m}\\times\\mathbb{R}\\to\\mathbb{R}^{d}$ and $\\mathbf{g}_{\\theta}:\\mathbb{R}^{m}\\times\\mathbb{R}\\to\\mathbb{R}^{d}$ are parameterized by a neural network. Notice that the drift term in DNF, ${\\bf f}_{\\theta}({\\bf x},t)$ , is a non-linear function in $\\mathbf{x}_{\\mathrm{0}}$ , and the same holds for MuLAN since in the drift term, $\\mathbf{f}_{\\theta}(\\mathbf{z},t)\\odot\\mathbf{x},\\mathbf{z}$ and $\\mathbf{x}$ depend on $\\mathbf{x}_{\\mathrm{0}}$ . Additionally, the diffusion coefficient, $\\mathbf{g}_{\\theta}(\\mathbf{z},t)$ , is multivariate and conditioned on $\\mathbf{x}_{\\mathrm{0}}$ via $\\mathbf{z}$ The two parameterizations are different: on one hand, DNF admits more general classes of neural networks because it does not require marginals to be tractable. On the other hand MuLAN admits a more flexible noise model $\\mathbf{g}_{\\theta}(\\mathbf{z},t)$ and admits more efficient training (see the summarized Table 5 below). ", "page_idx": 17}, {"type": "text", "text": "MuLAN has the advantage that it is simulation free; which means that given a data $\\mathbf{x}_{\\mathrm{0}}$ , the noisy sample $\\mathbf{x}_{t}$ can be computed in closed form; however, in Diffusion Normalizing Flow, to compute $\\mathbf{x}_{t}$ , one needs to simulate the forward SDE which is resource intensive and limits its scalability to larger denoising models. While MuLAN optimizes the ELBO, DNF optimizes an approximation for the ELBO. In particular, the DNF training objective does not involve a term that accounts for the entropy of the encoder. Thus, the objective is closer to that of a normal auto-encoder in that regard. ", "page_idx": 17}, {"type": "table", "img_path": "loMa99A4p8/tmp/64b37bb13f0e33cb3a1b92a03d3e8b66c2fa05c81f6af39c729bbd24f275c436.jpg", "table_caption": ["Table 5: The key differences between MULAN and DNF is listed below. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Other concurrent work seeks to improve log-likelihood estimation by learning the forward diffusion process in a simulation-free setting. In neural diffusion models (NDMs), the noise schedule is fixed, but the mean of each marginal $q(\\mathbf{x}_{t}|\\mathbf{x}_{0})$ is learned. This requires denoising $\\mathbf{x}$ in a transformed space, which prevents using noise parameterization, a design choice that is important for performance. Their denoising family also induces a parameterization that limits the kinds of samplers that can be sued. Lastly, NDMs use a model that is $2\\mathbf{x}$ larger than a regular diffusion model, while ours only adds $10\\%$ more parameters. ", "page_idx": 18}, {"type": "text", "text": "The DiffEnc framework adds an extra learned correction term to $q$ to adjust the mean of each marginal $q(\\mathbf{x}_{t}|\\mathbf{x}_{0})$ . This noise choice also requires using certain parameterizations for $\\mathbf{x}$ that are not compatible with noise parameterization; while their approach supports v-parameterization, it also requires training a mean parameterization network. Similarly to NDMs, the noise schedule remains fixed, while the mean of each marginal is adjusted by the network. Our approach towards learning the noise schedule yields better empirical performance and is, in our opinion, simpler; it can also be combined with this prior work on learning the marginals\u2019 means. ", "page_idx": 18}, {"type": "text", "text": "A.4 Optimal Transport ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In techniques based on optimal transport, the goal is to learn a noise process that minimizes the transport cost from data to noise, which in practice produces smoother diffusion trajectories that facilitate sampling. ", "page_idx": 18}, {"type": "text", "text": "Minimizing Trajectory Curvature (MTC) of ODE-based generative models Lee et al. [27], the primary goal is to design the forward diffusion process that is optimal for fast sampling; however, MuLAN strives to learn a forward process that optimizes for log-likelihood. In the former, the marginals $\\mathbf{x}_{t}$ in the forward process are given as ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf x}_{t}=(1-t){\\bf x}_{0}+t{\\bf z};{\\bf z}\\sim q_{\\phi}({\\bf z}|{\\bf x}_{0})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{x}_{t},\\mathbf{x}_{0},F\\,\\,\\,\\in\\,\\,\\,\\mathbb{R}^{d}$ . However for MuLAN the marginals are $\\begin{array}{r l r}{{\\mathbf{x}}_{t}}&{{}\\!=\\!}&{\\alpha_{\\phi}(\\mathbf{z},t)\\,\\odot\\,{\\mathbf{x}}_{0}\\;+}\\end{array}$ $\\sqrt{1-\\alpha_{\\phi}^{2}(\\mathbf{z},t)}\\odot\\epsilon\\,;\\epsilon\\sim{\\mathcal{N}}(0,\\mathbf{I}_{d})\\,;\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})$ where $\\alpha_{\\phi}(\\mathbf{z},t):\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}_{\\geq0}^{d}$ , $\\mathbf{z}\\in\\{0,1\\}^{m}$ , $\\epsilon\\in\\mathbb{R}^{d}$ Notice that in the MTC formula, the coefficient of $\\mathbf{x}_{\\mathrm{0}}$ , the time integral of the drift term, is a scalar and linear function of, and is independent of the input $\\mathbf{x}_{\\mathrm{0}}$ . In MuLAN, that term is a multivariate non-linear function in $t$ , and conditioned on $\\mathbf{x}_{\\mathrm{0}}$ via the auxiliary latent variable ${\\bf z}$ . This implies that the forward diffusion process in MuLAN is more expressive than MTC. The simplistic forward process in MTC enables faster sampling whereas the richer / more expressive forward process in MuLAN leads to improved likelihood estimates. Table 6 summarizes the key differences. ", "page_idx": 18}, {"type": "table", "img_path": "loMa99A4p8/tmp/549235a91e9724481e2ea0178c0a5915eb902da788efef1a5226e3f5ab003c67.jpg", "table_caption": ["Table 6: Comparison between Minimizing Trajectory Curvature and MuLAN methods. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "An alternative approach to learning a forward process that performs optimal transport is via the theory of Schrodinger bridges [47, 6, 59, 37] . Similarly to the DNF framework, these methods do not admit analytical marginals and therefore involve computing full trajectories from noisy and clean data. Additionally, they are typically trained using an iterative procedure that generalizes the sinkhorn algorithm and involves iteratively training $q$ and $p$ . As such, these types of methods are typically more expensive to train and competitive results on standard benchmarks (e.g., CIFAR10, ImageNet) are not yet available to our knowledge. ", "page_idx": 19}, {"type": "text", "text": "Appendix B Standard Diffusion Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We have a Gaussian diffusion process that begins with the data $\\mathbf{x}_{\\mathrm{0}}$ , and defines a sequence of increasingly noisy versions of $\\mathbf{x}_{\\mathrm{0}}$ which we call the latent variables $\\mathbf{x}_{t}$ , where $t$ runs from $t=0$ (least noisy) to $t=1$ (most noisy). Given, $T$ , we discretize time uniformly into $T$ timesteps each with a width $1/T$ . We define $t(i)=i/T$ and $s(i)=(i-1)/T$ . ", "page_idx": 19}, {"type": "text", "text": "B.1 Forward Process ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{t}|\\mathbf{x}_{s})=\\mathcal{N}(\\alpha_{t|s}\\mathbf{x}_{s},\\sigma_{t|s}^{2}\\mathbf{I}_{n})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{t|s}=\\frac{\\alpha_{t}}{\\alpha_{s}}}\\\\ {\\displaystyle\\sigma_{t|s}^{2}=\\sigma_{t}^{2}-\\frac{\\alpha_{t|s}^{2}}{\\sigma_{s}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Reverse Process ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Kingma et al. [20] show that the distribution $q(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})$ is also gaussian, ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathcal{N}\\left(\\mu_{q}=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{0},\\;\\Sigma_{q}=\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\mathbf{I}_{n}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since during the reverse process, we don\u2019t have access to $\\mathbf{x}_{\\mathrm{0}}$ , we approximate it using a neural network $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ with parameters $\\theta$ . Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t})=\\mathcal{N}\\left(\\mu_{p}=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t),\\ \\Sigma_{p}=\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\mathbf{I}_{n}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.3 Variational Lower Bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This corruption process $q$ is the following markov-chain as $\\begin{array}{r}{q(\\mathbf{x}_{0:1})=q(\\mathbf{x}_{0})\\left(\\prod_{i=1}^{T}q(\\mathbf{x}_{t(i)}\\vert\\mathbf{x}_{s(i)})\\right)}\\end{array}$ . In the Reverse Process, or the Denoising Process, $p_{\\theta}$ , a neural network (with parameters $\\theta$ ) is used to denoise the noising process $q$ . The Reverse Process factorizes as: $p_{\\theta}(\\mathbf{x}_{0:1})\\;\\;=$ $\\begin{array}{r}{p_{\\theta}(\\mathbf{x}_{1})\\prod_{i=1}^{T}p_{\\theta}(\\mathbf{x}_{s(i)}\\vert\\mathbf{x}_{t(i)})}\\end{array}$ . Let $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ be the reconstructed input by a neural network from $\\mathbf{x}_{t}$ . Similar to Sohl-Dickstein et al. [51], Kingma et al. [20] we decompose the negative lower bound (VLB) as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\log p_{\\theta}(\\mathbf{x}_{0})\\leq}&{\\underbrace{\\mathbb{E}_{\\psi_{\\theta}}\\left[-\\log\\frac{p_{\\theta}(\\mathbf{x}_{0};t_{0}(\\mathbf{x}_{1};t_{0});t_{0})}{q(\\mathbf{x}_{1};t_{0})(\\mathbf{x}_{1};t_{0})}\\right]}_{\\mathbf{R}_{\\mathrm{BSO}}(p_{\\theta}(\\mathbf{x}_{1});q(\\mathbf{x}_{1});q(\\mathbf{x}_{1});t_{0})\\leq t_{0}}}\\\\ &{=\\mathbb{E}_{\\mathbf{x}_{0};\\tau_{0};\\tau_{1}\\in[0,t_{1}]}[-\\log p_{0}(\\mathbf{x}_{0})\\kappa_{1}(1)]}\\\\ &{\\phantom{\\sum_{\\ell=1}^{\\infty}\\mathbb{E}_{\\mathbf{x}_{\\ell}(\\mathbf{x}_{1};\\tau_{0})\\leq t_{\\ell}}\\left[\\kappa_{0}(\\mathbf{x}_{1})\\left[\\ln_{\\ell}\\left(\\mathbf{x}_{1};(\\mathbf{x}_{1})\\right)\\right]\\right.}}\\\\ &{\\phantom{\\sum_{\\ell=1}^{\\infty}\\sum_{1}^{\\infty}\\mathbb{E}_{\\mathbf{x}_{\\ell}(\\mathbf{x}_{1})\\left[\\ln_{\\ell}\\left(\\mathbf{x}_{1}\\right)\\right]\\mathbf{D}_{\\ell}}\\left[\\kappa_{0}(\\mathbf{x}_{1})\\left[\\ln_{\\ell}(\\mathbf{x}_{1})\\right]\\right]q(\\mathbf{x}_{\\ell}_{\\ell}|\\mathbf{x}_{\\ell}|,\\mathbf{x}_{0})|}{\\left[\\kappa_{0}(\\mathbf{x}_{1};(\\mathbf{x}_{1};0)\\mathbf{x}_{1})\\right]}}\\\\ &{=\\underbrace{\\mathbb{E}_{\\mathbf{x}_{0};\\tau_{1}\\in[0,t_{1}]}\\left[\\left[\\kappa_{0}(\\mathbf{x}_{1}|\\mathbf{x}_{0})\\right]\\right]}_{\\mathbf{E}_{\\mathrm{cons}}}}\\\\ &{\\phantom{\\sum_{\\ell=1}^{\\infty}\\sum_{1}^{\\infty}\\sum_{i=1}^{\\infty}\\left[\\kappa_{0}(\\mathbf{x}_{1};(\\mathbf{x}_{1})\\left[\\ln_{\\ell}\\left(\\mathbf{x}_{1}\\right)\\right])\\right]}}\\\\ &\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The prior loss, $\\mathcal{L}_{\\mathrm{prior}}$ , and reconstruction loss, ${\\mathcal{L}}_{\\mathrm{recons}}$ , can be (stochastically and differentiably) estimated using standard techniques; see Kingma $\\&$ Welling [23]. The diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}$ , varies with the formulation of the noise schedule. We provide an exact formulation for it in the subsequent sections. ", "page_idx": 20}, {"type": "text", "text": "B.4 Diffusion Loss ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For brevity, we use the notation $s$ for $s(i)$ and $t$ for $t(i)$ . From Eq. 31 and Eq. 32 we get the following expression for $q(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})$ : ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{l}{\\displaystyle\\mathrm{D}_{\\mathrm{KL}}(q(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})||p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t}))}\\\\ {\\displaystyle=\\frac{1}{2}\\left((\\mu_{q}-\\mu_{p})^{\\top}\\Sigma_{\\theta}^{-1}(\\mu_{q}-\\mu_{p})+\\mathrm{tr}\\left(\\Sigma_{q}\\Sigma_{p}^{-1}-\\mathbf{I}_{n}\\right)-\\log\\frac{|\\Sigma_{q}|}{|\\Sigma_{p}|}\\right)}\\\\ {\\displaystyle=\\frac{1}{2}(\\mu_{q}-\\mu_{p})^{\\top}\\Sigma_{\\theta}^{-1}(\\mu_{q}-\\mu_{p})}\\end{array}$   \nSubstituting $\\mu_{q},\\Sigma_{q},\\mu_{p},\\Sigma_{p}$ from equation 20 and equation 19; for the exact derivation see Kingma et al. [20]   \n$=\\frac{1}{2}\\left(\\nu(s)-\\nu(t)\\right)\\|({\\bf x}_{0}-{\\bf x}_{\\theta}({\\bf x}_{t},t))\\|_{2}^{2}$ ", "page_idx": 20}, {"type": "text", "text": "(22) ", "page_idx": 20}, {"type": "text", "text": "Thus $\\mathcal{L}_{\\mathrm{diffusion}}$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{L}}_{\\mathrm{diffusion}}}\\\\ {\\displaystyle=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{T}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n}),i\\sim U\\left\\{2,T\\right\\}}\\mathrm{D}_{\\mathrm{KL}}\\big[p_{\\theta}\\big(\\mathbf{x}_{s\\left(i\\right)}|\\mathbf{x}_{t\\left(i\\right)}\\big)\\big\\|q_{\\phi}\\big(\\mathbf{x}_{s\\left(i\\right)}|\\mathbf{x}_{t\\left(i\\right)},\\mathbf{x}_{0}\\big)\\big]}\\\\ {\\displaystyle=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{2}\\sum_{i=2}^{T}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left(\\nu(s)-\\nu(t)\\right)\\big\\|\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t},t\\big)\\big\\|_{2}^{2}}\\\\ {\\displaystyle=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\operatorname*{lim}_{T\\rightarrow\\infty}\\sum_{i=2}^{T}\\left(\\nu(s)-\\nu(t)\\right)\\big\\|\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t},t\\big)\\big\\|_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\operatorname*{lim}_{T\\rightarrow\\infty}\\sum_{i=2}^{T}T\\left(\\nu(s)-\\nu(t)\\right)\\Vert\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\Vert_{2}^{2}\\frac{1}{T}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\int_{0}^{1}\\nu^{\\prime}(t)\\|\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\|_{2}^{2}\\right]\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In practice instead of computing the integral is computed by MC sampling. ", "page_idx": 21}, {"type": "equation", "text": "$$\n=-\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n}),t\\sim U[0,1]}\\left[\\nu^{\\prime}(t)\\lVert\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\rVert_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Appendix C Multivariate noise schedule ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For a multivariate noise schedule we have $\\alpha_{t},\\pmb{\\sigma}_{t}\\in\\mathbb{R}_{+}^{d}$ where $t\\in[0,1]$ . $\\alpha_{t},\\sigma_{t}$ are vectors. The timesteps $s,t$ satisfy $0\\leq s<t\\leq1$ . Furthermore, we use the following notations where arithmetic division represents element wise division between 2 vectors: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{t|s}=\\frac{\\alpha_{t}}{\\alpha_{s}}}\\\\ {\\displaystyle\\pmb\\sigma_{t|s}^{2}=\\pmb\\sigma_{t}^{2}-\\frac{\\alpha_{t|s}^{2}}{\\pmb\\sigma_{s}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.1 Forward Process ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{t}|\\mathbf{x}_{s})=\\mathcal{N}\\left(\\alpha_{t|s}\\mathbf{x}_{s},\\pmb{\\sigma}_{t|s}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Change of variables. We can write $\\mathbf{x}_{t}$ explicitly in terms of the signal-to-noise ratio, $\\nu(t)$ , and input $\\mathbf{x}_{\\mathrm{0}}$ in the following manner: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{\\nu}_{t}=\\frac{\\alpha_{t}^{2}}{\\pmb{\\sigma}_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\implies\\frac{1-\\sigma_{t}^{2}}{\\sigma_{t}^{2}}=\\nu_{t}}\\\\ {\\displaystyle\\implies\\sigma_{t}^{2}=\\frac{1}{1+\\nu_{t}}\\quad\\mathrm{and}\\quad\\displaystyle\\alpha_{t}^{2}=\\frac{\\nu_{t}}{1+\\nu_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nu_{t}=\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\implies\\frac{1-\\sigma_{t}^{2}}{\\sigma_{t}^{2}}=\\nu_{t}}\\\\ {\\displaystyle\\implies\\sigma_{t}^{2}=\\frac{1}{1+\\nu_{t}}\\quad\\mathrm{and}\\quad\\alpha_{t}^{2}=\\frac{\\nu_{t}}{1+\\nu_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we write $\\mathbf{x}_{t}$ in terms of the signal-to-noise ratio in the following manner: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{x}_{\\pmb{\\nu}(t)}=\\alpha_{t}\\mathbf{x}_{0}+\\pmb{\\sigma}_{t}\\epsilon_{t};~\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\\\ {=\\frac{\\sqrt{\\pmb{\\nu}(t)}}{\\sqrt{1+\\pmb{\\nu}(t)}}\\mathbf{x}_{0}+\\frac{1}{\\sqrt{1+\\pmb{\\nu}(t)}}\\epsilon_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.2 Reverse Process ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The distribution of $\\mathbf{x}_{t}$ given $\\mathbf{x}_{s}$ is given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathcal{N}\\left(\\mu_{q}=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{0},\\;\\Sigma_{q}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ be the neural network approximation for $\\mathbf{x}_{\\mathrm{0}}$ . Then we get the following reverse process: ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t})=\\mathcal{N}\\left(\\mu_{p}=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t),\\ \\Sigma_{p}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.3 Diffusion Loss ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For brevity we use the notation $s$ for $s(i)$ and $t$ for $t(i)$ . From Eq. 31 and Eq. 32 we get the following expression for $q(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}(q(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0})\\|p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t}))}\\\\ &{\\ =\\frac{1}{2}\\left((\\mu_{q}-\\mu_{p})^{\\top}\\Sigma_{\\theta}^{-1}(\\mu_{q}-\\mu_{p})+\\operatorname{tr}\\left(\\Sigma_{q}\\Sigma_{p}^{-1}-\\mathbf{I}_{n}\\right)-\\log\\frac{|\\Sigma_{q}|}{|\\Sigma_{p}|}\\right)}\\\\ &{\\ =\\frac{1}{2}(\\mu_{q}-\\mu_{p})^{\\top}\\Sigma_{\\theta}^{-1}(\\mu_{q}-\\mu_{p})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting $\\pmb{\\mu}_{q},\\pmb{\\mu}_{p},\\pmb{\\Sigma}_{p}$ from equation 32 and equation 31. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{1}{2}\\left(\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{0}-\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right)^{\\top}\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\right)^{-1}\\left(\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{0}-\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right)}\\\\ &{=\\frac{1}{2}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\right)^{\\top}\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\right)^{-1}\\mathrm{diag}\\left(\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))}\\\\ &{=\\frac{1}{2}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\odot\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{s}^{2}\\sigma_{t|s}^{2}}\\odot\\frac{\\sigma_{t|s}^{2}\\alpha_{s}}{\\sigma_{t}^{2}}\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))}\\\\ &{=\\frac{1}{2}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\sigma_{t|s}^{2}\\alpha_{s}^{2}}{\\sigma_{t}^{2}\\sigma_{s}^{2}}\\right)(\\mathbf{x}_{0}-\\mathbf \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Simplifying the expression using Eq. 25 and Eq. 26 we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n=\\frac{1}{2}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\alpha_{s}^{2}}{\\sigma_{s}^{2}}-\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the relation $\\pmb{\\nu}(t)=\\pmb{\\alpha}_{t}^{2}/\\pmb{\\sigma}_{t}^{2}$ we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n=\\frac{1}{2}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\pmb{\\nu}(s)-\\pmb{\\nu}(t)\\right)\\left(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Like Kingma et al. [20] we train the model in the continuous domain with $T\\rightarrow\\infty$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{2}\\sum_{i=2}^{T}\\mathbb{E}_{\\ell\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{x}_{s(i)}|\\mathbf{x}_{t(i)},\\mathbf{x}_{0})\\big|\\big|p_{\\theta}(\\mathbf{x}_{s(i)}|\\mathbf{x}_{t(i)})\\big)}\\\\ {\\displaystyle=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{2}\\sum_{i=2}^{T}\\mathbb{E}_{\\ell\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\big(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t(i)},t(i)\\big)\\big)^{\\top}\\mathrm{diag}\\left(\\nu_{s(i)}-\\nu_{t(i)}\\right)\\big(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t(i)},t\\big)\\big)}\\\\ {\\displaystyle=\\frac{1}{2}\\mathbb{E}_{\\ell\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\operatorname*{lim}_{T\\rightarrow\\infty}^{T}\\!\\!\\!\\!\\!\\sum_{i=2}^{T}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t(i)},t(i)\\big))^{\\top}\\mathrm{diag}\\left(\\nu_{s(i)}-\\nu_{t(i)}\\right)\\big(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t(i)},t\\big)\\big)\\right]}\\\\ {\\displaystyle=\\frac{1}{2}\\mathbb{E}_{\\ell\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\operatorname*{lim}_{T\\rightarrow\\infty}^{T}\\!\\!\\!\\!\\sum_{i=2}^{T}\\!\\!\\!T(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t(i)},t(i)\\big))^{\\top}\\mathrm{diag}\\left(\\nu_{s(i)}-\\nu_{t(i)}\\right)\\big(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}\\big(\\mathbf{x}_{t(i)},t\\big)\\big)\\frac{1}{T}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\operatorname*{lim}_{T\\to\\infty}T(\\pmb{\\nu}_{s(i)}-\\pmb{\\nu}_{t(i)})=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\pmb{\\nu}(t)$ denote the scalar derivative of the vector $\\nu(t)$ w.r.t $t$ $=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\int_{0}^{1}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))\\mathrm{d}t\\right]$ ", "page_idx": 23}, {"type": "text", "text": "In practice instead of computing the integral is computed by MC sampling. ", "page_idx": 23}, {"type": "equation", "text": "$$\n=-\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n}),t\\sim U[0,1]}\\left[(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.4 Vectorized Representation of the diffusion loss ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let $\\nu(t)$ be the vectorized representation of the diagonal entries of the matrix $\\nu(t)$ . We can rewrite the integral in 34 in the following vectorized form where $\\odot$ denotes element wise multiplication and $\\langle,\\rangle$ denotes dot product between 2 vectors. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\mathrm{diffsion}}}\\\\ &{=-\\cfrac{1}{2}\\int_{0}^{1}\\big(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\big)^{\\top}\\mathrm{diag}\\left(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t))\\mathrm{d}t}\\\\ &{=-\\cfrac{1}{2}\\int_{0}^{1}\\bigg\\langle\\left(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right)\\odot\\left(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right),\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\bigg\\rangle\\,\\mathrm{d}t}\\\\ &{\\mathrm{Uing~chang~eof~variables~sa~mentiond~in\\,sec.~3.2~whave}}\\\\ &{=-\\cfrac{1}{2}\\int_{0}^{1}\\bigg\\langle\\left(\\mathbf{x}_{0}-\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu}(t),\\nu(t))\\right)\\odot\\left(\\mathbf{x}_{0}-\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu}(t),\\nu(t))\\right),\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\bigg\\rangle\\,\\mathrm{d}t}\\\\ &{\\mathrm{Let\\,}\\ell_{0}(\\mathbf{x}_{0},\\nu(t))=\\big(\\mathbf{x}_{0}-\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu(t)},\\nu(t))\\big)\\odot\\left(\\mathbf{x}_{0}-\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu(t)},\\nu(t))\\right)}\\\\ &{=\\int_{0}^{1}\\left\\langle\\ell_{0}(\\mathbf{x}_{0},\\nu(t)),\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\right\\rangle\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus $\\mathcal{L}_{\\mathrm{diffusion}}$ can be interpreted as the amount of work done along the trajectory $\\nu(0)\\to\\nu(1)$ in the presence of a vector field $\\mathbf{\\bar{f}}_{\\theta}(\\mathbf{x}_{0},\\nu(\\mathbf{z},t))$ . From the perspective of thermodynamics, this is precisely equal to the amount of heat lost into the environment during the process of transition between 2 equilibria via the noise schedule specified by $\\nu(t)$ . ", "page_idx": 23}, {"type": "text", "text": "C.5 Log likelihood and Noise Schedules: A Thermodynamics perspective ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "A diffusion model characterizes a quasi-static process that occurs between two equilibrium distributions: $q(\\mathbf{x}_{0})\\rightarrow q(\\mathbf{x}_{1})$ , via a stochastic trajectory [51]. According to Spinney & Ford [55], it is demonstrated that the diffusion schedule or the noising process plays a pivotal role in determining the \"measure of irreversibility\" for this stochastic trajectory which is expressed as log PPFB ((xx01::10)). PF (x0:1) represents the probability of observing the forward path $\\mathbf{x}_{0:1}$ and $P_{B}(\\mathbf{x}_{1:0})$ represents the probability of observing the reverse path $\\mathbf{x}_{1:0}$ . It\u2019s worth noting that log $;\\,\\,{\\frac{P_{F}(\\mathbf{x}_{0:1})}{P_{B}(\\mathbf{x}_{1:0})}}$ corresponds precisely to the ELBO Eq. 1 that we optimize when training a diffusion model. Consequently, thermodynamics asserts that the noise schedule indeed has an impact on the log-likelihood of the diffusion model which contradicts Kingma et al. [20]. ", "page_idx": 23}, {"type": "text", "text": "Appendix D Multivariate noise schedule conditioned on context ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let\u2019s say we have a context variable $\\textbf{c}\\in\\mathbb{R}^{m}$ that captures high level information about $\\mathbf{x}_{\\mathrm{0}}$ . $\\pmb{\\alpha}_{t}(\\mathbf{c}),\\pmb{\\dot{\\sigma}}_{t}(\\mathbf{c})\\,\\in\\,\\mathbb{R}_{+}^{d}$ are vectors. The timesteps $s,t$ satisfy $0\\,\\leq\\,s\\;<\\;t\\,\\leq\\,1$ . Furthermore, we use the following notations: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\alpha_{t|s}({\\bf c})=\\frac{\\alpha_{t}({\\bf c})}{\\alpha_{s}({\\bf c})}}}\\\\ {{\\displaystyle\\sigma_{t|s}^{2}({\\bf c})=\\sigma_{t}^{2}({\\bf c})-\\frac{\\alpha_{t|s}^{2}({\\bf c})}{\\sigma_{s}^{2}({\\bf c})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The forward process for such a method is given as: ", "page_idx": 24}, {"type": "equation", "text": "$$\nq_{\\phi}(\\mathbf{x}_{t}|\\mathbf{x}_{s},\\mathbf{c})=\\mathcal{N}\\left(\\pmb{\\alpha}_{t|s}(\\mathbf{c})\\mathbf{x}_{s},\\pmb{\\sigma}_{t|s}^{2}(\\mathbf{c})\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The distribution of $\\mathbf{x}_{t}$ given $\\mathbf{x}_{s}$ is given by (the derivation is similar to Hoogeboom & Salimans [17]): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{\\phi}(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0},\\mathbf{c})}\\\\ &{\\,=\\mathcal{N}\\left(\\pmb{\\mu}_{q}=\\frac{\\alpha_{t|s}(\\mathbf{c})\\sigma_{s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}(\\mathbf{c})\\alpha_{s}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{0},\\ \\Sigma_{q}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}(\\mathbf{c})\\sigma_{t|s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.1 context is available during the inference time. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Even though c represents the input $\\mathbf{x}_{\\mathrm{0}}$ , it could be available during during inference. For example c could be class labels [10] or prexisting embeddings from an auto-encoder [40]. ", "page_idx": 24}, {"type": "text", "text": "D.1.1 Reverse Process: Approximate ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)$ be an approximation for $\\mathbf{x}_{\\mathrm{0}}$ . Then we get the following reverse process (for brevity we write $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)$ as $\\mathbf{x}_{\\theta}$ ): ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{c})=\\mathcal{N}\\left(\\mu_{p}=\\frac{\\alpha_{t|s}(\\mathbf{c})\\sigma_{s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}(\\mathbf{c})\\alpha_{s}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\mathbf{x}_{\\theta},\\ \\Sigma_{p}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}(\\mathbf{c})\\sigma_{t|s}^{2}(\\mathbf{c})}{\\sigma_{t}^{2}(\\mathbf{c})}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.1.2 Diffusion Loss ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Similar to the derivation of multi-variate $\\mathcal{L}_{\\mathrm{diffusion}}$ in Eq. 33 we can derive $\\mathcal{L}_{\\mathrm{diffusion}}$ for this case too: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{diffusion}}=-\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n}),t\\sim U[0,1]}\\left[(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t))^{\\top}\\mathrm{diag}\\left(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t))\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.1.3 Limitations of this method ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This approach is very limited where the diffusion process is only conditioned on class labels. Using pre-existing embeddings like Diff-AE [40] is also not possible in general and is only limited to tasks such as attribute manipulation in datasets. ", "page_idx": 24}, {"type": "text", "text": "D.2 context isn\u2019t available during the inference time. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "If the context, $\\mathbf{c}$ is an explicit function of the input $\\mathbf{x}_{\\mathrm{0}}$ things become challenging because $\\mathbf{x}_{\\mathrm{0}}$ isn\u2019t available during the inference stage. For this reason, Eq. 40 can\u2019t be used to parameterize $\\pmb{\\mu}_{p},\\pmb{\\Sigma}_{p}$ in $p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t})$ . Let $\\begin{array}{r}{p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t})=\\mathcal{N}(\\bar{\\mu}_{p}(\\mathbf{x}_{t},t),\\Sigma_{p}(\\mathbf{x}_{t},t))}\\end{array}$ where $\\pmb{\\mu}_{p},\\pmb{\\Sigma}_{p}$ are parameterized directly by a neural network. Using Eq. 4 we get the following diffusion loss: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{diffusion}}=T\\,\\mathbb{E}_{i\\sim U[0,T]}\\mathrm{D}_{\\mathrm{KL}}\\left(q(\\mathbf{x}_{s(i)}|\\mathbf{x}_{t(i)},\\mathbf{x}_{0})\\|p_{\\theta}(\\mathbf{x}_{s(i)}|\\mathbf{x}_{t(i)})\\right)}\\\\ &{\\qquad\\quad=\\mathbb{E}_{q_{\\phi}}\\left(\\underbrace{\\frac{T}{2}(\\mu_{q}-\\mu_{p})^{\\top}\\mathbf{E}_{\\theta}^{-1}(\\mu_{q}-\\mu_{p})}_{\\mathrm{term~}1}+\\underbrace{\\frac{T}{2}\\left(\\mathrm{tr}\\left(\\Sigma_{q}\\Sigma_{p}^{-1}-\\mathbf{I}_{n}\\right)-\\log\\frac{|\\Sigma_{q}|}{|\\Sigma_{p}|}\\right)}_{\\mathrm{term~}2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2.1 Reverse Process: Approximate ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Due to the challenges associated with parameterizing $\\pmb{\\mu}_{p},\\pmb{\\Sigma}_{p}$ directly using a neural network we parameterize c using a neural network that approximates c in the reverse process. Let $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ be an approximation for $\\mathbf{x}_{\\mathrm{0}}$ . Then we get the following reverse Rrocess (for brevity we write $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)$ as $\\mathbf{x}_{\\theta}$ , and $\\mathbf{c}_{\\theta}$ denotes an approximation to $\\mathbf{c}$ in the reverse process.): ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{s}|\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n=\\mathcal{N}\\left(\\mu_{p}=\\frac{\\alpha_{t|s}(\\mathbf{c}_{\\theta})\\sigma_{s}^{2}(\\mathbf{c}_{\\theta})}{\\sigma_{t}^{2}(\\mathbf{c}_{\\theta})}\\mathbf{x}_{t}+\\frac{\\sigma_{t|s}^{2}(\\mathbf{c}_{\\theta})\\alpha_{s}(\\mathbf{c}_{\\theta})}{\\sigma_{t}^{2}(\\mathbf{c}_{\\theta})}\\mathbf{x}_{\\theta},\\ \\Sigma_{p}=\\mathrm{diag}\\left(\\frac{\\sigma_{s}^{2}(\\mathbf{c}_{\\theta})\\sigma_{t|s}^{2}(\\mathbf{c}_{\\theta})}{\\sigma_{t}^{2}(\\mathbf{c}_{\\theta})}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider the limiting case where $T\\to\\infty$ . Let\u2019s analyze the 2 terms in Eq. 43 separately. ", "page_idx": 25}, {"type": "text", "text": "Using Eq. 4 and Eq. 6, term 1 in Eq. 43 simplifies in the following manner: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{T}{2}(\\boldsymbol{\\mu_{q}}-\\boldsymbol{\\mu_{p}})^{\\top}\\mathbf{\\Sigma}_{\\theta}^{-1}(\\boldsymbol{\\mu_{q}}-\\boldsymbol{\\mu_{p}})}\\\\ &{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{T}{2}\\displaystyle\\sum_{i=1}^{d}\\frac{((\\boldsymbol{\\mu_{q}})_{i}-(\\boldsymbol{\\mu_{p}})_{i})^{2}}{(\\mathbf{\\Sigma}_{\\theta})_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting $1\\,/\\,\\mathrm{T}$ as $\\delta$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\delta\\rightarrow0^{+}}{\\operatorname*{lim}}\\displaystyle\\sum_{i=1}^{d}\\frac{1}{\\delta\\sigma_{i}2\\big(\\mathbf{x}_{\\theta},t-\\delta\\big)}\\frac{1}{\\big(1-\\frac{\\nu_{i}(\\mathbf{x}_{\\theta},t)}{\\nu_{i}(\\mathbf{x}_{\\theta},t-\\delta)}\\big)}\\times}\\\\ &{\\qquad\\qquad\\left[\\frac{\\alpha_{i}\\big(\\mathbf{x},t-\\delta\\big)}{\\alpha_{i}\\big(\\mathbf{x},t\\big)}\\frac{\\nu_{i}\\big(\\mathbf{x},t\\big)}{\\nu_{i}\\big(\\mathbf{x},t-\\delta\\big)}\\mathbf{z}_{t}+\\alpha_{i}\\big(\\mathbf{x},t-\\delta\\big)\\left(1-\\frac{\\nu_{i}\\big(\\mathbf{x},t\\big)}{\\nu_{i}\\big(\\mathbf{x},t-\\delta\\big)}\\right)x_{i}\\right.}\\\\ &{\\qquad\\qquad\\left.-\\left.\\frac{\\alpha_{i}\\big(\\mathbf{x}_{\\theta},t-\\delta\\big)}{\\alpha_{i}\\big(\\mathbf{x}_{\\theta},t\\big)}\\frac{\\nu_{i}\\big(\\mathbf{x}_{\\theta},t\\big)}{\\nu_{i}\\big(\\mathbf{x}_{\\theta},t-\\delta\\big)}\\mathbf{z}_{t}+\\alpha_{i}\\big(\\mathbf{x}_{\\theta},t-\\delta\\big)\\left(1-\\frac{\\nu_{i}\\big(\\mathbf{x}_{\\theta},t\\big)}{\\nu_{i}\\big(\\mathbf{x}_{\\theta},t-\\delta\\big)}\\right)\\left(x_{\\theta}\\right)_{i}\\right]^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider the scalar case: substituting $\\delta=1/T$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\delta\\rightarrow0}{\\operatorname*{lim}}\\frac{1}{\\delta\\sigma^{2}\\left(\\mathbf{x}_{\\theta},t-\\delta\\right)\\,\\left(1-\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t-\\delta\\right)}\\right)}\\times}\\\\ &{\\left[\\frac{\\alpha\\left(\\mathbf{x},t-\\delta\\right)}{\\alpha\\left(\\mathbf{x},t\\right)}\\frac{\\nu\\left(\\mathbf{x},t\\right)}{\\nu\\left(\\mathbf{x},t-\\delta\\right)}\\mathbf{z}_{t}+\\alpha(\\mathbf{x},t-\\delta)\\left(1-\\frac{\\nu\\left(\\mathbf{x},t\\right)}{\\nu\\left(\\mathbf{x},t-\\delta\\right)}\\right)\\mathbf{x}\\right.}\\\\ &{\\left.-\\left.\\frac{\\alpha\\left(\\mathbf{x}_{\\theta},t-\\delta\\right)}{\\alpha\\left(\\mathbf{x}_{\\theta},t\\right)}\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t-\\delta\\right)}\\mathbf{z}_{t}+\\alpha(\\mathbf{x}_{\\theta},t-\\delta)\\left(1-\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t-\\delta\\right)}\\right)\\mathbf{x}_{\\theta}\\right]^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notice that this equation is in indeterminate for when we substitute $\\delta=0$ . One can apply L\u2019Hospital rule twice or break it down into 3 terms below. For this reason let\u2019s write it as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{xpression\\}\\mathbb{1}:\\operatorname*{lim}_{\\delta\\rightarrow0}\\frac{1}{\\delta}\\times\\left[\\frac{\\alpha\\left(\\mathbf{x},t\\mathrm{-}\\delta\\right)}{\\alpha\\left(\\mathbf{x},t\\right)}\\frac{\\nu\\left(\\mathbf{x},t\\right)}{\\nu\\left(\\mathbf{x},t\\mathrm{-}\\delta\\right)}\\mathbf{z}_{t}+\\alpha(\\mathbf{x},t\\mathrm{-}\\delta)\\left(1-\\frac{\\nu\\left(\\mathbf{x},t\\right)}{\\nu\\left(\\mathbf{x},t\\mathrm{-}\\delta\\right)}\\right)\\mathbf{x}\\right.}}\\\\ {{\\left.\\qquad\\qquad\\qquad-\\left.\\frac{\\alpha\\left(\\mathbf{x}_{\\theta},t\\mathrm{-}\\delta\\right)}{\\alpha\\left(\\mathbf{x}_{\\theta},t\\right)}\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t\\mathrm{-}\\delta\\right)}\\mathbf{z}_{t}+\\alpha(\\mathbf{x}_{\\theta},t\\mathrm{-}\\delta)\\left(1-\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)\\mathrm{-}\\delta\\right)}\\right)\\mathbf{x}_{\\theta}\\right]}}\\\\ {{\\mathrm{xpression\\}2:\\operatorname*{lim}_{\\delta\\rightarrow0}\\frac{1}{\\delta-\\operatorname*{lim}_{\\delta}\\left(1-\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t\\mathrm{-}\\delta\\right)}\\right)}\\times\\left[\\frac{\\alpha\\left(\\mathbf{x},t\\mathrm{-}\\delta\\right)}{\\alpha\\left(\\mathbf{x},t\\right)}\\frac{\\nu\\left(\\mathbf{x},t\\right)}{\\nu\\left(\\mathbf{x},t\\mathrm{-}\\delta\\right)}\\mathbf{z}_{t}+\\alpha(\\mathbf{x},t\\mathrm{-}\\delta)\\left(1-\\frac{\\nu\\left(\\mathbf{x},t\\right)}{\\nu\\left(\\mathbf{x},t\\mathrm{-}\\delta\\right)}\\right)\\mathbf{x}\\right.}}\\\\ {{\\left.\\qquad\\qquad\\qquad-\\left.\\frac{\\alpha\\left(\\mathbf{x}_{\\theta},t\\mathrm{-}\\delta\\right)}{\\alpha\\left(\\mathbf{x}_{\\theta},t\\right)}\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t\\mathrm{-}\\delta\\right)}\\mathbf{z}_{t}+\\alpha(\\mathbf{x}_{\\theta},t-\\delta)\\left(1-\\frac{\\nu\\left(\\mathbf{x}_{\\theta},t\\right)}{\\nu\\left(\\mathbf{x}_{\\theta},t\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying L\u2019Hospital rule in expression 1 we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{d}{d\\delta}}\\left({\\frac{\\alpha(\\mathbf{x},t-\\delta)}{\\alpha(\\mathbf{x},t)}}{\\frac{\\nu(\\mathbf{x},t)}{\\nu(\\mathbf{x},t-\\delta)}}\\right)\\Bigg\\vert_{\\delta=0}={\\frac{\\nu(\\mathbf{x},t)}{\\alpha(\\mathbf{x},t)}}{\\frac{-\\nu(\\mathbf{x},t)\\alpha^{\\prime}(\\mathbf{x},t)+\\alpha(\\mathbf{x},t)\\nu^{\\prime}(\\mathbf{x},t)}{\\nu^{2}(\\mathbf{x},t)}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{={\\frac{-\\alpha^{\\prime}(\\mathbf{x},t)}{\\alpha(\\mathbf{x},t)}}+{\\frac{\\nu^{\\prime}(\\mathbf{x},t)}{\\nu(\\mathbf{x},t)}}}\\\\ &{{\\frac{d}{d\\delta}}\\alpha(\\mathbf{x},t-\\delta)\\left(1-{\\frac{\\nu(\\mathbf{x},t)}{\\nu(\\mathbf{x},t-\\delta)}}\\right){\\Bigg|}_{\\delta=0}=-\\alpha(\\mathbf{x},t){\\frac{\\nu^{\\prime}(\\mathbf{x},t)}{\\nu(\\mathbf{x},t)}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left[\\left(\\frac{-{\\alpha}^{\\prime}(\\mathbf{x},t)}{\\alpha(\\mathbf{x},t)}+\\frac{{\\nu}^{\\prime}(\\mathbf{x},t)}{\\nu(\\mathbf{x},t)}+\\frac{{\\alpha}^{\\prime}(\\mathbf{x}_{\\theta},t)}{\\alpha(\\mathbf{x}_{\\theta},t)}-\\frac{{\\nu}^{\\prime}(\\mathbf{x}_{\\theta},t)}{\\nu(\\mathbf{x}_{\\theta},t)}\\right)\\mathbf{z}_{t}\\right.}\\\\ {\\displaystyle\\left.-\\alpha(\\mathbf{x},t)\\frac{{\\nu}^{\\prime}(\\mathbf{x},t)}{\\nu(\\mathbf{x},t)}\\mathbf{x}+\\alpha(\\mathbf{x}_{\\theta},t)\\frac{{\\nu}^{\\prime}(\\mathbf{x}_{\\theta},t)}{\\nu(\\mathbf{x}_{\\theta},t)}\\mathbf{x}_{\\theta}\\right]^{2}\\times\\frac{{\\nu}(\\mathbf{x},t)}{{\\nu}^{\\prime}(\\mathbf{x},t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus the final result: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{d}\\Biggl[\\biggl(\\frac{-\\alpha_{i}^{\\prime}\\bigl(\\mathbf{x},t\\bigr)}{\\alpha_{i}\\bigl(\\mathbf{x},t\\bigr)}+\\frac{\\nu_{i}^{\\prime}\\bigl(\\mathbf{x},t\\bigr)}{\\nu_{i}\\bigl(\\mathbf{x},t\\bigr)}+\\frac{\\alpha_{i}^{\\prime}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)}{\\alpha_{i}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)}-\\frac{\\nu_{i}^{\\prime}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)}{\\nu_{i}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)}\\biggr)\\,\\mathbf{z}_{t}}\\\\ &{\\qquad\\qquad\\qquad-\\alpha_{i}(\\mathbf{x},t)\\frac{\\nu_{i}^{\\prime}\\bigl(\\mathbf{x},t\\bigr)}{\\nu_{i}\\bigl(\\mathbf{x},t\\bigr)}\\mathbf{x}+\\alpha_{i}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)\\frac{\\nu_{i}^{\\prime}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)}{\\nu_{i}\\bigl(\\mathbf{x}_{\\theta},t\\bigr)}\\mathbf{x}_{\\theta}\\Biggr]^{2}\\times\\frac{\\nu_{i}\\bigl(\\mathbf{x},t\\bigr)}{\\nu_{i}^{\\prime}\\bigl(\\mathbf{x},t\\bigr)}}\\\\ &{=\\Lambda^{\\top}\\mathrm{diag}\\left(\\frac{\\nu\\bigl(\\mathbf{x},t\\bigr)}{\\nu^{\\prime}\\bigl(\\mathbf{x},t\\bigr)}\\right)\\Lambda}\\\\ &{\\mathrm{where~}\\Lambda=\\Biggl[\\biggl(\\frac{-\\alpha^{\\prime}(\\mathbf{x},t)}{\\alpha(\\mathbf{x},t)}+\\frac{\\nu^{\\prime}(\\mathbf{x},t)}{\\nu(\\mathbf{x},t)}+\\frac{\\alpha^{\\prime}(\\mathbf{x}_{\\theta},t)}{\\alpha(\\mathbf{x},t)}-\\frac{\\nu^{\\prime}(\\mathbf{x}_{\\theta},t)}{\\nu(\\mathbf{x}_{\\theta},t)}\\biggr)\\,\\mathbf{z}_{t}-\\alpha(\\mathbf{x},t)\\frac{\\nu^{\\prime}(\\mathbf{x},t)}{\\nu(\\mathbf{x},t)}\\mathbf{x}+\\alpha(\\mathbf{x}_{\\theta},t)\\frac{\\nu^{\\prime}(\\mathbf{x}_{\\theta},t)}{\\nu(\\mathbf{x}_{\\theta},t)}\\mathbf{x}_{\\theta}\\Biggr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the second term we have the following: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\underset{r\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{T}{2}\\left(\\operatorname{tr}\\left(\\mathbf{\\Delta}_{\\mathbf{Z}}\\mathbf{)}\\mathbf{\\Sigma}_{p}^{-1}-\\mathbf{I}_{n}\\right)-\\log\\frac{\\left|\\mathbf{\\Delta}_{\\mathbf{Z}_{p}}\\right|}{\\left|\\mathbf{\\Delta}_{\\mathbf{Z}_{p}}\\right|}\\right)}\\\\ {=\\underset{r\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{T}{2}\\left[\\mathrm{tr}\\left(\\mathrm{diag}\\left(\\sigma^{2}(\\mathbf{c},s)\\left(1-\\frac{\\nu(\\mathbf{c},t)}{\\nu(\\mathbf{c},s)}\\right)\\right)\\right)\\int\\mathrm{diag}\\left(\\sigma^{2}(\\mathbf{c}_{\\theta},s)\\left(1-\\frac{\\nu(\\mathbf{c}_{\\theta},t)}{\\nu(\\mathbf{c}_{\\theta},s)}\\right)\\right)-\\mathbf{I}_{n}\\right)}\\\\ {\\quad}&{\\rightarrow\\infty\\left|\\frac{\\mathrm{diag}\\left(\\sigma^{2}(\\mathbf{c},s)\\left(1-\\frac{\\nu(\\mathbf{c},t)}{\\nu(\\mathbf{c},s)}\\right)\\right)}{\\left|\\mathrm{diag}\\left(\\sigma^{2}(\\mathbf{c}_{\\theta},s)\\left(1-\\frac{\\nu(\\mathbf{c}_{\\theta},t)}{\\nu(\\mathbf{c}_{\\theta},s)}\\right)\\right)\\right|^{2}}\\right]}\\\\ {=\\underset{r\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{T}{2}\\left(\\frac{\\sigma_{t}^{2}(\\mathbf{c},s)\\left(1-\\frac{\\nu_{t}(\\mathbf{c},t)}{\\nu(\\mathbf{c},s)}\\right)}{\\sigma_{t}^{2}(\\mathbf{c}_{\\theta},s)\\left(1-\\frac{\\nu_{t}(\\mathbf{c},t)}{\\nu(\\mathbf{c}_{\\theta},s)}\\right)}-1-\\log\\frac{\\sigma_{t}^{2}(\\mathbf{c},s)\\left(1-\\frac{\\nu_{t}(\\mathbf{c},t)}{\\nu(\\mathbf{c},s)}\\right)}{\\sigma_{t}^{2}(\\mathbf{c}_{\\theta},s)\\left(1-\\frac{\\nu_{t}(\\mathbf{c},t)}{\\nu(\\mathbf{c}_{\\theta},s)}\\right)}\\right)}\\end{array}\\quad\\quad\\mathrm{(s)}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The sequence limT \u2192\u221eT2 $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\frac{T}{2}\\sum_{i=1}^{d}(p_{i}-1-\\log p_{i})}\\end{array}$ converges iff $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\sum_{i=1}^{d}(p_{i}-1-\\log p_{i})=0}\\end{array}$ . Notice that the function $\\overline{{f}}(x)=x-1-\\log x\\geq0\\;\\;\\forall x\\in\\mathbb{R}$ and the equality holds for $x=1$ . Thus, the condition $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\frac{T}{2}\\sum_{i=1}^{d}(p_{i}-1-\\log p_{i})}\\end{array}$ holds iff $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}p_{i}=0\\;\\;\\forall i\\in\\{1,\\ldots,d\\}}\\end{array}$ . Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}p_{i}=1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\implies\\operatorname*{lim}_{T\\rightarrow\\infty}\\left(\\frac{{\\sigma_{i}}^{2}(\\mathbf{c},s)\\left(1-\\frac{\\nu_{i}(\\mathbf{c},t)}{\\nu_{i}(\\mathbf{c},s)}\\right)}{{\\sigma_{i}}^{2}(\\mathbf{c}_{\\theta},s)\\left(1-\\frac{\\nu_{i}(\\mathbf{c}_{\\theta},t)}{\\nu_{i}(\\mathbf{c}_{\\theta},s)}\\right)}\\right)=1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\implies\\underset{\\delta\\rightarrow0^{+}}{\\operatorname*{lim}}\\left(\\frac{\\sigma_{i}{}^{2}\\left(\\mathbf{c},t-\\delta\\right)\\,\\left(1-\\frac{\\nu_{i}\\left(\\mathbf{c},t\\right)}{\\nu_{i}\\left(\\mathbf{c},t-\\delta\\right)}\\right)}{\\sigma_{i}{}^{2}\\left(\\mathbf{c}_{\\theta},t-\\delta\\right)\\,\\left(1-\\frac{\\nu_{i}\\left(\\mathbf{c}_{\\theta},t\\right)}{\\nu_{i}\\left(\\mathbf{c}_{\\theta},t-\\delta\\right)}\\right)}\\right)=1}\\\\ &{\\implies\\frac{{\\sigma_{i}}^{2}\\left(\\mathbf{c},t\\right)}{\\sigma_{i}{}^{2}\\left(\\mathbf{c}_{\\theta},t\\right)}\\,\\underset{\\delta\\rightarrow0^{+}}{\\operatorname*{lim}}\\,\\left(\\frac{1-\\frac{\\nu_{i}\\left(\\mathbf{c},t\\right)}{\\nu_{i}\\left(\\mathbf{c},t-\\delta\\right)}\\right)}{1-\\frac{\\nu_{i}\\left(\\mathbf{c}_{\\theta},t\\right)}{\\nu_{i}\\left(\\mathbf{c}_{\\theta},t-\\delta\\right)}}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\implies\\frac{{\\sigma_{i}}^{2}({\\bf c},t)}{{\\sigma_{i}}^{2}({\\bf c}_{\\theta},t)}\\left(\\frac{\\frac{-\\nu_{i}\\mathbf{\\nu}^{\\prime}({\\bf c},t)}{\\nu_{i}({\\bf c},t)})}{\\frac{-\\nu_{i}\\mathbf{\\nu}^{\\prime}({\\bf c}_{\\theta},t)}{\\nu_{i}({\\bf c}_{\\theta},t)}}\\right)=1}\\\\ &{\\implies\\frac{{\\sigma_{i}}^{2}({\\bf c},t)}{{\\sigma_{i}}^{2}({\\bf c}_{\\theta},t)}\\left(\\frac{\\nu_{i}\\mathbf{\\nu}^{\\prime}({\\bf c},t)\\nu_{i}({\\bf c}_{\\theta},t)}{\\nu_{i}({\\bf c},t){\\nu_{i}}^{\\prime}({\\bf c}_{\\theta},t))}\\right)=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In the vector form the above equation can be written as, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\pmb{\\sigma}_{t}^{2}(\\mathbf{c})\\pmb{\\nu}_{t}(\\mathbf{c}_{\\theta})\\nabla_{t}\\pmb{\\nu}(\\mathbf{c},t)}{\\pmb{\\sigma}_{t}^{2}(\\mathbf{c}_{\\theta})\\pmb{\\nu}_{t}(\\mathbf{c})\\nabla_{t}\\pmb{\\nu}(\\mathbf{c}_{\\theta},t)}=\\mathbf{1}_{\\mathbf{d}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Eq. 58 holds if: ", "page_idx": 27}, {"type": "text", "text": "\u2022 $x_{\\theta}=x_{0}$ i.e. the unet can perfectly map $\\mathbf{x}_{t}$ to $\\mathbf{x}_{0}\\,\\forall t\\in[0,1]$ which is unrealistic. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Clever parameterizations for $\\sigma,\\alpha,\\nu$ that ensure Eq. 58 holds. ", "page_idx": 27}, {"type": "text", "text": "Because of aforementioned challenges we evaluate this method with finite $T=1000$ . We demonstrate the performance of the model empirically in Fig. 4. ", "page_idx": 27}, {"type": "text", "text": "D.2.2 Recovering VDM ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "If we substitute $\\nu_{t}(\\mathbf{c}),\\nu_{t}(\\mathbf{c}_{\\theta})$ with $\\nu(t)$ (since the SNR isn\u2019t conditioned on the context c), ${\\pmb\\sigma}_{t}({\\bf c}_{\\theta}),{\\pmb\\sigma}_{t}({\\bf c})$ with $\\sigma_{t}$ and ${\\alpha_{t}}(\\mathbf{c}_{\\theta}),{\\alpha_{t}}(\\mathbf{c})$ with $\\pmb{\\alpha}_{t}$ , Eq. 45 reduces to the intermediate loss in VDM i.e. $\\begin{array}{r}{\\frac{1}{2}(\\mathbf{x}_{\\theta}-\\mathbf{x}_{0})^{\\top}\\nabla_{t}\\pmb{\\nu}(t)\\,\\left(\\mathbf{x}_{\\theta}-\\mathbf{x}_{0}\\right)}\\end{array}$ and Eq. 55 reduces to 0. ", "page_idx": 27}, {"type": "text", "text": "D.3 Experimental results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Fig. 4 we demonstrate that the multivariate diffusion processes where $\\mathbf{c}=$ \u201cclass labels\u201d or $\\mathbf{c}=\\mathbf{x}_{0}$ perform worse than VDM. Since a continuous time formulation i.e. $T\\rightarrow\\infty$ for the case when $\\mathbf{c}=\\mathbf{x}_{0}$ isn\u2019t possible (unlike MULAN or VDM) we evaluate these models in the discrete time setting where we use $T=1000$ . Furthermore we also ablate $T=10k$ , $100k$ for $\\mathbf{c}=\\mathbf{x}_{0}$ to show that the VLB degrades with increasing T whereas for VDM and MULAN it improves for increasing T; see Kingma et al. [20]. This empirical observation is consistent with our mathematical insights earlier. As these models consistently exhibit inferior performance w.r.t VDM, in line with our initial conjectures, we refrain from training them beyond $300\\mathrm{k}$ iterations due to the substantial computational cost involved. ", "page_idx": 27}, {"type": "image", "img_path": "loMa99A4p8/tmp/d1ff2a3e978f7ded181dd8012d472a5fb4318195c4c12e48f339af3253f81b43.jpg", "img_caption": ["Figure 4: For $\\mathbf{c}=$ \u201cclass labels\u201d or $\\mathbf{c}=\\mathbf{x}_{0}$ the likelihood estimates are worse than VDM. For $\\mathbf{c}=\\mathbf{x}_{0}$ , we see that the VLB degrades with increasing $\\mathrm{T},$ , but for VDM and MULAN, it improves with increasing T. This empirical observation is consistent with our mathematical insights earlier. As these models consistently exhibit inferior performance w.r.t VDM, in line with our initial conjectures, we refrain from training them beyond $300\\mathrm{k}$ iterations due to the substantial computational cost involved. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 7: Likelihood in bits per dimension (BPD) (mean and $95\\%$ confidence interval), on the test set of CIFAR-10 computed using VLB estimate. ", "page_idx": 28}, {"type": "table", "img_path": "loMa99A4p8/tmp/3392a792e7616c999c538c2d0dee3739e48afd21ad176a5c2b52c79bc60573b3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Appendix E MULAN: MUltivariate Latent Auxiliary variable Noise Schedule ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "E.1 Parameterization in the reverse process ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "E.1.1 Noise parameterization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Since the forward pass is given by $\\mathbf{x}_{t}=\\alpha_{t}(\\mathbf{z})\\mathbf{x}_{0}+\\pmb{\\sigma}_{t}(\\mathbf{z})\\epsilon_{t}$ , we can write the noise $\\epsilon_{t}$ in terms of $\\mathbf{x}_{\\mathrm{0}},\\mathbf{x}_{t}$ in the following manner: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{t}=\\frac{\\mathbf{x}_{t}-\\pmb{\\alpha}_{t}(\\mathbf{z})\\mathbf{x}_{0}}{\\pmb{\\sigma}_{t}(\\mathbf{z})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Following Dhariwal & Nichol [10], Kingma et al. [20], instead of parameterizing $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)$ using a neural network, we use Eq. 59 to parameterize the denoising model in terms of a noise prediction model $\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)=\\frac{\\mathbf{x}_{t}-\\alpha_{t}(\\mathbf{z})\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\sigma_{t}(\\mathbf{z})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "E.1.2 Velocity parameterization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Following Salimans & Ho [45], Zheng et al. [65], we explore another parameterization of the denoising network which is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)=\\frac{\\alpha_{t}(\\mathbf{z})\\mathbf{x}_{t}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\sigma_{t}(\\mathbf{z})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In practice, v-parameterization leads to a better performance than noise parameterization; as illustrated in Table 7. ", "page_idx": 28}, {"type": "text", "text": "E.2 Polynomial Noise Schedule ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let $f(x;\\psi)$ be a scalar-valued polynomial of degree $n$ with coefficients $\\psi\\in\\mathbb{R}^{n+1}$ expressed as: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(x;\\psi)=\\psi_{n}x^{n}+\\psi_{n-1}x^{n-1}+\\cdot\\cdot\\cdot+\\psi_{1}x+\\psi_{0},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and denote its derivative with respect to $x$ as $\\textstyle{\\frac{d}{d x}}f(x;\\psi)$ , represented by $f^{\\prime}(x;\\psi)$ . Now we\u2019d like to find least $n$ such that $f(x;\\psi)$ satisfies the following properties: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf^{\\prime}(x_{1};\\psi)=0,f^{\\prime}(x_{2};\\psi)=0\\,\\exists x_{1},x_{2}\\in\\mathbb{C},x_{1}\\neq x_{2},\\forall\\psi\\in\\mathbb{R}^{n+1}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the first condition to hold, we can design $f^{\\prime}(x;\\psi)$ such that it\u2019s a perfect square with real $/$ imaginary roots. That way $f^{\\prime}(x;\\psi)\\geq0\\;\\forall x\\;\\overline{{\\in}}\\;\\dot{\\mathbb{R}}$ , $\\boldsymbol{\\dot{\\psi}}\\in\\dot{\\mathbb{R}}^{n+1}$ . This means that $f^{\\prime}(x;\\psi)$ is an even degree polynomial, i.e. the degree of $f^{\\prime}(x;\\psi)$ can take the following values: $2,4,\\ldots$ Also, note that at least half of the roots of $f^{\\prime}(x;\\psi)$ are repeated since $f^{\\prime}(x;\\psi)$ can be expressed as a perfect square, i.e., if $f^{\\prime}(x;\\psi)$ has a degree 2 then it has exactly 1 unique root (real $/$ imaginary), if $\\bar{f}^{\\prime}(x;\\psi)$ has a degree 4 then it has at most 2 unique roots (real / imaginary), and so on. ", "page_idx": 29}, {"type": "text", "text": "For the second condition to hold, $f^{\\prime}(x;\\psi)$ needs to have at least 2 unique roots $\\exists\\psi\\in\\mathbb{R}^{n+1}$ . For this reason $f^{\\prime}(x;\\psi)$ is a polynomial of degree 4. Thus, $f^{\\prime}(x;\\psi)$ can be written as $f^{\\prime}(x;\\psi)=(\\psi_{3}x^{2}+$ $\\psi_{2}x+\\psi_{1})^{2}$ . This ensures that $\\exists\\psi\\in\\mathbb{R}^{5}$ s.t. $f^{\\prime}(x;\\psi)=0$ twice in $x\\in\\mathbb R$ , and $f^{\\prime}(x;\\psi)\\geq0\\,\\forall\\psi\\in\\mathbb{R}^{5}$ . ", "page_idx": 29}, {"type": "text", "text": "Thus, $f(x;\\psi)$ takes the following functional form: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{f(x;\\psi)=\\int(\\psi_{3}x^{2}+\\psi_{2}x+\\psi_{1})^{2}d x}}\\ ~}\\\\ {{\\displaystyle{=\\frac{\\psi_{3}^{2}}{5}x^{5}+\\frac{\\psi_{3}\\psi_{2}}{2}x^{4}+\\frac{\\psi_{2}^{2}+2\\psi_{3}\\psi_{1}}{3}x^{3}+\\psi_{2}\\psi_{1}x^{2}+\\psi_{1}^{2}x+\\mathrm{constant}.}}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the above-mentioned reasons we express $\\gamma_{\\phi}(\\mathbf{c},t):\\mathbb{R}^{m}\\times[0,1]\\to\\mathbb{R}^{d}$ as a degree 5 polynomial in $t$ . We define neural networks $\\mathbf{a}_{\\phi}(\\mathbf{c}):\\mathbb{R}^{m}\\to\\mathbb{R}^{d}$ , $\\mathbf{b}_{\\phi}(\\mathbf{c}):\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{d}$ , and $\\mathbf{d}_{\\phi}(\\mathbf{c}):\\mathbb{R}^{m}\\to\\mathbb{R}^{d}$ with parameters $\\phi$ . Let $f_{\\phi}:\\mathbb{R}^{m}\\times[0,1]\\to\\mathbb{R}^{d}$ be defined as: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\phi}(\\mathbf{c},t)=\\frac{\\mathbf{a}_{\\phi}^{2}(\\mathbf{c})}{5}t^{5}+\\frac{\\mathbf{a}_{\\phi}(\\mathbf{c})\\mathbf{b}_{\\phi}(\\mathbf{c})}{2}t^{4}+\\frac{\\mathbf{b}_{\\phi}^{2}(\\mathbf{c})+2\\mathbf{a}_{\\phi}(\\mathbf{c})\\mathbf{d}_{\\phi}(\\mathbf{c})}{3}t^{3}+\\mathbf{b}_{\\phi}(\\mathbf{c})\\mathbf{d}_{\\phi}(\\mathbf{c})t^{2}+\\mathbf{d}_{\\phi}^{2}(\\mathbf{c})t^{4}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the multiplication and division operations are elementwise. The the noise schedule, $\\gamma(\\mathbf{c},t)$ , is given as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma_{\\phi}(\\mathbf{c},t)=\\gamma_{\\operatorname*{min}}+(\\gamma_{\\operatorname*{max}}-\\gamma_{\\operatorname*{min}})\\frac{f_{\\phi}(\\mathbf{c},t)}{f_{\\phi}(\\mathbf{c},t=1)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Notice that $\\gamma_{\\phi}(\\mathbf{c},t)$ has these interesting properties: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Is increasing in $t\\in[0,1]$ which is crucial as mentioned in Sec. 3.5.   \n\u2022 $\\gamma_{\\phi}(\\mathbf{c},t)$ has end points at $t=0$ and $t=1$ which the user can specify via $\\gamma_{\\mathrm{min}}$ and $\\gamma_{\\mathrm{max}}$ . Specificaly, $\\gamma_{\\phi}(\\mathbf{c},t=0)=\\gamma_{\\mathrm{min}}\\mathbf{1_{d}}$ and $\\gamma_{\\phi}(\\mathbf{c},t=1)=\\gamma_{\\operatorname*{max}}\\mathbf{1_{d}}$ .   \n\u2022 Its time-derivative i.e. $\\nabla_{t}\\gamma_{\\phi}(\\mathbf{c},t)$ can be zero twice in $t\\in[0,1]$ . This isn\u2019t a necessary condition but it\u2019s nice to have a flexible noise schedule whose time-derivative can be 0 at the beginning and the end of the diffusion process. ", "page_idx": 29}, {"type": "text", "text": "E.3 Variational Lower Bound ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section we derive the VLB. For ease of reading we use the notation $\\mathbf{x}_{t}$ to denote $\\mathbf{x}_{t(i)}$ and $\\mathbf x_{t-1}$ to denote $\\mathbf{x}_{t(i-1)}\\equiv\\mathbf{x}_{s(i)}$ in the following derivation. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log p_{\\theta}(\\mathbf{x}_{0})}\\\\ &{\\le\\mathbb{E}_{q_{\\phi}}\\left[-\\log\\frac{p_{\\theta}(\\mathbf{z},\\,\\mathbf{x}_{0:T})}{q_{\\phi}\\left(\\mathbf{z},\\,\\mathbf{x}_{1:T}\\left|\\mathbf{x}_{0}\\right)}\\right]}\\\\ &{=\\mathbb{E}_{q_{\\phi}}\\left[-\\log\\frac{p_{\\theta}\\left(\\mathbf{x}_{0:T-1}\\left|\\mathbf{z},\\,\\mathbf{x}_{T}\\right)\\right.}{q_{\\phi}\\left(\\mathbf{z},\\,\\mathbf{x}_{1:T}\\left|\\mathbf{x}_{0}\\right)}-\\log p_{\\theta}(\\mathbf{x}_{T})-\\log p_{\\theta}(\\mathbf{z})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{\\theta_{1}}\\left[-\\ln\\theta_{2}(\\theta_{1}(\\theta_{2})-\\log_{1}(1+\\theta_{3}))-\\log_{1}(\\theta_{3}(\\theta_{3})-\\log_{2}(\\theta_{3}))-\\log_{2}(\\theta_{3}(\\theta_{3}))\\right]}\\\\ &{=\\mathbb{E}_{\\theta_{1}}\\left[-\\ln\\theta_{2}(\\theta_{3}(\\theta_{3})-\\log_{2}(\\theta_{3}))-\\log_{2}(\\theta_{3}(\\theta_{3}))\\right]}\\\\ &{=\\mathbb{E}_{\\theta_{2}}\\left[-\\sum_{l}\\ln\\theta_{3}(\\theta_{2}(\\theta_{3})-\\log_{2}(\\theta_{3})-\\log_{2}(\\theta_{3}))\\right]}\\\\ &{\\le\\mathbb{E}_{\\theta_{3}}\\left[\\sum_{l=1}^{\\infty}\\phi_{l}(\\theta_{3}(\\theta_{3})-\\log_{3})-\\log_{2}(\\theta_{3}(\\theta_{3})-\\log_{3}(\\theta_{3}))\\right]}\\\\ &{\\le-\\theta_{4}\\left[\\ln\\theta_{2}(\\theta_{3})\\right]}\\\\ &{=\\mathbb{E}_{\\theta_{3}}\\left[-\\ln\\theta_{2}(\\theta_{3})\\right]-\\frac{1}{\\sqrt{3}}\\log_{4}(\\theta_{3}(\\theta_{3})-\\log_{3}(\\theta_{3}))-\\log_{2}(\\theta_{3}(\\theta_{3}))}\\\\ &{\\le\\theta_{4}\\left[-\\ln\\theta_{2}(\\theta_{3})\\right]\\sin_{3}\\theta_{2}\\left[\\sin_{3}(\\theta_{2})-\\log_{2}(\\theta_{3})\\right]}\\\\ &{=\\mathbb{E}_{\\theta_{1}}\\left[-\\ln\\theta_{2}(\\theta_{3})\\right]\\sin_{3}\\theta_{2}\\left[\\sin_{3}(\\theta_{3})-\\log_{2}(\\theta_{3})\\right]}\\\\ &{=\\mathbb{E}_{\\theta_{2}}\\left[-\\ln\\theta_{2}(\\theta_{3})\\right]-\\frac{1}{\\sqrt{3}}\\log_{2}\\theta_{3}(\\theta_{3})\\log_{3}(\\theta_{3})-\\log_{2}(\\theta_{3})}\\\\ &{=\\mathbb{E}_{ \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Switching back to the notation used throughout the paper, the VLB is given as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log p_{\\theta}(\\mathbf{x}_{0})}\\\\ &{\\le\\mathbb{E}_{q_{\\phi}}\\left[\\underbrace{-\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{z},\\mathbf{x}_{1})}_{\\mathcal{L}_{\\operatorname{mex}}}+\\underbrace{\\sum_{i=2}^{T}\\mathrm{D}_{\\mathrm{KL}}[p_{\\theta}(\\mathbf{x}_{s(i)}|\\mathbf{z},\\mathbf{x}_{t(i)})\\|q_{\\phi}(\\mathbf{x}_{s(i)}|\\mathbf{x}_{t(i)},\\mathbf{z},\\mathbf{x}_{0})]}_{\\mathcal{L}_{\\mathrm{diffusion}}}\\right]}\\\\ &{\\quad+\\,\\mathbb{E}_{q_{\\phi}}\\left[\\underbrace{\\mathrm{D}_{\\mathrm{KL}}[p_{\\theta}(\\mathbf{x}_{1})\\|q_{\\phi}(\\mathbf{x}_{1}|\\mathbf{z},\\mathbf{x}_{0})]}_{\\mathcal{L}_{\\mathrm{ptw}}}+\\underbrace{\\mathrm{D}_{\\mathrm{KL}}[p_{\\theta}(\\mathbf{z})\\|q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})]}_{\\mathcal{L}_{\\mathrm{limin}}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E.4 Diffusion Loss ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To derive the diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}$ in Eq. 9, we first derive an expression for $\\mathrm{D}_{\\mathrm{KL}}\\big(q_{\\phi}(\\mathbf{x}_{s}|\\mathbf{z},\\mathbf{x}_{t},\\mathbf{x}_{0})\\big|\\big|p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{z},\\mathbf{x}_{t})\\big)$ using Eq. 4 and Eq. 6 in the following manner (details in Suppl. E): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}\\big(q_{\\phi}(\\mathbf{x}_{s}|\\mathbf{z},\\mathbf{x}_{t},\\mathbf{x}_{0})\\big\\|p_{\\theta}(\\mathbf{x}_{s}|\\mathbf{z},\\mathbf{x}_{t})\\big)}\\\\ &{\\ =\\frac{1}{2}\\left((\\mu_{q_{\\phi}}-\\mu_{p})^{\\top}\\Sigma_{\\theta}^{-1}(\\mu_{q_{\\phi}}-\\mu_{p})+\\mathrm{tr}\\left(\\Sigma_{q_{\\phi}}\\Sigma_{p}^{-1}-\\mathbf{I}_{n}\\right)-\\log\\frac{\\left|\\Sigma_{q_{\\phi}}\\right|}{\\left|\\Sigma_{p}\\right|}\\right)}\\\\ &{\\ =\\frac{1}{2}\\left((\\mathbf{x}_{0}-\\mathbf{x}_{\\theta})^{\\top}\\mathrm{diag}(\\nu(\\mathbf{z},s)-\\nu(\\mathbf{z},t))(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}T(\\pmb{\\nu}_{s}(z)-\\pmb{\\nu}_{t}(z))=-\\nabla_{t}\\pmb{\\nu}(\\mathbf{z},t)}\\end{array}$ be the partial derivative of the vector $\\nu(\\mathbf{z},t)$ w.r.t scalar $t$ . Then we derive the diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}$ , for the continuous case in the following manner (for brevity we use the notation $s$ for $s(i)=(i-1)/T$ and $t$ for $t(i)=i/T$ ): ", "page_idx": 31}, {"type": "text", "text": "Ldiffusion   \n$=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{2}\\sum_{i=2}^{T}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\mathrm{D}_{\\mathrm{KL}}\\big(q(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{x}_{0},\\mathbf{z})\\big|\\big|p_{\\theta}\\big(\\mathbf{x}_{s}|\\mathbf{x}_{t},\\mathbf{z}\\big)\\big)$ Using Eq. 66 we get,   \n$\\begin{array}{l}{\\displaystyle=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{2}\\sum_{i=2}^{T}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t(i)))^{\\top}\\mathrm{diag}\\left(\\nu(s(i),\\mathbf{z})-\\nu(t(i),\\mathbf{z})\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t(i)))}\\\\ {\\displaystyle=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{n})}\\left[\\operatorname*{lim}_{T\\rightarrow\\infty}\\sum_{i=2}^{T}T(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t(i)))^{\\top}\\mathrm{diag}\\left(\\nu(s(i),\\mathbf{z})-\\nu(t(i),\\mathbf{z})\\right)(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t(i)))\\frac{1}{T}\\right]}\\end{array}$ Using the fact that $\\operatorname*{lim}_{T\\to\\infty}T\\left(\\pmb{\\nu}(s,\\mathbf{z})-\\pmb{\\nu}(\\mathbf{z},t)\\right)=-\\nabla_{t}\\pmb{\\nu}(t,\\mathbf{z})$ we get,   \n$=-\\frac{1}{2}\\mathbb{E}_{t\\sim\\{0,\\dots,1\\}}\\left[\\left(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right)^{\\top}\\left(\\nabla_{t}\\nu_{t}(z)\\right)\\left(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},t)\\right)\\right]$ Substituting $\\mathbf{x}_{0}=\\alpha_{t}^{-1}(\\mathbf{z})(\\mathbf{x}_{t}-\\pmb{\\sigma}_{t}(\\mathbf{z})\\pmb{\\epsilon}_{t})$ from Eq. 59 and Substituting $\\mathbf{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)=\\alpha_{t}^{-1}(\\mathbf{z})(\\mathbf{x}_{t}-\\sigma_{t}(\\mathbf{z})\\epsilon_{\\theta}(\\mathbf{x}_{t},t))$ from Eq. 60 we get,   \n$=-\\frac{1}{2}\\mathbb{E}_{t\\sim[0,1]}\\left[(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\left(\\frac{\\sigma_{t}^{2}(\\mathbf{z})}{\\alpha_{t}^{2}(\\mathbf{z})}\\times\\nabla_{t}\\nu_{t}(\\mathbf{z})\\right)(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t))\\right]$ Let $\\pmb{\\nu}^{-1}(\\mathbf{z},t)$ denote the reciprocal of the values in the vector $\\boldsymbol{\\nu}(\\mathbf{z},t)$ .   \n$=-\\frac{1}{9}\\mathbb{E}_{t\\sim[0,1]}\\left[(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t))^{\\top}\\mathrm{diag}\\left(\\nu^{-1}(t)(\\mathbf{z})\\nabla_{t}\\nu_{t}(\\mathbf{z})\\right)(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t))\\right]$ Substituting $\\pmb{\\nu}(\\mathbf{z},t)=\\exp(-\\gamma(\\mathbf{z},t))$ from Sec. E.1.1   \n$\\begin{array}{r l}&{=-\\cfrac{1}{2}\\mathbb{E}_{t\\sim[0,1]}\\left[\\left(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)^{\\top}\\mathrm{diag}\\left(\\exp\\left(\\gamma(\\mathbf{z},t)\\right)\\nabla_{t}\\exp\\left(-\\gamma(\\mathbf{z},t)\\right)\\right)\\left(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)\\right]}\\\\ &{=\\cfrac{1}{2}\\mathbb{E}_{t\\sim[0,1]}\\left[\\left(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)^{\\top}\\mathrm{diag}\\left(\\exp\\left(\\gamma(\\mathbf{z},t)\\right)\\exp\\left(-\\gamma(\\mathbf{z},t)\\right)\\nabla_{t}\\gamma(\\mathbf{z},t)\\right)\\left(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)\\right]}\\\\ &{=\\cfrac{1}{2}\\mathbb{E}_{t\\sim[0,1]}\\left[\\left(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)^{\\top}\\mathrm{diag}\\left(\\nabla_{t}\\gamma(\\mathbf{z},t)\\right)\\left(\\epsilon_{t}-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)\\right]}\\end{array}$ (67) ", "page_idx": 31}, {"type": "text", "text": "E.5 Recovering VDM from the Vectorized Representation of the diffusion loss ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Notice that we recover the loss function in VDM when $\\pmb{\\nu}(\\mathbf{z},t)=\\nu(t)\\mathbf{1_{d}}$ where $\\nu_{t}\\in\\mathbb{R}^{+}$ and $\\mathbf{1_{d}}$ represents a vector of 1s of size $d$ and the noising schedule isn\u2019t conditioned on $\\mathbf{z}$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{0}^{1}\\langle\\mathbf{f}_{\\theta}(\\mathbf{x}_{0},\\nu(\\mathbf{z},t)),\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(t)\\rangle\\mathrm{d}t=\\displaystyle\\int_{0}^{1}\\langle\\mathbf{f}_{\\theta}(\\mathbf{x}_{0},\\nu(t)),\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nu(\\nu(t)\\mathbf{1}_{\\mathbf{n}})\\rangle\\mathrm{d}t}&{}\\\\ {\\displaystyle}&{=\\displaystyle\\int_{0}^{1}\\langle\\mathbf{f}_{\\theta}(\\mathbf{x}_{0},\\nu(t)),\\mathbf{1}_{\\mathbf{d}}\\rangle\\nu^{\\prime}(t)\\mathrm{d}t}\\\\ {\\displaystyle}&{=\\displaystyle\\int_{0}^{1}\\nu^{\\prime}(t)\\|\\mathbf{f}_{\\theta}(\\mathbf{x}_{0},\\nu(t))\\|_{1}^{1}\\mathrm{d}t}\\\\ {\\displaystyle}&{=\\displaystyle\\int_{0}^{1}\\nu^{\\prime}(t)\\|(\\mathbf{x}_{0}-\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu(t)},\\nu(t)))\\|_{2}^{2}\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$\\begin{array}{r}{\\int_{0}^{1}\\frac{d}{d t}\\nu(t)\\|(\\mathbf{x}_{0}-\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{\\nu(t)},\\pmb{\\nu}(t)))\\|_{2}^{2}d t}\\end{array}$ denotes the diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}$ , as used in VDM; see ", "page_idx": 31}, {"type": "image", "img_path": "loMa99A4p8/tmp/63d8d7690e45a017e125a37c21409aa2683a55b11d1703dc0327222bd001c206.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 5: (a) Imagine piloting a plane across a region with cyclones and strong winds, as shown in Fig. 5. Plotting a direct, straight-line course through these adverse weather conditions requires more fuel and effort due to increased resistance. By navigating around the cyclones and winds, however, the plane reaches its destination with less energy, even if the route is longer.This intuition translates into mathematical and physical terms. The plane\u2019s trajectory is denoted by $\\mathbf{r}(t)\\in\\mathbb{R}_{+}^{n}$ , while the forces acting on it are represented by $\\mathbf{f}(\\mathbf{r}(t))\\,\\in\\,\\mathbb{R}^{n}$ . The work required to navigate is given by $\\begin{array}{r}{\\int_{0}^{1}\\mathbf{f}(\\mathbf{r}(t))\\cdot\\frac{d}{d t}\\mathbf{r}(t),d t}\\end{array}$ . Here, the work depends on the trajectory because $\\mathbf{f}(\\mathbf{r}(t))$ is not a conservative field. ", "page_idx": 32}, {"type": "text", "text": "${\\bf(b)}$ This concept also applies to the diffusion NELBO. From Eq. 12, it\u2019s clear that the trajectory $\\mathbf{r}(t)$ is parameterized by the noise schedule $\\nu(\\mathbf{z},t)$ , which is influenced by complex forces, f (analogous to weather patterns), represented by the dimension-wise reconstruction error of the denoising model, $(\\mathbf{x}_{0}-\\mathbf{x}_{\\theta}(\\mathbf{\\dot{x}}_{t},\\mathbf{z},t))^{2}$ . Thus, the diffusion loss, $\\mathcal{L}_{\\mathrm{diffusion}}$ , can be interpreted as the work done along the trajectory $\\nu(\\mathbf{z},t)$ in the presence of these vector field forces f. By learning the noise schedule, we can avoid \u201chigh-resistance\u201d paths (those where the loss accumulates rapidly), thereby minimizing the overall \u201cenergy\u201d expended, as measured by the NELBO. ", "page_idx": 32}, {"type": "text", "text": "Appendix F Subset Sampling ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Sampling a subset of $k$ items from a collection of collection of $n$ items, $x_{1},x_{2},\\ldots,x_{3}$ belongs to a category of algorithms called reservoir algorithms. In weighted reservoir sampling, every $x_{i}$ is associated with a weight $w_{i}~\\geq~0$ . The probability associated with choosing the sequence $S_{\\mathrm{wrs}}\\,=\\,[i_{1},i_{2},\\cdot\\cdot\\cdot,i_{k}]$ be a tuple of indices. Then the probability associated with sampling this sequence is ", "page_idx": 32}, {"type": "equation", "text": "$$\np(S_{\\mathrm{wrs}}|\\mathbf{w})=\\frac{w_{i_{1}}}{Z}\\frac{w_{i_{2}}}{Z-w_{i_{1}}}\\dots\\frac{w_{i_{k}}}{Z-\\sum_{j=1}^{k-1}w_{i_{j}}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Efraimidis & Spirakis [13] give an algorithm for weighted reservoir sampling where each item is assigned a random key $\\bar{r_{i}}\\;=\\;u_{i}^{\\frac{1}{w_{i}}}$ where $u_{i}$ is drawn from a uniform distribution [0, 1] and $w_{i}$ is the weight of item $x_{i}$ . Let $\\mathrm{TopK}({\\bf r},k)$ which takes keys $\\mathbf{r}=[r_{1},r_{2},\\ldots,r_{n}]$ and returns a sequence $[i_{1},i_{2},\\ldots,i_{k}]$ . Efraimidis & Spirakis [13] proved that $\\mathrm{TopK}({\\bf r},k)$ is distributed according to $p(S_{\\mathrm{wrs}}|\\mathbf{\\bar{w}})$ . ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Let\u2019s represent a subset $S\\in\\{0,1\\}^{n}$ with exactly $k$ non-zero elements that are equal to 1. Then the probability associated with sampling $S$ is given as, ", "page_idx": 32}, {"type": "equation", "text": "$$\np(S|\\mathbf{w})=\\sum_{S_{\\mathrm{wrs}}\\in\\Pi(S)}p(S_{\\mathrm{wrs}}|\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\Pi(S)$ denotes all possible permutations of the sequence $S$ . By ignoring the ordering of the elements in $S_{\\mathrm{wrs}}$ we can sample using the same algorithm. Xie & Ermon [61] show that this sampling algorithm is equivalent to $\\mathrm{TopK}(\\hat{\\mathbf{r}},k)$ where $\\hat{\\mathbf{r}}=[\\hat{r}_{1},\\hat{r}_{2},\\ldots,\\hat{r}_{n}]$ where $\\hat{r}_{i}\\,=\\,-\\log(-\\log(r_{i}\\bar{\\mathbf{\\alpha}}))\\,\\bar{\\mathbf{\\alpha}}$ $\\log w_{i}+\\operatorname{Gumbel}(0,\\;1)$ . This holds true because the monotonic transformation $-\\log(-\\log(x))$ preserves the ordering of the keys and thus $\\mathrm{TopK}({\\bf r},k)\\equiv\\mathrm{TopK}(\\hat{\\bf r},k)$ . ", "page_idx": 32}, {"type": "text", "text": "Sum of Gamma Distribution. Niepert et al. [34] show that adding SOG noise instead of Gumbel noise leads to better performance. ", "page_idx": 33}, {"type": "text", "text": "Niepert et al. [34] show that $\\mathbf{z}\\sim p_{\\theta}(\\mathbf{z};\\theta)$ is equivalent to $\\mathbf{z}=\\arg\\operatorname*{max}_{y\\in Y}\\langle\\theta+\\epsilon_{g},y\\rangle$ where $\\epsilon_{g}$ is a sample from Sum-of-Gamma distribution given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{SoG}(k,\\tau,s)=\\frac{\\tau}{k}\\biggl(\\sum_{i=1}^{s}\\operatorname{Gamma}\\Bigl(\\frac{1}{k},\\frac{k}{i}\\Bigr)-\\log s\\biggr),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $s$ is a positive integer and $\\mathrm{Gamma}(\\alpha,\\beta)$ is the Gamma distribution with $(\\alpha,\\beta)$ as the shape and scale parameters. ", "page_idx": 33}, {"type": "text", "text": "And hence, given logits log w, we sample a $k$ -hot vector using $\\mathrm{TopK}(\\log\\mathbf{w}+\\epsilon)$ . We choose a categorical prior with uniform distribution across $n$ classes. Thus the KL loss term is given by: ", "page_idx": 33}, {"type": "equation", "text": "$$\n-\\sum_{i=1}^{n}{\\frac{w_{i}}{Z}}\\log\\left(n{\\frac{w_{i}}{Z}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Appendix G Experiment Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "G.1 Model Architecture ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Denoising network. Our model architecture is extremely similar to VDM. The UNet of our pixelspace diffusion has an unchanged architecture from Kingma et al. [20].This structure is specifically designed for optimal performance in maximum likelihood training. We employ features from VDM such as the elimination of internal downsampling/upsampling processes and the integration of Fourier features to enhance fine-scale prediction accuracy. In alignment with the configurations suggested by Kingma et al. (2021), our approach varies depending on the dataset: For CIFAR-10, we employ a U-Net with a depth of 32 and 128 channels; for ImageNet-32, the U-Net also has a depth of 32, but the channel count is increased to 256. Additionally, all these models incorporate a dropout rate of 0.1 in their intermediate layers. ", "page_idx": 33}, {"type": "text", "text": "Encoder network. $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ is modeled using a sequence of 4 Resnet blocks with a channel count of 128 for CIFAR-10 and 256 for ImageNet-32 with a drop out of 0.1 in their intermediate layers. ", "page_idx": 33}, {"type": "text", "text": "Noise schedule. For polynomial noise schedule, we use an MLP that maps the latent vector ${\\bf z}$ to ${\\bf a}_{\\phi}({\\bf z}),{\\bf b}_{\\phi}({\\bf z}),{\\bf c}({\\bf z})$ ; see Eq. E.2 for details. The MLP has 2 hidden layers of size 3072 with swish activation function. The final layer is a linear mapping to $3\\times3072$ values corresponding to ${\\bf a}_{\\phi}({\\bf z}),{\\bf b}_{\\phi}({\\bf z}),{\\bf c}({\\bf z}).$ . Note that $\\mathbf{a}_{\\phi}(\\mathbf{z}),\\mathbf{b}_{\\phi}^{\\dagger}(\\mathbf{z}),\\mathbf{c}(\\mathbf{z})$ have the same dimensionality of 3072. ", "page_idx": 33}, {"type": "text", "text": "G.2 Hardware. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For the ImageNet experiments, we used a single GPU node with 8-A100s. For the cifar-10 experiments, the models were trained on 4 GPUs spanning several GPUs types like V100, A5000s, A6000s, and 3090s with float32 precision. ", "page_idx": 33}, {"type": "text", "text": "G.3 Hyperparameters ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We follow the same default training settings as Kingma et al. [20]. For all our experiments, we use the Adam [21] optimizer with learning rate $2\\times10^{-4}$ , exponential decay rates of $\\beta_{1}\\,=\\,0.9$ , $\\beta_{2}=0.99$ and decoupled weight decay [29] coefficient of 0.01. We also maintain an exponential moving average (EMA) of model parameters with an EMA rate of 0.9999 for evaluation. For other hyperparameters, we use fixed start and end times which satisfy $\\gamma_{\\mathrm{min}}=-13.3$ , $\\gamma_{\\mathrm{max}}=5.0$ , which is used in Kingma et al. [20], Zheng et al. [65]. ", "page_idx": 33}, {"type": "text", "text": "Appendix H Datasets and Visualizations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section we provide a brief description of the datasets used in the paper and visualize the generated samples and the noise schedules. ", "page_idx": 33}, {"type": "text", "text": "The CIFAR-10 dataset [25] is a collection of images consisting of $60,000\\;32\\times32$ color images in 10 different classes, with each class representing a distinct object or scene. The dataset is divided into 50,000 training images and 10,000 test images, with each class having an equal representation in both sets. The classes in CIFAR-10 include: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck. ", "page_idx": 34}, {"type": "text", "text": "Randomly generated samples for the CIFAR-10 datasaet are provided in Fig. 6a for MULAN and Fig. 6b for VDM. We visualize the noise schedule in Fig. 13. ", "page_idx": 34}, {"type": "image", "img_path": "loMa99A4p8/tmp/cba7dbb9e1c660b4470a3b1f698e2d583bb71769f7a8b8bdd3a42e7d2b17e9ac.jpg", "img_caption": ["8M training iterations. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 6: CIFAR-10 samples generated by different methods. ", "page_idx": 34}, {"type": "text", "text": "H.2 ImageNet-32 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "ImageNet-32 is a dataset derived from ImageNet [7], where the original images have been resized to a resolution of $32\\times32$ . This dataset comprises 1,281,167 training samples and 50,000 test samples, distributed across 1,000 labels. ", "page_idx": 34}, {"type": "text", "text": "Randomly generated samples for the ImageNet datasaet are provided in Fig. 7 for MULAN and Fig. 8 for VDM. We visualize the noise schedule in Fig. 13. ", "page_idx": 34}, {"type": "image", "img_path": "loMa99A4p8/tmp/8d6c3d65c1b87aa262c8db1fc57a89f5481d82b518dc7d84f5f5019836ff35f5.jpg", "img_caption": ["Figure 7: MULAN with noise parameterization after 2M training iterations. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "loMa99A4p8/tmp/a0f0ce4509a9c094ea7c52925b5fad9baddca0700b0c30d04458d6483910bb21.jpg", "img_caption": ["Figure 8: VDM after 2M training iterations. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "To see if MULAN learns different noise schedules for parts of the images with different frequencies, we modify the images in the CIFAR-10 dataset where we modify an image where we randomly remove the low frequency component an image or remove the high frequency with equal probability. Fig. 9a shows the training samples. MULAN was trained for 500K steps. The samples generated by MULAN is shown in Fig. 9b. The corresponding noise schedules is shown in Fig. 13. As compared to CIFAR-10, we notice that the spatial variation in the noise schedule increases (SNRs for all the pixels form a wider band) while the variance of the noise schedule across instances decreases slightly. ", "page_idx": 36}, {"type": "image", "img_path": "loMa99A4p8/tmp/3830fcbdbf085ef976e8ed7630aaeb19a0aac1ed6a1066ba1153b86ce29fa65d.jpg", "img_caption": ["Figure 9: Frequency Split CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "H.4 Frequency-2 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "To see if MULAN learns different noise schedules for images with different frequencies, we modify the images in the CIFAR-10 dataset where we modify an image where we randomly remove the low frequency component an image or remove the high frequency with equal probability. Fig. 9a shows the training samples. MULAN was trained for 500K steps. The samples generated by MULAN is shown in Fig. 9b. The corresponding noise schedules is shown in Fig. 13. As compared to CIFAR-10, we notice that the spatial variation in the noise schedule increases (SNRs for all the pixels form a wider band) and the variance of the noise schedule across instances increases as well. ", "page_idx": 37}, {"type": "image", "img_path": "loMa99A4p8/tmp/fe841dc0338a715d13f44b03a2b931d499c4bcd47a4d3e91c7bb955ab0424b9c.jpg", "img_caption": ["Figure 10: Frequency Split-2 CIFAR-10 dataset. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "H.5 CIFAR-10: Intensity ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "To see if MULAN learns different noise schedules for images with different intensities, we modify the images in the CIFAR-10 dataset where we randomly convert an image into a low intensity or a high intensity image with equal probability. Originally, the CIFAR10 images are in the range [0, 255]. To convert an image into a low intensity image we multiply all pixel values by 0.5. To convert an image into a high intensity image we multiply all the pixel values by 0.5 and add 127.5 to them. Fig. 11a shows the training samples. MULAN was trained for 500K steps. The samples generated by MULAN is shown in Fig. 11b. The corresponding noise schedules is shown in Fig. 13. As compared to CIFAR-10, we notice that the spatial variation in the noise schedule slightly increases (SNRs for all the pixels form a wider band) while the variance of the noise schedule across instances slightly decreases. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "image", "img_path": "loMa99A4p8/tmp/98a5adc53302d705969eb4430fae57d8f009d82151ba35d688835b7e1b5eb08d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 11: Intensity CIFAR-10 dataset. ", "page_idx": 38}, {"type": "text", "text": "We modify the CIFAR-10 dataset where we randomly mask (i.e. replace with 0s) the top of an image or the bottom half of an image with equal probability. Fig. 12a shows the training samples. MULAN was trained for 500K steps. The samples generated by MULAN is shown in Fig. 12b. The corresponding noise schedules is shown in Fig. 13. As compared to CIFAR-10, we notice that the spatial variation in the noise schedule slightly increases (SNRs for all the pixels form a wider band) while the variance of the noise schedule across instances decreases. ", "page_idx": 38}, {"type": "image", "img_path": "loMa99A4p8/tmp/ef9dd33790374e0a3a8b3855955ec59cf4ce4c6d59d9fbcf8b13c926e1864bbf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 12: Intensity CIFAR-10 dataset. ", "page_idx": 39}, {"type": "image", "img_path": "loMa99A4p8/tmp/89e403c6abd78b0114f54b4af79c1dc1521d85d4e33c0a1fb3791ea75b954b7c.jpg", "img_caption": ["Figure 13: signal-to-noise ratio for different datasets. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Appendix I Likelihood Estimation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We used both Variance Lower Bound (VLB) and ODE-based methods to compute BPD. ", "page_idx": 41}, {"type": "text", "text": "I.1 VLB Estimate ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In the VLB-based approach, we employ Eq. 9. To compute $\\mathcal{L}_{\\mathrm{diffusion}}$ , we use $T=128$ in Eq. 10, discretizing the timesteps, $t\\in[0,1]$ into 128 bins. ", "page_idx": 41}, {"type": "text", "text": "I.2 Exact likelihood computation using Probability Flow ODE ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "A diffusion process whose marginal is given by (the same as in Eq. 2), ", "page_idx": 41}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\alpha_{t}\\mathbf{x}_{0},\\mathrm{diag}(\\pmb{\\sigma}_{t}^{2})),\\ \\mathbf{x}_{0}\\sim q_{0}(\\mathbf{x}_{0}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "can be modeled as the solution to an It\u02c6o Stochastic Differential Equation (SDE): ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\mathbf{f}(t)\\mathbf{x}_{t}\\mathrm{d}t+\\mathbf{g}(t)\\mathrm{d}\\mathbf{w}_{t},\\ \\mathbf{x}_{0}\\sim q_{0}(\\mathbf{x}_{0}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\mathbf{f}\\left(t\\right)\\in\\mathbb{R}^{d},\\mathbf{g}(t)\\in\\mathbb{R}^{d}$ take the following expressions [53]: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{f}(t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\log\\alpha_{t},}\\\\ {\\displaystyle\\mathbf{g}^{2}(t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\pmb{\\sigma}_{t}^{2}-2\\pmb{\\sigma}_{t}^{2}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\log\\alpha_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The corresponding reverse process, Eq. 4, can also be modelled by an equivalent reverse-time SDE: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\mathrm{d}\\mathbf{x}_{t}=[\\mathbf{f}(t)-\\mathbf{g}(t)^{2}\\nabla_{\\mathbf{x}_{t}}\\log q(\\mathbf{x}_{t}|\\mathbf{x}_{0})]\\mathrm{d}t+\\mathbf{g}(t)\\mathrm{d}\\bar{\\mathbf{w}}_{t},\\ \\mathbf{x}_{1}\\sim p_{\\theta}(\\mathbf{x}_{1}),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\bar{\\bf w}$ is a standard Wiener process when time flows backwards from $1~\\rightarrow~0$ , and $\\mathrm{d}t$ is an infinitesimal negative timestep. Song et al. [53] show that the marginals of Eq. 75 can be described by the following Ordinary Differential Equation (ODE) in the reverse process: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\left[\\mathbf{f}(t)\\mathbf{x}_{t}-\\frac{1}{2}\\mathbf{g}^{2}(t)\\nabla_{\\mathbf{x}_{t}}\\log q(\\mathbf{x}_{t})\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This ODE, also called the probablity flow ODE, allows us to compute the exact likelihood on any input data via the instantaneous change of variables formula as proposed in Chen et al. [2]. Note that during the reverse process, the term $q(\\mathbf{x}_{t})$ is unknown and is approximated by parameterized by $p_{\\theta}(\\mathbf{x}_{t})$ . For the probability flow defined in Eq. 76, Chen et al. [2] show that the log- likelihood of $p_{\\theta}(\\mathbf{x}_{0})$ can be computed using the following equation: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\log p_{\\theta}(\\mathbf{x}_{0})=\\log p_{\\theta}(\\mathbf{x}_{1})-\\int_{t=0}^{t=1}\\mathrm{tr}\\left(\\nabla_{\\mathbf{x}_{t}}\\mathbf{h}_{\\theta}(\\mathbf{x}_{t},t)\\right)\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "I.2.1 Probability Flow ODE for MULAN. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Similarly for the forward process conditioned on the auxiliary latent variable, ${\\bf z}$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\nq_{\\phi}(\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{z})=\\mathcal{N}(\\mathbf{x}_{t};\\alpha_{t}(\\mathbf{z})\\mathbf{x}_{0},\\mathrm{diag}(\\sigma_{t}^{2}(\\mathbf{z}))),\\ \\mathbf{x}_{0}\\sim q_{0}(\\mathbf{x}_{0}),\\ \\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we can extend Eq. 74 in the following manner, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\mathbf{f}(\\mathbf{z},t)\\mathbf{x}_{t}\\mathrm{d}t+\\mathbf{g}(\\mathbf{z},t)\\mathrm{d}\\mathbf{w}_{t},\\ \\mathbf{x}_{0}\\sim q_{0}(\\mathbf{x}_{0}),\\ \\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "to obtain the corresponding SDE formulation. Notice that the random variable $\\mathbf{z}$ in the above equation doesn\u2019t have a subscript $t$ , and hence, $\\mathbf{z}$ is drawn from $q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})$ once and the same $\\mathbf{z}$ is used as $\\mathbf{x}_{\\mathrm{0}}$ diffuses to $\\mathbf{x}_{1}$ . The expressions for $\\mathbf{f}(\\mathbf{z},t):\\mathbb{R}^{m}\\times[0,1]\\to\\mathbb{R}^{d}$ , $\\mathbf{g}(\\mathbf{z},t):\\mathbb{R}^{m}\\times[0,1]\\rightarrow\\mathbb{R}^{d}$ is given as follows: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{f}\\left(\\mathbf{z},t\\right)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\log\\alpha_{t}(\\mathbf{z}),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{g}^{2}(\\mathbf{z},t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\pmb{\\sigma}_{t}^{2}(\\mathbf{z})-2\\pmb{\\sigma}_{t}^{2}(\\mathbf{z})\\frac{\\mathrm{d}}{\\mathrm{d}t}\\log\\pmb{\\alpha}_{t}(\\mathbf{z})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Recall that $\\pmb{\\alpha}_{t}^{2}(\\mathbf{z})\\,=\\,\\mathrm{sigmoid}(-\\gamma_{\\phi}(\\mathbf{z},t))$ , $\\pmb{\\sigma}_{t}^{2}(\\mathbf{z})\\,=\\,\\mathrm{sigmoid}(\\gamma_{\\phi}(\\mathbf{z},t))$ . Substituting these in the above equations, the expressions for f ${\\bf\\Xi}({\\bf z},t)$ and ${\\bf g}^{2}({\\bf z},t)$ simplify to the following: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\bf{f}}({\\bf{z}},t)=-\\displaystyle\\frac{1}{2}\\mathrm{sigmoid}\\big(\\gamma_{\\phi}({\\bf{z}},t)\\big)\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\gamma_{\\phi}({\\bf{z}},t),}}\\\\ {{{\\bf{g}}^{2}({\\bf{z}},t)=\\mathrm{sigmoid}\\big(\\gamma_{\\phi}({\\bf{z}},t)\\big)\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\gamma_{\\phi}({\\bf{z}},t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The corresponding reverse-time SDE is given as: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{x}_{t}=[\\mathbf{f}(t)-\\mathbf{g}(t)^{2}\\nabla_{\\mathbf{x}_{t}}\\log q_{\\phi}(\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{z})]\\mathrm{d}t+\\mathbf{g}(t)\\mathrm{d}\\bar{\\mathbf{w}}_{t},\\ \\mathbf{x}_{1}\\sim p_{\\theta}(\\mathbf{x}_{1}),\\ \\mathbf{z}\\sim p_{\\theta}(\\mathbf{z}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\bar{\\bf w}$ is a standard Wiener process when time flows backwards from $1~\\rightarrow~0$ , and $\\mathrm{d}t$ is an infinitesimal negative timestep. Given, $\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z})$ , an approximation to the true score function, $\\nabla_{\\mathbf{x}_{t}}\\log q_{\\phi}(\\mathbf{x}_{t}|\\mathbf{x}_{0},\\mathbf{z})$ , Song et al. [53] show that the marginals of Eq. 80 can be described by the following Ordinary Differential Equation (ODE): ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\left[\\mathbf{f}(\\mathbf{z},t)-\\frac{1}{2}\\mathbf{g}^{2}(\\mathbf{z},t)\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z})\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Zheng et al. [65] show that the score function, $\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z})$ , for the noise and the $\\mathbf{V}$ -parameterization is given as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\ni_{\\theta}(\\mathbf{x}_{t},\\mathbf{z})=\\left\\{\\begin{array}{l l}{\\displaystyle-\\frac{\\epsilon_{\\theta}\\big(\\mathbf{x}_{t},t\\big)}{\\sigma_{t}(\\mathbf{z})}}&{\\mathrm{Noise~parameterization;see~Sec.~E.1.1~(82)}}\\\\ {\\displaystyle-\\mathbf{x}_{t}-\\exp\\left(-\\frac{1}{2}\\gamma_{\\phi}(\\mathbf{z},t)\\right)\\mathbf{v}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}&{\\mathrm{v}\\mathrm{-parameterization;see~Sec.~E.1.2~}}&{\\mathrm{(821)}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Applying the change of variables formula [2] on Eq. 81, $\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{z})$ can be computed in the following manner: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{z})=\\log p_{\\theta}(\\mathbf{x}_{1})-\\displaystyle\\int_{t=0}^{t=1}\\mathrm{tr}\\left(\\nabla_{\\mathbf{x}_{t}}\\mathbf{h}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)\\right)\\mathrm{d}t,}\\\\ {\\mathrm{where}\\,\\mathbf{h}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)\\equiv\\mathbf{f}(\\mathbf{z},t)-\\displaystyle\\frac{1}{2}\\mathbf{g}^{2}(\\mathbf{z},t)\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The expression for log-likelihood (Eq. 8) is as follows, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p_{\\theta}(\\mathbf{x}_{0})\\geq\\underline{{\\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})}[\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{z})]}}-\\mathrm{D}_{\\mathrm{KL}}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})||p_{\\theta}(\\mathbf{z}))}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{Using~Eq.~83,}}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})}\\left[\\log p_{\\theta}(\\mathbf{x}_{1})-\\int_{t=0}^{t=1}\\mathrm{tr}\\left(\\nabla_{\\mathbf{x}_{t}}\\mathbf{h}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{z})\\right)\\mathrm{d}t\\right]-\\mathrm{D}_{\\mathrm{KL}}(q_{\\phi}(\\mathbf{z}|\\mathbf{x}_{0})||p_{\\theta}(\\mathbf{z}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Computing tr $\\big(\\nabla_{\\mathbf{x}_{t}}\\mathbf{h}_{\\theta}\\big(\\mathbf{x}_{t},t,\\mathbf{z}\\big)\\big)$ is expensive and we follow Chen et al. [2], Zheng et al. [65], Grathwohl et al. [15] to estimate it with Skilling-Hutchinson trace estimator [50, 19]. In particular, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{tr}\\left(\\nabla_{\\mathbf{x}_{t}}{\\mathbf{h}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{z})\\right)=\\mathbb{E}_{p(\\epsilon)}\\left[\\epsilon^{\\top}\\nabla_{\\mathbf{x}_{t}}{\\mathbf{h}}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{z})\\epsilon\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the random variable $\\epsilon$ satisfies $\\mathbb{E}_{p(\\epsilon)}[\\epsilon]=\\mathbf{0}$ and ${\\bf C}\\mathrm{ov}_{p(\\epsilon)}[\\epsilon]={\\bf I}$ . Common choices for $p(\\epsilon)$ include Rademacher or Gaussian distributions. Notably, the term $\\nabla_{{\\mathbf{x}}_{t}}\\mathbf{h}_{\\theta}({\\mathbf{x}}_{t},t,{\\mathbf{z}})\\epsilon$ can be computed efficiently using \u201cJacobian-vector-product\u201d computation in JAX. In our experiments, we follow the exact evaluation procedure for computing likelihood as outlined in Song et al. [53], Grathwohl et al. [15]. Specifically, for the computation of Eq. 85, we employ a Rademacher distribution for $p(\\epsilon)$ . To calculate the integral in Eq. 84, we utilize the RK45 ODE solver [11] provided by scipy.integrate.solve_ivp with atol $_{=1\\ominus-5}$ and $\\mathtt{r t o l=1e-5}$ . ", "page_idx": 42}, {"type": "text", "text": "I.2.2 Dequantization. ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Real-world datasets for images or texts often consist of discrete data. Attempting to learn a continuous density model directly on these discrete data points can lead to degenerate outcomes [56] and fail to provide meaningful density estimations. Dequantization [46, 16, 65] is a common solution in such cases. To elaborate, let $x_{0}$ represent 8-bit discrete data scaled to [-1, 1]. Dequantization methods assume that we have trained a continuous model distribution $p_{\\theta}$ for $x_{0}$ , and define the discrete model distribution by ", "page_idx": 43}, {"type": "equation", "text": "$$\nP_{\\theta}(\\mathbf{x}_{0})=\\int_{[-\\frac{1}{256},\\frac{1}{256})^{d}}p_{\\theta}(\\mathbf{x}_{0}+u)\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "To train $P_{\\theta}(\\mathbf{x}_{0})$ by maximum likelihood estimation, variational dequantization [16, 65] introduces a dequantization distribution $q(u|\\mathbf{x}_{0})$ and jointly train $p_{\\mathrm{model}}$ and $q(u|\\mathbf{x}_{0})$ by a variational lower bound: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log P_{\\theta}(\\mathbf{x}_{0})\\ge\\mathbb{E}_{q(u|\\mathbf{x}_{0})}[p_{\\theta}(\\mathbf{x}_{0}+u)-\\log q(u|\\mathbf{x}_{0})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Truncated Normal Dequantization. Zheng et al. [65] show that truncated Normal distribution, ", "page_idx": 43}, {"type": "equation", "text": "$$\nq(u|\\mathbf{x}_{0})=\\mathcal{T N}\\left(\\mathbf{0},\\mathbf{I},-\\frac{1}{256},\\frac{1}{256}\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with mean 0, covariance $\\mathbf{I}$ , and bounds $\\left[-{\\frac{1}{256}},{\\frac{1}{256}}\\right]_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ along each dimension, leads to a better likelihood estimate. Thus, Eq. 86 simplifies to the following (for details please refer to section A. in Zheng et al. [65]): ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log P_{\\theta}(\\mathbf{x}_{0})\\geq\\!\\!\\mathbb{E}_{\\hat{\\epsilon}\\sim T\\mathcal{N}(\\mathbf{0},\\mathbf{I},-\\tau,\\tau)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}_{0}+\\frac{\\sigma_{\\epsilon}}{\\alpha_{\\epsilon}}\\hat{\\epsilon}\\right)\\right]+\\frac{d}{2}(1+\\log(2\\pi\\sigma_{\\epsilon}^{2}))-0.01522\\times d}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times\\mathrm{ith}\\ \\frac{\\sigma_{\\epsilon}}{\\alpha_{\\epsilon}}=\\exp(-\\frac{1}{2}\\times13.3),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\sigma_{\\epsilon}=\\mathrm{sqrt}(\\mathrm{sigmoid}(-13.3)),\\mathrm{~and~}\\tau=3.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Importance Weighted Estimator. Eq. 87 can also be extended to obtain an importance weighted likelihood estimator to get a tighter bound on the likelihood. The variational bound is given by (for details please refer to section A. in Zheng et al. [65]): ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\ge\\!\\mathbb{E}_{\\ell^{(1)},\\dots,\\hat{\\epsilon}^{(K)}\\sim\\mathcal{T N}(\\mathbf{0},\\mathbf{I},-\\tau,\\tau)}\\left[\\log\\left(\\frac{1}{K}\\displaystyle\\sum_{i=1}^{K}\\frac{p_{\\theta}\\left(\\mathbf{x}_{0}+\\frac{\\sigma_{\\epsilon}}{\\alpha_{\\epsilon}}\\hat{\\epsilon}^{(k)}\\right)}{q\\left(\\hat{\\epsilon}^{(i)}\\right)}\\right)\\right]+d\\log\\sigma_{\\epsilon}}\\\\ &{\\quad\\mathrm{with}\\ \\frac{\\sigma_{\\epsilon}}{\\alpha_{\\epsilon}}=\\exp(-\\frac{1}{2}\\times13.3),\\log\\sigma_{\\epsilon}=\\frac{1}{2}(-13.3+\\mathrm{softplus}(-13.3)),}\\\\ &{q(\\hat{\\epsilon})=\\frac{1}{(2\\pi Z)^{2}}\\mathrm{exp}\\left(-\\frac{1}{2}\\|\\hat{\\epsilon}\\|_{2}^{2}\\right),Z=0.9974613,\\mathrm{and}\\ \\tau=3.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note that for $K=1$ , Eq. 88 is equivalent to Eq. 87; see Zheng et al. [65]. $\\begin{array}{r}{\\log p_{\\theta}\\left(\\mathbf{x}_{0}+\\frac{\\sigma_{\\epsilon}}{\\alpha_{\\epsilon}}\\hat{\\epsilon}\\right)}\\end{array}$ is evaluated using Eq. 84. In Table 8, we report BPD values for MULAN on CIFAR10 (8M training steps, v-parameterization) and ImageNet (2M training steps, noise parameterization) using both the VLB-based approach, and the ODE-based approach with $K=1$ and $K=20$ importance samples. ", "page_idx": 43}, {"type": "text", "text": "Table 8: NLL (mean and $95\\%$ Confidence Interval for MULAN) on CIFAR10 (8M training steps, v-parameterization) and ImageNet (2M training steps, noise parameterization) using both the VLBbased approach, and the ODE-based approach. $K=1$ means that we do not use importance weighted estimator since Eq. 88 is equivalent to Eq. 87 for this case; see Zheng et al. [65]. ", "page_idx": 44}, {"type": "table", "img_path": "loMa99A4p8/tmp/de8e3867cfac2a0a1ce142664245c303c298b1f97231f6ff5c7349d7cb4adabc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: See our introduction for a list of claims including getting SOTA results on density estimation. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: Yes, our model does not get state of the art FID due to it not having a lower frequency bias. See the paper for more details. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: Please see our detailed proofs. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: Not only do we show all equations and train on standard datasets, we will open source the code. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [No] ", "page_idx": 46}, {"type": "text", "text": "Justification: We will open source after paper acceptance. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Yes, we include all hyperparameters in the paper and will open source code. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: we report the deviatations for BPD in Table 2. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide this in the paper. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our paper is just a diffusion model useful for compression. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: We do not believe our method will have a high risk of abuse as our models are not perceptually SOTA, they only provide for state of the art logliklihood. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: We are using standard benchmark datasets. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 48}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: No crowdsource or research with human subjets ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]