[{"figure_path": "abuQMKDVkW/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Advantages of SAR image: independent of weather conditions, sunlight and land cover. (b) Number of papers (thousands) retrieved from Google Scholar using keywords \"SAR Detection\".", "description": "The figure demonstrates the benefits of SAR imagery in comparison with traditional optical images.  Panel (a) shows examples of RGB and SAR images taken under various conditions (night, cloud, snow, land cover).  The SAR images maintain clarity and detail regardless of the weather or lighting. Panel (b) displays a bar graph showing a significant increase in the number of research papers published on SAR detection between 2014 and 2023, indicating growing interest in this area of research.", "section": "1 Introduction"}, {"figure_path": "abuQMKDVkW/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of the significant domain gap exists between Nature RGB dataset and remote sensing SAR datasets. (a) showcases the WST feature space significantly narrows the domain gap. (b) demonstrates that the remote sensing RGB dataset serves as an effective domain transit bridge, facilitating smoother domain transfer.", "description": "This figure illustrates the domain gap between natural RGB images and SAR images and how the proposed method bridges this gap.  The top half shows a large difference in pixel-level distributions between RGB and SAR data. The bottom half demonstrates that using Wavelet Scattering Transform (WST) features reduces this gap. The figure also depicts a two-stage pretraining strategy where optical remote sensing data acts as an intermediary bridge between natural RGB and SAR datasets.", "section": "4 Multi-Stage with Filter Augmentation Pretraining Framework"}, {"figure_path": "abuQMKDVkW/figures/figures_5_1.jpg", "caption": "Figure 3: Conceptual illustration of traditional ImageNet pretrain and our proposed Multi-Stage with Filter Augmentation (MSFA) pretrain framework.", "description": "This figure illustrates the difference between the traditional ImageNet pretraining approach and the proposed Multi-Stage with Filter Augmentation (MSFA) pretraining framework. In the traditional approach, a backbone network is pretrained on ImageNet and then fine-tuned on a SAR dataset for object detection. This process suffers from a domain gap and a model gap. The MSFA framework aims to address these issues by introducing a multi-stage pretraining process: 1) filter augmentation and pretraining on ImageNet, 2)  detection model pretraining on an optical remote sensing dataset (acting as a bridge to SAR data), and 3) finetuning the entire detection model on the SAR dataset. This approach helps to reduce the domain gap and model gap, leading to improved performance in SAR object detection.", "section": "4 Multi-Stage with Filter Augmentation Pretraining Framework"}, {"figure_path": "abuQMKDVkW/figures/figures_8_1.jpg", "caption": "Figure 4: Generalization of MSFA on different detection frameworks (a) and different backbones (b). Models are finetuned and tested on SARDet-100K dataset. INP: Traditional ImageNet Pretrain on backbone network only.", "description": "This figure demonstrates the generalizability of the proposed Multi-Stage with Filter Augmentation (MSFA) pretraining framework.  Subfigure (a) is a radar chart showing the performance of MSFA across various detection frameworks (Faster R-CNN, Cascade R-CNN, RetinaNet, FCOS, GFL, DETR, Deformable DETR, Sparse R-CNN, and Grid R-CNN), highlighting its consistent improvement over traditional ImageNet pretraining (INP). Subfigure (b) is a line graph illustrating the performance of MSFA with different backbones (ResNet, ConvNext, VAN, Swin Transformer) across varying model sizes (parameter counts), further showcasing the consistent performance gains of MSFA regardless of the chosen backbone architecture.", "section": "5 Experiments and Analysis"}, {"figure_path": "abuQMKDVkW/figures/figures_15_1.jpg", "caption": "Figure S5: Visualization of sample images from the proposed SARDet-100K dataset.", "description": "This figure visualizes sample images from the SARDet-100K dataset. It shows representative samples for each of the six object categories: Ship, Tank, Bridge, Harbor, Aircraft, and Car.  The images are displayed in a grid format, allowing for a visual comparison of the different object types and their appearances within the dataset. The color-coded bounding boxes highlight the instances of each category.", "section": "A.1 SARDet-100K Dataset Visualization"}, {"figure_path": "abuQMKDVkW/figures/figures_16_1.jpg", "caption": "Figure S6: (a) SARDet-100K dataset standardization process, encompassing set splitting, large image slicing, and label annotation format unification. (b) Percentage of instances for each category and average instance area (in pixels) in SARDet-100K.", "description": "This figure illustrates the SARDet-100K dataset standardization process.  Panel (a) shows the steps involved in dataset preparation, including splitting the dataset into training, validation, and testing sets; slicing large images into smaller patches; and unifying label annotations into a consistent format (COCO).  Panel (b) displays the proportion of instances for each of the six object categories (Ship, Tank, Bridge, Harbor, Aircraft, and Car) within the dataset, along with the average instance area in pixels for each category, indicating the dataset's diversity and scale.", "section": "A.2 Standardization"}, {"figure_path": "abuQMKDVkW/figures/figures_17_1.jpg", "caption": "Figure S7: Visualization of handcrafted features on SAR images. (To facilitate visualization, the features are average pooled and represented as a single channel.", "description": "This figure visualizes the results of applying six different handcrafted feature extraction methods (HOG, Canny, GRE, Haar, and WST) to two sample SAR images.  Each method highlights different aspects of the image, showcasing their unique strengths and the variety of information that can be extracted from SAR data using these traditional techniques. The average pooling helps in easier comparison between features obtained from different methods.", "section": "A.3 Handcrafted Feature Descriptors"}, {"figure_path": "abuQMKDVkW/figures/figures_18_1.jpg", "caption": "Figure 3: Conceptual illustration of traditional ImageNet pretrain and our proposed Multi-Stage with Filter Augmentation (MSFA) pretrain framework.", "description": "This figure illustrates the difference between the traditional ImageNet pretraining approach and the proposed Multi-Stage with Filter Augmentation (MSFA) pretraining framework. The traditional approach involves pretraining a backbone network on ImageNet and then finetuning the entire detection model on a SAR dataset, leading to domain and model gaps. In contrast, the MSFA framework utilizes a multi-stage pretraining strategy. In the first stage, it uses a filter augmentation approach to modify the input data, addressing the data domain gap. In the second stage, it uses an optical remote sensing detection dataset as a bridge, further reducing the domain gap and improving the model migration. The MSFA approach demonstrates the improved efficacy in bridging domain and model gaps, leading to enhanced performance in SAR object detection.", "section": "4 Multi-Stage with Filter Augmentation Pretraining Framework"}, {"figure_path": "abuQMKDVkW/figures/figures_21_1.jpg", "caption": "Figure S9: Evaluate different backbone models on SARDet-100K dataset and other previous popular benchmarks (SSDD [84] and HRSID [71]). Backbones are plugged under Faster-RCNN [53] detection framework.", "description": "This figure compares the performance of different backbone models (ResNet, VAN, Swin) on three different SAR object detection datasets: SARDet-100K, SSDD, and HRSID.  The x-axis represents the number of model parameters (in millions), and the y-axis represents the mean average precision (mAP) at IoU threshold of 0.5.  The results show how the model performance changes with the model size and the dataset used. Each backbone is evaluated in conjunction with the Faster-RCNN detection framework. The plot enables readers to compare the relative performance across models and datasets; also observe the saturation effects on smaller datasets, SSDD and HRSID.", "section": "A.5 Comparison of SARDet-100K with other datasets"}, {"figure_path": "abuQMKDVkW/figures/figures_23_1.jpg", "caption": "Figure S10: MSFA better than traditional ImageNet backbone pretrain in (a) missing detection, (b) false detection and (c) inaccurate localization", "description": "This figure demonstrates the improved performance of the proposed MSFA pretraining framework compared to the traditional ImageNet backbone pretraining method.  It showcases three scenarios: (a) missing detection, where MSFA correctly identifies objects missed by the ImageNet method; (b) false detection, where MSFA reduces false positive detections compared to ImageNet; and (c) inaccurate localization, where MSFA shows more precise bounding boxes around detected objects than ImageNet. Each scenario provides visual comparisons between Ground Truth (GT), ImageNet (INP), and MSFA, highlighting MSFA's superior performance in various aspects of SAR object detection.", "section": "A.7 Detection Result Visualizations"}, {"figure_path": "abuQMKDVkW/figures/figures_24_1.jpg", "caption": "Figure S10: MSFA better than traditional ImageNet backbone pretrain in (a) missing detection, (b) false detection and (c) inaccurate localization", "description": "This figure demonstrates the improved performance of the proposed Multi-Stage with Filter Augmentation (MSFA) pretraining framework compared to traditional ImageNet backbone pretraining.  It shows three examples highlighting MSFA's advantages: (a) fewer missed detections, (b) fewer false detections, and (c) more accurate localization.  The ground truth (GT) bounding boxes are compared against the results using ImageNet pretraining (INP) and MSFA. The visualization highlights instances where MSFA either correctly detects objects missed by INP, avoids false positive detections made by INP, or provides significantly better localization.", "section": "A.7 Detection Result Visualizations"}]