[{"figure_path": "dheDf5EpBT/tables/tables_3_1.jpg", "caption": "Table 1: Comparison of approximate MU methods. We decompose the steepest descent direction into three parts: the weight saliency matrix (S), the forgetting part (F), and the remaining part (R) as in (3) and (4). Only SA and our method consider the remain-preserving manifold, and we further approximate up-to-date Hessian.", "description": "This table compares various approximate machine unlearning (MU) methods, analyzing their components (weight saliency matrix, forgetting part, remaining part) and the manifold metric used. It highlights that only Selective Amnesia (SA) and the proposed SFR-on method consider the remain-preserving manifold, with SFR-on further approximating the up-to-date Hessian.", "section": "3 Approximate MU from Perspective of Steepest Descent"}, {"figure_path": "dheDf5EpBT/tables/tables_6_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table summarizes the performance of several machine unlearning methods on two image classification tasks: CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T.  It compares the proposed SFR-on method against six baselines and the retraining (RT) approach, measuring forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), average disparity (Avg.D) from RT, Kullback-Leibler divergence (DKL) from RT, and runtime efficiency (RTE).  The table shows that SFR-on achieves the best performance.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_7_1.jpg", "caption": "Table 3: Class-wise forgetting performance on CIFAR10 with DDPM and ImageNet with DiT. The best unlearning performance for each forgetting class is highlighted in bold for FA and FID.", "description": "This table presents the results of class-wise forgetting experiments on CIFAR-10 using a Denoising Diffusion Probabilistic Model (DDPM) and on ImageNet using a Diffusion Transformer (DiT).  For each forgetting class, the table shows the Forgetting Accuracy (FA) and Fr\u00e9chet Inception Distance (FID) metrics.  The best performing method (lowest FA and FID) is highlighted in bold for each class. The number of steps needed for unlearning is also shown. This helps illustrate the performance of different methods in effectively removing the influence of specific classes from the model while maintaining the overall quality of generated images for other classes.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_8_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table presents a performance comparison of several machine unlearning (MU) methods on two image classification datasets: CIFAR-10 and TinyImageNet.  The methods are evaluated using various metrics, including forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), KL divergence to the retrained model (DKL), and runtime efficiency (RTE).  The table highlights the effectiveness and efficiency of the proposed SFR-on method by showing its superior performance compared to existing methods.  Ablation studies are also included, demonstrating the importance of the various components of the SFR-on method.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_22_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table summarizes the performance of various machine unlearning methods on two image classification tasks.  It compares the performance of the proposed SFR-on method against several baselines (including retraining, FT, GA, RL, SalUn, BT, SCRUB) across key metrics: forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), KL divergence to Retraining (DKL), and runtime efficiency (RTE).  The table uses CIFAR-10 with ResNet-18 and TinyImageNet with Swin-T datasets, evaluating the unlearning of 10% of randomly selected samples.  Performance discrepancies from the retraining model are highlighted to show the effectiveness of each method.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_22_2.jpg", "caption": "Table 3: Class-wise forgetting performance on CIFAR10 with DDPM and ImageNet with DiT. The best unlearning performance for each forgetting class is highlighted in bold for FA and FID.", "description": "This table presents the results of class-wise forgetting experiments conducted on CIFAR-10 using a diffusion probabilistic model (DDPM) and on ImageNet using a diffusion transformer (DiT).  The table shows the forgetting accuracy (FA) and Fr\u00e9chet Inception Distance (FID) for several methods, highlighting the best performance in bold for each forgetting class. The results illustrate the effectiveness of different methods in removing specific classes from the models while maintaining the overall image quality.", "section": "Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_24_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table summarizes the performance of several machine unlearning (MU) methods on two image classification datasets: CIFAR-10 and TinyImageNet.  It compares the performance of these methods to a retrained model (RT), considered the gold standard.  Metrics include forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), average disparity (Avg.D) from RT, Kullback-Leibler divergence (DKL) to RT, and runtime efficiency (RTE).  The table highlights the effectiveness of the proposed method (SFR-on) by showing its performance is closer to the RT gold standard than other methods.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_24_2.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table summarizes the performance of several machine unlearning (MU) methods on two image classification datasets: CIFAR-10 and TinyImageNet.  It compares the performance of the proposed method (SFR-on) against several baseline methods, including retraining (RT).  Metrics used include forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), average disparity from RT (Avg.D), Kullback-Leibler divergence to RT (DKL), and runtime efficiency (RTE).  The table shows that SFR-on achieves results closest to RT, indicating more effective unlearning.", "section": "Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_25_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table compares the performance of several machine unlearning (MU) methods on two image classification tasks.  It shows the forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), average disparity from the retraining model (Avg.D), Kullback-Leibler divergence to retraining (DKL), and runtime efficiency (RTE) for each method.  The table includes results for retraining (RT) as a baseline and highlights the performance of the proposed SFR-on method in relation to existing techniques.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_25_2.jpg", "caption": "Table 3: Class-wise forgetting performance on CIFAR10 with DDPM and ImageNet with DiT. The best unlearning performance for each forgetting class is highlighted in bold for FA and FID.", "description": "This table presents the results of class-wise forgetting experiments conducted on two different datasets: CIFAR-10 using a diffusion probabilistic model (DDPM) and ImageNet using a diffusion transformer (DiT).  For each dataset and model, several methods were used to perform unlearning of specific classes. The table shows the forgetting accuracy (FA) and Fr\u00e9chet Inception Distance (FID) for each method and each class. The best performing methods for each class are highlighted in bold for both FA and FID metrics.  This allows for a comparison of various unlearning methods in terms of their effectiveness in forgetting specific classes while maintaining performance on other classes.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_26_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table compares the performance of several machine unlearning (MU) methods on two image classification datasets: CIFAR-10 and TinyImageNet.  It shows the forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack success rate (MIA), average disparity (Avg.D) from the Retrained model (RT), Kullback-Leibler divergence (DKL) to RT, and run-time efficiency (RTE) for each method. The table helps assess the effectiveness and efficiency of different MU methods.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_26_2.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table summarizes the performance of different machine unlearning methods on two image classification datasets: CIFAR-10 and TinyImageNet.  It compares several baselines, including fine-tuning (FT), gradient ascent (GA), random labeling (RL), saliency-based unlearning (SalUn), bad teacher (BT), and SCRUB, against the proposed method (SFR-on) and an ablation study of its components.  Key metrics include forgetting accuracy (FA), remaining accuracy (RA), test accuracy (TA), membership inference attack success rate (MIA), Kullback-Leibler divergence (DKL) to the retrained model (RT), and runtime efficiency (RTE).  The table highlights how close each method's performance comes to that of the ideal RT model and shows the runtime efficiency of each approach.", "section": "5 Experiments"}, {"figure_path": "dheDf5EpBT/tables/tables_29_1.jpg", "caption": "Table 2: Performance summary of MU methods for image classification (including RT, six baselines, our proposed SFR-on, and ablations on our designed components), assessing unlearning 10% random subset of CIFAR-10 using ResNet-18 and TinyImageNet using Swin-T. All results are presented as mean and standard deviation across 10 independent trials. Performance discrepancies from RT are indicated with (\u26ab), highlighting that more effective unlearning is reflected by performance closer to RT. The 'Averaging Disparity' (Avg.D) metric is calculated by the average of the gaps measured in accuracy-related metrics, including FA, RA, TA, and MIA. DKL denotes the KL divergence to RT. RTE is recorded in minutes.", "description": "This table summarizes the performance of several machine unlearning (MU) methods on two image classification tasks.  It compares the proposed SFR-on method against six baseline methods and the retraining approach (RT) which serves as the gold standard.  Metrics include forgetting accuracy (FA), remaining accuracy (RA), testing accuracy (TA), membership inference attack (MIA), average disparity from RT, KL divergence from RT, and runtime efficiency (RTE).  Higher FA, RA, and TA are better while lower MIA, Avg.D, and DKL are better. RTE refers to the runtime in minutes.", "section": "5 Experiments"}]