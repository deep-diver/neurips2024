[{"type": "text", "text": "Fine-Tuning Out-of-Vocabulary Item Recommendation with User Sequence Imagination ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruochen Liu1, Hao Chen2, Yuanchen Bei3, Qijie Shen4, Fangwei Zhong5, Senzhang Wang1\u2217, Jianxin Wang1 ", "page_idx": 0}, {"type": "text", "text": "1Central South University, 2City University of Macau, 3Zhejiang University, 4Alibaba Group, 5Beijing Normal University {ruochen, szwang\u2217, jxwang}@csu.edu.cn, sundaychenhao@gmail.com yuanchenbei@zju.edu.cn, qjshenxdu@gmail.com, fangweizhong@bnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recommending out-of-vocabulary (OOV) items is a challenging problem since the in-vocabulary (IV) items have well-trained behavioral embeddings but the OOV items only have content features. Current OOV recommendation models often generate \u2018makeshift\u2019 embeddings for OOV items from content features and then jointly recommend with the \u2018makeshift\u2019 OOV item embeddings and the behavioral IV item embeddings. However, merely using the \u2018makeshift\u2019 embedding will result in suboptimal recommendation performance due to the substantial gap between the content feature and the behavioral embeddings. To bridge the gap, we propose a novel User Sequence IMagination (USIM) fine-tuning framework, which first imagines the user sequences and then refines the generated OOV embeddings with the user behavioral embeddings. Specifically, we frame the user sequence imagination as a reinforcement learning problem and develop a recommendationfocused reward function to evaluate to what extent a user can help recommend the OOV items. Besides, we propose an embedding-driven transition function to model the embedding transition after imaging a user. USIM has been deployed on a prominent e-commerce platform for months, offering recommendations for millions of OOV items and billions of users. Extensive experiments demonstrate that USIM outperforms traditional generative models in OOV item recommendation performance across traditional collaborative flitering and GNN-based collaborative filtering models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recommendation systems, such as collaborative flitering models, learn behavioral embeddings from historical interactions to represent the behavioral characteristics of billions of users and items [1\u20134]. For instance, the embeddings of interacted user-item pairs have higher inner products, whereas those of un-interacted pairs have lower inner products. However, besides the items with user interactions, thousands of out-of-vocabulary (OOV) items\u2014such as short videos, photos, and posts\u2014are generated or uploaded every second. In the age of AGI, the generation speed of AI-made OOV content, including text, images, and videos, will far exceed the speed of human consumption. To avoid being overwhelmed by the OOV content, it is essential to replicate how humans handle these items, recommending them without disrupting in-vocabulary (IV) items. ", "page_idx": 0}, {"type": "text", "text": "Traditional OOV recommendation models usually generate \u2018makeshift\u2019 embeddings from the content features and then use them to recommend OOV items. The research can be classified into two categories. (a) Generative models aim to generate realistic embeddings. GAR [5] uses a generative adversarial structure to ensure the embedding distribution of generated OOV embeddings is similar to IV embeddings. ALDI [6] distills knowledge from IV items to OOV items. (b) Dropout models increase the robustness of recommender systems. Dropout [7] randomly substitutes IV embeddings with \u201cmakeshift\u201d ones to enhance system robustness. Heater [8] and CLCRec [9] further utilize a mix of experts and contrastive learning techniques to improve OOV recommendation performance. ", "page_idx": 0}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/6776ac28f21eb2e3accbc2092a8d445a72df5bd54bb1cb0504038bd5b668b01c.jpg", "img_caption": ["Figure 1: Comparison between (a) traditional \u2018makeshift\u2019 embedding OOV recommendation framework, and (b) user sequence imagination OOV recommendation framework. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As shown on the left of Figure 1, current recommender systems typically learn the embeddings for any given items by initializing/generating the embedding and then optimizing them through usersequence backpropagation. However, prevalent OOV recommendation models primarily concentrate on generating improved or robust embeddings, overlooking the potential for further optimization from imagining user sequences. This oversight limits these models due to the following issues. 1. Content-Behavior Gap. \u2018Makeshift\u2019 embeddings are generated from content features, while behavioral embeddings are trained using backpropagation. The substantial difference between content features and behavioral embeddings may lead to discrepancies between IV and OOV items, impacting IV, OOV, or both. 2. Potential Suboptimality. Focusing only on embedding generation overlooks potential improvements from backpropagation, which could finely tune the embeddings to adapt to user preferences and current recommender systems, possibly leading to suboptimal recommendation performance and a reduction in revenue. ", "page_idx": 1}, {"type": "text", "text": "While imagining the sequential optimization process for OOV item embeddings shows promise, its implementation presents three challenges. 1. Absence of Historical Interactions. The lack of historical user interactions for OOV item embeddings hinders the definition of a clear backpropagation and optimization process. 2. Ambiguous Imagination Objectives. Formulating objectives, stopping criteria, and user selection for imagining OOV item interactions remains an open challenge. 3. Navigating the Vast User Space. Efficiently identifying suitable user sequences to imagine for OOV items within the massive user space of recommender systems. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we introduce an User Sequence Imagination (USIM) pipeline that further optimizes the embedding of OOV items. It first imagines potential users who may interact with the OOV item and then refines the item embeddings through backpropagation. Specifically, we propose a RL-based USIM solution, which formulates the sequential optimization as a Markov Decision Process and introduces recommender-oriented PPO (RecPPO) to maximize the final recommendation performance of OOV items. In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce USIM, a novel approach that fundamentally addresses the Out-of-Vocabulary (OOV) problem by imagining user sequences and performing user-sequence backpropagation.   \n\u2022 We formally define the Reinforcement Learning (RL) formulation of USIM, including the formulation of the Markov Decision Process (MDP) for user sequence imagination, and provide the state transition function framework for user-sequence backpropagation.   \n\u2022 We implement USIM on a major e-commerce platform\u2014Alibaba, successfully optimizing millions of OOV items and recommending them to billions of users. The source code is publicly available at https://github.com/Ruochen1003/USIM.   \n\u2022 We validate the effectiveness of our approach on two benchmark datasets using both traditional collaborative filtering and graph-based collaborative filtering backbones. Extensive experiments demonstrate that USIM outperforms existing state-of-the-art OOV recommendation models in terms of OOV recommendation performance and overall recommendation quality. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Let $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{T}$ denote the sets of users and items, respectively. We partition $\\mathcal{T}$ into in-vocabulary (IV) items $\\mathcal{T}_{i v}$ (items with interaction history) and out-of-vocabulary (OOV) items $\\mathcal{Z}_{o o v}$ (items without interaction history). We denote the cardinality of these sets as $|\\mathcal{U}|,|\\mathcal{Z}_{i v}|$ , and $\\lvert\\mathcal{Z}_{o o v}\\rvert$ , respectively. For clarity in the following discussion, we also use $u_{i}$ to represent the set of users who have interacted with item $i$ . ", "page_idx": 2}, {"type": "text", "text": "The embedding matrices for users, IV items, and OOV items are denoted as $E_{u}\\in\\mathbb{R}^{|\\mathcal{U}|\\times d}$ , $E_{i v}\\in$ $\\mathbb{R}^{|\\mathcal{Z}_{i v}|\\times d}$ , and $\\bar{\\boldsymbol{E}}_{o o v}\\in\\mathbb{R}^{|\\mathcal{Z}_{o o v}|\\times d}$ , respectively, where $d$ represents the embedding dimension. For an individual user $u$ and item $i$ , their corresponding embeddings are denoted as $\\boldsymbol{e_{u}}^{\\breve{}{}}\\in\\mathbb{R}^{d}$ and $e_{i}\\in\\mathbb{R}^{d}$ . Since OOV items lack behavioral embeddings initially, we leverage content features, denoting the content feature vector of item $i$ as $c_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Backpropagated Embedding of IV Items. The embeddings of IV items are initialized using standard initialization techniques such as Xavier initialization [10]. These embeddings are subsequently optimized through backpropagation using historical interaction data [11, 4], ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{e}_{i}=\\boldsymbol{e}_{i}-\\sum_{u\\in\\mathcal{U}_{i}}\\nabla L(u,i),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L$ represents the loss function (e.g., BPR loss). ", "page_idx": 2}, {"type": "text", "text": "Makeshift Embedding of OOV Items. Due to the absence of historical interactions for OOV items, various approaches have been proposed to generate makeshift embeddings that enable joint recommendation with IV items. These approaches can be broadly categorized into generative models [6, 5, 12] and dropout models [9, 7], which transform content features $c_{i}$ into embeddings through a generator function $G$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\ne_{i}=G(c_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $G$ is optimized using various objective functions, including similarity-based losses [12], adversarial losses [5], and knowledge distillation losses [6]. ", "page_idx": 2}, {"type": "text", "text": "MDP Formulation of Back Propagation. We formulate the OOV embedding optimization process as a Markov Decision Process (MDP) to narrow the optimization gap between IV and OOV item embeddings. This formulation enables simultaneous user imagination and embedding optimization through backpropagation. ", "page_idx": 2}, {"type": "text", "text": "An MDP at time step $t$ is defined by the quintuple $(S,{\\mathcal{A}},\\rho,R,\\gamma)$ , where: ", "page_idx": 2}, {"type": "text", "text": "\u2022 $\\boldsymbol{S}$ represents the state space, with each state $s\\in S$ capturing the environment configuration \u2022 $\\boldsymbol{\\mathcal{A}}$ denotes the action space, where each action $a\\in A$ represents a possible agent decision \u2022 $\\rho:{\\mathcal{S}}\\times{\\mathcal{A}}\\rightarrow{\\mathcal{S}}$ defines the state transition function, with $\\pmb{s}_{t+1}=\\rho(\\pmb{s}_{t},a_{t})$ \u2022 $R:S\\times A\\to\\mathbb{R}$ specifies the reward function, where $r_{t}=R(s_{t},a_{t})$ \u2022 $\\gamma\\in[0,1]$ represents the discount factor for future rewards ", "page_idx": 2}, {"type": "text", "text": "3 Proposed User Sequence Imagination Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Framework Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To bridge the fundamental gap between IV and OOV item embedding generation caused by disparate interaction histories, we propose USIM, which fine-tunes OOV item embedding by imagining appropriate user sequences. Specifically, in each step, USIM simulates a user interaction and optimizes the item embedding accordingly. We formulate this process within a reinforcement learning paradigm, as illustrated in Figure 2. ", "page_idx": 2}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/cbfe801510cbc6e3ab885da06a4332ab46fb3328d709c2603c7f05a1f49e1e39.jpg", "img_caption": ["Figure 2: The overview framework of USIM. USIM fine-tunes the generated OOV item embeddings through sequential user interaction imagination, guided by exploration set construction, state transition, and a tailored reward mechanism. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "State Space. At time step $t$ , the state $s_{t}$ encapsulates the essential information required for item embedding optimization, defined as $\\pmb{s}_{t}=[h_{t},l_{t}]$ . The state representation $\\boldsymbol{h}_{t}\\in\\mathbb{R}^{d}$ , which is refined through optimization, resides in the same embedding space as $e_{i}$ , and its final representation will serve as the OOV item embedding. To specifically denote the state of item $i$ at time step $t$ , we define $\\pmb{s}_{i,t}=[h_{i,t},l_{i,t}]$ . In contrast, $\\scriptstyle s_{t}$ serves as a more general denotation. The initial state presentation is obtained by the generator in Eq. (2) as $h_{i,0}=G(c_{i})$ . Details can refer to Appendix B. ", "page_idx": 3}, {"type": "text", "text": "The temporal component $l_{t}$ is used as a countdown mechanism, tracking the remaining optimization steps to encourage efficient convergence [13]. Given a maximum action limit $N$ , at time step $t$ , the countdown value is computed as $l_{t}=N-t$ . ", "page_idx": 3}, {"type": "text", "text": "Action Space. Given state $\\scriptstyle s_{t}$ , the agent selects an action $a_{t}$ from the action space $\\begin{array}{r}{\\mathcal{A}=\\mathcal{U}\\cup\\{a_{e n d}\\}}\\end{array}$ , where $a_{e n d}$ denotes the termination action. This selection process entails either imagining a user or terminating the optimization process. The action embedding $e_{a}$ corresponds to the user embedding from $E_{u}$ when $a\\in\\mathcal{U}$ , and defaults to 0 for the termination action. ", "page_idx": 3}, {"type": "text", "text": "Policy Network. The agent\u2019s decision-making process is governed by policy $\\pi\\big(\\pmb{\\mathscr{s}}_{t}\\big)$ , which maps the current state $\\scriptstyle s_{t}$ to a probability distribution over possible actions. To effectively utilize existing embeddings while accommodating the special termination action, the policy distribution is given as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi(a_{t}|s_{t})=\\left\\{\\!\\!\\begin{array}{l l}{(1-\\sigma(W_{2}s_{t}^{\\top}+c))\\cdot\\frac{\\exp(e_{a_{t}}W_{1}s_{t}^{\\top})}{\\sum_{a\\in\\mathcal{U}}\\exp(e_{a}W_{1}s_{t}^{\\top})},}&{\\mathrm{if~}a_{t}\\in\\mathcal{U};}\\\\ {\\sigma(W_{2}s_{t}^{\\top}+c),}&{\\mathrm{if~}a_{t}=a_{e n d},}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{1}\\in\\mathbb{R}^{d\\times(d+1)}$ , $W_{2}\\in\\mathbb{R}^{d+1}$ , and $c$ are parameters, and $\\sigma$ represents the sigmoid function. ", "page_idx": 3}, {"type": "text", "text": "Reward and State Transition. The agent receives an immediate reward $r_{t}=R(s_{t},a_{t})$ after each action, guiding the optimization trajectory. To align with the imagination process, we design an efficient state transition function $\\pmb{s}_{t+1}=\\rho(\\pmb{s}_{t},a_{t})$ that facilitates the progressive refinement from content-based to interaction-based embeddings. The experience tuples $(\\pmb{s}_{t},a_{t},r_{t},\\pmb{s}_{t+1})$ are collected in a replay buffer for subsequent training iterations. ", "page_idx": 3}, {"type": "text", "text": "3.2 State Transition Function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "State transition involves modifying $h_{t}$ , which will ultimately be used as the OOV item embedding and each step of the state transition corresponds to optimizing $h_{t}$ using the imagined user(last action $a_{t}\\,,$ ). Therefore, to design a state transition function that aligns with this optimization process, we must ", "page_idx": 3}, {"type": "text", "text": "first determine the objective of this optimization. Considering that most current recommendation algorithms calculate relevance scores between users and items for recommendations [11, 14, 4], we adopt the following objective as our optimization goal, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{e_{i}}-\\mathbb{E}_{u\\in\\mathcal{U}_{i}}\\hat{y}_{u,i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{y}_{u,i}$ is predicted score between $e_{i}$ and $e_{u}$ . ", "page_idx": 4}, {"type": "text", "text": "Based on the above idea, we can view the user imagination process as the solution to the optimization objective (Eq. (4)). Specifically, we assume that the users imagined by the agent are the users who have interacted with this item, and by using backpropagation, we can optimize the content-based initialized embeddings. Thus, the transition function $\\rho_{h}$ of $h_{t}$ can be written as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t+1}=\\rho_{h}(h_{t},a_{t})=\\left\\{h_{t}+\\lambda\\nabla\\hat{y}_{a_{t},i}\\ \\ \\mathrm{~if~}a_{t}\\in\\mathcal{U};\\atop{\\mathrm{~if~}}a_{t}=a_{e n d},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda$ is a hyperparameter and can be understood as the learning rate. If $\\hat{y}_{a_{t},i}$ is computed using dot product, the final transition function of $h_{t}$ can be written as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t+1}=\\rho_{h}(h_{t},a_{t})=\\displaystyle\\left\\{h_{t}+\\lambda\\cdot e_{a_{t}}\\quad\\mathrm{if~}a_{t}\\in\\mathcal{U};\\atop{h_{t}}\\right.}\\\\ {\\quad\\left.h_{t+1}=a_{e n d}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "And $l_{t}$ can be updated as $l_{t+1}=l_{t}-1$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Reward Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The objective of the USIM is to optimize the initialized embeddings by imagining user sequences. To facilitate this process, we design a reward function to guide the reinforcement learning approach. Our reward function consists of three components. ", "page_idx": 4}, {"type": "text", "text": "Embedding Alignment Reward. We believe that the item embeddings generated by the IV model represent the best solution for our optimization process. Consequently, our objective is to closely align the final state representation with actual item embedding (i.e., the corresponding embedding from the IV model). To achieve this, we calculate the reward based on the concept of similarity, ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{e m b}({h}_{i,t},{a}_{t})=D({h}_{i,t},{e}_{i})-D({h}_{i,t+1},{e}_{i}),{h}_{i,t+1}=\\rho_{h}({h}_{i,t},{a}_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D(\\cdot,\\cdot)$ denotes the Euclidean distance between embeddings. This reward represents the change in similarity between the state representation and the actual item embedding, before and after the state transition. ", "page_idx": 4}, {"type": "text", "text": "Recommendaion Performance Reward. Although the embedding alignment reward encourages the state representation $h_{t}$ to be close to the actual item embedding, it does not differentiate between states when multiple representations are equally distant from the actual embedding. Additionally, it does not fully utilize the insights from existing user interactions. Therefore, we design a reward function based on recommendation performance as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R_{r e c}(h_{i,t},a_{t})=f(h_{i,t},e_{i})-f(h_{i,t+1},e_{i}),h_{i,t+1}=\\rho_{h}(h_{i,t},a_{t}),}}\\\\ {{f(h_{i,t},e_{i})=\\displaystyle\\frac{1}{|\\mathcal{U}_{i}|}\\sum_{u_{j}\\in\\mathcal{U}_{i}}|h_{i,t}\\cdot e_{u_{j}}-e_{i}\\cdot e_{u_{j}}|,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f(\\boldsymbol h_{i,t},\\boldsymbol e_{i})$ represents the predictive power of state representation $\\boldsymbol{h}_{i,t}$ for users in $u_{i}$ . The final performance reward is derived from the change in $f$ before and after the state transition. This change represents the variation in the embedding\u2019s predictive capability. ", "page_idx": 4}, {"type": "text", "text": "Step Regulation. To encourage the agent to achieve the goal in as few steps as possible, we impose a penalty for each action it takes. Therefore, the final reward function is as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{t}=R(s_{i,t},a_{t})=R_{e m b}(h_{i,t},a_{t})+R_{r e c}(h_{i,t},a_{t})-p,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p$ is a hyperparameter that represents the penalty. ", "page_idx": 4}, {"type": "text", "text": "3.4 Exploration Set Construction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the large user base in the recommendation dataset, sampling actions solely by probability during initial reinforcement learning often results in negative rewards, slowing convergence and reducing performance. To explore actions more efficiently, we construct an exploration set according to state $s_{t}$ , comprising three components. ", "page_idx": 5}, {"type": "text", "text": "Positive Action. In the USIM framework (Section 3.2), the agent assumes that the imagined users correspond to those who have interacted with the item, so these users are included in the exploration set. While optimizing $\\boldsymbol{h}_{i,t}$ to match $e_{i}$ theoretically requires only the interaction set $u_{i}$ , achieving this often demands combining multiple actions $a\\in\\mathcal{U}_{i}$ , making exploration complex. To streamline this, we select users most likely to bridge $\\boldsymbol{h}_{i,t}$ and $e_{i}$ , accelerating the process. The Positive Action set $\\mathcal{U}_{p o s}$ is constructed as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{U}_{p o s}=\\mathrm{Top}_{k_{1}}((e_{i}-h_{i,t}),e_{u})\\cup\\mathcal{U}_{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{Top}_{k_{1}}((e_{i}-h_{i,t}),e_{u})$ represents the set of $k_{1}$ users with the highest cosine similarity to $e_{i}-h_{i,t}$ , forming a vector that directly points to the actual item embedding. ", "page_idx": 5}, {"type": "text", "text": "Random Action. Relying solely on the aforementioned action sets may overly restrict actions, limiting state space coverage and reducing model generalization. Moreover, sampling actions with negative rewards can also benefti training [15]. Thus, we augment the action set by randomly selecting $k_{2}$ actions from the remaining pool, denoted as $\\mathcal{U}_{r a d}$ . ", "page_idx": 5}, {"type": "text", "text": "Termination Action. To enable the agent to learn when to terminate, we also incorporate the termination action $a_{e n d}$ into the final action set. This inclusion allows the agent to determine the appropriate timing for ending the optimization process. ", "page_idx": 5}, {"type": "text", "text": "For simplicity, we set $k_{1}=k_{2}=k$ . And exploration set $\\mathcal{A}_{s a m p}$ can be represented as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{A}_{s a m p}=\\mathcal{U}_{p o s}\\cup\\mathcal{U}_{r a d}\\cup\\{a_{e n d}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "During the training phase, as the sampling range is narrowed from the full action set $\\boldsymbol{\\mathcal{A}}$ to a specific action set $\\mathcal{A}_{s a m p}$ , we rewrite the Eq. (3) as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi(a_{t}|s_{t})=\\left\\{\\begin{array}{l l}{(1-\\sigma(W_{2}s_{t}^{\\top}+c))\\cdot\\frac{\\exp\\{e_{a_{t}}W_{1}s_{t}^{\\top}\\}}{\\sum_{a\\in A_{s a m p}\\setminus\\{a_{e n d}\\}}\\exp\\{e_{a}W_{1}s_{t}^{\\top}\\}},}&{\\mathrm{if~}a_{t}\\in A_{s a m p}\\setminus\\{a_{e n d}\\};}\\\\ {\\sigma(W_{2}s_{t}^{\\top}+c),}&{\\mathrm{if~}a_{t}=a_{e n d};}\\\\ {0,}&{\\mathrm{if~}a_{t}\\not\\in A_{s a m p}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.5 Training with RecPPO ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We incorporate recommendation-specific supervision signals into PPO [16], referring to this enhanced approach as Recommender-Oriented PPO (RecPPO), to train our USIM. In the scenario of OOV item recommendation, the optimal action following certain states is clear, allowing the cumulative expected rewards for these states to be calculated directly. When the state representation $\\boldsymbol{h}_{i,t}$ of a specific item is equal to its item embedding $e_{i}$ , according to our designed reward function, the expected value should be 0 because any subsequent actions, except for termination will lead to negative rewards. ", "page_idx": 5}, {"type": "text", "text": "So we use these supervision signals to assist in training the value network $V_{\\omega}$ , the specific loss function of the value network is defined as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\omega)=\\frac{1}{|B|}\\sum_{(s_{t},r_{t},s_{t+1})\\in B}\\Big[(r_{t}+\\gamma V_{\\omega}(s_{t+1})-V_{\\omega}(s_{t}))^{2}\\Big]+\\frac{1}{|Z|}\\sum_{i\\in\\mathcal{Z}}V_{\\omega}([e_{i},\\mathrm{random}(0,N)])^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $B$ denotes tuples sampled from the buffer pool, and random $(0,N)$ is a random number between 0 and $N$ . The first term of the loss is the Temporal Difference loss used in value network training, while the second term includes our recommendation-oriented supervision signals. Regardless of previous actions, when the state representation $h_{t}^{i}$ matches $e_{i}$ , the agent should terminate. Here, random $(0,N)$ acts as the timer for each termination state. As for the policy network, we train it using the same method as PPO. The whole training process can be found in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct comprehensive experiments on two benchmark datasets aiming to address the following three questions: RQ1: Can USIM achieve superior OOV item recommendation performance compared to state-of-the-art OOV item recommendation models? RQ2: How key components of USIM affect its performance? RQ3: Is the proposed USIM more effective than representative RL methods? RQ4: What is the tendency of performance during USIM\u2019s imagination process? RQ5: How does USIM perform in real-world industrial recommendations? RQ6: How does USIM achieve efficiency compared to other baselines? ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1: Overall, OOV, and IV item recommendation performance comparison over two representative recommender backbones (MF and GNN). The best and second-best results in each column are highlighted in bold font and underlined, $\\star\\star$ indicates the statistical significance $\\mathrm{p}<0.01$ compared to the best-performed baseline, $\\star$ indicates the statistical significance $\\mathrm{p}<0.05$ compared to the best-performed baseline. ", "page_idx": 6}, {"type": "table", "img_path": "JyWAFGCJPl/tmp/0a192d46e427c0ebbb73a44f02710cf0b9bbb8faa44ccec502e6fecbb3b13c59.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Datesets. We evaluate the performance of USIM on OOV items using the widely used datasets: CiteULike [8] and MovieLens [17]. Specifically, CiteULike contains 5,551 users, 16,980 articles (items), and 204,986 interactions. MovieLens comprises 6,040 users, 3,883 movies (items), and 1,000,210 interactions. Details about these datasets are shown in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "Baselines.To assess the effectiveness and universality of USIM, we conduct a comparative analysis with 8 leading-edge models in OOV item recommendations across two distinct datasets. These models include two main groups. (i) Dropout-based methods: DropoutNet [7], MTPR [18], Heater [8], and CLCRec [9]. (ii) Generative-based methods: DeepMusic [12], MetaEmb [19], GAR [5], and ALDI [6]. Details about these models are shown in Appendix D.2. To further verify the generalization ability, we adopted both the widely used collaborative flitering model MF [20] and GNN-based model NGCF [3] as the recommender, respectively. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. Following the evaluation of existing OOV recommendation [6, 9], we conduct three different tasks in our experiments: (1) Overall Recommendation, (2) OOV Recommendation, and (3) IV Recommendation. We employ the full-ranking evaluation approach to assess the performance of overall, IV, and OOV recommendations. Following previous works [4, 3], we utilize Recall $@\\,\\mathrm{K}$ and Normalized Discounted Cumulative Gain ${\\mathrm{NDCG}}@\\mathrm{K})$ as metrics. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We implement the baselines using their officially provided version. See Appendix E for the detailed implementation. The best hyperparameters are found for each dataset. For fairness, we use the same options and follow the designs in their articles for all baselines. ", "page_idx": 6}, {"type": "table", "img_path": "JyWAFGCJPl/tmp/a50848e781483af60379a28c7a06f449810919ba021c32c7dc560f1e8f7d7469.jpg", "table_caption": ["Table 2: Ablation study results between USIM with its four variants on CiteULike. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Main Results (RQ1) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The main comparison results of overall, IV, and OOV item recommendation results can be found in Table 1. From the results, we can have the following observations. ", "page_idx": 7}, {"type": "text", "text": "USIM can generally achieve significant improvements over state-of-the-art methods on both overall and OOV item recommendations while keeping the IV item recommendation. From the tables, we observe that the USIM achieves the highest average Recall and NDCG performance across both MF and GNN recommenders. These comparison results verify the superiority of the imagined embeddings over traditional one-step generated embeddings. ", "page_idx": 7}, {"type": "text", "text": "Dropout-based baselines have the performance drop in the IV item recommendation. We find that the USIM and the generative-based models can keep the IV item recommendation. However, dropout-based models will lead to a performance drop on IV items. This suggests that there is a difference between OOV items and IV items. Pre-training representations of IV items in advance to generate representations for OOV items may be better for retaining information for IV items. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study (RQ2) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate the effectiveness of the individual components in our model, we compared the full model against four variants: (i) w/o ct removes the cosine similarity-based top- $\\cdot\\mathbf{k}$ user selection when constructing the positive action set. (ii) w/o ra removes the randomly sampled actions in the exploration set. (iii) w/o es does not construct an exploration set and directly explores the entire action set. (iv) w/o pr removes the performance reward, only using the similarity reward in the RL processing. From the results in Table 2, we make the following observations. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of components in the exploration set construction. The w/o ct result indicates that the cosine similarity-based selection of the top- $k$ users can simplify the exploration process by quickly identifying users related to the item embedding. Then, the performance of $w/o$ ra reveals the importance of including negative samples to provide a more comprehensive exploration signal, preventing the agent from overestimating action values by only considering positive feedback. Further, the poor performance of the w/o es approach highlights the challenge of effectively exploring an extremely large action space, emphasizing the need for a well-designed exploration strategy to guide the agent towards promising actions. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of components in the reward function, Removing the performance reward component leads to inferior results. This suggests that the reward function should not only consider the distance between the generated embedding and the target embedding but also explicitly incorporate the downstream recommendation performance of the generated embedding. ", "page_idx": 7}, {"type": "text", "text": "4.4 Comparison with Representative RL Methods (RQ3) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Figure 3, we compared our proposed model with our initialized MLP and two other traditional reinforcement learning methods: Wolpertinger Policy [21] (WP) and Hierarchical Reinforcement Learning [22](HRL). WP utilizes the similarity of action representations and value estimation to solve the large discrete action space problem in reinforcement learning, while HRL employs a hierarchical decomposition of action space and sub-task solution to improve exploration efficiency. ", "page_idx": 7}, {"type": "text", "text": "Our model significantly outperforms the other two methods in both OOV recommendation and overall recommendation. Moreover, it can be observed that the other two methods do not show apparent improvements compared to their initial states. This supports the effectiveness of our tailored exploration method in improving performance in OOV recommendations. ", "page_idx": 7}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/18c9a7bcab6f558a055d56a3c6c01e45ad61d82afae58d63b6c3e9c8a817c56e.jpg", "img_caption": ["Figure 3: Comparing USIM with other RL methods for overall and OOV recommendation performance in CiteULike dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/9ce028d3796e2f2f3dac5d1098cad8339c4a06ec750af0f83454864527285afa.jpg", "img_caption": ["Figure 4: Performance analysis of different generation methods on the CiteUlike dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 Case Study (RQ4) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To investigate the performance trend of our proposed method during the optimization process, and to validate the importance of the way of sampling in the embedding optimization process, we compare USIM with the following two user selection strategies: randomly sampling users at each step, and randomly selecting from the top 20 users with the highest relevance scores at each step. The results are shown in Figure 4, and according to the result, we can draw the following conclusions. ", "page_idx": 8}, {"type": "text", "text": "(i) Our method outperforms the random selection of top-20 users and the entire user set in both OOV and overall scenarios, indicating it can effectively identify users beneficial for optimization. However, performance declines as more users are imagined, likely due to reduced exploration by the agent. ", "page_idx": 8}, {"type": "text", "text": "(ii) When randomly selecting users from the top 20, we can observe that the performance in OOV recommendation increases after the first step. This suggests that sampling from high-scoring users is more likely to select users who are beneficial to the optimization process. However, the performance then continuously declines, indicating that this approach is not suitable for all states and has limited help for the optimization process. ", "page_idx": 8}, {"type": "text", "text": "(iii) Randomly selecting a user at each step leads to a drastic and non-recoverable decline in performance in both the OOV and overall scenarios after the first step. This suggests that in recommendation scenarios, it is extremely difficult to sample users from the massive user set who are beneficial to the optimization, and the majority of users are highly detrimental to the optimization process. ", "page_idx": 8}, {"type": "text", "text": "More experimental hyperparameter analysis can be found in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "4.6 Online Evaluation (RQ5) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the performance of USIM in an industrial setting, we conducted a two-week online A/B test on a major e-commerce platform with $5\\%$ of users in each group. USIM was compared against three baselines: Random, MetaEmb [19], and ALDI [6]. Details about our test platform and evaluation metrics are provided in Appendix G. Table 3 presents the results of these online A/B tests. ", "page_idx": 8}, {"type": "text", "text": "These remarkable improvements across all metrics underscore the effectiveness of the USIM in addressing the OOV item recommendation problem in real-world recommender systems. The consistent and substantial performance gains, particularly in OOV item GMV, highlight the practical impact of our approach on business outcomes in e-commerce settings. ", "page_idx": 8}, {"type": "table", "img_path": "JyWAFGCJPl/tmp/c7cbb87d315185e97432b19568b59da9ddf9afa9268d9ba3272a511d3ddaf169.jpg", "table_caption": ["Table 3: Results of online A/B test in the industrial platform. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.7 Efficiency Analysis (RQ6) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To evaluate the time efficiency of USIM, especially in comparison to SOTA baselines, we recorded the total training time(Training Time), total convergence epochs(Converge Epochs), Time Per Epoch, and Inference Time for USIM and each baseline on the CiteULike and MovieLens datasets. The results are presented in Table 4. Based on these results, we can draw the following conclusions: ", "page_idx": 9}, {"type": "table", "img_path": "JyWAFGCJPl/tmp/0d2894b171d8b4b41c4466b56bc07fff61b1daadf3a5f4913d3eb822e709f6ba.jpg", "table_caption": ["Table 4: Results of time efficiency. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "(i) USIM is Faster than Heater and CLCRec: USIM computes embeddings only for OOV items, whereas Heater and CLCRec must compute embeddings for both OOV and IV items. ", "page_idx": 9}, {"type": "text", "text": "(ii) USIM is Comparable with MetaEmb and ALDI: USIM imagines sequences only for OOV items, resulting in inference times comparable to MetaEmb and ALDI. ", "page_idx": 9}, {"type": "text", "text": "(iii) USIM is Efficient in Training: By fundamentally addressing OOV recommendation, USIM converges in fewer epochs, making training more efficient. ", "page_idx": 9}, {"type": "text", "text": "More experiments about online recommendation efficiency can be found in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Recommending out-of-vocabulary (OOV) items is challenging due to the lack of well-trained behavioral embeddings. Current models use \"makeshift\" embeddings from content features, leading to suboptimal performance. We introduced the User Sequence Imagination (USIM) framework to refine OOV embeddings by imagining user sequences and incorporating behavioral embeddings. By framing this as a reinforcement learning problem and creating a recommendation-focused reward function, USIM effectively enhances OOV recommendations. Extensive experiments demonstrate its superior performance and the ablation study further illustrates the effectiveness of USIM. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was funded by the National Science Foundation of China (No.62172443) and Hunan Provincial Natural Science Foundation of China (No.2022JJ30053). This work was carried out in part using computing resources at the High-Performance Computing Center of Central South University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 974\u2013983, 2018.   \n[2] Hao C., Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang Wang, and Xiao Huang. Macro graph neural networks for online billion-scale recommender systems. In Proceedings of the ACM on Web Conference 2024, pages 3598\u20133608, 2024.   \n[3] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative flitering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, page 165\u2013174, 2019.   \n[4] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, page 639\u2013648, 2020.   \n[5] Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, Peng He, and Zhoujun Li. Generative adversarial framework for cold-start item recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, page 2565\u20132571, 2022.   \n[6] Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen. Aligning distillation for cold-start item recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, page 1147\u20131157, 2023.   \n[7] Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. Dropoutnet: addressing cold start in recommender systems. In Proceedings of the 31st International Conference on Neural Information Processing Systems, page 4964\u20134973, 2017.   \n[8] Ziwei Zhu, Shahin Sefati, Parsa Saadatpanah, and James Caverlee. Recommendation for new users and new items via randomized training and mixture-of-experts transformation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, page 1121\u20131130, 2020.   \n[9] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. Contrastive learning for cold-start recommendation. In Proceedings of the 29th ACM International Conference on Multimedia, page 5382\u20135390, 2021.   \n[10] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256, 2010.   \n[11] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, page 452\u2013461, 2009.   \n[12] A\u00e4ron van den Oord, Sander Dieleman, and Benjamin Schrauwen. Deep content-based music recommendation. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, page 2643\u20132651, 2013.   \n[13] Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, pages 4045\u20134054, 2018.   \n[14] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1441\u20131450, 2019.   \n[15] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. Recommendations with negative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, page 1040\u20131048, 2018.   \n[16] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[17] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1\u201319, 2015.   \n[18] Xiaoyu Du, Xiang Wang, Xiangnan He, Zechao Li, Jinhui Tang, and Tat-Seng Chua. How to learn item representation for cold-start multimedia recommendation? In Proceedings of the 28th ACM International Conference on Multimedia, page 3469\u20133477, 2020.   \n[19] Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, and Juan Cao. Learning to warm up cold item embeddings for cold-start recommendation with meta scaling and shifting networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1167\u20131176, 2021.   \n[20] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, 2009.   \n[21] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679, 2015.   \n[22] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   \n[23] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining, pages 263\u2013272, 2008.   \n[24] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, 2009.   \n[25] Hao Chen, Yue Xu, Feiran Huang, Zengde Deng, Wenbing Huang, Senzhang Wang, Peng He, and Zhoujun Li. Label-aware graph convolutional networks. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, page 1977\u20131980, 2020.   \n[26] Hao Chen, Zhong Huang, Yue Xu, Zengde Deng, Feiran Huang, Peng He, and Zhoujun Li. Neighbor enhanced graph convolutional networks for node classification and recommendation. Knowledge-Based Systems, 246:108594, 2022.   \n[27] Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, and Xiao Huang. Knowledge-to-sql: Enhancing sql generation with data expert llm. arXiv preprint arXiv:2402.11517, 2024.   \n[28] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), pages 197\u2013206, 2018.   \n[29] Dietmar Jannach and Malte Ludewig. When recurrent neural networks meet the neighborhood for session-based recommendation. In Proceedings of the eleventh ACM conference on recommender systems, pages 306\u2013310, 2017.   \n[30] Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao Huang. Next-generation database interfaces: A survey of llm-based text-to-sql. arXiv preprint arXiv:2406.08426, 2024.   \n[31] Huachi Zhou, Hao Chen, Junnan Dong, Daochen Zha, Chuang Zhou, and Xiao Huang. Adaptive popularity debiasing aggregator for graph collaborative filtering. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 7\u201317, 2023.   \n[32] Peiyan Zhang, Yuchen Yan, Xi Zhang, Liying Kang, Chaozhuo Li, Feiran Huang, Senzhang Wang, and Sunghun Kim. Gpt4rec: Graph prompt tuning for streaming recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1774\u20131784, 2024.   \n[33] Yijie Zhang, Yuanchen Bei, Hao Chen, Qijie Shen, Zheng Yuan, Huan Gong, Senzhang Wang, Feiran Huang, and Xiao Huang. Multi-behavior collaborative flitering with partial order graph convolutional networks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 6257\u20136268, 2024.   \n[34] Feiran Huang, Zhenghang Yang, Junyi Jiang, Yuanchen Bei, Yijie Zhang, and Hao Chen. Large language model interaction simulator for cold-start item recommendation. arXiv preprint arXiv:2402.09176, 2024.   \n[35] Xiaoxiao Xu, Chen Yang, Qian Yu, Zhiwei Fang, Jiaxing Wang, Chaosheng Fan, Yang He, Changping Peng, Zhangang Lin, and Jingping Shao. Alleviating cold-start problem in ctr prediction with a variational embedding learning framework. In Proceedings of the ACM Web Conference 2022, page 27\u201335, 2022.   \n[36] Shaoyun Shi, Min Zhang, Xinxing Yu, Yongfeng Zhang, Bin Hao, Yiqun Liu, and Shaoping Ma. Adaptive feature sampling for recommendation with missing content feature values. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, page 1451\u20131460, 2019.   \n[37] Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. Warm up cold-start advertisements: Improving ctr predictions via learning to learn id embeddings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, page 695\u2013704, 2019.   \n[38] Yuanfei Wang, fangwei zhong, Jing Xu, and Yizhou Wang. Tom2c: Target-oriented multi-agent communication and cooperation with theory of mind. In International Conference on Learning Representations, 2022.   \n[39] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou Wang. Fast peer adaptation with context-aware exploration. In Proceedings of the 41st International Conference on Machine Learning, pages 33963\u201333982, 2024.   \n[40] Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, and Yizhou Wang. Richelieu: Self-evolving llm-based agents for ai diplomacy. arXiv preprint arXiv:2407.06813, 2024.   \n[41] Fangwei Zhong, Kui Wu, Hai Ci, Churan Wang, and Hao Chen. Empowering embodied visual tracking with visual foundation models and offline rl. arXiv preprint arXiv:2404.09857, 2024.   \n[42] Yuanchen Bei, Sheng Zhou, Qiaoyu Tan, Hao Xu, Hao Chen, Zhao Li, and Jiajun Bu. Reinforcement neighborhood selection for unsupervised graph anomaly detection. In 2023 IEEE International Conference on Data Mining (ICDM), pages 11\u201320, 2023.   \n[43] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative adversarial user model for reinforcement learning based recommendation system. In Proceedings of the 36th International Conference on Machine Learning, pages 1052\u20131061, 2019.   \n[44] Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin. Reinforcement learning to optimize long-term user engagement in recommender systems. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2810\u20132818, 2019.   \n[45] Lixin Zou, Long Xia, Pan Du, Zhuo Zhang, Ting Bai, Weidong Liu, Jian-Yun Nie, and Dawei Yin. Pseudo dyna-q: A reinforcement learning framework for interactive recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining, page 816\u2013824, 2020.   \n[46] Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. \"deep reinforcement learning for search, recommendation, and online advertising: a survey\" by xiangyu zhao, long xia, jiliang tang, and dawei yin with martin vesely as coordinator. SIGWEB Newsl., 2019(Spring), 2019.   \n[47] Dong Liu and Chenyang Yang. A deep reinforcement learning approach to proactive content pushing and recommendation for mobile users. IEEE Access, 7:83120\u201383136, 2019.   \n[48] Zefang Liu, Shuran Wen, and Yinzhu Quan. Deep reinforcement learning based group recommender system. arXiv preprint arXiv:2106.06900, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "General Recommendation. General recommendation systems typically predict which items users will prefer by leveraging collaborative information. Based on how collaborative information is utilized, general recommendation methods typically include approaches such as Matrix Factorization-based, Graph-based, and Sequential recommendation. Matrix Factorization-based recommendation methods derive user and item embeddings by decomposing the interaction matrix into two feature matrices using Matrix Factorization techniques [11, 23, 24]. Graph-based recommendations incorporate graph techniques [25] to model high-order relationships between users and items [4, 3, 26]. Sequential recommendation methods focus on capturing the temporal patterns in user interactions, modeling the order of users\u2019 actions to predict their future preferences [27, 28, 14, 29\u201332]. However, these methods are generally ineffective in addressing the OOV item, ", "page_idx": 14}, {"type": "text", "text": "Out-of-Vocabulary Item Recommendation. Out-of-vocabulary (OOV) item recommendation aims to address the problem of recommending a completely new item that has no prior user interactions with users [33, 34]. Its core idea lies in how to map the content information of the cold item into the space defined by the warm item embeddings trained from interactions with users. To achieve this goal, existing methods can be mainly categorized into two approaches. One category is the dropout model, which learns a content map function using the dropout approach [18, 35, 36, 8, 7, 9]. The other category is the generative model, which directly learns the relationship between the content of the OOV item and the IV behavior embeddings to obtain the mapping function [6, 12, 37, 5]. ", "page_idx": 14}, {"type": "text", "text": "Reinforcement Learning-based Recommendation. Reinforcement learning (RL) is a famous type of learning strategy where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward, which is a widely used technique for modeling human behavior [38\u201342]. Many studies also focus on simulating user behavior within the recommendation community. Cascading DQN [43] uses GAN to learn and simulate real users from historical interactions to obtain the reward function. In [15, 44], the simulator is trained on user historical data to simulate user feedback. In Pseudo Dyna-Q [45], a world model (user simulator) is trained by minimizing the error between online and offline rewards. Another common simulation approach is based on collaborative filtering. LIRD [46]builds a memory with (s, a, r) tuples seen in the log dataset and uses a similarity method based on cosine similarity to find the closest state-action pair to the current state and recommended action. DRR [47] and DRGR [48] use the same intuition but based on different factorizations, respectively. ", "page_idx": 14}, {"type": "text", "text": "B Details of initial model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Loss function Our initial model $G$ is an MLP trained to minimize the Euclidean distance between the output of $G$ and the item embedding. Specifically, the loss function for $G$ is defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\frac{1}{|\\mathcal{Z}_{i v}|}\\sum_{i\\in\\mathcal{Z}_{i v}}\\|e_{i}-G_{\\theta}(\\pmb{c}_{i})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Training details The initial model shares the same training data as USIM. We use the Adam optimizer with a learning rate of 0.001 and apply early stopping by monitoring NDCG $@\\,\\mathrm{K}$ on the validation set. The batch size and regularization weight are set to 1024 and 0.001, respectively. ", "page_idx": 14}, {"type": "text", "text": "C Model Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our complete training process is shown in Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Dataset Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Datasets. We evaluate the USIM \u2019s performance on cold-start items using the CiteULike and MovieLens datasets. ", "page_idx": 14}, {"type": "text", "text": "Input: Policy network $\\pi$ , Value network $\\omega$ , episode length $N$ , initial state generator $G$ , experience   \nreplay buffer $D=\\emptyset$   \nfor each iteration do for each batch do Initialize state $s_{0}\\gets G(c_{i})$ for $t$ in range $(N)$ do Sample action $a_{t}$ according to Eq. equation 12 Receive reward $r_{t}$ according to Eq.equation 9 Transition to next state $_{s_{t+1}}$ according to Eq.equation 6 Store transition in buffer: $\\begin{array}{r}{\\dot{D}\\gets D\\bigcup(\\bar{s}_{t},a_{t},\\bar{r}_{t},\\bar{s}_{t+1})}\\end{array}$ end for for each gradient step do Sample transitions from $D$ for gradient calculation Update the Policy network $\\pi$ using the PPO loss Update the Value network $\\omega$ with Eq. equation 13 end for end for   \nend for   \nOutput: Trained Policy $\\pi$ , Value network $\\omega$ ", "page_idx": 15}, {"type": "text", "text": "\u2022 CiteULike2 [8] The dataset contains 5,551 users, 16,980 articles, and 204,986 interactions. On CiteULike, registered users create scientific article libraries and save articles for future reference. The goal is to leverage these libraries to recommend relevant new articles to each user. The articles are represented by 300-dimensional vectors as item content features. ", "page_idx": 15}, {"type": "text", "text": "\u2022 MovieLens3 [17] MovieLens comprises 6,040 users, 3,883 items, and 1,000,210 interactions. The content features of items are represented using 200-dimensional vectors. ", "page_idx": 15}, {"type": "text", "text": "In this paper, the content features of items are represented using 200-dimensional vectors. For each dataset, $20\\%$ of items are designated as OOV items, with interactions split into a OOV validation set and testing set (1:1 ratio). Records of the remaining $80\\%$ of items are divided into training, validation, and testing sets, using an 8:1:1 ratio. ", "page_idx": 15}, {"type": "text", "text": "D.2 Baseline Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Baselines.To assess the effectiveness and universality of USIM, we conducted a comparative analysis with 8 leading-edge models in the domain of cold-start recommendations. This comparison was carried out across two distinct datasets. The models we benchmarked against include two main groups: (i) Dropout-based models: DropoutNet [7], MTPR [18], Heater [8], and CLCRec [9]. (ii) Generative models: DeepMusic [12], MetaEmb [19], GAR [5], and ALDI [6]. ", "page_idx": 15}, {"type": "text", "text": "\u2022 DeepMusic utilizes deep neural networks to model the mean squared error (MSE) difference between generated and warm embeddings. ", "page_idx": 15}, {"type": "text", "text": "\u2022 MetaEmb trains a meta-learning-based generator for rapid convergence. ", "page_idx": 15}, {"type": "text", "text": "\u2022 GAR generates embeddings through a generative adversarial relationship with the warm recommendation model. ", "page_idx": 15}, {"type": "text", "text": "\u2022 ALDI employs distillation, using warm items as \"teachers\" to transfer behavioral information to cold items, referred to as \"students\". ", "page_idx": 15}, {"type": "text", "text": "\u2022 DropoutNet enhances cold-start robustness by randomly discarding embeddings. ", "page_idx": 15}, {"type": "text", "text": "\u2022 MTPR generates counterfactual cold embeddings considering dropout and Bayesian Personalized Ranking (BPR). ", "page_idx": 15}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/e8a27363b7b858eb46aa477eb883985d724d98e2b5ba245349ae1978aa696140.jpg", "img_caption": ["Figure 5: Hyperparameter analysis of MF backbone on CiteULike dataset "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/b7b935b8e8611bc6b1bc7a07120b8eb3eafa435921bce7bae2e3e572b86dc981.jpg", "img_caption": ["Figure 6: Hyperparameter analysis of GNN backbone on CiteULike dataset "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "\u2022 Heater improves DropoutNet by using a mix-of-experts network and considering embedding similarity.   \n\u2022 CLCRec models cold-start recommendation using contrastive learning from an informationtheoretic perspective. ", "page_idx": 16}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We implement the baselines using their official implementations. Specifically, for GAR, we use the updated version provided in the official repository, which is evaluated under the same CLCRec settings as used in our paper4. The embedding dimension is set to 200 for all models. We employ the Adam optimizer with learning rates of 0.001 for the critic and 0.0005 for the actor. Early stopping is applied by monitoring NDCG $@\\,\\mathrm{K}$ on the validation set. The training batch size and regularization weight are set to 1024 and 0.001, respectively.The experiment was conducted on an NVIDIA GeForce RTX 3090 with 24GB of memory. Hyperparameters are tuned using grid search, and the optimal parameters are identified for each dataset. For fairness, we use the same settings and follow the design choices in their respective articles for all baselines. ", "page_idx": 16}, {"type": "text", "text": "F Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conducted a parameter analysis on the CiteULike dataset, and the results are shown in Figures 5 and 6. Except for the parameters, all other parameters exhibited the same trends on both the MF and GNN backbones. ", "page_idx": 16}, {"type": "text", "text": "For the parameter $k$ , the performance first increased and then decreased as $k$ increased, reaching the optimal values at 30 and 20, respectively. ", "page_idx": 16}, {"type": "text", "text": "For the parameter $N$ , the OOV recommendation performance first increased and then decreased, reaching the maximum at $N\\,=\\,9$ , while the overall recommendation performance consistently decreased. ", "page_idx": 16}, {"type": "text", "text": "For the parameter $p$ , the OOV performance first increased and then decreased, while the IV performance first decreased and then increased. ", "page_idx": 16}, {"type": "text", "text": "For the parameter $\\lambda$ , on the MF backbone, the OOV performance consistently increased, while the overall performance first increased and then decreased, reaching the optimal value at $\\lambda=0.07$ . On the GNN backbone, both the OOV and IV performances exhibited a trend of first increasing and then decreasing. ", "page_idx": 16}, {"type": "text", "text": "G Details of online test ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Platform. We have implemented our USIM on the homepage of one of the largest e-commerce platforms, which boasts hundreds of millions of users and billions of items. The homepage features a feed recommendation system that recommends items to users. Thousands of new items are uploaded every hour. ", "page_idx": 17}, {"type": "text", "text": "Framework. Our online implementation consists of two core components: 1. Online Recommendation; and 2. USIM Imagination. We present our framework in Figure 7 . When an OOV item is uploaded, we utilize the Large Language Model to embed the content features, including the product name and description. We then employ the USIM structure to predict the most suitable user sequence and optimize the embedding accordingly. Finally, we use the USIM-produced as the IV embeddings in the online recommendation model. ", "page_idx": 17}, {"type": "text", "text": "Evaluation Metrics. We employed three tailored metrics to assess the performance of USIM against existing baselines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Page Views (OOV item PV): The number of user clicks during the OOV period.   \n\u2022 Page Click-Through Rate (OOV item PCTR): The ratio of clicks to impressions during the OOV period.   \n\u2022 Gross Merchandise Value (OOV item GMV): The total value of user purchases during the OOV period. ", "page_idx": 17}, {"type": "image", "img_path": "JyWAFGCJPl/tmp/2c4ba3efeaed549080611250ea9ad333f90ede38e9285ad5f66222f6099de559.jpg", "img_caption": ["", "Figure 7: Overall framework of online implementation. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "H Online Recommendation Efficiency ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To evaluate the online recommendation efficiency of USIM, we recorded the LLM-based content feature extraction time (LLM Content Feature Extraction Time) and the inference time for OOV item embeddings (OOV Time) of MetaEmb, ALDI and USIM. The results are shown in Table 5. ", "page_idx": 17}, {"type": "text", "text": "The results of the online recommendation efficiency indicate that USIM does not serve as a bottleneck for online recommendation, as the LLM is the main time consumer while USIM\u2019s speed remains comparable to that of MetaEmb and ALDI. Additionally, the OOV recommendation is an offline, one-time process that has no impact on online recommendations, as shown in Figure 7. And given that the platform supports parallel processing, USIM computations for OOV items can be managed efficiently. ", "page_idx": 17}, {"type": "text", "text": "I Limitation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "One limitation of our proposed model is the large number of hyperparameters that need to be tuned. Our model involves 4 key hyperparameters. Tuning these hyperparameters can be a time-consuming and computationally expensive process, as it often requires extensive grid search or random search to find the optimal configuration. ", "page_idx": 17}, {"type": "table", "img_path": "JyWAFGCJPl/tmp/7861acc37ed3fbf21f44cec0c43e6c56bcd15555dd02f98d6fdbfa9ef81a067c.jpg", "table_caption": ["Table 5: Results of online recommendation efficiency "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Another limitation is the slow generation speed of our model due to its autoregressive generation approach. In our model, the output is generated sequentially, with each token being predicted conditioned on the previously generated tokens. This autoregressive generation process can be computationally intensive. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions made in the paper and important assumptions and limitation We propose the User Sequence IM agination (USIM) framework along with an RL-based solution to maximize out-of-vocabulary (OOV) item recommendation performance. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have a limitation in the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All the theorems, formulas, and proofs in the paper are numbered and crossreferenced. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper describe the architecture clearly ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Source code is available at https://anonymous.4open.science/r/USIM-D776. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experimental settings are presented in the core of the paper. And full details are provided appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have statistical significance in the main experiment. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We indicate the type of compute workers GPU. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We checked and ensured that our paper conforms with the NeurlPS Code of Ethics in every respect. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 22}, {"type": "text", "text": "there is no societal impact of the work performed ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "the paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "We cite the original paper that produced the code package or dataset.We state which version of the asset is used ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: the paper does not release new assets ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]