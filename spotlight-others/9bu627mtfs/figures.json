[{"figure_path": "9bu627mTfs/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of feature aggregation. (a) VoxFormer [23] employs a set of shared context-independent queries for different input images, which fails to capture distinctions among them and may lead to undirected feature aggregation. Besides, due to the ignorance of depth information, multiple 3D points may be projected to the same 2D point, causing depth ambiguity. (b) Our CGFormer initializes the voxel queries based on individual input images, effectively capturing their unique features and aggregating information within the region of interest. Furthermore, the deformable cross-attention is extended from 2D to 3D pixel space, enabling the points with similar image coordinates to be distinguished based on their depth coordinates.", "description": "This figure compares feature aggregation methods between VoxFormer and CGFormer.  (a) shows VoxFormer's use of shared, context-independent queries, leading to undirected feature aggregation and depth ambiguity due to the lack of depth information. (b) illustrates CGFormer's context-dependent queries, which are tailored to individual input images, resulting in improved feature aggregation and depth disambiguation.", "section": "1 Introduction"}, {"figure_path": "9bu627mTfs/figures/figures_3_1.jpg", "caption": "Figure 2: Schematics and detailed architectures of CGFormer. (a) The framework of the proposed CGFormer for camera-based semantic scene completion. The pipeline consists of the image encoder for extracting 2D features, the context and geometry aware voxel (CGVT) transformer for lifting the 2D features to 3D volumes, the 3D local and global encoder (LGE) for enhancing the 3D volumes and a decoding head to predict the semantic occupancy. (b) Detailed structure of the context and geometry aware voxel transformer. (c) Details of the Depth Net.", "description": "This figure shows the overall architecture of CGFormer, a neural network for semantic scene completion. It is divided into four parts: feature extraction, view transformation using a context and geometry aware voxel transformer (CGVT), a 3D local and global encoder (LGE) for enhancing the 3D features, and a decoding head to output the final semantic occupancy prediction. The CGVT module includes context-aware query generation and 3D deformable cross and self-attention mechanisms. The depth net refines depth estimation using information from stereo depth estimation.  The figure provides detailed schematics of each component, including the CGVT and the depth net.", "section": "3 CGFormer"}, {"figure_path": "9bu627mTfs/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of the sampling locations for different small objects. The yellow dot represents the query point, while the red dots indicate the locations of the deformable sampling points. The sampling points of the context-dependent query (a) tend to be distributed within the regions of interest. Beneficial from this, CGFormer achieve better performance than previous methods.", "description": "This figure visualizes how the proposed context-aware query generator in CGFormer focuses the sampling points of the deformable cross-attention on the regions of interest for individual input images, enhancing the accuracy and efficiency of feature aggregation compared to context-independent methods.", "section": "3.2 View Transformation"}, {"figure_path": "9bu627mTfs/figures/figures_6_1.jpg", "caption": "Figure 2: Schematics and detailed architectures of CGFormer. (a) The framework of the proposed CGFormer for camera-based semantic scene completion. The pipeline consists of the image encoder for extracting 2D features, the context and geometry aware voxel (CGVT) transformer for lifting the 2D features to 3D volumes, the 3D local and global encoder (LGE) for enhancing the 3D volumes and a decoding head to predict the semantic occupancy. (b) Detailed structure of the context and geometry aware voxel transformer. (c) Details of the Depth Net.", "description": "This figure shows the architecture of CGFormer, a novel neural network for semantic scene completion. It illustrates the four main components: feature extraction, view transformation, 3D local and global encoder, and a decoding head.  It details the context and geometry-aware voxel transformer (CGVT), which lifts 2D image features into 3D volumes while considering individual image contexts and geometry. Finally, it provides a close-up of the Depth Net module.", "section": "3 CGFormer"}, {"figure_path": "9bu627mTfs/figures/figures_6_2.jpg", "caption": "Figure 2: Schematics and detailed architectures of CGFormer. (a) The framework of the proposed CGFormer for camera-based semantic scene completion. The pipeline consists of the image encoder for extracting 2D features, the context and geometry aware voxel (CGVT) transformer for lifting the 2D features to 3D volumes, the 3D local and global encoder (LGE) for enhancing the 3D volumes and a decoding head to predict the semantic occupancy. (b) Detailed structure of the context and geometry aware voxel transformer. (c) Details of the Depth Net.", "description": "This figure shows the overall architecture of CGFormer, a novel neural network for semantic scene completion.  It is broken down into four main parts: feature extraction, view transformation, 3D local and global encoding, and the decoding head. The view transformation module is further detailed, showing its use of a context-aware query generator and deformable 3D attention. Finally, the details of the depth estimation network are also shown.", "section": "3 CGFormer"}, {"figure_path": "9bu627mTfs/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative visualization results on the SemanticKITTI [1] validation set.", "description": "The figure qualitatively compares the scene completion results of MonoScene [3], VoxFormer [23], OccFormer [59], and CGFormer (ours) on the SemanticKITTI validation set.  Each row shows a different scene, with the input RGB image and the predicted semantic scene completion from each method, alongside the ground truth.  The red boxes highlight areas where the methods differ significantly, illustrating CGFormer's improved accuracy and detail in semantic prediction, especially regarding the clearer depiction of geometric structures and smaller objects compared to the other methods.", "section": "4.3 Qualitative Results"}, {"figure_path": "9bu627mTfs/figures/figures_15_1.jpg", "caption": "Figure 2: Schematics and detailed architectures of CGFormer. (a) The framework of the proposed CGFormer for camera-based semantic scene completion. The pipeline consists of the image encoder for extracting 2D features, the context and geometry aware voxel (CGVT) transformer for lifting the 2D features to 3D volumes, the 3D local and global encoder (LGE) for enhancing the 3D volumes and a decoding head to predict the semantic occupancy. (b) Detailed structure of the context and geometry aware voxel transformer. (c) Details of the Depth Net.", "description": "This figure shows the overall architecture of CGFormer, a novel neural network for semantic scene completion. It illustrates the four main parts of the network: feature extraction from 2D images, view transformation (lifting 2D features to 3D volumes using CGVT), 3D local and global encoding (enhancing 3D features using LGE), and a decoding head for semantic occupancy prediction.  Sub-figures (b) and (c) provide detailed breakdowns of the CGVT and Depth Net components, respectively.", "section": "3 CGFormer"}, {"figure_path": "9bu627mTfs/figures/figures_15_2.jpg", "caption": "Figure 5: Visualization of the sampling locations for different small objects. The yellow dot represents the query point, while the red dots indicate the locations of the deformable sampling points. The sampling points of the context-dependent query (a) tend to be distributed within the regions of interest. Beneficial from this, CGFormer achieve better performance than previous methods.", "description": "This figure visualizes how the proposed context-aware query generator in CGFormer effectively focuses on the region of interest when aggregating features. It compares the sampling points of context-independent queries (like in VoxFormer) against the context-dependent queries in CGFormer, highlighting the improved feature aggregation and performance due to the focus on relevant areas.", "section": "3.2 View Transformation"}, {"figure_path": "9bu627mTfs/figures/figures_16_1.jpg", "caption": "Figure 6: More qualitative comparison results on the SemanticKITTI [1] validation set.", "description": "This figure shows a qualitative comparison of semantic scene completion results on the SemanticKITTI validation set.  It compares the output of four different methods: MonoScene [3], VoxFormer [23], OccFormer [59], and the proposed CGFormer, against the ground truth. Each row represents a different scene, and each column represents the output from a different method. The figure visually demonstrates the strengths and weaknesses of each method in terms of accuracy, detail, and overall scene understanding. CGFormer aims to outperform other methods by providing clearer geometric structures and improved semantic discrimination.", "section": "4.3 Qualitative Results"}, {"figure_path": "9bu627mTfs/figures/figures_17_1.jpg", "caption": "Figure 3: Visualization of the sampling locations for different small objects. The yellow dot represents the query point, while the red dots indicate the locations of the deformable sampling points. The sampling points of the context-dependent query (a) tend to be distributed within the regions of interest. Beneficial from this, CGFormer achieve better performance than previous methods.", "description": "This figure visualizes how the context-aware query generator in CGFormer focuses the attention on the region of interest.  It compares the sampling locations of context-independent queries (as used in previous methods) with the context-dependent queries introduced by CGFormer. The context-dependent queries' sampling points are concentrated in the relevant areas, leading to improved feature aggregation and better performance in semantic scene completion.", "section": "3.2 View Transformation"}]