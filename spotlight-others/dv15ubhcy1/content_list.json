[{"type": "text", "text": "Are Language Models Actually Useful for Time Series Forecasting? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingtian Tan Mike A. Merrill Vinayak Gupta University of Virginia University of Washington University of Washington wtd3gz@virginia.edu mikeam@cs.washington.edu vinayak@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Tim Althoff University of Washington althoff@cs.washington ", "page_idx": 0}, {"type": "text", "text": "Thomas Hartvigsen University of Virginia hartvigsen@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) are being applied to time series forecasting. But are language models actually useful for time series? In a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance\u2014in most cases, the results even improve! We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series analysis is a critical problem across many domains, including disease propagation forecasting [8], retail sales analysis [3], healthcare [26, 17] and finance [31]. A great deal of recent work in time series analysis (constituting repositories with more than 1200 total stars on GitHub) has focused on adapting pretrained large language models (LLMs) to classify, forecast, and detect anomalies in time series [15, 50, 22, 4, 5, 32, 14, 44, 16]. These papers posit that language models, being advanced models for sequential dependencies in text, may generalize to the sequential dependencies in time series data. This hypothesis is unsurprising given language models are now pervasive in machine learning research. However, direct connections between language modeling and time series forecasting remain largely undefined. So to what extent is language modeling really beneficial for traditional time series tasks? ", "page_idx": 0}, {"type": "text", "text": "Our claim is simple but profound: popular LLM-based time series forecasters perform the same or worse than basic LLM-free ablations, yet require orders of magnitude more compute. Derived from extensive ablations, this reveals a worrying trend in contemporary time series forecasting literature. Our goal is not to imply that language models will never be useful for time series. In fact, recent works point to many exciting and promising ways that language and time series interact, like time series reasoning [25, 7, 45, 42, 37], social understanding [6] and financial reasoning [36, 20]. Rather, we aim to highlight surprising findings that existing methods do very little to use the innate reasoning power of pretrained language models on established time series tasks. ", "page_idx": 0}, {"type": "text", "text": "We substantiate our claim by performing three ablations of three popular and recent LLM-based forecasting methods [50, 15, 22] using eight standard benchmark datasets from reference methods and another five datasets from MONASH [13]. First, we successfully reproduce results from the original publications. Then, we show that replacing language models with simple attention layers, basic transformer blocks, randomly-initialized language models, and even removing the language model entirely, yields comparable or better performance. The same performance was observed on another five datasets that were not studied by the reference methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Next, we compare the training and inference speed of these methods against their ablations, showing that these simpler methods reduce training and inference time by up to three orders of magnitude while maintaining comparable performance. Then, to investigate the source of LLM forecaster\u2018s performance, we further explore time series encoders. We find that a simple linear model with an encoder composed of patching and attention can achieve forecasting performance similar to that of LLMs. Next, we test whether the sequence modeling capabilities of LLMs transfer to time series by shuffling input time series and find no appreciable change in performance. Finally, we show that LLMs do not even help forecasting in few-shot settings with $10\\%$ of the training data. We discuss the implications of our findings and suggest that time series methods that use large language models are better left to multimodal applications [4, 12, 38] that require textual reasoning. ", "page_idx": 1}, {"type": "text", "text": "The key contributions we make in this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose three straightforward ablation methods for methods that pass time series into LLMs for forecasting. We then ablate three top-tier methods on thirteen standard datasets and find that LLMs fail to convincingly improve time series forecasting. However, they significantly increase computational costs in both training and inference.   \n\u2022 We study the impact of an LLM\u2019s pretraining by re-initializing their weights prior to forecasting. We find that this has no impact on forecasting performance. Additionally, in shuffling input time series, we find no evidence the LLMs successfully transfer sequence modeling abilities from text to time series and no indication they help in few-shot settings.   \n\u2022 We find a very simple model, with patching and attention as encoder, can achieve performance similar to LLMs. This suggests a massive gap between the beneftis LLMs pose and the time series forecasting problem, despite a rapid rush to adopt LLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Here, we summarize the key works relevant to LLM-based time series models. They can be broadly classified into three sections: (i) time series forecasting using LLMs; (ii) encoders in LLM time series models; and (iii) smaller and efficient neural models for time-series. ", "page_idx": 1}, {"type": "text", "text": "Time Series Forecasting Using LLMs. Recently, with the development of Large Language Models (LLMs) [10, 29, 34] and their demonstrated multi-modal capabilities, more researchers have successfully applied LLMs to time series forecasting tasks [14, 16, 5, 4]. Chang et al., [5] used finetuning the transformer module and positional encoding in GPT-2 to align pre-trained LLMs with time series data for forecasting tasks. Zhou et al. [50] proposed a similar finetuning method, named \u201cOneFitsAll\u201d, for time series forecasting with GPT-2. Additionally, Jin et al. [15] introduced a reprogramming method to align LLM\u2019s Word Embedding with time series embeddings, showing good representation of time series data on LLaMA [34]. Similarly, CALF [22] and TEST [32] adapted word embeddings to enable LLMs to forecast time series data effectively. In addition to time-series forecasting models, Liu et al. [23] show that these models can be extended to classifying health-time series, such as heart-rate and daily-footsteps. These models have also been shown to outperform supervised neural models in few-shot settings. ", "page_idx": 1}, {"type": "text", "text": "Encoders in LLM Time Series Models. In order for an LLM to learn from text it must first be discretized and encoded as word tokens which are $1\\times d$ vectors [10, 29, 34]. Similarly, LLM-based methods for time series learn discrete time series tokens. One method is to segment the time series into overlapping patches, which effectively shortens the time series while retaining its features [15, 50, 5, 4, 28, 27, 11]. Other methods decompose time series into trend, seasonal components, and residual components [4, 28]. Lastly, Liu et al. [22] feed the multivariate time series using a Transformer to enable different channels to learn the dynamics of other channels. These embedding procedures are followed by a linear neural network layer that projects the time series encoding to the same dimensions used by the pre-trained LLM. ", "page_idx": 1}, {"type": "table", "img_path": "DV15UbHCY1/tmp/a654d7b6988a8cf3d49aeb3884e4b00f4b66c3ba556ba878bb3170953fe006ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "DV15UbHCY1/tmp/4a3b848125f89f7a67d2301636de7288556bc336ff5d9a81981c1855b9d12f35.jpg", "table_caption": ["Table 1: Statistics for all datasets used in reference methods [50, 22, 15]. ", "Table 2: Three popular methods for time series forecasting with Large Language Models. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Small and Efficient Neural Forecasters. In addition to LLMs, there has been a large body of research on smaller yet efficient frameworks that outperform their bulky counterparts in time series forecasting [19, 47, 33, 24, 2]. For example, Zeng et al. [46] present DLinear, an incredibly simple model that combines decomposition techniques and achieves better forecasting performance than state-of-the-art transformer-based time series architectures at the time, such as Informer [48], FEDformer [49], and Autoformer [40]. Furthermore, Xu et al. [43] introduces a lightweight model with only 10k parameters, which captures both amplitude and phase information in the time-series to outperform transformer-based models. ", "page_idx": 2}, {"type": "text", "text": "3 Experimental Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use three state-of-the-art methods for time series forecasting and propose three ablation methods for LLMs: (i) \u201cw/o LLM\u201d; (ii) \u201cLLM2Attn\u201d; (iii) and \u201cLLM2Trsf\u201d. To evaluate the effectiveness of LLMs in time series forecasting, we test these methods on eight standard datasets. ", "page_idx": 2}, {"type": "text", "text": "3.1 Reference Methods for Language Models and Time Series ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We experiment with three recent methods for time series forecasting using LLMs. All models were published between December 2023 and May 2024 and are popular, with their GitHub repositories collectively amassing 1,245 stars. These methods are summarized in Table 2, and use either GPT2 [29] or LLaMA [34] as base models, with different alignment and fine-tuning strategies. ", "page_idx": 2}, {"type": "text", "text": "\u2022 OneFitsAll [50]: OneFitsAll, sometimes called GPT4TS, applies instance norm and patching to the input time series and then feeds it into a linear layer to obtain a input representation for the language model. The multi-head attention and feed forward layers of the language model are frozen while the positional embeddings and layer norm are optimized during training. A final linear layer is used to transform the language model\u2019s final hidden states into a prediction. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Time-LLM [15]: In Time-LLM the input time series is tokenized via patching and aligned with a low-dimensional representation of word embeddings using multi-head attention. The outputs of this alignment, combined with the embeddings of descriptive statistical features, are passed to a frozen pre-trained language model. The output representations of the language model are then flattened and passed through a linear layer to obtain a forecast. ", "page_idx": 2}, {"type": "text", "text": "\u2022 CALF [22]: CALF embeds the input time series by treating each channel as a token. One half of the architecture is a \u201ctextual branch\u201d which uses cross attention to align the time series representation with a low dimensional representation of the language model\u2019s word embeddings. This representation is then passed through a pretrained, frozen language model to obtain a \u201ctextual prediction\u201d. Simultaneously, a \u201ctemporal\u201d branch learns a low-rank adapter for a pretrained language model based on the input time series to produce a \u201ctemporal prediction\u201d which is used for inference. The model includes additional loss terms that enforce similarity between these representations. ", "page_idx": 2}, {"type": "text", "text": "Reproducibility Note. While experimenting with each model, we tried to replicate the conditions of their original papers. We used the original hyper-parameters, runtime environments, and code, including model architectures, training loops, and data-loaders. To ensure a fair comparison, we have included error metrics from the original papers alongside our results wherever possible. ", "page_idx": 2}, {"type": "image", "img_path": "DV15UbHCY1/tmp/746966cdb7c56c381ed160ca6597f773ba62ba20ad156cac177ae02b701b6472.jpg", "img_caption": ["Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15, 14], while in others, they undergo fine-tuning [50, 22, 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Proposed Ablations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To isolate the influence of the LLM in an LLM-based forecaster, we propose three ablations: removing the LLM component or replacing it with a simple block. Specifically, for each of the three methods we make the following three modifications: ", "page_idx": 3}, {"type": "text", "text": "\u2022 w/o LLM (Figure 1 (b)). We remove the language model entirely, instead passing the input tokens directly to the reference method\u2019s final layer.   \n\u2022 LLM2Attn (Figure 1 (c)). We replace the language model with a single randomly-initialized multi-head attention layer.   \n\u2022 LLM2Trsf (Figure 1 (d)). We replace the language model with a single randomly-initialized transformer block. ", "page_idx": 3}, {"type": "text", "text": "In the above ablations, we keep left parts of the forecasters unchanged (trainable). For example, as shown in Figure 1 (a), after removing the LLM, the input encodings are passed directly to the output projection. Alternatively, as shown in Figure 1 (b) or (c), after replacing the LLM with attention or a transformer, they are trained along with the remaining structure of the original method. ", "page_idx": 3}, {"type": "text", "text": "3.3 Datasets and Evaluation Metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Benchmark Datasets. We evaluate on the following real-world datasets: (1) ETT [21]: encompasses seven factors related to electricity transformers across four subsets: ETTh1 and ETTh2, which have hourly recordings, and ETTm1 and ETTm2, which have recordings every 15 minutes; (2) Illness [40]: includes the weekly recorded influenza illness among patients from the Centers for Disease Control, which describes the ratio of patients seen with influenza-like illness to the total number of patients; (3) Weather [40]: local climate data from 1,600 U.S. locations, between 2010 and 2013, and each data point consists of 11 climate features; (4) Traffic [40]: is an hourly dataset from California transportation department, and consists of road occupancy rates measured on San Francisco Bay area freeways; (5) Electricity [35]: contains the hourly electricity consumption of 321 customers from 2012 to 2014. The train-val-test split for ETT datasets is $60\\dot{\\%}{-}20\\%{-}2\\bar{0}\\%$ , and for Illness, Weather, and Electricity datasets is $70\\%{-}10\\%{-}20\\%$ respectively. The statistics for all datasets is given in Table 1. We highlight that these datasets, with the same splits and size, have been extensively used to evaluate time-series forecasting ability of LLM-based and other neural models for time-series data [48, 50, 4, 15, 5, 46, 40, 49]. (6) Exchange Rate [18]: collected between 1990 and 2016, it contains daily exchange rates for the currencies of eight countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore). (7) Covid Deaths [13]: contains daily statistics of COVID-19 deaths in 266 countries and states between January and August 2020. (8) Taxi $\\mathbf{(30\\min)}$ [1]: contains taxi rides from 1,214 locations in New York City between January 2015 and January 2016. The data is collected every 30 minutes, with an average of 1,478 samples. (9) NN5 (Daily) [13]: contains daily cash withdrawal data from 111 ATMs in the UK, with each ATM having ", "page_idx": 3}, {"type": "image", "img_path": "DV15UbHCY1/tmp/04eb672a26aeb4d3dac99e144401ae398c1c5f2c233c9532f7634b0cf27bfdaa.jpg", "img_caption": ["Figure 2: In the above examples, only OneFitsAll \u201cw/ LLM\u201d performs better than the ablation methods on ETTh1, but there is substantial overlap in bootstraped confidence intervals. The figures show the comparison of OneFitsAll, CALF, and Time-LLM using LLMs and ablations (i.e., w/o LLM, LLM2Attn, and LLM2Trsf) on ETTh1, ETTm2, and Electricity, and the vertical dashed lines represent the results from the original work. Others Figures for MSE and other datasets are available in Figure 5 and Figure 6 in the Appendix. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "791 data points. (10) FRED-MD [13]: contains 107 monthly macroeconomic indices released by the Federal Reserve Bank since 01/01/1959. It was extracted from the FRED-MD database. ", "page_idx": 4}, {"type": "text", "text": "Evaluation Metrics and Setup. We report the results in terms of mean absolute error (MAE) and mean squared error (MSE) between predicted and true values of the time-series. Mathematically, given a test-set with $\\mathcal{D}$ elements, $\\begin{array}{r}{\\mathrm{MAE}\\stackrel{\\cdot}{=}\\frac{1}{|\\mathcal{D}|}\\sum_{t_{i}\\in\\mathcal{D}}[|c_{i}-\\widehat{c}_{i}|]}\\end{array}$ and $\\begin{array}{r}{\\mathrm{MSE}=\\frac{1}{|\\mathcal{D}|}\\sum_{t_{i}\\in\\mathcal{D}}(c_{i}-\\widehat{c}_{i})^{2}}\\end{array}$ , where $c_{i}$ and ${\\widehat{c}}_{i}$ denote the true value and predicted value at the $i$ -th index of the time-series respectively. ", "page_idx": 4}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide the details of our comprehensive evaluation of all baseline LLM models for time-series forecasting. Specifically, we ask the following research questions. (RQ1) Do pretrained language models contribute to forecasting performance? (RQ2) Are LLM-based methods worth the computational cost? (RQ3) Does language model pretraining help performance on forecasting tasks? (RQ4) Do LLMs represent sequential dependencies in time series? (RQ5) Do LLMs help with few-shot learning? (RQ6) Where does the performance come from? ", "page_idx": 4}, {"type": "text", "text": "4.1 Do pretrained language models contribute to forecasting performance? (RQ1) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our results show that pretrained LLMs are not useful for time series forecasting tasks yet. Overall, as shown in Table 3, across 13 datasets and two metrics, ablations out perform Time-LLM methods in $26/26$ cases, CALF in $22/26$ cases, and OneFitsAll in $19/26$ cases. We averaged results over different predicting lengths, as in [50, 15, 22]. Across all prediction lengths (thirteen datasets and four prediction lengths) ablations outperformed Time-LLM, CALF, and OneFitsAll in $35/40,31/40$ , and $\\bar{2}9/40$ cases as measured by MAE, respectively. To ensure a fair comparison, we also report results from each method\u2019s original paper alongside our replication. For specific results refer to Appendix E.1. To better evaluate the effectiveness of LLMs and ablation methods, we include $95\\%$ bootstrapped confidence intervals for each task. In tasks where LLMs performed better, such as OneFitsAll with ETTh1, shown in Figure 2, there is still substantial overlap in the confidence intervals with the ablation method \u201cw/o LLM\u201d in MAE. Other datasets results and MSE metrics are shown in Figure 5 and Figure 6 in the Appendix. To summarize, our results on the evaluation above, it is hard to conclude that LLMs are effective in time series forecasting. ", "page_idx": 4}, {"type": "text", "text": "4.2 Are LLM-based methods worth the computational cost? (RQ2) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we showed that LLMs do not meaningfully improve performance on time series forecasting tasks. Here, we evaluate the computational intensity of these methods with their ", "page_idx": 4}, {"type": "table", "img_path": "DV15UbHCY1/tmp/619ecfeec367599ba834c177272696fac0de4e4579ff4f6ab639ae285b696563.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper. ", "page_idx": 5}, {"type": "text", "text": "nominal performance in mind. The language models in our reference methods use hundreds of millions and sometimes billions of parameters to perform time series forecasting. Even when the parameters of the language models are frozen they still contribute to substantial overhead during training and inference. For instance, Time-LLM has $6642\\,\\mathrm{M}$ parameters and takes 3003 minutes to train on the Weather dataset whereas ablation methods have only $0.245\\;\\mathrm{M}$ parameters and take 2.17 minutes on average. Information about training other methods on ETTh1 and Weather datasets are shown in Table 4. In the case of inference time, we divide by the maximum batch size to give an estimate of inference time per example. Time-LLM, OneFitsAll, and CALF take, on average, 28.2, ", "page_idx": 5}, {"type": "image", "img_path": "DV15UbHCY1/tmp/946dbe5e61f5daed6ac62c22b273b533781f659000c296a2106a23af990d290d.jpg", "img_caption": ["Figure 3: Ablation methods consume less time for inference while providing better forecasting performance. The figure above shows the inference time and prediction accuracy of Time-LLM, OneFitsAll, and CALF on ETTm2, Traffic, and Electricity datasets, averaged across prediction lengths. For more datasets and MSE metrics refer to Figure 7 and Figure 8 in the Appendix. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "DV15UbHCY1/tmp/467f020e4c5769956a23cb363f596f6037826fc5332b58ad1b85f34d0a7e0530.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 4: In time series tasks, LLM (LLaMA and GPT-2) significantly increases training time. The table shows the number of model parameters (in millions) and total training time (in minutes) for three methods predicting over a length of 96 on ETTh1 and Weather data. Compared with original method \u201cw/ LLM\u201d are \u201cw/o LLM\u201d, \u201cLLM2Attn\u201d and \u201cLLM2Trsf\u201d. ", "page_idx": 6}, {"type": "text", "text": "2.3, and 1.2 times longer than the modified models. Examples can be seen in Figure 3, where the green marks (ablation methods) are typically below the red one (LLM) and are positioned towards the left of the axis, indicating a lower computational costs and better forecasting performance. Other datasets and MSE metric refer to Figure 7 and Figure 8 in Appendix. In conclusion, the computational intensity of LLMs in time series forecasting tasks does not result in a corresponding performance improvement. ", "page_idx": 6}, {"type": "text", "text": "4.3 Does language model pretraining help performance on forecasting tasks? (RQ3) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our evaluation in this section indicates that pretraining with language datasets is unnecessary for time series forecasting. To test whether the knowledge learned during pretraining meaningfully improves forecasting performance we experimented with different combinations of pretraining and finetuning CALF\u2019s [22] language model on time series. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Pretrain $^+$ Finetune $(\\mathbf{Pre+FT})$ . This is the original method, wherein a pretrained language model is finetuned on time series data. In the case of CALF, the base language model is frozen and low rank adapters (LoRA) are learned. \u2022 Random Initialization $^+$ Finetune $\\mathbf{\\Delta}\\mathbf{WoPre+FT})$ ). Does the textual knowledge from pretraining aid time series forecasting? In this method we randomly initialize the weights of the language model (thereby erasing any effect of pretraining) and train the LLM from scratch. \u2022 Pretrain $\\mathbf{+\\DeltaNo}$ Finetuning (Pre+woFT). How much does finetuning on time series improve prediction performance? For this baseline we again leave the language model frozen and forgo ", "page_idx": 6}, {"type": "table", "img_path": "DV15UbHCY1/tmp/c59223802b81572a9e0faa3493118ab9caf7931e82d7e05a3bc2a9a3596b1aa8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "DV15UbHCY1/tmp/4aff9927b858122e907585b4f38cf5d5f175e5825e75607b3495f0d80dba9c1b.jpg", "table_caption": ["Table 5: Randomly initializing LLM parameters and training from scratch (woPre) achieved better results than using a pretrained (Pre) model. \u201cwoFT\u201d and \u201cFT\u201d refer to whether the LLM parameters are frozen or trainable. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 6: For the input shuffling/masking experiments on ETTh1 (predict length is 96) and Illness (predict length is 24), the impact of shuffling the input on the degradation of time series forecasting performance does not change significantly before and after model modifications. Results of other predict lengths refer to table 21 in Appendix. ", "page_idx": 7}, {"type": "text", "text": "learning LoRAs. Results from this model are therefore indicative of the base language model\u2019s performance without additional guidance on processing time series. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Random Initialization $\\mathbf{+\\DeltaN_{0}}$ Finetuning (woPre+woFT). This baseline is effectively a random projection from the input time series to a forecasting prediction and serves as a baseline comparison with the other methods. ", "page_idx": 7}, {"type": "text", "text": "Overall, as shown in Table 5, across 8 datasets using MAE and MSE metrics, the \"Pretraining $^+$ Finetune\" method performed the best 3 times, while \"Random Initialization $^+$ Finetune\" achieved this 8 times. This indicates that language knowledge offers very limited help for forecasting. However, \"Pretrain $+\\,\\mathrm{No}$ Finetuning\" and the baseline \"Random Initialization $+\\,\\mathrm{No}$ Finetuning\" performed the best 5 times and 0 times, respectively, suggesting that Language knowledge does not contribute meaningfully during the finetuning process. Detailed results refer to Table 20 in Appendix. ", "page_idx": 7}, {"type": "text", "text": "In summary, textual knowledge from pretraining provides very limited aids for time series forecasting. ", "page_idx": 7}, {"type": "text", "text": "4.4 Do LLMs represent sequential dependencies in time series? (RQ4) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Most time series forecasting methods that use LLMs finetune the positional encoding to help understand the position of time steps in the sequence [4, 50, 22, 5, 32]. We would expect a time series model with good positional representations to show a significant drop in predictive performance when the input is shuffled [46]. We applied three types of shuffling to the time series: shuffling the entire sequence randomly (\"sf-all\"), shuffling only the first half of the sequence (\"sf-half\"), and swapping the first and second halves of the sequence (\"ex-half\"). As shown in Table 6, LLM-based methods were no more vulnerable to input shuffling than their ablations. This implies that LLMs do not have unique capabilities for representing sequential dependencies in time series. ", "page_idx": 7}, {"type": "table", "img_path": "DV15UbHCY1/tmp/f6f5af639e79b254425542474e1bb8205418ddacaad9d1a387aff9c587381dd7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "DV15UbHCY1/tmp/2a636451cd79515c3cea58296e0150e80191d2a7300dd4e82f984abfb30ac4ca.jpg", "table_caption": ["Table 7: In few-shot scenarios ( $10\\%$ dataset), LLaMA (Time-LLM) performs similarly to the ablation methods. LLaMA and \u201cw/o LLM\u201d each outperformed the other 8 times. Note that the results of Time-LLM is from the original paper [15]. ", "Table 8: In few-shot scenarios ( $10\\%$ dataset), Ablation methods perform much better than GPT-2 (CALF). Without LLMs, 12 out of 14 cases showed better performance. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Do LLMs help with few-shot learning in forecasting? (RQ5) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, our evaluation demonstrates that LLMs are still not meaningfully useful in few-shot learning scenarios. ", "page_idx": 8}, {"type": "text", "text": "While our results indicate that LLMs are not useful for time series forecasting, it is nonetheless possible that knowledge encoded in pretrained weights could help performance in few-shot settings where data are scarce. To evaluate whether this is the case we trained models and their ablations on $10\\%$ of each dataset. Specifically, we evaluated LLaMA in Time-LLM methods. The results for LLaMA, shown in Table 7, compared LLaMA with completely removing the LLM (w/o LLM). There was no difference, with each performing better in 8 cases. We conducted similar experiments with CALF, a GPT-2-based method. Our results in Table 8 indicate that our ablations can perform better than LLMs in few-shot scenarios. ", "page_idx": 8}, {"type": "text", "text": "4.6 Where does the performance come from? (RQ6) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate common encoding techniques used in LLM time series models. We find that combining patching with one-layer attention is a simple and effective choice. ", "page_idx": 8}, {"type": "text", "text": "In subsection 3.2, we found that simple ablations of LLM-based methods did not decrease performance. To understand why such simple methods work so well we selected some popular techniques used for encoding in LLM time series tasks, such as patching [50, 15, 5, 32, 22, 11], decomposition [4, 28]. A basic transformer block also can be used to aid in encoding [22]. ", "page_idx": 8}, {"type": "text", "text": "The specific results, shown in Table 18 in the Appendix, indicate that a structure combining patching and attention, named \u201cPAttn\u201d, performs better than most other encoding methods on small datasets (with time stamps less than 1 million) and is even comparable to LLM methods. Its detailed structure, as shown in Figure 4, involves applying \"instance norm\" to the time series, followed by patching and projection. Then, one-layer attention enables feature learning between patches. For larger datasets, such as Traffic (\\~15 million) and Electricity (\\~8 million), a model named \"LTrsf,\" using the encoder from CALF [22], performs better. In those methods, finally, time series embedding will be projected with a single linear layer to forecast. Details of other encoders is in Appendix subsection D.3. ", "page_idx": 8}, {"type": "image", "img_path": "DV15UbHCY1/tmp/b0276d4c1657213e600462ab79e083668296ba48c8cd9e0489d7bf356e0da3d6.jpg", "img_caption": ["Figure 4: PAttn Model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Overall, patching plays a crucial role in encoding. Additionally, basic Attention and Transformer blocks also effectively aid in encoding. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we showed that despite the recent popularity of LLMs in time series forecasting they do not appear to meaningfully improve performance. We experimented with simple ablations, showing that they maintain or improve the performance of the LLM-based counterparts while requiring considerably less compute. Once more, our goal is not to suggest that LLMs have no place in time series analysis. To do so would likely prove to be a shortsighted claim. Rather, we suggest that the community should dedicate more focus to the exciting tasks could be unlocked by LLMs at the interface of time series and language such as time series reasoning [25, 7, 45, 37], or social understanding [6]. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the University of Virginia\u2019s Research Computing team for maintaining the excellent high-performance computing resources that allowed us to conduct this research. This research was supported in part by NSF IIS-1901386, NSF CAREER IIS-2142794, NIH R01MH125179, the Bill & Melinda Gates Foundation (INV-004841), the Office of Naval Research (#N00014-21-1-2154), and the National Security Data & Policy Institute, Contracting Activity #2024-24070100001. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. F. Ansari, L. Stella, C. Turkmen, X. Zhang, P. Mercado, H. Shen, O. Shchur, S. S. Rangapuram, S. P. Arango, S. Kapoor, J. Zschiegner, D. C. Maddix, H. Wang, M. W. Mahoney, K. Torkkola, A. G. Wilson, M. Bohlke-Schneider, and Y. Wang. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.   \n[2] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharif,i D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, et al. Audiolm: a language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.   \n[3] J.-H. B\u00f6se, V. Flunkert, J. Gasthaus, T. Januschowski, D. Lange, D. Salinas, S. Schelter, M. Seeger, and Y. Wang. Probabilistic demand forecasting at scale. VLDB, 2017.   \n[4] D. Cao, F. Jia, S. O. Arik, T. Pfister, Y. Zheng, W. Ye, and Y. Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting. In ICLR, 2024.   \n[5] C. Chang, W.-C. Peng, and T.-F. Chen. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.   \n[6] J. Cheng and P. Chin. Sociodojo: Building lifelong analytical agents with real-world text and time series. In ICLR, 2024.   \n[7] W. Chow, L. Gardiner, H. T. Hallgr\u00edmsson, M. A. Xu, and S. Y. Ren. Towards time series reasoning with llms. arXiv preprint arXiv:2409.11376, 2024. [8] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \n[9] A. Das, W. Kong, R. Sen, and Y. Zhou. A decoder-only foundation model for time-series forecasting. 2024.   \n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[11] S. Gao, T. Koker, O. Queen, T. Hartvigsen, T. Tsiligkaridis, and M. Zitnik. UniTS: Building a unified time series model. In NeurIPS, 2024.   \n[12] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023.   \n[13] R. Godahewa, C. Bergmeir, G. I. Webb, R. J. Hyndman, and P. Montero-Manso. Monash time series forecasting archive. In NeurIPS, 2021.   \n[14] N. Gruver, M. Finzi, S. Qiu, and A. G. Wilson. Large language models are zero-shot time series forecasters. In NeurIPS, 2023.   \n[15] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen, Y. Liang, Y.-F. Li, S. Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. In ICLR, 2024.   \n[16] M. Jin, Y. Zhang, W. Chen, K. Zhang, Y. Liang, B. Yang, J. Wang, S. Pan, and Q. Wen. Position paper: What can large language models tell us about time series analysis. In ICML, 2024.   \n[17] M. J\u00f6rke, S. Sapkota, L. Warkenthien, N. Vainio, P. Schmiedmayer, E. Brunskill, and J. Landay. Supporting physical activity behavior change with llm-based conversational agents. arXiv preprint arXiv:2405.06061, 2024.   \n[18] G. Lai, W.-C. Chang, Y. Yang, and H. Liu. Modeling long-and short-term temporal patterns with deep neural networks. In SIGIR, 2018.   \n[19] S. Lee, T. Park, and K. Lee. Learning to embed time series patches independently. In ICLR, 2024.   \n[20] Y. Li, B. Luo, Q. Wang, N. Chen, X. Liu, and B. He. A reflective llm-based agent to guide zero-shot cryptocurrency trading. In EMNLP, 2024.   \n[21] Z. Li, S. Qi, Y. Li, and Z. Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. arXiv preprint arXiv:2305.10721, 2023.   \n[22] P. Liu, H. Guo, T. Dai, N. Li, J. Bao, X. Ren, Y. Jiang, and S.-T. Xia. Calf: Aligning llms for time series forecasting via cross-modal fine-tuning. arXiv preprint arXiv:2403.07300, 2024.   \n[23] X. Liu, D. McDuff, G. Kovacs, I. Galatzer-Levy, J. Sunshine, J. Zhan, M.-Z. Poh, S. Liao, P. Di Achille, and S. Patel. Large language models are few-shot health learners. arXiv preprint arXiv:2305.15525, 2023.   \n[24] Y. Liu, T. Hu, H. Zhang, H. Wu, S. Wang, L. Ma, and M. Long. iTransformer: Inverted transformers are effective for time series forecasting. In ICLR, 2024.   \n[25] M. A. Merrill, M. Tan, V. Gupta, T. Hartvigsen, and T. Althoff. Language models still struggle to zero-shot reason about time series. In EMNLP, 2024.   \n[26] M. A. Morid, O. R. L. Sheng, and J. Dunbar. Time series prediction using deep learning methods in healthcare. ACM Trans. Manage. Inf. Syst., 14(1), 2023.   \n[27] Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In ICLR, 2023.   \n[28] Z. Pan, Y. Jiang, S. Garg, A. Schneider, Y. Nevmyvaka, and D. Song. $s^{2}$ ip-llm: Semantic space informed prompt learning with llm for time series forecasting. In Forty-first International Conference on Machine Learning, 2024.   \n[29] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[30] K. Rasul, A. Ashok, A. R. Williams, A. Khorasani, G. Adamopoulos, R. Bhagwatkar, M. Bilo\u0161, H. Ghonia, N. Hassen, A. Schneider, et al. Lag-llama: Towards foundation models for time series forecasting. In R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023.   \n[31] O. B. Sezer, M. U. Gudelek, and A. M. Ozbayoglu. Financial time series forecasting with deep learning: A systematic literature review: 2005\u20132019. Applied soft computing, 90, 2020.   \n[32] C. Sun, Y. Li, H. Li, and S. Hong. Test: Text prototype aligned embedding to activate llm\u2019s ability for time series. In ICLR, 2024.   \n[33] S. J. Talukder and G. Gkioxari. Time series modeling at scale: A universal representation across tasks and domains. 2023.   \n[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models (2023). arXiv preprint arXiv:2302.13971, 2023.   \n[35] A. Trindade. ElectricityLoadDiagrams20112014. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C58C86.   \n[36] S. Wang, T. Ji, L. Wang, Y. Sun, S.-C. Liu, A. Kumar, and C.-T. Lu. Stocktime: A time series specialized large language model architecture for stock price prediction. arXiv preprint arXiv:2409.08281, 2024.   \n[37] X. Wang, M. Feng, J. Qiu, J. Gu, and J. Zhao. From news to forecast: Iterative event reasoning in llm-based time series forecasting. In Neural Information Processing Systems, 2024.   \n[38] C. Wimmer and N. Rekabsaz. Leveraging vision-language models for granular market change prediction. arXiv preprint arXiv:2301.10166, 2023.   \n[39] G. Woo, C. Liu, A. Kumar, C. Xiong, S. Savarese, and D. Sahoo. Unified training of universal time series forecasting transformers. 2024.   \n[40] H. Wu, J. Xu, J. Wang, and M. Long. Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting. NeurIPS, 2021.   \n[41] H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023.   \n[42] Z. Xu, Y. Bian, J. Zhong, X. Wen, and Q. Xu. Beyond trend and periodicity: Guiding time series forecasting with textual cues. arXiv preprint arXiv:2405.13522, 2024.   \n[43] Z. Xu, A. Zeng, and Q. Xu. Fits: Modeling time series with $10k$ parameters. In ICLR, 2024.   \n[44] H. Xue and F. D. Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[45] W. Ye, Y. Zhang, W. Yang, L. Tang, D. Cao, J. Cai, and Y. Liu. Beyond forecasting: Compositional time series reasoning for end-to-end task execution, 2024. URL https: //arxiv.org/abs/2410.04047.   \n[46] A. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In AAAI, 2023.   \n[47] L. Zhao and Y. Shen. Rethinking channel dependence for multivariate time series forecasting: Learning from leading indicators. In ICLR, 2024.   \n[48] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI, 2021.   \n[49] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022.   \n[50] T. Zhou, P. Niu, L. Sun, R. Jin, et al. One fits all: Power general time series analysis by pretrained lm. In NeurIPS, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Are Language Models Actually Useful for Time Series Forecasting? (Appendix) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we discuss the limitations of our paper. ", "page_idx": 13}, {"type": "text", "text": "1. We evaluate the ability of LLMs using time-series forecasting. However, to get a better picture of how LLMs can work with time-series, this ability should be evaluated across other downstream tasks as well, such as time-series classification and question-answering.   \n2. Our evaluation is limited to only time-series datasets, i.e., sequences with even time-intervals. However, there also exists a large fraction of data in the form of non-uniform series, such as payment records, online purchases, etc. Understanding the ability of LLMs in forecasting non-uniform sequences is also necessary to verify our claim on the usefulness of LLMs for time-series data. ", "page_idx": 13}, {"type": "text", "text": "B Broader Societal Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "One of the major impacts our study will have is on the influx of models that use LLMs for modeling time-series. Our results will help researchers to not simply follow the trend of using LLMs in all applications, but to check their usability in detail. Specifically, these findings will help them determine if the LLM component is necessary and if the computational costs are reasonable for the specific setting. In addition to the research community, our findings on the better performance of smaller and simpler models will help develop scalable models that are easy to understand, interpret, and can be deployed cheaply in real-world applications. While we agree that a majority of our results are experimental and limited to selected datasets, we feel that these results will also help researchers narrow down their search space for better models in time-series forecasting, and not simply neglect the simpler models. ", "page_idx": 13}, {"type": "text", "text": "C License ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All our contributions will be released under the MIT License. ", "page_idx": 13}, {"type": "text", "text": "D Additional Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 System Configuration ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We train and evaluate each reference method and each architecture modification using the same device. For Time-LLM [15], applying LlaMA-7B [34], we use NVIDIA A100 GPU with 80GB memory. For other methods [50, 22], applying GPT-2 [29], we use NVIDIA RTX A6000 GPU with 48GB memory. Though an analysis of memory footprint is beyond scope of this research, we note that training the baselines in the absence of LLM can be done within smaller GPUs as well. ", "page_idx": 13}, {"type": "text", "text": "D.2 Baseline Hyper-Parameter Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "When reproducing the reference methods, we used the original repository\u2019s hyper-parameters and model structures. In the ablation study, due to the smaller model parameters, we adjusted the learning rate or increased the batch size in some cases. All other training details remained identical with the reference methods. The specific training details and hyper-parameters are provided in the code and run scripts of the repository2. Note that the training process and hyper-parameters for the simple methods can also be accessed via this link. ", "page_idx": 13}, {"type": "text", "text": "D.3 Details of Encoder Exploration and Simple Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To investigate the source of LLM method performance, we conducted further research on encoders. We used various encoders to encode time series data, followed by a linear layer to project the time series embeddings to the forecast. The encoder structure combining patching and attention is shown in Figure 4. The key difference between PAttn and PatchTST [27] is the absence of position embedding and the Feed Forward in PatchTST [27], or, more specifically, replacing the Transformer Encoder with a simple single-layer Attention structure. It could be a simple yet effective method to help evaluate the trade-off between cost and performance for newly proposed methods. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "In addition, we propose three different neural models (i) \u201cLTrsf\u201d: which performs better on larger datasets, uses CALF \u2019s [22] encoder without cross-modal attention. This could be a potential source of CALF \u2019s performance; (ii) \u201cD-LTrsf\u201d; and (iii) \u201cD-PAttn\u201d: both of them decompose the time series into three sub-sequences and forecast each using the above two methods respectively, and then linearly combine the results for the final forecast. Across our results in Table 18, we note that even simpler models significantly outperform LLM-based time-series models. In detail, the LLM-based models, all combined we able to appear 33 times as the best and the second-best performer. However, \u201cPAttn\u201d was outperforming them by appearing 34 times as the best and the second-best performer. In addition, the comparison between \u201cPAttn\u201d, \u201cLTrsf\u201d and other state-of-the-art methods is shown in Table 19. ", "page_idx": 14}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we present the results of additional experiments that also highlight the ability of LLMs in modeling time-series data. ", "page_idx": 14}, {"type": "text", "text": "E.1 Confidence Intervals for Forecasting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Since LLMs and deep learning models in general are probabilistic in nature, their predictions can vary across different runs and different random initializations. Thus, we report the confidence intervals (CIs) for all MAE and MSE predictions made by our baseline models. We report the CIs for MAE prediction by Time-LLM, CALF, OneFitsAll in Tables 9, 11, and 13, for MSE predictions in Tables 10, 12, and 14, respectively. Across all results, we note that the range of variation is quite small, and these intervals do not affect our observations. To illustrate the subtle differences more clearly, we present the visualized results in Figure 5 and Figure 6. ", "page_idx": 14}, {"type": "text", "text": "Confidence Intervals for other Datasets. To evaluate the generality of the ablations in the paper, we introduce five additional datasets that have not been studied by the reference methods [50, 15, 22]. The above datasets are used in many time series forecasting studies [30, 9, 39, 1, 46]. The prediction lengths for the \u201cExchange Rate\u201d are \"96, 192, 336, 720\", as in [46, 18]. The prediction lengths for the other four datasets are 30, 48, 56, and 12, respectively, following the settings in Chronos [1]. As shown in Tables 15, 16, and 17, the forecasting performance on the five new datasets, using the three methods [50, 15, 22] we referenced, still demonstrates that language models are unnecessary for forecasting tasks. ", "page_idx": 14}, {"type": "text", "text": "E.2 Complete Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we provide the results for all methods and datasets that we were unable to add to the main paper. ", "page_idx": 14}, {"type": "text", "text": "Inference Times. All results regarding \u201cinference time\u201d and forecast performance are shown in Figure 7 and Figure 8 respectively. ", "page_idx": 14}, {"type": "text", "text": "Randomized Parameters and Random-Shuffilng of Inputs. The results of the randomized parameters are shown in Table 20. The remaining results for shuffled input are shown in Table 21,22,23 and 24. ", "page_idx": 14}, {"type": "table", "img_path": "DV15UbHCY1/tmp/d1d79e6506ba91e16dd868cf469df0deda9c420e96c4bdeb10c159df75130f5b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "DV15UbHCY1/tmp/d98515332b25a8740453631216d5f4600a926ad8c83f1a1a15af02c206c37270.jpg", "table_caption": ["Table 9: Confidence Intervals for MAE predictions of Time-LLM. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. ", "Table 10: Confidence Intervals for MSE predictions of Time-LLM. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "DV15UbHCY1/tmp/356aed092c76ea146e726c1fc42c714a082983afdb9a134ec3eb29303f2c3da6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "DV15UbHCY1/tmp/1aac9691bfaab0b6b63d6ee50f459b44a9c3df820fd074c8317cb13be3253b09.jpg", "table_caption": ["Table 11: Confidence Intervals for MAE predictions of CALF. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. ", "Table 12: Confidence Intervals for MSE predictions of CALF. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. Note that \u2019-\u2019 means the dataset has not been included in the original paper. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "DV15UbHCY1/tmp/d1b3075f4372a42a004590a75b839de001fc78224457fc7a3943653529d730fc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "DV15UbHCY1/tmp/532d1ea135fe112bda9e042083aae803b04af32f96bc3beeac37e31f504aa4e3.jpg", "table_caption": ["Table 13: Confidence Intervals for MAE predictions of OneFitsAll. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. ", "Table 14: Confidence Intervals for MSE predictions of OneFitsAll. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "DV15UbHCY1/tmp/0c1608b0d89d3bec41be1f2d421ffd012934c77b63edb989ab96a1e193bfd9f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "DV15UbHCY1/tmp/01e87e9222c35d1e8a59fa63c310ce1a507e7f61923ddd06f0061aea4553e5f6.jpg", "table_caption": ["Table 15: Confidence Intervals for MAE and MSE predictions of Time-LLM. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. The ablation results in the table above are from datasets that have not been studied by the reference methods [50, 15, 22]. ", "Table 16: Confidence Intervals for MAE and MSE predictions of CALF. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. The ablation results in the table above are from datasets that have not been studied by the reference methods [50, 15, 22]. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "DV15UbHCY1/tmp/7bf887707328d7f8db9be14c87c11a50f4a2d83f4697e55ff3cce4ed2dfaf9a9.jpg", "table_caption": ["Table 17: Confidence Intervals for MAE and MSE predictions of OneFitsAll. The best performing model in highlighted in Red color text. #Wins refers to the total number of times the method performed best. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "DV15UbHCY1/tmp/4aef831ba407de1ef0b0ef9e38c50d2d1a8a7cd07f7ba1e8066517fbba841f39.jpg", "img_caption": ["Figure 5: Ablation studies indicate that when different methods remove the LLM (\u201cw/o LLM\u201d) or replace it with a single-layer attention (\u201cLLM2Attn\u201d) or Transformer (\u201cLLM2Trsf\u201d), the performance on time series forecasting tasks with MAE metric does not decline and even improves, compared with original methods, such as \u201cGPT-2\u201d or \u201cLLaMA\u201d. The vertical dashed line in the figures represents the results from the original paper. Above figures are from \u2019ETTh2\u2019, \u2019ETTm1\u2019, \u2019Illness\u2019, \u2019Weather\u2019, and \u2019Traffic\u2019 datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "DV15UbHCY1/tmp/4ba03ab0ace4fd4db7fd402eab32813ee804123c93b54835720501e1306bdd64.jpg", "img_caption": [], "img_footnote": ["Figure 6: Ablation studies indicate that when different methods remove the LLM (\u201cw/o LLM\u201d) or replace it with a single-layer attention (\u201cLLM2Attn\u201d) or Transformer (\u201cLLM2Trsf\u201d), the performance on time series forecasting tasks with MSE metric does not decline and even improves, compared with original methods, such as \u201cGPT- $2^{\\bullet}$ or \u201cLLaMA\u201d. The vertical dashed line in the figures represents the results from the original paper. "], "page_idx": 20}, {"type": "table", "img_path": "DV15UbHCY1/tmp/f04d14ed8bcf73371e459596b6fb3bc4f94d06ea9f759b3c367982e644a55cca.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 21}, {"type": "table", "img_path": "DV15UbHCY1/tmp/f7cc4f0e839f9ba51a2817b0d7b13bafff8454f457a77624769f6baeaba48250.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 22}, {"type": "table", "img_path": "DV15UbHCY1/tmp/5c4beb95d222ca4884140efd492850f3540fab21496aefb7ce97d9062b54fefc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 20: Pretraining on language datasets is not necessary for time series forecasting tasks. The table shows the performance of using pretraining models versus not using pretraining, as well as the combination of fine-tuning and not fine-tuning LLMs in time series forecasting. ", "page_idx": 23}, {"type": "table", "img_path": "DV15UbHCY1/tmp/0662cc37725d7bb9062f8d3f7791f54e02747dba92fff2735dc052521143c988.jpg", "table_caption": [], "table_footnote": ["Table 21: Results for input shuffling/masking for Time-LLM, CALF, and OneFitsAll methods on ETTh1 (predict length are \"96, 192, 336 and 720\") and Illness (predict length are \"24, 36, 48 and $60\"$ ), the impact of shuffilng the input on the degradation of time series forecasting performance does not change significantly before and after model modifications. "], "page_idx": 24}, {"type": "table", "img_path": "DV15UbHCY1/tmp/c11a7f9f179e6c58e036e8bfa1119f5d5b5e8b3791a94b087375719e1d94c1eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "DV15UbHCY1/tmp/1ff99954247189cae74ba66cbcb512d9dbfe32a22bda7f34036fd0474d7827a8.jpg", "table_caption": ["Table 22: For the input shuffling/masking experiments on ETTh2 and Electricity, the impact of shuffling the input on the degradation of time series forecasting performance does not change significantly before and after model modifications. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "DV15UbHCY1/tmp/c75fe2603eceec3cc61bcebdf0acf41b779fb21065f71b0f7adc96464e9dd45d.jpg", "table_caption": ["Table 23: For the input shuffilng/masking experiments on ETTm1 and ETTm2, the impact of shuffilng the input on the degradation of time series forecasting performance does not change significantly before and after model modifications. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 24: For the input shuffilng/masking experiments on Weather and Traffic, the impact of shuffilng the input on the degradation of time series forecasting performance does not change significantly before and after model modifications. ", "page_idx": 25}, {"type": "image", "img_path": "DV15UbHCY1/tmp/556f76ed3ac6b6af6754adabb865b8764eed9c72e7981d45056234f35ff8f3e0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 7: Ablation methods consume less time for inference while providing better forecasting performance in most cases. The figure above shows the inference time and prediction accuracy of Time-LLM, OneFitsAll, and CALF on ETTh1, ETTh2, ETTm1, Illness, and Weather, Traffic datasets, averaged across prediction lengths. Results of other datasets refer to Figure 3. ", "page_idx": 26}, {"type": "image", "img_path": "DV15UbHCY1/tmp/c5f0c81fbc6c95572febce83b91fbe2cbcac2c4fba53db7981c291a344ea7bb7.jpg", "img_caption": ["(a) Inference Time and Performance for ETTh1, ETTh2, ETTm1, and ETTm2. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "DV15UbHCY1/tmp/e549b775d02346bdeaeefd973097d996cca25ca9dffbcc98873a965f1d89059b.jpg", "img_caption": ["(b) Inference Time and Performance for Illness, Weather, Traffic, and Electricity. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: Ablation methods consume less time for inference while providing better forecasting performance in most cases. The figure above shows the inference time and prediction accuracy of Time-LLM, OneFitsAll, and CALF on all the datasets, averaged across prediction lengths in MSE metric. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Abstract and the Introduction sections include the the paper\u2019s contributions and scope. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Appendix A discusses the limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not make any theoretical contributions. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Section 4 and Appendix D shows the details of the baselines used and their hyper-parameter details respectively. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have shared the code as a .zip file in the supplementary material. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Experiments 3.3 and Appendix D describe them. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the details of confidence intervals for all our results in Appendix E.1 ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of computing workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We mention the hardware details in Appendix D ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we do follow the Code Of Ethics. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Appendix B describes broader impact. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper properly credited, and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the license details in Appendix C. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented, and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We attached the code in supplementary material. All implementation and hyper-parameter details are also reported in the paper. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 29}]