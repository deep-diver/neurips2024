{"references": [{"fullname_first_author": "M. Fayyaz", "paper_title": "Adaptive token sampling for efficient vision transformers", "publication_date": "2022-10-23", "reason": "This paper introduces Adaptive Token Sampling (ATS), a key token sparsification mechanism analyzed in the DeSparsify attack."}, {"fullname_first_author": "L. Meng", "paper_title": "AdaViT: Adaptive vision transformers for efficient image recognition", "publication_date": "2022-06-01", "reason": "AdaViT, another crucial token sparsification technique targeted by DeSparsify, is introduced and evaluated in this reference."}, {"fullname_first_author": "H. Yin", "paper_title": "A-ViT: Adaptive tokens for efficient vision transformer", "publication_date": "2022-06-01", "reason": "This work presents A-ViT, a third significant token sparsification method that DeSparsify assesses, providing a range of techniques for comparison."}, {"fullname_first_author": "I. Shumailov", "paper_title": "Sponge examples: Energy-latency attacks on neural networks", "publication_date": "2021-05-01", "reason": "This foundational paper introduces the concept of 'sponge attacks,' which DeSparsify builds upon to target the availability of vision transformers."}, {"fullname_first_author": "H. Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-07-01", "reason": "This paper provides the DeiT model architecture, the primary target for the DeSparsify attack, establishing a baseline for the evaluation of the attack's impact."}]}