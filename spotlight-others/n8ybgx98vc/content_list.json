[{"type": "text", "text": "TFG: Unified Training-Free Guidance for Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haotian Ye1 \\* Haowei Lin2\\* Jiaqi Han1 \\* Minkai Xu1 Sheng Liu1 Yitao Liang? Jianzhu Ma? James Zoul Stefano Ermon1 1Stanford University 2Peking University 3Tsinghua University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performanceby $8.5\\%$ on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in generative models, particularly diffusion models [61, 21, 62, 66], have demonstrated remarkable effectiveness across vision [65, 48, 52], small molecules [74, 73, 24], proteins[1, 72], audio [35, 29], 3D objects [40, 41], and many more. Diffusion models estimate the gradient of log density (i.e., Stein score, [67]) of the data distribution [65] via denoising learning objectives, and can generate new samples via an iterative denoising process. With impressive scalability to billions of data [58], future diffusion models have the potential to serve as foundational generative models across a wide range of applications. Consequently, the problem of conditional generation based on these models, i.e., tailoring outputs to satisfy user-defined criteria such as labels, attributes, energies, and spatial-temporal information, is becoming increasingly important [63, 2]. ", "page_idx": 0}, {"type": "text", "text": "Conditional generation methods like classifier-based guidance [66, 7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, training-free guidance aims to generate samples that align with certain targets specified through an off-the-shelf differentiable target predictor without involving any additional training. Here, a target predictor can be any classifier, loss function, probability function, or energy function used to score the quality of the generated samples. ", "page_idx": 0}, {"type": "text", "text": "In classifier-based guidance [66, 7], where a noise-conditional classifier is specifically trained to predict the target property on both clean and noisy samples, incorporating guidance in the diffusion process is straightforward since the gradient of the classifier is an unbiased driving term. Trainingfree guidance, however, is fundamentally more difficult. The primary challenge lies in leveraging a target predictor trained solely on clean samples to offer guidance on noisy samples. Although various approaches have been proposed [18, 63, 6, 2, 78] and are effective for some individual tasks, theoretical grounding and comprehensive benchmarks are still missing. Indeed, existing methods fail to produce satisfactory samples for label guidance even on simple datasets such as CIFAR10 (Figure 1). Moreover, the lack of quantitative comparisons between these methods makes it difficult for practitioners to identify an appropriate algorithm for a new application scenario. ", "page_idx": 0}, {"type": "image", "img_path": "N8YbGX98vc/tmp/7a79bbd1b36a2da6aeaa4945261343b3537f2c614698d72728547c936e9ecf57.jpg", "img_caption": ["Figure 1: (a) Mllustration of the unified search space of our proposed TFG, where the height (color) stands for performance. Existing algorithms search along sub-manifolds, while TFG results in improved guidance thanks to its extended search space. (b) The label accuracy (higher the better) and Fr\u00e9chet inception distance (FID, lower the better) of different methods for the label guidance task on CIFAR10 [30], averaged across ten labels. Ours (TFG-4) performs much closer to training-based methods. $\\scriptstyle\\mathbf{\\left(c\\simh\\right)}$ TFG generated samples across various tasks in vision, audio, and geometry domains. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper proposes a novel and general algorithmic framework for (and also named as) Training Free Guidance (TFG). We show that existing approaches are special cases of the TFG as they correspond to particular hyper-parameter subspace in our unified space. In other words, TFG naturally simplifies and reduces the study of training-free guidance, as well as the comparisons between existing methods, into the analysis of hyper-parameter choices in our unified design space. Within our framework, we analyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive experiments to identify their influence. Our systematic study offers novel insights into the principles behind training-free guidance, allowing for a transparent and efficient survey of the problem. ", "page_idx": 1}, {"type": "text", "text": "Based on the framework, we propose a hyper-parameter searching strategy for general downstream tasks. We comprehensively benchmark TFG and existing algorithms across 16 tasks (ranging from images to molecules) and 40 targets. TFG achieves superior performance across all datasets, outperforming existing methods by $8.5\\%$ on average. In particular, it excels in generating userrequired samples in various scenarios, regardless of the complexity of targets and datasets. ", "page_idx": 2}, {"type": "text", "text": "In summary, we (1) propose TFG that unifies existing algorithms into a design space, (2) theoretically and empirically analyze the space to propose an effective space-searching strategy for general problems, and (3) benchmark all methods on numerous qualitatively different tasks to present the superiority of TFG and the guideline for future research in training-free conditional generation algorithms. This advancement demonstrates the efficacy of TFG and establishes a robust and comprehensive benchmark for future research in training-free conditional generation algorithms. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Generative diffusion model. A generative diffusion model is a neural network that can be used to sample from an unconditional distribution $p_{0}(x)$ with the support on any continuous sample space $\\mathcal{X}$ [21, 62, 64, 27]. For instance, $\\mathcal{X}$ could be $[-1,1]^{d\\times d\\times3}$ representing the RGB colors of $d\\times d$ images [4, 22], or $\\mathbb{R}^{3d}$ representing the 3D coordinates of molecules with $d$ atoms [24, 74, 73]. Given a data $\\pmb{x}_{0}$ sampled from $p_{0}({\\pmb x})$ , a time step $t\\in[T]\\triangleq\\{1,\\cdot\\cdot\\cdot,T\\}$ , a corresponding noisy datapoint is constructed as $\\mathbf{\\bar{\\alpha}}x_{t}=\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon$ where $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$ and $\\{\\bar{\\alpha}_{t}\\}_{t=1}^{T}$ is a set of pre-defined monotonically decreasing parameters used to control the noise level. Following [21], we further define $\\alpha_{t}=\\bar{\\alpha}_{t}/\\bar{\\alpha}_{t-1}$ for $t>1$ and $\\alpha_{1}=\\bar{\\alpha}_{1}$ . The diffusion model $\\epsilon_{\\theta}:\\mathcal{X}\\times[T]\\mapsto\\bar{\\mathcal{X}}$ parameterized by $\\theta$ is trained to predict the noise $\\epsilon$ that was added on $\\pmb{x}_{t}$ with p.d.f $\\begin{array}{r}{p_{t}(\\mathbf{\\boldsymbol{x}}_{t})=\\dot{\\int}_{\\mathbf{\\boldsymbol{x}}_{0}}\\,p_{0}(\\mathbf{\\bar{\\boldsymbol{x}}}_{0})p_{t|0}(\\mathbf{\\boldsymbol{x}}_{t}|\\mathbf{\\boldsymbol{x}}_{0})\\mathrm{{\\dot{d}}}\\mathbf{\\boldsymbol{x}}_{0}^{2}}\\end{array}$ In theory, this corresponds to learning the score of $p_{t}(\\pmb{x})$ [65], i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathop{\\arg\\operatorname*{min}}_{\\epsilon_{\\theta}}\\sum_{t=1}^{T}\\mathbb{E}_{x_{0}\\sim p_{0}(x_{0}),\\epsilon\\sim\\mathcal{N}(\\mathbf{0},I)}\\|\\epsilon_{\\theta}(x_{t},t)-\\epsilon\\|=-\\sqrt{1-\\bar{\\alpha}_{t}}\\nabla\\log p_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For sampling, we start from $\\pmb{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ and gradually sample $\\pmb{x}_{t-1}\\sim p_{t-1|t}(\\pmb{x}_{t-1}|\\pmb{x}_{t})$ This conditional probability is not directly computable, and in practice, DDIM [62] samples $x_{t-1}$ via ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}_{t-1}=\\sqrt{\\bar{\\alpha}_{t-1}}\\pmb{x}_{0\\mid t}+\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_{t}^{2}}\\frac{\\pmb{x}_{t}-\\sqrt{\\bar{\\alpha}_{t}}\\pmb{x}_{0\\mid t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}+\\sigma_{t}\\epsilon,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{\\sigma_{t}\\}_{t=1}^{T}$ are DDIM parameters, $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$ , and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}_{0\\mid t}=m(\\pmb{x}_{t})\\triangleq\\frac{\\pmb{x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}(\\pmb{x}_{t},t)}{\\sqrt{\\bar{\\alpha}_{t}}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "is the predicted sample given $\\pmb{x}_{t}$ . According to the Tweedie's formula [11, 51], $\\pmb{x}_{0|t}$ equals to the conditional expectation $\\mathbb{E}[{\\pmb x}_{0}|{\\pmb x}_{t}]$ under perfect optimization of $\\epsilon_{\\theta}$ in Equation (1). It has been theoretically established that the above sampling process results in $\\pmb{x}_{0}\\sim p(\\pmb{x})$ under certain assumptions. ", "page_idx": 2}, {"type": "text", "text": "Target predictor. For a user required target $c$ , we use a predictor $f_{c}(\\mathbf{x}):\\mathcal{X}\\mapsto\\mathbb{R}_{+}\\cup\\{0\\}$ 3 to represent how well a sample $\\textbf{\\em x}$ is aligned with the target (higher the better). Here $f_{c}(x)$ can be a conditional probability $p_{0}(c|\\pmb{x})$ for a label $c$ [62, 14], a Boltzmann distribution $\\exp^{-e_{c}(\\pmb{x})}$ for any pre-defined energy function $e_{c}$ [31, 63, 38], the similarity of two features [47], or even their combinations. The goal is to samples from the conditional distribution ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{0}(\\mathbf{\\boldsymbol{x}}|c)\\triangleq\\frac{p_{0}(\\mathbf{\\boldsymbol{x}})f_{c}(\\mathbf{\\boldsymbol{x}})}{\\int_{\\tilde{\\mathbf{\\boldsymbol{x}}}}p_{0}(\\tilde{\\mathbf{\\boldsymbol{x}}})f_{c}(\\tilde{\\mathbf{\\boldsymbol{x}}})\\mathrm{d}\\tilde{\\mathbf{\\boldsymbol{x}}}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Training-based guidance for diffusion models. [66] proposes to train a time-dependent classifier to fit $f_{c}(\\mathbf{\\boldsymbol{x}}_{t},t)\\triangleq\\mathbb{E}_{\\mathbf{\\boldsymbol{x}}_{0}\\sim p_{0\\mid t}(\\cdot|\\mathbf{\\boldsymbol{x}}_{t})}f_{c}(\\mathbf{\\boldsymbol{x}}_{0})$ . This can be regarded as a predictor over noisy samples. Since ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{t}}\\log{p_{t}(\\mathbf{x}_{t}|c)}=\\nabla_{x_{t}}\\log\\int_{x_{0}}p_{t|0}(x_{t}|x_{0})p_{0}(x_{0}|c)\\mathrm{d}x_{0}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{t}}\\log\\int_{x_{0}}p_{t}(x_{t})p_{0|t}(x_{0}|x_{t})f_{c}(x_{0})\\mathrm{d}x_{0}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{t}}\\log p_{t}(x_{t})+\\nabla_{x_{t}}\\log f_{c}(x_{t},t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "if we denote the trained classifier as $f(\\pmb{x}_{t})$ (that implicitly depends on $c$ and model parameters), we can replace $\\epsilon_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t)$ in Equation (3) by $\\epsilon_{\\theta}(\\mathbf{x}_{t},t)-\\sqrt{1-\\bar{\\alpha}_{t}}\\nabla_{\\mathbf{x}_{t}}\\log f(\\mathbf{x}_{t})$ upon sampling to obtain unbiased sample $\\pmb{x}_{0}\\sim p_{0}(\\pmb{x}_{0}|c)$ . On the other hand, [23] proposes the classifier-free diffusion guidance approach. Instead of training a time-dependent predictor $f$ , it encodes conditions $c$ directly into the diffusion model as $\\epsilon_{\\theta}(\\mathbf{\\boldsymbol{x}},c,t)$ and trains this condition-aware diffusion model with samplecondition pairs. Both methods have been proven effective when training resources are available. ", "page_idx": 3}, {"type": "text", "text": "This paper in contrast focuses on conditional generation in a training-free manner: given a diffusion model $\\epsilon_{\\theta}(\\boldsymbol{x},t)$ and an off-the-shelf target predictor $f({\\pmb x})$ (we omit the subscript $c$ below), we aim to generate samples from $p_{0}({\\pmb x}|{\\pmb c})$ without any additional training. Unlike training-based methods that can accurately estimate $f(\\pmb{x}_{t},t)$ , training-free guidance is significantly more difficult since it involves guiding a noisy data $\\pmb{x}_{t}$ using $f({\\boldsymbol{x}})$ defined over the clean data space. ", "page_idx": 3}, {"type": "text", "text": "2.1 Existing algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Most existing methods take advantage of the predicted sample ${\\pmb x}_{0\\mid t}$ defined in Equation (3) and use the gradient of $f({\\boldsymbol{x}})$ for guidance. We review and summarize five existing approaches below, and provide a schematic and a copy of pseudo-code in Appendix B for the sake of reference. Due to the variety in underlying intuitions and implementations, coupled with a lack of quantitative comparisons among these methods, it is challenging to discern which operations are crucial and which are superfluous, a problem we address in Section 3. ", "page_idx": 3}, {"type": "text", "text": "DPS [6] was initially proposed to solve general noisy inverse problems for image generation: for a given condition $\\textit{\\textbf{y}}$ and a transformation operator $\\boldsymbol{\\mathcal{A}}$ we aim to generate image $\\textbf{\\em x}$ such that $||\\mathcal{A}(\\pmb{x})\\!-\\!\\pmb{y}||_{2}$ is small. For instance, in super-resolution task [71], the operator $\\boldsymbol{\\mathcal{A}}$ is a down-sampling operator, and $\\textit{\\textbf{y}}$ is a low-resolution image. DPS replaces $\\nabla\\log f(\\mathbf{\\boldsymbol{x}}_{t},t)$ in Equation (5) by $\\nabla_{\\pmb{x}_{t}}\\log f(m(\\pmb{x}_{t}))$ .As suggested in [63], this corresponds to a point estimation of the conditional density $p_{0|t}(\\pmb{x}_{0}|\\pmb{x}_{t})$ ", "page_idx": 3}, {"type": "text", "text": "LGD [63] replaces the point estimation in DPS and proposes to estimate $f(\\pmb{x}_{t},t)$ with a Gaussian kernel $\\mathbb{E}_{\\pmb{x}\\sim\\mathcal{N}(\\pmb{x}_{0\\mid t},\\sigma_{t}^{2}\\pmb{I})}f(\\pmb{x},t)$ , where the expectation is computed using Monte-Carlo sampling [59]. ", "page_idx": 3}, {"type": "text", "text": "FreeDoM [78] generalizes DPS by introducing a \u201crecurrent strategy\" (called \u201ctime-travel strategy\" [39, 10, 70]) that iteratively denoises $x_{t-1}$ from $\\pmb{x}_{t}$ and adds noise to $x_{t-1}$ to regenerate $\\pmb{x}_{t}$ back and forth. This strategy empirically enhances the strength of the guidance at the cost of additional computation. FreeDoM also points out the importance of altering guidance strength at different time steps $t$ , but a comprehensive study on which schedule is better is not provided. ", "page_idx": 3}, {"type": "text", "text": "MPGD [18] is proposed for manifold preserving tasks, e.g., the target predictor is supposed to generate samples on a given manifold. It computes the gradient of $\\log f({\\pmb x}_{0|t})$ to ${\\pmb x}_{0\\mid t}$ instead of $\\pmb{x}_{t}$ , i.e., $\\nabla_{\\mathbf{\\alpha}_{0\\mid t}}\\log f(\\mathbf{\\alpha}_{0\\mid t})$ to avoid the back-propagation through the diffusion model $\\epsilon_{\\theta}$ that is highly inefficient. This strategy is effective in manifold-preserving problems, but whether it can be generalized to general training-free problems is unclear. In addition to the computation difference, theoretical understanding on the difference between gradients to $\\pmb{x}_{0|t}$ and $\\pmb{x}_{t}$ is missing.4 ", "page_idx": 3}, {"type": "text", "text": "UGD [2] builds on FreeDoM, with the difference that it additionally solves a backward optimization problem $\\Delta_{0}=\\arg\\operatorname*{max}_{\\Delta}f(\\pmb{x}_{0\\mid t}+\\Delta)$ and guides $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ and $\\pmb{x}_{t}$ simultaneously. UGD also implements the \u201crecurrent strategy\u201d to further improve generation quality. ", "page_idx": 3}, {"type": "text", "text": "3 TFG: A Unified Framework for Training-free Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite the array of algorithms available and their reported successes in various applications, we conduct a case study on CIFAR10 [30] to illustrate the challenging nature of training-free guidance and the insufficiency of existing methods. Specifically, for each of the ten labels, we use the pretrained diffusion model and classifiers from [7, 9] to generate 2048 samples, where the hyper-parameters are selected via a grid search for the fairness of comparison. We compute the FID and the label accuracy evaluated by another classifier [20] and present results in Figure 1. Even in such a relatively simple setting, all training-free approaches significantly underperform training-based guidance, with a significant portion of generated images being highly unnatural (when guidance is strong) or irrelevant to the label (when guidance is weak). These findings reveal the fundamental challenges and highlight the necessity of a comprehensive study. Unfortunately, comparisons and analyses of existing approaches are missing or primarily qualitative, limiting deeper investigation in this field. ", "page_idx": 4}, {"type": "text", "text": "3.1 Unification and extension ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This sections introduces our unified framework for training-free guidance (TFG, Algorithm 1) and formally defines its design space in Definition 3.1. We demonstrate the advantage of TFG by drawing connections between TFG and other algorithms to show that existing algorithms are encompassed as special cases. Based on this, all comparisons and studies of training-free algorithms automatically become the study within the hyper-parameter space of our framework. This allows us to analyze the techniques theoretically and empirically, and choose an appropriate hyper-parameter for a specific downstream task efficiently and effectively, as shown in Section 4. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Training-Free Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Input: Unconditional diffusion model $\\epsilon_{\\theta}$ , target predictor $f$ , guidance strength $\\rho,\\pmb{\\mu},\\bar{\\gamma}$ , number of steps   \n$T,N_{\\mathrm{recur}},N_{\\mathrm{iter}}$   \n2: $\\pmb{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$   \n3: for $t=T,\\cdots,1$ do   \n4: Define function $\\widetilde{f}(\\pmb{x})=\\mathbb{E}_{\\delta\\sim\\mathcal{N}(\\mathbf{0},I)}f(\\pmb{x}+\\bar{\\gamma}\\sqrt{1-\\bar{\\alpha}_{t}}\\delta)$   \n5: for $r=1,\\cdots$ , Nrecur do   \n6: ${\\pmb x}_{0\\mid t}=({\\pmb x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}({\\pmb x}_{t},t))/\\sqrt{\\bar{\\alpha}_{t}}$ Obtain the predicted data   \n7: \u25b3t = ptVmt log f(xolt)   \n8: 0 =o + \u03bctVmolt log f(olt + \u25b30) $\\triangleright$ Iterate $N_{\\mathrm{iter}}$ times starting from $\\Delta_{0}=\\mathbf{0}$   \n9: $\\pmb{x}_{t-1}=\\mathrm{Sample}(\\pmb{x}_{t},\\pmb{x}_{0\\mid t},t)+\\Delta_{t}/\\sqrt{\\alpha}_{t}+\\sqrt{\\bar{\\alpha}_{t-1}}\\Delta_{0}$ Sample follows Equation (2)   \n10: t \\~ N(\u221a\u03b1t\u221et-1,\u221a1-\u03b1tI) Recurrent strategy   \n11: end for   \n12: end for   \n13: Output: Conditional sample $\\mathbf{\\nabla}_{x_{0}}$ ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. Given a denoising step $T$ , the hyper-parameter space (design space) of Algorithm 1 is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{\\mathrm{TFG}}=\\big\\{\\big(N_{\\mathrm{recur}},N_{\\mathrm{iter}},\\bar{\\gamma},\\rho,\\mu\\big):N_{\\mathrm{recur}},N_{\\mathrm{recur}}\\in\\mathbb{N},\\bar{\\gamma}\\geq0,\\rho,\\mu\\in\\big(\\mathbb{R}_{+}\\cup\\{0\\}\\big)^{T}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Weuse $\\mathcal{H}_{\\mathrm{TFG}}$ to represent the complete hyper-parameter space and $\\mathcal{H}_{\\mathrm{TFG}}(N_{\\mathrm{recur}}=N_{0})$ to represent the subspace constrained on $N_{\\mathrm{recur}}=N_{0}$ ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 defines the hyper-parameter space spanned by TFG, where one hyper-parameter in $\\mathcal{H}_{\\mathrm{TFG}}$ is an instantiation of the framework. Intuitively, $N_{r e c u r}$ controls the recurrence of the algorithm, $N_{i t e r}$ controls the iterating when computing $\\Delta_{0}$ (Line 8), $\\bar{\\gamma}$ controls the extent we smooth the original guidance function $f$ (Line 4), and $\\rho,\\pmb{\\mu}$ control the strength of two types of guidance (Lines 7 and 8). A comprehensive explanation of the effect of each hyper-parameter can be found in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Below is the major theorem showing that all algorithms presented in Section 2.1 correspond to special cases of TFG, thus unifying them into our framework and obviating the need for separate analyses. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. The hyper-parameter space of \u00b7 MPGD [18] $\\mathcal{H}_{M P G D}$ is equivalent to $\\mathcal{H}_{T F G}(N_{r e c u r}=N_{i t e r}=1,\\rho=\\mathbf{0},\\bar{\\gamma}=0)$ \u00b7LGD [63] $\\mathcal{H}_{L G D}$ is equivalent to $\\mathcal{H}_{T F G}(N_{r e c u r}=1,N_{i t e r}=0,\\mu=\\mathbf{0})$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "\u00b7 UGD [2] HuGD is equivalent to $\\mathcal{H}_{T F G}(\\bar{\\gamma}=0)$ \u00b7 DPS [6] $\\mathcal{H}_{D P S}$ is equivalent to $\\mathcal{H}_{T F G}(N_{r e c u r}=1,N_{i t e r}=0,\\mu=\\mathbf{0},\\bar{\\gamma}=0).$ \u00b7 FreeDoM [78] HFreeDom is equivalent to $\\mathcal{H}_{T F G}(N_{i t e r}=0,\\pmb{\\mu}=\\mathbf{0},\\bar{\\gamma}=0)$ ", "page_idx": 5}, {"type": "text", "text": "The complete analysis and proof of Theorem 3.2 is postponed to Appendix C. It implies that existing algorithms are limited in expressivity, covering only a subset of $\\mathcal{H}_{\\mathrm{TFG}}$ . In contrast, TFG covers the entire space and is guaranteed to perform better. In addition, TFG streamlines nuances between existing methods, allowing for a unified way to compare and study different techniques. Consequently, the versatile framework that TFG provides can simplify its adaptation to various applications. ", "page_idx": 5}, {"type": "text", "text": "3.2   Algorithm and design space analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now present a concrete analysis of TFG and its design space $\\mathcal{H}$ in detail. Similar to standard classifier-based guidance, TFG guides $\\pmb{x}_{t}$ at each denoising step $t$ To provide appropriate and informative guidance, TFG essentially leverages four techniques for guidance: Mean Guidance (Line 8) controlled by $N_{\\mathrm{iter}},\\mu$ , Variance Guidance (Line 7) controlled by $\\rho$ , Implicit Dynamic (Line 4) controlled by $\\bar{\\gamma}$ , and Recurrence (Line 5) controlled by $N_{\\mathrm{recur}}$ ", "page_idx": 5}, {"type": "text", "text": "Mean Guidance computes the gradient of $\\tilde{f}(\\pmb{x})$ to $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ and is the most straightforward approach. However, this method can yield inaccurate guidance. To show this, notice that under perfect optimization we have ${\\pmb x}_{0\\mid t}=\\mathbb E[{\\pmb x}_{0}|{\\pmb x}_{t}]$ , and when $p_{0}(\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}])$ is close to zero, the predictor has rarely been trained on data from the region close to $\\pmb{x}_{0|t}$ , making the gradient unstable and noisy. To mitigate this, one can iteratively add gradients of $\\tilde{f}(\\pmb{x})$ to $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ , encouraging $\\pmb{x}_{0|t}$ to escape low-probability regions. ", "page_idx": 5}, {"type": "text", "text": "Variance Guidance provides an alternative approach for improving the gradient estimation. The reason why we dub it variance guidance might be ambiguous, as the only difference is that the gradient is taken with respect to $\\pmb{x}_{t}$ (Line 7) instead of $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ (Line 8). The lemma below demonstrates that this essentially corresponds to a covariance re-scaled guidance. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.3. If the model is optimized perfecty,i. $\\epsilon_{\\theta}({\\pmb x},t)=-\\sqrt{1-\\bar{\\alpha}_{t}}\\nabla\\log p_{t}({\\pmb x}),$ wehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{t}=\\frac{\\sqrt{\\bar{\\alpha}_{t}}}{1-\\alpha_{t}}\\mathbf{\\Sigma}_{0\\mid t}\\nabla_{\\mathbf{x}_{0\\mid t}}\\tilde{f}(\\mathbf{x}_{0\\mid t}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{\\Sigma_{0\\mid t}}\\triangleq\\int_{\\pmb{x}}p_{0\\mid t}(\\pmb{x}|\\pmb{x}_{t})(\\pmb{x}-\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}])(\\pmb{x}-\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}])^{\\top}\\mathrm{d}\\pmb{x}}\\end{array}$ is the covariance of $\\mathbf{\\boldsymbol{x}}_{0}|\\mathbf{\\boldsymbol{x}}_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.3 suggests that variance guidance refines mean guidance by incorporating the secondorder informationof $\\pmb{x}_{0}|\\pmb{x}_{t}$ , specifically considering the correlation among components within $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ Consequently, positively correlated components could have guidance mutually reinforced, while negatively correlated components could have guidance canceled. This also implies that mean guidance and variance guidance are intrinsically leveraging different orders of information for guidance. In TFG, variance guidance is controlled by $\\rho_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Implicit Dynamic transforms the predictor $f$ into its convolution via a Gaussian kernel $\\bar{\\mathcal{N}}(\\mathbf{0},\\bar{\\gamma}(1-\\bar{\\alpha}_{t})\\pmb{I})$ . This operation is initially introduced by LGD [63] to estimate $p_{0|t}(\\pmb{x}_{0}|\\pmb{x}_{t})$ . However, it is unclear why the form is preselected as a Gaussian distribution. We argue that this technique is effective because it creates an implicit dynamic on $\\pmb{x}_{0|t}$ . Specifically, starting from $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ , it iteratively adds noise to $\\pmb{x}_{0|t}$ , evaluates gradient, and moves $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ based on the gradient. The repeating process converges to the density proportional to $f(x)$ when $N_{\\mathrm{iter}}$ goes to infinity, driving ${\\pmb x}_{0\\mid t}$ to high-density regions. This explanation is justified by Table 1: the performance remains nearly un", "page_idx": 5}, {"type": "text", "text": "Table 1:Influence of the number of Monte-Carlo samples in estimating the expectation of Line 4. Both the FID and the accuracy remain unchanged when #Samples varies, suggesting that the number of samples is less important. More details are in Appendix E.1. ", "page_idx": 5}, {"type": "table", "img_path": "N8YbGX98vc/tmp/51445d7c99b75b5ca9133dc2be8809613f4c6b3a1d14656c91117b1f17c73904.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "changed as we gradually decrease the number of Monte-Carlo samples in estimating the expectation (Line 4) down to 1, implying that the preciseness of estimation is not essential, but adding noises is. ", "page_idx": 5}, {"type": "text", "text": "Recurrence helps strengthen the guidance by iterating the previous three techniques to obtain $x_{t-1}$ andresample $\\pmb{x}_{t}$ back and forth. This can be understood as an Ornstein-Uhlenbeck process[42] on $x_{t-1}$ whereLine $6{\\sim}9$ corresponds to the drift term and ${\\bf\\nabla}x_{t-1}\\rightarrow{\\bf\\nabla}x_{t}$ (Line 10) the white noise term. Intuitively, it finds a trade-off between the error inherited from previous steps (the more you recur, the less previous error stays) and the accumulated error in this step (the more you recur, the more error in the current guidance you suffer). Empirically, we also find that the generation quality improves and then deteriorates as we increase $N_{\\mathrm{recur}}$ ", "page_idx": 5}, {"type": "image", "img_path": "N8YbGX98vc/tmp/854283434efda728b6feca59c4467146fc64b2288731e11644f15d849ed2aa70.jpg", "img_caption": [], "img_footnote": ["Figure 2: Comparison of three structures in Equation (8) of $\\rho$ and $\\pmb{\\mu}$ on CIFAR10 and ImageNet, under different choices of the rest hyper-parameters in $\\mathcal{H}_{\\mathrm{TFG}}$ .We set ${\\pmb\\rho}={\\bf0},\\bar{\\boldsymbol\\gamma}=0$ when studying structures of $\\pmb{\\mu}$ , and similarly for $\\rho$ . Results are averaged across all labels. The comparative relationship between structures remains unchanged when the rest of the parameters vary. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4  Design Space of TFG: Analysis and Searching Strategy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Admittedly, a more extensive design space only yields a better performance if an effective and robust hyper-parameter searching strategy can be applied. For example, arbitrarily complex neural networks are guaranteed to have better optimal performance than simple linear models, but finding the correct model parameters is significantly more difficult. This section dives into this core problem by comprehensively analyzing the hyper-parameter space structure of $\\mathcal{H}_{\\mathrm{TFG}}$ , and further proposing a general searching algorithm applicable for any general downstream tasks. ", "page_idx": 6}, {"type": "text", "text": "The hyper-parameters of $\\mathcal{H}_{\\mathrm{TFG}}$ can be categorized into two parts: time-dependent vectors $\\rho,\\pmb{\\mu}$ and time-independent sacalars $N_{\\mathrm{{recur}}},N_{\\mathrm{{iter}}},\\bar{\\gamma}$ :While a grid search can potentially result in the best performance, performing such an extensive search in $\\mathcal{H}_{\\mathrm{TFG}}$ is highly impractical, especially considering the vector parameters $\\rho,\\pmb{\\mu}$ . Fortunately, below we demonstrate that, if we decompose $\\rho$ into $\\bar{\\rho}\\cdot s_{\\rho}(t)$ (same for $\\pmb{\\mu}$ )where $\\bar{\\rho}$ is a scalar and $s_{\\rho}(t)$ is a \u201cstructure\u201d (a non-negative function) such that $\\begin{array}{r}{\\sum_{t}s_{\\rho}(t)=T}\\end{array}$ , then some structures are consistently better than others regardless of the other hyper-parameters. This allows us to pre-locate an appropriate structure for the given task and efficiently optimize the rest of the scalar hyper-parameters. Our analysis is conducted on the label guidance task on CIFAR-10 [30] and ImageNet [55], with experimental settings identical to Section 3. ", "page_idx": 6}, {"type": "text", "text": "Structure analysis. Motivated by the default structure selected in UGD and LGD, we consider three structuresforboth $s_{\\rho}(t)$ and $s_{\\mu}(t)$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\ns(t)=\\frac{\\alpha_{t}}{\\sum_{t=1}^{T}\\alpha_{t}}(\\mathrm{increase}),\\;s(t)=\\frac{(1-\\alpha_{t})}{\\sum_{t=1}^{T}(1-\\alpha_{t})}(\\mathrm{decrease}),\\;s(t)=1(\\mathrm{constant}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "These structures are selected to be qualitatively different, while each is justified to be reasonable under certain conditions [18, 78, 2]. We leave the study of more structures to future works. The rest of the parameters are grid-searched for the comprehensiveness of the analysis. For $s_{\\rho}(t)$ ,we set $N_{\\mathrm{recur}}=\\lbrace1,2,4\\rbrace$ and $\\bar{\\bar{\\rho}}=\\{0.25,0.5,1.0,2.0,4.\\bar{0}\\}$ ; and for $s_{\\mu}(t)$ , we set $N_{\\mathrm{iter}}=\\{1,2,4\\}$ and $\\bar{\\mu}=\\{0.25,0.5,1.0,2.0,4.0\\}$ We run label guidance for each configuration and each of the ten labels on CIFAR10 (four labels on ImageNet, due to computation constraints). ", "page_idx": 6}, {"type": "text", "text": "As presented in Figure 2, the relationship between different structures remains unchanged when the rest of the parameters vary. For instance, on both datasets, the Validity-FID performance curves consistently move top-left (implying a better performance) when we switch from \u201cdecrease\u201d structure (red lines) to \u201cconstant\u2019 structure (yellow lines) to \u201cincrease\u201d structure (blue lines) for both $\\rho,\\pmb{\\mu}$ and different values of $N_{\\mathrm{recur}}$ and $N_{\\mathrm{iter}}$ This invariant relationship is essential as it allows for an effcient hyper-parameters search in $\\mathcal{H}_{\\mathrm{TFG}}$ by first determining appropriate structures for $s_{\\rho}(t),s_{\\mu}(t)$ under a simple subspace, and then selecting the rest scalar parameters. ", "page_idx": 6}, {"type": "table", "img_path": "N8YbGX98vc/tmp/087ed26b49d493c1851f7bda11055fe94c8925fde9e63cb064daec560544dd9d.jpg", "table_caption": ["Table 2: List of 14 task types we benchmark. Each task is run with multiple individual targets (38 in total). We evaluate the guidance validity (how well a sample is aligned with the target predictor) and the guidance fidelity (how well a sample is aligned with the unconditional distribution) according to the task type. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Computation cost analysis. Among the scalars parameters, $N_{\\mathrm{recur}}$ and $N_{\\mathrm{iter}}$ directly influence the total computational cost, while $\\bar{\\rho},\\bar{\\mu},\\bar{\\gamma}$ do not. With a certain range, performance increases when the value of $N_{\\mathrm{recur}},N_{\\mathrm{iter}}$ increase?, and the tradeoff between generation quality and computation time is presented in Figure 3: recurrence leads to a $N_{\\mathrm{recur}}$ times cost with clear performance gain; iteration (on $\\pmb{x}_{0\\mid t})$ results in less increase of computation time, and its effect plateaus. In practice, users can determine their values based on computation resources, but an upper bound of 4 suffices to unlock a near-optimal performance. ", "page_idx": 7}, {"type": "image", "img_path": "N8YbGX98vc/tmp/0be55d8b789b6112e8194bd9ab38a5188dc271fe7ebf506e56a58effd95e6957.jpg", "img_caption": ["Figure 3: Accuracy and FID on CIFAR10 under different $N_{\\mathrm{recur}}$ and $N_{\\mathrm{iter}}$ $s_{\\rho}(t),s_{\\mu}(t)$ are fixed to \u201cincrease\u201d structure, and $\\rho=\\gamma=0$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Searching strategy. The above analysis successfully simplify the task-specific hyper-parameter search problem without significant performance sacrifice. It remains to be decided the scalar values $\\rho,\\mu,\\gamma$ . Here we propose a strategy based on beam search to effectively and efficiently select their values. Specifically, our searching strategy starts with an initial set $T=\\{(\\bar{\\rho}_{\\mathrm{init}},\\bar{\\mu}_{\\mathrm{init}},\\bar{\\gamma}_{\\mathrm{init}})\\}$ , where these initial values are small enough to approximate TFG as an unconditional generation. At each searching step, for each tuple in $T$ , we separately double the values of $\\bar{\\rho},\\bar{\\mu}$ and $\\bar{\\gamma}$ to generate up to $3|T|$ new configurations. We conduct a small-sized generation trial for each new configuration and update $T$ to be the top $K$ configurations with the highest evaluation results that are determined by user requirements (e.g., accuracy, FID, or a combination). This iterative process is repeated until $T$ stabilizes or the maximum number of search steps is reached. Notice that this process is conducted with a much smaller sample size, and consequently, the computation time is highly controllable. ", "page_idx": 7}, {"type": "text", "text": "5 Benchmarking ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section comprehensively benchmarks training-free guidance under the TFG framework and the design searching strategy in Section 4. We consider 7 datasets, 16 different tasks, and 40 individual targets with a total experimental cost of more than 2,000 A100 GPU hours. For comparison, we also run experiments for each of the existing methods (where the design searching is conducted in the corresponding subspace). All methods, tasks, search strategies, and evaluations are unified in our codebase, with details specified in Appendices D and E. ", "page_idx": 7}, {"type": "text", "text": "5.1 Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Diffusion models. (1) CIFAR10-DDPM [48] is a U-Net [54] model trained on CIFAR10 [30] images. (2) ImageNet-DDPM [7] is an larger U-Net model trained on ImageNet-1k [55] images. (3) Cat-DDPM is trained on Cat [12] images. (4) CelebA-DDPM is trained on CelebA-HQ dataset [26] that consists millions of human facial images. (5) Molecule-EDM [24] is an equivariant diffusion model pretrained on molecule dataset QM9 [50] that performs molecule generation from scratch. (6)Stable-Diffusion $(\\nu l.5)$ [53] is a latent text-to-image model that generate images with text prompts. (7) Audio-Diffusion? is a audio diffusion model based on DDPM trained to generate mel spectrogramsof $256\\mathrm{x}256$ corresponding to 5 seconds of audio. ", "page_idx": 7}, {"type": "table", "img_path": "N8YbGX98vc/tmp/cad2496cb7bcb6cc10172021e215423711adf24d9e0f0ecf561fa1d40b20f03b.jpg", "table_caption": ["Table 3: Benchmarking TFG and existing algorithms on 16 task types and 40 individual targets. Each cell presents the guidance validity/generation fidelity averaged across multiple targets in the task (e.g., labels, image styles). The best guidance validity is bold, and the second best underline. The relative improvement of guidance validity is computed between TFG and the existing method with the highest guidance validity. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Tasks. Our tasks (Table 2) cover a wide range of interests, including Gaussian deblur, super-resolution, label guidance, style transfer, molecule property guidance, audio declipping, audio inpainting, and guidance combination. Each task is run on multiple datasets or with multiple targets (e.g., different labels, molecular properties, styles). ", "page_idx": 8}, {"type": "text", "text": "Other settings. We consistently set the time step $T=100$ and the DDIM parameter $\\eta=1$ .We consider $N_{\\mathrm{recur}}=1,N_{\\mathrm{iter}}=4$ and use a single sample for Implicit Dynamic (Line 4) throughout all experiments and methods for fair comparison. For TFG, the structures of $\\rho$ and $\\pmb{\\mu}$ are set to \"increase\u201d and the scalars $\\bar{\\rho},\\bar{\\mu},\\bar{\\gamma}$ are determined via our searching strategy. We follow the setting in original papers if they specify their hyper-parameters. blueFor specific tricks in the code that are not mentioned in papers, we choose to align with original papers. Otherwise, values are determined via searching with $1/8$ of the sample size and a maximum search step of 6. For fairness of comparison, we use accuracy as the metric during the search and compare different algorithms on the metric, but we report both accuracy and FID. ", "page_idx": 8}, {"type": "text", "text": "5.2  Benchmarking results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare all six methods in Table 3. TFG outperforms existing algorithms in 13 over 14 settings, achieving an average guidance validity improvement of $7.4\\%$ compared to the best existing algorithm. Notice that we do not compare with the best algorithm in terms of generation fidelity because obtaining high realness samples is not our objective in training-free guidance, and an unconditional model suffices to generate high realness samples (with extremely low validity). Interestingly, different methods achieve the second best performance on different tasks, suggesting the variance of these methods, while TFG is consistent thanks to the unification. ", "page_idx": 8}, {"type": "text", "text": "We want to highlight that despite the superior performance of TFG, the key intention of our experiments is not restrained to comparing TFG with existing methods, but more importantly to systematically benchmark under the training-free guidance setting to see how much we have achieved in various tasks with different difficulties. Below we go through each task separately and conduct relevant ablation studies to provide a more fine-grained analysis. ", "page_idx": 8}, {"type": "text", "text": "Fine-grained label guidance. In addition to the standard label guidance, we for the first time study the out-of-distribution fine-grained label guidance under the training-free setting, a problem where no existing training-based methods are available. We consider the bird-species guidance using an ", "page_idx": 8}, {"type": "text", "text": "EfficienNet trained to classify 525 fine-grained bird species. This problem remains highly difficult for leading text-to-image generative models such as DALLE. Under recurrence, TFG can generate at most $2.24\\%$ of accurate birds, compared with the unconditional generation rate of 0. ", "page_idx": 9}, {"type": "text", "text": "Recurrence on label guidance. We go back to the failure case we study in Section 3, i.e., the standard label guidance problem on CIFAR10 where the training-based method offers an $85\\%$ accuracy, while the accuracy of TFG without recurrence accuracy is $52\\%$ only. As presented in Table 4, increasing $N_{\\mathrm{recur}}$ significantly closes the gap from $33\\%$ to $8\\%$ . Similar improvement is observed in other datasets as well. ", "page_idx": 9}, {"type": "text", "text": "Multiple guidance and bias mitigation. We next consider the scenario with multiple targets: control the generation of human faces based on gender and hair color (or age) using two predictors. It is well known that the label imbalance in CelebA-HQ causes classifiers tofocus on spurious correlations $I76J$ ,such as using hair colors to classify gender, a biased feature we aim to avoid. The stratified performance of TFG on \u201cgender $^+$ age\" and \u201cgender $^+$ hair guidance are presented in Table 5. Despite the highly disparate performance, training-free guidance largely alleviates the imbalance: only $1\\%$ of images in CelebA are \u201cmale $^+$ blonde hair\", while the generated accuracy is $46.7\\%$ ", "page_idx": 9}, {"type": "table", "img_path": "N8YbGX98vc/tmp/bbf7943fa4a39e73e603bf2e2ca03d30a7136d84ed08d41f5b9a43bce8623e25.jpg", "table_caption": ["Table 4: The accuracy / FID for TFG with different recurrence step $N_{\\mathrm{recur}}$ on three label guidance datasets, averaged across all labels. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5: The accuracy of multi-label guidance on CelebA, where labels O and 1 correspond to female and male (gender), nonblonde and blonde (hair color), and young and old (age). The accuracy is lower for minority groups, indicating an implicit bias in the generation process. Despite this, it is still much higher than unconditional generation. ", "page_idx": 9}, {"type": "table", "img_path": "N8YbGX98vc/tmp/45c93df7a5413f0032bd9a2ec8efd75b7a725c7940faf71219e8aed0864f5e49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Molecule property guidance. To our knowledge, we are the first to study training-free guidance for molecule generation. We interestingly find in Table 3 that TFG is effective in guiding molecules towards desirable properties, yielding the highest guidance validity on 5 out of 6 targets with $5.64\\%$ MAE improvement over existing methods, verifying the generality of our approach as a unified framework in completely unseen domains. Notice that, unlike images, molecules with better validity usually have lower generation fidelity, a finding reflected in previous work [3]. ", "page_idx": 9}, {"type": "text", "text": "Audio Guidance. We extend our investigation to the audio modality, where TFG achieves significant relative improvements over existing methods. Given that the audio domain is rarely explored in training-free guidance literature, our benchmarks will contribute to future research in this area. ", "page_idx": 9}, {"type": "text", "text": "6  Discussions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Recently, training-free guidance for diffusion models has gained increasing attention and has been adopted in various applications. TFG is based on an extensive literature review over ten algorithmic papers for different purposes, including images, audio, molecules, and motions [34, 8, 43, 32, 19, 13, 16, 15, 45, 38, 68]. While we incorporate several key algorithms into our framework, we acknowledge that encompassing all approaches is impossible, as it would make the unification bloated and less practical. We seek to find a balance point by unifying most representative algorithms while keeping the techniques clear and easily studied. ", "page_idx": 9}, {"type": "text", "text": "An often discussed problem is why we care about training-free guidance, given the ever-growing community of language-based generative models such as the image generator of GPT4. In practice, there are countless conditional generation tasks where the conditions are hard to accurately convey to or represent by language encoders. For instance, it can fail to under a complex property of a molecule or generate CelebA-style faces. We give an illustrative analysis in Appendix A.1. Despite that training-free guidance is important, this paper does not systematically analyze what types of conditional generation are, in general, more suitable for the framework and what types are for language-based models. That said, training-free guidance is fundamentally difficult due to the misalignment between the training objective of target predictors and the diffusion, with a more detailed discussion in Appendix A.2. This paper does not comprehensively analyze this misalignment, and the gap between training-based and TFG remains high in some tasks like molecule property guidance. We hope that future works can analytically dive into these problems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1-3, 2024.   \n[2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 843-852, 2023.   \n[3] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energy-guided SDE for inverse molecular design. In The Eleventh International Conference on Learning Representations, 2023.   \n[4]  Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schonlieb, and Christian Etmann. Conditional image generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021.   \n[5]  Mikotaj Binkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv: 1801.01401, 2018.   \n[6]  Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical image analysis, 80:102479, 2022.   \n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.   \n[8]  Kieran Didi, Francisco Vargas, Simon V Mathis, Vincent Dutordoir, Emile Mathieu, Urszula J Komorowska, and Pietro Lio. A framework for conditional diffusion modelling with applications in motif scaffolding for protein design, 2024.   \n[9]  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[10] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pages 8489-8510. PMLR, 2023.   \n[11] Bradley Efron. Tweedie's formula and selection bias. Journal of the American Statistical Association, 106(496):1602-1614, 2011.   \n[12]  Jeremy Elson, John (JD) Douceur, Jon Howell, and Jared Saul. Asirra: A captcha that exploits interest-aligned manual image categorization. In Proceedings of 14th ACM Conference on Computer and Communications Security (CCS). Association for Computing Machinery, Inc., October 2007.   \n[13] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. arXiv preprint arXiv:2401.18085, 2024.   \n[14]  Alexandros Graikos, Srikar Yellapragada, and Dimitris Samaras. Conditional generation from unconditional diffusion models using denoiser representations. arXiv preprint arXiv:2306.01900, 2023.   \n[15] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, and Josh Susskind. Control3diff: Learning controllable 3d diffusion models from single-view images. In 2024 International Conference on 3D Vision (3DV), pages 685-696. IEEE, 2024.   \n[16] Xu Han, Caihua Shan, Yifei Shen, Can Xu, Han Yang, Xiang Li, and Dongsheng Li. Trainingfree multi-objective diffusion model for 3d molecule generation. In The Twelfth International Conference on Learning Representations, 2023.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.   \n[18]  Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, J Zico Kolter, Ruslan Salakhutdinov, and Stefano Ermon. Manifold preserving guided difusion. In The Twelth International Conference on Learning Representations, 2024.   \n[19] Carlos Hernandez-Olivan, Koichi Saito, Naoki Murata, Chieh-Hsin Lai, Marco A MartinezRamirez, Wei-Hsiang Liao, and Yuki Mitsufuji. Vrdmg: Vocal restoration via diffusion posterior sampling with multiple guidance. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 596-600. IEEE, 2024.   \n[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[21]  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.   \n[22] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fdelity image generation. Journal of Machine Learning Research, 23(47):1-33, 2022.   \n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[24] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusionfor molecule generation in3d. In International conference onmachine larning, ages 8867-8887. PMLR, 2022.   \n[25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694-711. Springer, 2016.   \n[26]  Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv: 1710.10196, 2017.   \n[27]  Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593-23606, 2022.   \n[28]  Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharif. Fr\\'echet audio distance: A metric for evaluating music enhancement algorithms. arXiv preprint arXiv: 1812.08466, 2018.   \n[29] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.   \n[30]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[31] Yann LeCun, Sumit Chopra, Raia Hadsell M Ranzato, and Fujie Huang. A tutorial on energybased learning. Predicting structured data, 1(0), 2006.   \n[32]  Jean-Marie Lemercier, Julius Richter, Simon Welker, Eloi Moliner, Vesa Valimaki, and Timo Gerkmann. Diffusion models for audio restoration. arXiv preprint arXiv:2402.09821, 2024.   \n[33]  Anat Levin, Yair Weiss, Fredo Durand, and William T Freeman. Understanding and evaluating blind deconvolution algorithms. In 2009 IEEE conference on computer vision and pattern recognition, pages 1964-1971. IEEE, 2009.   \n[34] Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Controllable music production with diffusion models and guidance gradients. arXiv preprint arXiv:2311.00613, 2023.   \n[35] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023.   \n[36] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell and Saining Xie. A convnet for the 2020s, 2022.   \n[37] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018.   \n[38] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offine reinforcement learning. In International Conference on Machine Learning, pages 22825-22855. PMLR, 2023.   \n[39] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11461-11471, June 2022.   \n[40]  Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2837-2845, 2021.   \n[41] Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, and Dahua Lin. A conditional point diffusion-refinement paradigm for 3d point cloud completion. arXiv preprint arXiv:2112.03530, 2021.   \n[42] Ross A Maller, Gernot Muller, and Alex Szimayer. Ornstein-uhlenbeck processes and extensions. Handbook of financial time series, pages 421-437, 2009.   \n[43] Eloi Moliner, Jaakko Lehtinen, and Vesa Valimaki. Solving audio inverse problems with a diffusion model. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.   \n[44] Meinard Muller. Dynamic time warping. Information retrieval for music and motion, pages 69-84, 2007.   \n[45] Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit, Ye Wang, Toshiaki Koike-Akino, Vishal M Patel, and Tim K Marks. Steered diffusion: A generalized framework for plug-andplay conditional image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20850-20860, 2023.   \n[46]  Kamal Nasrollahi and Thomas B Moeslund. Super-resolution: a comprehensive survey. Machine vision and applications, 25:1423-1468, 2014.   \n[47]  Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[48]  Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162-8171. PMLR, 2021.   \n[49] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.   \n[50] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.   \n[51]  Herbert E Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory, pages 388-394. Springer, 1992.   \n[52] Robin Rombach, Andreas Blatmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.   \n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684-10695, June 2022.   \n[54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part Il 18, pages 234-241. Springer, 2015.   \n[55]  Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challnge. International Journal of Computer Vsion (IJCV), 115(3):211-252, 2015.   \n[56]  Babak Saleh and Ahmed Elgammal. Large-scale classification of fne-art paintings: Learning the right metric on the right feature. arXiv preprint arXiv: 1505.00855, 2015.   \n[57]  Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. arXiv preprint arXiv:2102.09844, 2021.   \n[58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278-25294, 2022.   \n[59]  Alexander Shapiro. Monte carlo sampling methods. Handbooks in operations research and management science, 10:353-425, 2003.   \n[60]  Herbert A Simon. Spurious correlation: A causal interpretation. Journal of the American statistical Association, 49(267):467-479, 1954.   \n[61] Jascha Sohl-Dickstein, Eric Weis, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256-2265. PMLR, 2015.   \n[62] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising difusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[63] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided difusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 32483-32498. PMLR, 2023.   \n[64] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415- 1428, 2021.   \n[65]  Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[66] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[67]  Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, volume 6, pages 583-603. University of California Press, 1972.   \n[68] Haoyuan Sun, Bo Xia, Yongzhe Chang, and Xueqian Wang. Generalizing alignment paradigm of text-to-image generation with preferences through $f$ -divergence minimization. arXiv preprint arXiv:2409.09774, 2024.   \n[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention, 2021.   \n[70] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In The Eleventh International Conference on Learning Representations, 2023.   \n[71]  Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep learning for image super-resolution: A survey. IEEE transactions on pattern analysis and machine intelligence, 43(10):3365-3387, 2020.   \n[72] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976): 1089-1100, 2023.   \n[73] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592-38610. PMLR, 2023.   \n[74] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.   \n[75] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. Openood: Benchmarking generalized out-of-distribution detection. 2022.   \n[76] Haotian Ye, James Zou, and Linjun Zhang. Freeze then train: Towards provable representation learning under spurious correlations and feature noise. In International Conference on Artificial Intelligence and Statistics, pages 8968-8990. PMLR, 2023.   \n[77] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.   \n[78] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Trainingfree energy-guided conditional diffusion model. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n[79] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-based r-cnns for finegrained category detection. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 113, pages 834-849. Springer, 2014.   \n[80] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586-595, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A  The Motivation of Studying Training-free Guidance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we argue that training-free guidance using off-the-shelf models is a crucial and timely research problem deserving more attention and effort. We begin by providing an illustrative analysis that highlights the limitations of current strong text-to-image generative models, underscoring the necessity for training-free guidance. Furthermore, we assert that training-free guidance remains a significant challenge, with previous literature underestimating its complexity. Given its necessity and inherent difficulties, we call for increased focus from the research community on this problem and offer our benchmarks and code base to help accelerate progress in this area. ", "page_idx": 15}, {"type": "text", "text": "A.1  Failure case of image generation with GPT4 ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "N8YbGX98vc/tmp/d75853c85a3b55c8f4ad03fe09f4084fd66b0a28f2dab982d672695b5be7cec5.jpg", "img_caption": ["Figure 4: Prompting GPT4 to generate property guided molecules. It is hard for the image generator to understand the target and generate faithful samples. In this dialog, GPT4 claims to generate a benzene molecule but the sample is apparently not a benzene. There are also many invalid carbon atoms with more than 4 bonds and the polarizability target is not achieved. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "It's hard for GPT4 to understand targets. In Figure 4, we ask GPT-4 to generate a molecule with polarizability $\\alpha=3$ , which is a task we use to evaluate training-free guidance (refer to Figure 16 for visualization). We found that the GPT-4 generated molecule is apparently invalid and unrealistic: the generated molecule contains many carbon atoms with more than 4 bonds (the maximum allowed number is 4); and the generated molecule is apparently not a benzene which is claimed by the text outputs. From this case we may understand that it is hard to follow diverse user-defined instructions for the foundational generative models, where the user-defined targets may be subtle, fine-grained, combinatorial, and open-ended. ", "page_idx": 15}, {"type": "text", "text": "To this end, training-free guidance offers two key advantages: (1) It allows for greater specificity in target requirements by enabling the use of a differentiable objective function, making the generation process more sterable; (2) The objective function is plug-and-play, facilitating the addition of new targets and tasks to a pre-trained generative model. Since there is no need to retrain the diffusion or prediction models, this approach makes the generative process lightweight and applicable to various downstreamtasks. ", "page_idx": 16}, {"type": "text", "text": "It's hard for GPT4 to capture the targeted distribution. _Another important metrit for trainingfree guidance is the fexibility of choosing diffusion models. For the same target, we can switch from different diffusion models to change the unconditional background distribution. For example, it is hard for GPT4 to generate CelebA-like samples though it \u201cknows CelebA dataset very well': ", "page_idx": 16}, {"type": "image", "img_path": "N8YbGX98vc/tmp/2acf9cae2427db21b163ac5467d21f18312aee81b134069425833817c8b6e6c9.jpg", "img_caption": ["Figure 5: Prompting GPT4 to generate CelebA-like images. We first prompt ChatGPT to probe its knowledge of CelebA dataset and then ask it to generate a young man figure in CelebA style. However, the generated figure is apprently not in the distribution of CelebA (refer to Figure 12) for comparison. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "The fexibility to use different diffusion models provides an opportunity to generate a wider range of user-defined targets. With training-free guidance, individuals can select their preferred diffusion model to establish the background distribution and use the prediction model to steer the generation towards specific properties. This approach may represent a future direction for human-AI interaction. ", "page_idx": 16}, {"type": "image", "img_path": "N8YbGX98vc/tmp/15b3ff472ac82888e1292c1b350d77c0998a587359c847e807dc4e0a80b0803f.jpg", "img_caption": ["Figure 6: (Left) The accuracy and FID of different methods under different settings on CIFAR10 [30], average across ten labels and 2048 samples per label. The suffix number in UGD and FreeDoM represents recurrent step $N_{\\mathrm{recur}}$ , and (fake) stands for a synthetic setting where we apply a trainingbased classifier but set $t=0$ and use the same training-free guidance methods. A huge performance gap between different settings suggests the intrinsic difficulty of training-free guidance. (Right) Illustration of generated \u201cship\u201d\u2019 using MPGD under different settings (top) and the sampling trajectory of the predicted clean image $\\mathbf{\\boldsymbol{x}}_{0\\mid t}$ (down). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.2   The fundamental challenge of training-free guidance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Despite the array of algorithms available and their reported successes in various applications, we conduct a case study on CIFAR10 [30] to illustrate the challenging nature of training-free guidance and the insufficiency of existing methods. Specifically, we compare the training-based approach and training-free approach for the label guidance task on CIFAR10, with the diffusion model pretrained by [7], the training-based time-dependent classifier $f({\\pmb x},t)$ by [7], and the training-free standard label classifier $f(x)$ pretrained only on clean CIFAR10 by [9]. and a \u201cfake\u201d training-free classifier defined as $f({\\boldsymbol{x}},t)|_{t=0}$ . The first serves as the oracle benchmark, while the second corresponds to the standard training-free guidance. The third setting, as considered in LGD [63], uses a \u201cfake\" training-free classifier since its parameters are shared across different time steps $t$ during training, resulting in an implicit regularization that is not available for practical predictors. This setting serves as a comparison to help identify the difficulty of training-free guidance. ", "page_idx": 17}, {"type": "text", "text": "Quantitative and qualitative results are shown in Figure 6. All training-free approaches significantly under-perform training-based guidance, with a significant portion of generated images being highly unnatural (when guidance is strong) or irrelevant to the label (when guidance is weak). A more clear illustration on this can be found in Figure 7. In terms of \u201cfake'\u201d classifiers, it leads to a remarkable difference from real training-free classifiers even under identical experimental settings. It generates much less messy images due to the implicit regularization from the training process, where noisy images are also \u201cseen\u201d (although $t$ is fixed to O upon guidance). Unfortunately, such types of predictors are inaccessible in practice (otherwise, we can use classifier-based guidance directly). From the comparison, we know that the key challenge of training-free guidance is the lack of a \"smoothing\"\u201d classifier that can produce faithful guidance in the \u201cunseen\u201d noisy image space. ", "page_idx": 17}, {"type": "image", "img_path": "N8YbGX98vc/tmp/b4db4554b6a7bf04f3df7a2c11dd9bb70fc0f73d3b6f8df822d5388faebd43ef.jpg", "img_caption": ["Figure 7: Illustration on CIFAR10 dogs generated with different algorithms. Compared with trainingbased method, training-free methods fall behind but TFG significantly outperforms existing methods. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "The observation largely uncovers the essential difficulties of training-free guidance, and motivates us to systematically study techniques that can improve generation quality. Unfortunately, comparisons between existing techniques are ambiguous since different methods are tested on distinct and primarily qualitative applications, which in turn hinders the in-depth study in this field. To this end, we resolve to revisit this complicated scenario and design a clear and comprehensive framework for training-free guidance. ", "page_idx": 18}, {"type": "text", "text": "B Pseudo-code and schematics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We have presented the pseudo-code of TFG in Algorithm 1. Below, we provide a copy of the DPS (Algorithm 2), MPGD (Algorithm 3), FreeDoM (Algorithm 4), UGD (Algorithm 5), and LGD (Algorithm 6). Notice that LGD does not provide a pseudo-code, and we present their algorithm following their paper as a modification of DPS. We do not change the original algorithms? notations for reference. Please see the proof in Appendix C for the equivalence analysis. We provide a schematic of existing algorithms in Figure 8. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 2 DPS - Gaussian   \nRequire: N, y, {Ci}1,{}1 $x_{N}\\sim\\mathcal{N}(0,I)$ for $i=N-1$ to 0 do $\\hat{s}\\gets s_{\\theta}(x_{i},i)$ $\\begin{array}{r}{\\hat{x}_{0}\\gets\\frac{1}{\\sqrt{\\bar{\\alpha}_{i}}}(x_{i}+(1-\\bar{\\alpha}_{i})\\hat{s})}\\end{array}$ z \\~ N(0, I) $\\begin{array}{r}{x_{i-1}^{\\prime}\\leftarrow\\frac{\\sqrt{\\alpha_{i}}(1-\\bar{\\alpha}_{i-1})}{1-\\bar{\\alpha}_{i}}x_{i}+\\frac{\\sqrt{\\bar{\\alpha}_{i-1}}\\beta_{i}}{1-\\bar{\\alpha}_{i}}\\hat{x}_{0}+\\tilde{\\sigma}_{i}z}\\end{array}$ $x_{i-1}\\leftarrow x_{i-1}^{\\prime}-\\zeta_{i}\\nabla_{x_{i}}\\|y-A(\\hat{x}_{0})\\|_{2}^{2}$ end for return Xo ", "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "N8YbGX98vc/tmp/ccd9dc0776166ad1aed3700865065fde110a07860f69cbb3f58759587f2b04ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Algorithm 4 FreeDoM $^+$ Efficient Time-Travel Strategy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Require: condition c, unconditional score estimator $\\overline{{s(\\cdot,t)}}$ time-independent distancemeasuring functin $\\overline{{\\mathcal{D}_{\\theta}(\\mathbf{c},\\cdot)}}$ pre-defined parameters $\\beta_{t}$ $\\bar{\\alpha}_{t}$ , learning rate $\\rho_{t}$ , and the repeat times of time travel of each step $\\{r_{1},\\cdot\\cdot\\cdot,r_{T}\\}$ $\\mathbf{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ for $t=T,...,1$ do for $i=r_{t},...,1$ do $\\begin{array}{r l}&{\\mathbf{e}_{1}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{1})\\,\\mathrm{tr}\\,{\\boldsymbol{\\vdash}}\\,\\mathbf{1},\\mathbf{else}\\,\\mathbf{\\epsilon}_{1}=\\mathbf{0}.}\\\\ &{\\mathbf{x}_{t-1}=(1+\\frac{1}{2}\\beta_{t})\\mathbf{x}_{t}+\\beta_{t}s(\\mathbf{x}_{t},t)+\\sqrt{\\beta_{t}}\\mathbf{\\epsilon}_{1}}\\\\ &{\\mathbf{x}_{0\\mid t}=\\frac{1}{\\sqrt{\\alpha_{t}}}(\\mathbf{x}_{t}+(1-\\bar{\\alpha}_{t})s(\\mathbf{x}_{t},t))}\\\\ &{g_{t}=\\nabla_{\\mathbf{x}_{t}}\\mathcal{D}_{\\theta}(\\mathbf{c},\\mathbf{x}_{0\\mid t}(\\mathbf{x}_{t})))}\\\\ &{\\mathbf{x}_{t-1}=\\mathbf{x}_{t-1}-\\rho_{t}g_{t}}\\\\ &{\\mathbf{if}\\,{\\boldsymbol{\\vdash}}\\,\\mathbf{1}\\,\\mathbf{then}}\\\\ &{\\mathbf{\\epsilon}_{2}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}\\\\ &{\\mathbf{x}_{t}=\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1}+\\sqrt{\\beta_{t}}\\mathbf{\\epsilon}_{2}}\\end{array}$ end if end for end for return $\\mathbf{x}_{\\mathrm{0}}$ ", "page_idx": 19}, {"type": "text", "text": "Algorithm 5 Universal Guidance (its $\\alpha$ is our $\\bar{\\alpha}$ ", "page_idx": 20}, {"type": "text", "text": "Parameter: Recurrent steps $k$ , gradient steps $m$ for backward guidance and guidance strength $s(t)$   \nRequired: $z_{T}$ sampled from ${\\mathcal{N}}(0,I)$ , diffusion model $\\epsilon_{\\theta}$ , noise scales $\\{\\alpha_{t}\\}_{t=1}^{T}$ , guidance function f, loss   \nfunction $\\ell$ , and prompt $c$   \nfor $t=T,T-1,\\ldots,1$ do for $n=1,2,\\ldots,k$ do Calculate 2o as t-(\u221a-\u03b1t)eo(zt.t) $\\hat{\\epsilon}_{\\boldsymbol{\\theta}}(z_{t},t)=\\epsilon_{\\boldsymbol{\\theta}}(z_{t},t)+\\boldsymbol{\\dot{s}}(t)\\cdot\\nabla_{z_{t}}\\ell(c,\\mathbf{f}(\\hat{z}_{0}))$ $m>0$ then Calculate $\\Delta z_{0}$ by minimizing $\\ell(c,\\mathbf{f}(\\hat{z}_{0}+\\Delta))$ . with $m$ steps of gradient descent Perform backward universal guidance by $\\hat{\\epsilon}_{\\theta}\\leftarrow\\hat{\\epsilon}_{\\theta}-\\sqrt{\\alpha_{t}/(1-\\alpha_{t})}\\Delta z_{0}$ end if $\\begin{array}{r l}&{z_{t-1}\\leftarrow S(z_{t},\\hat{\\epsilon}_{\\theta},t)}\\\\ &{\\epsilon^{\\prime}\\sim\\mathcal{N}(0,I)}\\\\ &{z_{t}\\leftarrow\\sqrt{\\alpha_{t}/\\alpha_{t-1}}z_{t-1}+\\sqrt{1-\\alpha_{t}/\\alpha_{t-1}}\\epsilon^{\\prime}}\\end{array}$ end for   \nend for ", "page_idx": 20}, {"type": "text", "text": "Algorithm 6 LGD (from DPS) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Require: N, y, {S}=1, {o}=1, n $x_{N}\\sim\\mathcal{N}(0,I)$ for $i=N-1$ to 0 do \u2190 s\u03b8(xi,i) $\\begin{array}{r}{\\hat{x}_{0}\\gets\\frac{1}{\\sqrt{\\bar{\\alpha}_{i}}}(x_{i}+(1-\\bar{\\alpha}_{i})\\hat{s})}\\end{array}$ z \\~ N(0, I) $\\begin{array}{r}{x_{i-1}\\leftarrow\\frac{\\sqrt{\\alpha_{i}}(1-\\bar{\\alpha}_{i-1})}{1-\\bar{\\alpha}_{i}}x_{i}+\\frac{\\sqrt{\\bar{\\alpha}_{i-1}}\\beta_{i}}{1-\\bar{\\alpha}_{i}}\\hat{x}_{0}+\\tilde{\\sigma}_{i}z}\\end{array}$ Ci-1 \u2190 Ci-1 -GVx log ( Z=1exp(-ey(x() \u25b3 $x_{i}^{(j)}$ sampled i.d. from N(to, 1 end for return xo ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "N8YbGX98vc/tmp/bc735aa4479e57a9445fea4eaa0ab3b7677036ec1f589271cb9fd1ff3eeb6de6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: (a) The reversed diffusion process. (b) Mllustration of different training-free guidance algorithms at the $t$ -th reversed diffusion step. ", "page_idx": 20}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We prove Theorem 3.2 and Lemma 3.3 below. ", "page_idx": 21}, {"type": "text", "text": "C.1Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. For each algorithm, we prove the equivalence of design space separately below. Notice that when $\\bar{\\gamma}=0$ $\\tilde{f}$ degradesbackto $f$ ", "page_idx": 21}, {"type": "text", "text": "MPGD (Algorithm 3). Below we demonstrate that any hyper-parameter $\\{c_{t}\\}_{t=1}^{T}$ in Algorithm 3 is equivalent to the TFG with $f(\\mathbf{x}_{0\\mid t})=\\exp\\{-L(\\mathbf{x}_{0\\mid t};y)\\}$ and hyper-parameter ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N_{r e c u r}=1,N_{i t e r}=1,\\bar{\\gamma}=0,\\rho=\\mathbf{0},\\mu=(c_{1},\\cdots\\,,c_{T})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show this, notice that since both $\\rho$ and $\\bar{\\gamma}$ are zero, Line 4 and Line 7 take no effect. When using the identical sampling function (Line 9), TFG generates $x_{t-1}$ using ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t-1}=\\mathrm{Sample}(x_{t},x_{0\\mid t},t)+c_{t}\\sqrt{\\bar{\\alpha}_{t-1}}\\nabla_{x_{0\\mid t}}\\log f(x_{0\\mid t})}\\\\ &{\\qquad=\\sqrt{\\bar{\\alpha}_{t-1}}x_{0\\mid t}+\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_{t}^{2}}\\frac{x_{t}-\\sqrt{\\bar{\\alpha}_{t}}x_{0\\mid t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}+\\sigma_{t}\\epsilon_{t}+c_{t}\\sqrt{\\bar{\\alpha}_{t-1}}\\nabla_{x_{0\\mid t}}\\log f(x_{0\\mid t})}\\\\ &{\\qquad=\\sqrt{\\bar{\\alpha}_{t-1}}(x_{0\\mid t}-c_{t}\\nabla_{x_{0\\mid t}}L(x_{0\\mid t};y))+\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_{t}^{2}}\\epsilon_{\\theta}(x_{t},t)+\\sigma_{t}\\epsilon_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is exactly the formula used in MPGD. ", "page_idx": 21}, {"type": "text", "text": "DPS (Algorithm 2) and FreeDoM (Algorithm 4). We prove both algorithms together as DPS is a special case of FreeDoM (without recurrence). Specifically, any hyper-parameter $\\{\\rho_{t}\\}_{t=1}^{T}$ ,time travel step $r$ and distance function $\\mathcal{D}(\\pmb{c},\\cdot)$ in Algorithm 4 is equivalent to TFG with $f(\\pmb{x}_{0\\mid t})\\,=$ $\\exp\\{-D(\\pmb{c},\\pmb{x}_{0\\mid t})\\}$ and hyper-parameter ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N_{r e c u r}=r,N_{i t e r}=0,\\bar{\\gamma}=0,\\rho=\\big(\\sqrt{\\alpha_{1}}\\rho_{1},\\cdots,\\sqrt{\\alpha_{T}}\\rho_{T}\\big)^{\\top},\\mu=\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show this, notice that both algorithms have the identical resampling step from $x_{t-1}$ to $\\pmb{x}_{t}$ ,So it suffices to prove that the formula to generate $x_{t-1}$ in each recurrent step is the same. For FreeDoM, wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf x_{t-1}=\\operatorname{Sample}(\\mathbf x_{t},x_{0\\mid t},t)-\\rho_{t}\\nabla_{\\mathbf x_{t}}\\mathcal D(\\mathbf c,\\mathbf x_{0\\mid t})}\\\\ &{\\qquad=\\operatorname{Sample}(\\mathbf x_{t},x_{0\\mid t},t)-(\\sqrt{\\alpha_{t}}\\rho_{t})\\nabla_{\\mathbf x_{t}}\\log f(\\mathbf x_{0\\mid t})/\\sqrt{\\alpha_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the last line equals the combination of Line 7 and Line 7 in TFG. ", "page_idx": 21}, {"type": "text", "text": "LGD (Algorithm 6). Any hyper-parameter $\\{\\zeta_{t}\\}_{t=1}^{T},n$ in LGD is equivalent to TFG with $f({\\pmb x})=$ $\\exp\\{-\\ell_{y}(x)\\}$ , sample size $n$ in Line 4, and hyper-parameter ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N_{r e c u r}=1,N_{i t e r}=0,\\bar{\\gamma}=1,\\rho=\\mathbf{0},\\mu=(\\zeta_{1},\\cdots\\,,\\zeta_{T})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that ${\\bar{\\alpha}}_{t}$ in TFG equals $1/(1+\\sigma_{t}^{2})$ in the DPS algorithm. With this, the equivalence is clear from the pseudo-code of both algorithms. ", "page_idx": 21}, {"type": "text", "text": "UGD (Algorithm 5). \u03b2 Any hyper-parameter $k,m,s(t)$ in UGD is equivalent to TFG with $f({\\pmb x})=$ $\\exp\\{-\\ell(\\bar{c_{,}}\\,\\mathbf{f}(x))\\}$ and hyper-parameter ", "page_idx": 21}, {"type": "equation", "text": "$$\nN_{r e c u r}=k,N_{i t e r}=m,\\bar{\\gamma}=0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\rho=(-\\sqrt{\\alpha_{1}}s(1)\\delta_{1},\\cdots,-\\sqrt{\\alpha_{T}}s(T)\\delta_{T})^{\\top},\\mu=(-\\sqrt{\\frac{\\alpha_{1}}{1-\\bar{\\alpha}_{1}}}\\delta_{1},\\cdots,-\\sqrt{\\frac{\\alpha_{T}}{1-\\bar{\\alpha}_{T}}}\\delta_{T})^{\\top},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\delta_{t}$ is the coefficient of $\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)$ in sampler $S$ in the UGD algorithm (which is negative). To show this, notice that $\\Delta_{t}=\\rho_{t}\\nabla_{x_{t}}\\log\\tilde{f}(x_{0\\mid t})=-\\rho_{t}\\nabla_{x_{t}}\\ell(c,\\mathbf{f}(x_{0\\mid t}))=\\sqrt{\\alpha_{t}}s(t)\\delta_{t}\\nabla_{x_{t}}\\ell(c,\\mathbf{f}(x_{0\\mid t}))$ By replacing this into Line 9, the equivalence of guidance Variance Guidance can be easily observed. A similar deduction can be made for the mean guidance as well. ", "page_idx": 21}, {"type": "text", "text": "C.2Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{\\mathbf{\\boldsymbol{x}}_{0\\mid t}=\\frac{\\mathbf{\\boldsymbol{x}}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}\\left(\\mathbf{\\boldsymbol{x}}_{t},t\\right)}{\\sqrt{\\bar{\\alpha}_{t}}}}\\end{array}$ .Accordin tsmle chainrul wehae ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{t}=\\rho_{t}\\nabla_{\\mathbf{x}_{t}}\\log\\tilde{f}(\\mathbf{x}_{0\\mid t})}\\\\ &{\\quad=\\rho_{t}\\frac{I-\\sqrt{1-\\bar{\\alpha}_{t}}\\nabla_{\\mathbf{x}_{t}}\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t\\right)}{\\sqrt{\\bar{\\alpha}_{t}}}\\nabla_{\\mathbf{x}_{0\\mid t}}\\log\\tilde{f}(\\mathbf{x}_{0\\mid t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The perfect optimization assumption implies the relationship between $\\epsilon_{\\theta}$ and the score of $p_{t}(\\pmb{x}_{t})$ which we leverage and obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{t}=\\rho_{t}\\frac{\\displaystyle I-\\sqrt{1-\\bar{\\alpha}_{t}}\\nabla_{x_{t}}\\epsilon_{\\theta}\\left(x_{t},t\\right)}{\\sqrt{\\bar{\\alpha}_{t}}}\\nabla_{x_{0\\mid t}}\\log\\tilde{f}(\\mathbf{x}_{0\\mid t})}\\\\ &{\\quad=\\rho_{t}\\frac{\\displaystyle I+(1-\\bar{\\alpha}_{t})\\nabla_{x_{t}}^{2}\\log p_{t}(\\mathbf{x}_{t})}{\\sqrt{\\bar{\\alpha}_{t}}}\\nabla_{x_{0\\mid t}}\\log\\tilde{f}(\\mathbf{x}_{0\\mid t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, it remains to draw a connection between the conditional covariance between $\\Sigma_{0|t}$ and $\\nabla_{\\pmb{x}_{t}}^{2}\\log p_{t}(\\pmb{x}_{t})$ . Omit the subscript $\\pmb{x}_{t}$ $\\nabla_{\\pmb{x}_{t}}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla^{2}\\log p_{t}({\\pmb x}_{t})=\\frac{\\nabla^{2}p_{t}({\\pmb x}_{t})}{p_{t}({\\pmb x}_{t})}-\\nabla\\log p_{t}({\\pmb x}_{t})(\\nabla\\log p_{t}({\\pmb x}_{t}))^{\\top}}\\ ~}\\\\ {{\\displaystyle=\\frac{1}{p_{t}({\\pmb x}_{t})}\\int_{{\\pmb x}_{0}\\in{\\pmb x}}p_{0}({\\pmb x}_{0})\\nabla^{2}p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})\\mathrm{d}{\\pmb x}_{0}-\\nabla\\log p_{t}({\\pmb x}_{t})(\\nabla\\log p_{t}({\\pmb x}_{t}))^{\\top}}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Notice that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla^{2}p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})=p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})\\nabla^{2}\\log p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})+\\frac{1}{p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})}\\nabla p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})(\\nabla p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0}))^{\\top}}}\\\\ {{=-\\frac{p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})}{1-\\bar{\\alpha}_{t}}I+p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})(\\frac{{\\pmb x}_{t}-\\sqrt{\\bar{\\alpha}_{t}}x_{0}}{1-\\bar{\\alpha}_{t}})(\\frac{{\\pmb x}_{t}-\\sqrt{\\bar{\\alpha}_{t}}x_{0}}{1-\\bar{\\alpha}_{t}})^{\\top}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Replacing_the LHS in the above equation in Equation (14), and noticing that $\\mathbb{E}[\\pmb{x}_{0}|\\pmb{x}_{t}]\\ =$ mt+(1-at) log pt(et) , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}\\log p_{t}(x_{t})=-\\frac{1}{1-\\bar{\\alpha}_{t}}I+\\int_{\\arg(\\boldsymbol{\\alpha})(x_{t})(\\frac{x_{t}-\\sqrt{\\alpha_{t}}x_{0}}{1-\\bar{\\alpha}_{t}})(\\frac{x_{t}-\\sqrt{\\alpha_{t}}x_{0}}{1-\\bar{\\alpha}_{t}})^{\\top}\\mathrm{d}x_{0}}\\\\ &{\\qquad\\qquad\\qquad-\\nabla\\log p_{t}(x_{t})(\\nabla\\log p_{t}(x_{t}))^{\\top}}\\\\ &{=-\\frac{1}{1-\\bar{\\alpha}_{t}}I+\\int_{\\arg(\\boldsymbol{\\alpha})(x_{t})(\\frac{x_{t}-\\sqrt{\\alpha_{t}}x_{0}}{1-\\bar{\\alpha}_{t}})(\\frac{x_{t}-\\sqrt{\\alpha_{t}}x_{0}}{1-\\bar{\\alpha}_{t}})^{\\top}\\mathrm{d}x_{0}}\\\\ &{\\qquad\\qquad\\qquad-\\left(\\frac{\\sqrt{\\alpha_{t}}\\mathbb{E}[x_{0}|x_{t}]-x_{t}}{1-\\bar{\\alpha}_{t}}\\right)(\\frac{\\sqrt{\\alpha_{t}}\\mathbb{E}[x_{0}|x_{t}]-x_{t}}{1-\\bar{\\alpha}_{t}})^{\\top}}\\\\ &{=-\\frac{1}{1-\\bar{\\alpha}_{t}}I+\\mathrm{Cov}\\Big[\\frac{\\sqrt{\\alpha_{t}}x_{0}-x_{t}}{1-\\bar{\\alpha}_{t}}|x_{t}\\Big]}\\\\ &{=-\\frac{1}{1-\\bar{\\alpha}_{t}}I+\\frac{\\bar{\\alpha}_{t}}{(1-\\alpha_{t})^{2}}\\Sigma_{0_{1}|t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The proof is finished by substituting $\\nabla^{2}\\log p_{t}(\\pmb{x}_{t})$ in Equation (12) by the above result. ", "page_idx": 22}, {"type": "text", "text": "D Task details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1  Gaussian Deblur ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In computer vision, the Gaussian deblur task addresses the challenge of restoring clarity to images that have been blurred by a Gaussian process. Gaussian blur, a common image degradation, simulates effects such as out-of-focus photography or atmospheric disturbances. It is characterized by the convolution of an image with a Gaussian kernel, a process that spreads the pixel values outwards, leading to a smooth, uniform blur [33]. The deblurring task seeks to reverse this effect, aiming to retrieve the original sharp image. ", "page_idx": 23}, {"type": "text", "text": "Guidance target. Specifically, we apply a $61\\times61$ sized Gaussian blur with kernal intensity 3.0 to original $256\\times256$ images. A random noise with a variance of $\\sigma^{2}\\,=\\,0.05^{2}$ is added to the noisy images. If we denote the above process as a blurring operator $\\mathcal{A}_{\\mathrm{blur}}\\,:\\,x\\,\\to\\,y$ , where E R256\u00d7256x3 $\\pmb{y}\\in\\mathbb{R}^{256\\times256\\times3}$ are original images and noisy images,then the target of Gaussian Deblur is to generate a clean image $\\pmb{x}_{0}$ such that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}\\exp(-\\|\\pmb{A}_{\\mathrm{blur}}(\\pmb{x}_{0})-\\pmb{y}\\|_{2}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which means if we project the generated image into the noisy space, the noisy samples $A_{\\mathrm{blur}}(x)$ should be similar to the ground truth noisy images $\\textit{\\textbf{y}}$ ", "page_idx": 23}, {"type": "text", "text": "Evaluation metrics. In our experiments, we evaluate each guidance method on a set of 256 samples generated by Cat-DDPM. We apply the FID [26] to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS) [80] to evaluate the guidance validity. ", "page_idx": 23}, {"type": "image", "img_path": "N8YbGX98vc/tmp/610c24aadea33484a41f98424fe42b6456a8925b83883dc579cb7fd0897ca5b0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "N8YbGX98vc/tmp/8a2503fe276b321e4c6da993b834c1358b24ff36e9b0ea99baacd60761b545f0.jpg", "img_caption": ["Figure 9: Quantitative comparison of different training-free guidance methods on Gaussian deblur task. Our TFG method can produce clean images without background noise (unlike FreeDoM and UGD), faithful image features (unlike DPS) and vivid image details (compared to LGD). $N_{\\mathrm{recur}}$ is set to 1 for all methods. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.2 Super Resolution ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Super-resolution in computer vision refers to the process of enhancing the resolution of an imaging system, aimed at reconstructing a high-resolution image from one or more low-resolution observations. This technique is fundamental in overcoming the inherent limitations of imaging sensors and improving the detail and quality of digital images. Super-resolution has broad applications, ranging from satellite imaging and surveillance to medical imaging and consumer photography [46]. ", "page_idx": 25}, {"type": "text", "text": "Guidance target. Specifically, we simply down-sample to original $256\\,\\times\\,256$ images to the resolution of $64\\times64$ A random noise with a variance of $\\sigma^{2}=\\bar{0}.05^{2}$ is also added to the noisy images. If we denote the above process as a degradation operator $\\mathbb{R}^{25\\overline{{6}}\\times256\\times3}$ $\\pmb{y}\\in\\mathbb{R}^{256\\times256\\times3}$ are riginalimagesandown-sampled images thenthe arget o super- $A_{\\mathrm{down}}\\,:\\,x\\,\\to\\,y$ ,where $_x\\in$ is to generate a high resolution image $\\scriptstyle x_{0}$ such that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}\\exp(-\\|\\pmb{A}_{\\mathrm{down}}(\\pmb{x}_{0})-\\pmb{y}\\|_{2}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which means if we project the generated image into the downsampled image space, the downsampled samples ${\\mathcal{A}}_{\\mathrm{down}}(x)$ should be similar to the ground truth downsampled images $\\textit{\\textbf{y}}$ ", "page_idx": 25}, {"type": "text", "text": "Evaluation metrics. Similar to Gaussian Deblur, we evaluate each guidance method on a set of 256 samples generated by Cat-DDPM. We apply FID [26] to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS) [8O] to evaluate the guidance validity. ", "page_idx": 25}, {"type": "image", "img_path": "N8YbGX98vc/tmp/d60d91d3a9c689873d1a8e961a6a5e5610b779302cf7d74d0d1983de3ff9a02a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "N8YbGX98vc/tmp/59f84565785bddf2a7b96399f17bee792d96df3cc2d14456c80e6fd328a42718.jpg", "img_caption": ["Figure 10: Quantitative comparison of different training-free guidance methods on super-resolution task. Our TFG method can produce clean images, faithful image features (unlike DPS, MPGD) and vivid image details (compared to LGD, UGD). $N_{\\mathrm{recur}}$ is set to 1 for all methods. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.3 Label Guidance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Label guidance is a standard task for conditional generation studied in previous literature [7, 23]. The target is to generate images conditioned on a certain label. We found this standard task is rarely studied in training-free guidance work and there exist an evident performance gap between training-based guidance and existing training-free guidance as shown in Section 3. ", "page_idx": 27}, {"type": "text", "text": "Label sets. In our experiments, we studied labels from CIFAR10 [30] and ImageNet [55]. We average the results on 10 labels from CIFAR10 if there is no extra explanation. For ImageNet, which is resource-hungry to do comprehensive inference, we randomly select 4 labels (111, 222, 333, 444) to evaluate the methods. The corresponding label-ID and their names are as follows: ", "page_idx": 27}, {"type": "table", "img_path": "N8YbGX98vc/tmp/3a2487c89d5df0eba5b0a633a796407558ba4e90cb079c14cdad2f5c63a4097c.jpg", "table_caption": ["Table 6: CIFAR-10 Dataset Labels "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "N8YbGX98vc/tmp/857c7be1cfbe9927b63906e06d0a20664c063be98112256f63b5ce3a91e6e586.jpg", "table_caption": ["Table 7: Selected ImageNet-1K Dataset Labels "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Guidance target. For each dataset, we use the output probability of a pre-trained classifier $h(\\cdot)$ as the target probability. Our target is to maximize the certain classification probability of a given label, i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}\\mathrm{softmax}(h(\\pmb{x}_{0}))_{i},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $i$ is the label-ID of the target label, and $h(\\cdot)$ is the logits of $\\pmb{x}_{0}$ produced by the pre-trained classifier. For CIFAR10 and ImageNet, we use a pre-trained classifier based on ResNet [17] and VIT [9] that are provided from [75] and [9] respectively. The image resolution for CIFAR10 and ImageNet are $32\\times32$ and $256\\times256$ respectively. ", "page_idx": 27}, {"type": "text", "text": "Evaluation metrics. We follow the image generation literature to use Fr\u00e9chet inception distance (FID) [20] to assess the fidelity of generated images. The reference images are filtered from the entire dataset of CIFAR10 or ImageNet with the target label, and we set sample sizes as 2560 and 256 for CIFAR and ImageNet respectively. For validity, we use another pre-trained classifier to compute accuracy other than the one used in providing guidance to avoid over-confidence: ", "page_idx": 27}, {"type": "equation", "text": "$$\na c c u r a c y=\\frac{\\#c l a s s i f t e d\\,a s\\;t a r g e t\\;l a b e l}{\\#g e n e r a t e d\\;s a m p l e s}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For CIFAR10, we use a pre-trained ConvNeXT [36] downloaded from HuggingFace $\\mathrm{Hub}^{7}$ .And for ImageNet, we use the pre-trained DeiT [69] downloaded from HuggingFace $\\mathrm{H}\\overline{{\\mathfrak{u}}}\\mathfrak{d}^{8}$ ", "page_idx": 27}, {"type": "image", "img_path": "N8YbGX98vc/tmp/5ef1e32a9d7b34cfeed5db714b52a07397629c3fabd9eb69331176d499efd550.jpg", "img_caption": ["Figure 11: Quantitative comparison of different training-free guidance methods on ImageNet label guidance (with target $=222$ , Kuvasz). The suffix of FreeDoM, UGD, TFG represents the number of recurrence $N_{\\mathrm{recur}}$ . Notice that all the samples are generated based on the same seed and we do not conduct cherry-picking. It is apprent that TFG generates the most valid samples among all the compared methods. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.4  Combined Guidance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "An interesting scenario for conditional generation is to assign multiple targets for a single sample. Conditional generation with multiple conditions is crucial in machine learning for enhancing the relevance and applicability of AI across complex, real-world scenarios. It enables models to produce more contextually appropriate and personalized outputs, crucial for fields requiring high customization. ", "page_idx": 29}, {"type": "text", "text": "Motivations. Combined guidance is to use multiple target functions to guide the same sample towards multiple targets for the same sample. It is more efficient for training-free guidance to do combined guidance as the space of combinatorial targets is potentially huge, which makes it unrealistic to train all target combinations for training-based guidance. We also find it related to the topic of spurious correlations [60], where certain combinations of attributes may dominant the other combinations. For example, hair color may have a strong correlation with gender in CelebA dataset [37]. It is beneficial to explore training-free guidance on reducing the bias of generation models trained on these biased data and address the concerns related to fairness and equality. ", "page_idx": 29}, {"type": "text", "text": "Guidance target.  We study combined guidance on CelebA-DDPM, which is trained on CelebAHQ [26] dataset. The image resolution is $256\\times256$ We choose two settings of combined guidance with two attributes: (gender, hair color) and (gender, age). Each attribute has two labels, where genderE {male, female}, $\\mathrm{age\\in\\{young,old\\}}$ , and hair colon $\\in$ {black, blonde}. We have a binary classifier for each attribute that is downloaded from HuggingFace $\\mathrm{Hub}^{91011}$ . The target is to sample images that maximize the marginal probability: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p_{\\mathrm{combined}}(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}p_{\\mathrm{target1}}(\\pmb{x}_{0})p_{\\mathrm{target2}}(\\pmb{x}_{0}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $p_{\\mathrm{target}}(\\pmb{x}_{0})$ is computed using label guidance as shown in Appendix D.3. ", "page_idx": 29}, {"type": "text", "text": "Evaluation metrics.  As it is hard to filter many reference images for combined targets in CelebAHQ dataset, we adopt Kernel Inception distance (KID) [5] to assess fidelity of generated samples using 1,000 random sampled images of CelebA-HQ as reference images. We generate 256 samples for each evaluated method. We follow MPGD [18] to use the logarithm of KID, i.e. KID (log). For validity, we use another three attribute classifiers to compute the accuracy considering the conjunction of attributes: ", "page_idx": 29}, {"type": "equation", "text": "$$\na c c u r a c y=\\frac{\\#\\land_{t a r g e t\\;l a b e l}\\left(c l a s s i f i e d\\;a s\\;t a r g e t\\;l a b e l\\right)}{\\#g e n e r a t e d\\;s a m p l e s}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The evaluation classifiers are also downloaded from HuggingFace Hub121314. ", "page_idx": 29}, {"type": "image", "img_path": "N8YbGX98vc/tmp/6bf8ad594e821a8cd58bb555c0a9946aa2601b0c2969a8e975e821653bf8bd45.jpg", "img_caption": ["Figure 12: Quantitative comparison of different training-free guidance methods on combined guidance task (male+young). Our TFG method can produce images with high fidelity and validity compared to all the baselines. Notice that we use the fixed seed for all the methods in this figure and do not conduct cherry picking. $N_{\\mathrm{recur}}$ is set to 1 for all methods. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "D.5 Fine-grained Guidance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Fine-grained classification is a specialized task in computer vision that focuses on distinguishing between highly similar subcategories within a larger, general category. This task is particularly challenging due to the subtle differences among the objects or entities being classified. For example, in the context of animal species recognition, fine-grained classification would involve not just distinguishing a bird from a cat, but identifying the specific species of birds, such as differentiating between a crow and a raven [79]. ", "page_idx": 30}, {"type": "text", "text": "Studying fine-grained generation in the context of generative models like Stable Diffusion or DALL-E presents unique challenges due to the inherent complexity of generating highly detailed and specific images. Fine-grained generation involves creating images that not only belong to a broad category but also capture the subtle nuances and specific characteristics of a narrowly defined subcategory. For example, generating images of specific dog breeds in distinct poses or environments requires the model to understand and replicate minute details that distinguish one breed from another. ", "page_idx": 30}, {"type": "text", "text": "Motivations. To develop personalized AI, it is important to explore if the foundational generative models can synthesize fine-grained, accurate target samples according to user-defined target. However, this usually requires high-quality and detailed training data, and the model should be highly sensitive to small variations in input to accurately produce the desired output, which can be difficult for strong text2image generation models DALL- $\\check{\\ E}^{15}$ or Imagen16. We first study this problem in a training-free guidance scenario. ", "page_idx": 31}, {"type": "text", "text": "Guidance target.  We study the out-of-distribution fine-grained label guidance on ImageNet-DDPM, which learns the generation of some species of birds but not comprehensively. We use an EfficientNet trained to classify 525 fine-grained bird species downloaded from HuggingFace Hub.17 The classifier is trained on Bird Species dataset on Kaggle18. We follow the same way in label-guidance to maximize softmax probability for target fine-grianed label. We randomly sample 4 labels (111, 222, 333, 444) in Bird Species dataset, which are: ", "page_idx": 31}, {"type": "table", "img_path": "N8YbGX98vc/tmp/ba88273a12b23c2b628b6d5ba4fc16f805e6049f536ed1f8f41868ad98b4295e.jpg", "table_caption": ["Table 8: Selected Bird Species Dataset Labels "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Evaluation metrics. Similar to label guidance, we use FID to evaluate generation fidelity by filtering data of the target label as reference images. We also compute the FID with 256 sampled images. For accuracy, we adopt another downloaded pre-trained classifier trained on Bird Species dataset from HuggingFace Hub.19. ", "page_idx": 31}, {"type": "image", "img_path": "N8YbGX98vc/tmp/f461504bfe7327df12ac29eb81b7adf0fe905b356273d7a95bf4138a3e2e943f.jpg", "img_caption": ["Figure 13: The sampled 256 images for fine-grained guidance with target label 222 (fairy tern) by ImageNet-DDPM with TFG. A key feature of fairy tern is its black-colored head. We observe that TFG generally samples images with black, round shapes and successfully generates some birds with target feature (red circled). "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "D.6  Style Transfer ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Style transfer is a significant task in computer vision (CV) that focuses on applying the stylistic elements of one image onto another while preserving the content of the target image. This task is pivotal because it bridges the gap between artistic expression and technological innovation, allowing for the creation of novel and aesthetically pleasing visual content. Applications of style transfer are vast, ranging from enhancing user engagement in digital media and advertising to aiding in architectural design by visualizing changes in real-time. ", "page_idx": 32}, {"type": "text", "text": "Guidance target.  The target in our experiments is to guide a text-to-image latent diffusion model Stable-Diffusion-v-1-5 [53j20 to generate images that fit both the text input prompts and the style of the reference images. The guidance objective involves calculating the Gram matrices [25] of the intermediate layers of the CLIP image encoder for both the generated images and the reference style images. The Frobenius norm of these matrices serves as the metric for the objective function. Specifically, for a reference style image $\\mathbf{\\deltax_{\\mathrm{ref}}}$ and a decoded image $D(z_{0\\mid t})$ generated from the estimated clean latent variable $z_{0|t}$ , we compute the Gram matrices $G(x_{\\mathrm{{ref}}})$ and $G(D(z_{0\\mid t}))$ .These matrices are derived from the features of the 3rd layer of the image encoder, in accordance with the methodologies described in MPGD [18] and FreeDoM [78]. The target function is computed as follows: ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p_{\\mathrm{style}}(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}\\exp(-\\|G(\\pmb{x}_{\\mathrm{ref}})-G(D(\\pmb{z}_{0|t}))\\|_{F}^{2}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\|\\cdot\\|_{F}^{2}$ denotes the Frobenius norm of a matrix. ", "page_idx": 33}, {"type": "text", "text": "Evaluation metrics. We use Style score and CLIP score to assess the guidance validity and fidelity, respectively. For reference style images and text prompts, we select 4 images from WikiArt [56] that are also used by MPGD [18], and 64 text prompts from Partiprompts [77]. For each style, we generate 64 images based on the 64 different text prompts. To avoid over-confidence of CLIP score, we use two different CLIP models downloaded from HuggingFace Hub to compute guidance and evaluation metrics, respectively.2122. The style images and examplar prompts are shown as follows. ", "page_idx": 33}, {"type": "image", "img_path": "N8YbGX98vc/tmp/e4e5fc2d8d8af8d05ceb1209261f5f2b4ffafd6d71b16b13b077724c0e109f3a.jpg", "img_caption": ["Figure 14: Four style images used in style transfer task. "], "img_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "N8YbGX98vc/tmp/52a13639446f115d4d93d616f964b0607f578dac742a2d2465c4ba1d90bb00c9.jpg", "table_caption": ["Table 9: Examples for used prompts for style transfer tasks "], "table_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "N8YbGX98vc/tmp/e81b7c57809b85feebb55e9fbd8190f43ca0a1818ffd59e580c25be7aa65b1a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "N8YbGX98vc/tmp/dd90135695c0d976073b243830977e2b43a0e427fb1a171b535a0130342fd913.jpg", "img_caption": ["Figure 15: Quantitative comparison of different training-free guidance methods on style transfer task with the target image as The Starry Night by Von Gogh. Our TFG generates the images with the most faithful style, while DPS, LGD, FreeDoM, and UGD fail to capture the target style. The images of MPGD is also of good quality, but the style score is also inferior than TFG by a large margin. We set $N_{\\mathrm{recur}}=1$ for all methods. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Setup. Our benchmark setup generally follows [24, 3] but with certain specifications to guarantee the overall framework abides by the training-free regime. For dataset, we employ QM9 [50] and adopt the split in [24] with 100,000 training samples. Following [24] and [3], the training set is further split into two halves that guarantees there is no data leakage. The first half is leveraged to train a property prediction network with EGNN [57] as the backbone, which serves as the ground truth oracle to provide the label used for MAE computation. We reuse the checkpoints released by [3] for the labeling network for all 6 properties. The second half is used to train the diffusion model as well as the guidance network. We adopt the unconditional generation version of EDM [24] as the diffusion model. The guidance network in general takes the same architecture as defined by [3] that, again, features EGNN as the backbone but outputs a single scalar as the predicted quantum mechanics property. The only difference lies in at training time we mask the diffusion time step by zeros and always use the original clean molecule structure as input, ensuring training-free objective. All the pretrained models are trained separately for different properties. At sampling time, we employ DDIM [62] sampler with 100 sampling steps, as opposed to [24, 3] that take 1000 sampling steps. ", "page_idx": 35}, {"type": "text", "text": "Guidance target. We study training-free guided generation of molecules on 6 quantum mechanics properties, including polarizability $(\\alpha)$ , dipole moment $(\\mu)$ , heat capacity $(C_{v})$ , highest orbital energy (EHOMo), lowest orbital energy (ELUMo) and their gap $(\\Delta\\epsilon)$ . Denote the oracle property prediction network as $\\mathcal{E}$ . Our guidance target in this case is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\nf(\\pmb{x},c):=\\exp(-\\|\\mathcal{E}(\\pmb{x})-c\\|_{2}^{2}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\textbf{\\em x}$ is the input molecule and $c$ is the target property value. ", "page_idx": 35}, {"type": "text", "text": "Evaluation metrics. We use Mean Absolute Error (MAE) and validity as our evaluation metrics. In particular, MAE is computed between the target value and the predicted value gathered from the labeling network. Validity is computed by RDKit which measures whether the molecule is chemically feasible. We generate 4096 molecules for each property for evaluation. ", "page_idx": 35}, {"type": "image", "img_path": "N8YbGX98vc/tmp/10e568c724fa5cff2413e6c1534f174a536a01dd41587fb36d53d222b84dcc4b.jpg", "img_caption": [], "img_footnote": ["Figure 16: Quanlitative comparison of different training-free guidance methods on molecule generation task with the target property $\\alpha$ (polarizability). Our TFG generates valid molecules with better design target, while baselines often fail to produce valid molecules or offer poor guidance towards the design target. The molecules generated by our approach are increasingly polarizable as $\\alpha$ goesup. "], "page_idx": 36}, {"type": "text", "text": "D.8  Audio Declipping ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Audio declipping is a task in digital audio restoration where distorted audio signals are repaired. Clipping occurs when the amplitude or frequency of an audio signal exceeds the maximum limit that the recording system can handle, leading to a harsh, distorted sound with portions of the waveform \"cut off.\" Declipping aims to reconstruct the missing parts of these clipped waveforms, restoring the audio's original dynamics and reducing distortion. This process improves the quality and clarity of the audio, making it more pleasant to listen to while preserving the original sound's integrity. ", "page_idx": 37}, {"type": "text", "text": "Guidance target. Specifically, we apply a distortion operation to zero out the high frequency and low frequency (for the highest and lowest 40 dimensions) signal in the space of mel spectrograms. If we denote the above process as a blurring operator $\\mathcal{A}_{\\mathrm{blur}}:x\\rightarrow y$ where $\\pmb{x}\\in\\mathbb{R}^{\\hat{2}56\\times25\\breve{6}},\\pmb{y}\\in$ $\\mathbb{R}^{256\\times256}$ aremelgrasandnsgrafioarf Declipping is to generate a clean audio $\\pmb{x}_{0}$ such that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}\\exp(-\\|\\pmb{\\mathcal{A}}_{\\mathrm{blur}}(\\pmb{x}_{0})-\\pmb{y}\\|_{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which means if we project the generate mel spectrogram into the noisy space, the noisy samples $A_{\\mathrm{blur}}(x)$ should be similar to the ground truth noisy mel spectrogram $\\textit{\\textbf{y}}$ ", "page_idx": 37}, {"type": "text", "text": "Evaluation metrics. In our experiments, we evaluate each guidance method on a set of 256 samples generated by Audio-diffusion. We apply the Dynamic time warping (DTW) [44] to assess the guidance validity, and Fr\u00e9chet Audio Distance (FAD) [28] to assess the generation fidelity. ", "page_idx": 37}, {"type": "text", "text": "D.9  Audio Inpainting ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Audio inpainting is a digital audio restoration task that involves filling in missing or corrupted segments of an audio signal. Similar to image inpainting in the visual domain, this technique reconstructs the missing portions of sound by analyzing the surrounding context and seamlessly restoring the lost information. Applications of audio inpainting range from repairing damaged recordings to reconstructing gaps in audio streams due to data loss. The goal is to produce a natural-sounding result that preserves the continuity and overall quality of the original audio. ", "page_idx": 38}, {"type": "text", "text": "Guidance target. Specifically, we apply a distortion operation to zero out the middle 80 dimensions in the space of mel spectrograms. If we denote the above process as a blurring operator $\\mathcal{A}_{\\mathrm{blur}}:x\\rightarrow y$ where  ER256x256 $\\pmb{x}\\in\\mathbb{R}^{256\\times25\\dot{6}},\\pmb{y}\\in\\mathbb{R}^{256\\times256}$ are mel spectrograms and noisy mel spectrograms for 5s audios, then the target of Audio Declipping is to generate a clean audio $\\scriptstyle x_{0}$ such that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{x}_{0}}p(\\pmb{x}_{0})=\\operatorname*{max}_{\\pmb{x}_{0}}\\exp(-\\|\\pmb{\\mathcal{A}}_{\\mathrm{blur}}(\\pmb{x}_{0})-\\pmb{y}\\|_{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which means if we project the generate mel spectrogram into the noisy space, the noisy samples $A_{\\mathrm{blur}}(x)$ should be similar to the ground truth noisy mel spectrogram $\\textit{\\textbf{y}}$ ", "page_idx": 38}, {"type": "text", "text": "Evaluation metrics. In our experiments, we evaluate each guidance method on a set of 256 samples generated by Audio-diffusion. We apply the Dynamic time warping (DTW) [44] to assess the guidance validity, and Fr\u00e9chet Audio Distance (FAD) [28] to assess the generation fidelity. ", "page_idx": 38}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "E.1 Details of Table 1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In Table 1, we study the effect of Monte-Carlo sample sizes in estimating the expectation of Line 4 in TFG algorithm. As the noise is added on both Mean Guidance $(\\Delta_{0})$ and Variance Guidance $(\\Delta_{t})$ ,we decouple the effect into adding noise solely on $\\Delta_{0}$ (Mean only) or $\\Delta_{t}$ (Variance only). In the setting of Variance only, we set $\\pmb{\\mu}=0$ $N_{\\mathrm{iter}}=0$ $N_{\\mathrm{recur}}=4$ $s_{\\rho}(t)=$ \"increase\", and pick the best $\\bar{\\rho}$ and $\\bar{\\gamma}$ via hyper-parameter search. In the seting of Mean only, we set $\\rho=0$ $N_{\\mathrm{iter}}=4$ $N_{\\mathrm{recur}}=1$ \uff0c $s_{\\mu}(t)=$ \"increase\", and pick the best $\\bar{\\mu}$ and $\\bar{\\gamma}$ via hyper-parameter search. We found that the sample size used in Monte-Carlo method play a neglect-able role on the performance if we set the optimal hyper-parameter. It is also noteworthy that the Monte-Carlo sampling does affect the performance of generated quality. For example, we can find that different targets shown in Appendix E.3 have different searched $\\bar{\\gamma}$ . This indicates that the best $\\bar{\\gamma}$ for many targets are apparently not zero. ", "page_idx": 39}, {"type": "text", "text": "E.2 Comparison with grid search ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We compare the performance of our beam search parameters with the full grid search ones on CIFAR10 label guidance task (Table 10). Overall, the performance of both search methods is identical, while grid search is much slower than our search strategy, indicating that our beam search strategy is effective and efficient. ", "page_idx": 39}, {"type": "text", "text": "Table 10: The searched $(\\bar{\\rho},\\bar{\\mu},\\bar{\\gamma})$ of exhaustive grid search and our beam search strategy on the CIFAR-10 label guidance task. We show the validity metric of the corresponding results and the gap $\\Delta=\\|\\nu a l i d i t y_{\\mathrm{beam}}-\\nu a l i d i t y_{\\mathrm{grid}}\\|$ . Overall, the performance of both methods is identical. ", "page_idx": 39}, {"type": "table", "img_path": "N8YbGX98vc/tmp/7ea801d66180beebe8f55a51479acd9e88c4ac1fc9d6da1584e07779194ea265.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "E.3 Detailed results of each target and hyper-parameters ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we present the hyper-parameters searched via the strategy introduced in Section 4 and the corresponding experimental results for TFG as shown in Table 11. We list several observations below. ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Overall, optimal parameters vary widely between problems and datasets. For example, even with the same model and objective (e.g., label classifier on ImageNet or CIFAR10), the best hyperparameters vary widely from target to target. This highlights the importance of hyperparametersearch. ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The improvement of TFG over existing methods depends heavily on the difference between the optimal parameters and the subspaces of existing methods. For example, the $\\bar{\\rho}$ forUGD is the same as TFG for gender-age guidance task, where TFG only has $0.133\\%$ validity improvement over UGD. On the contrary, their values differ on the fine-grained classification task, and TFG has an $18.7\\%$ validity improvement over UGD. Overall, we suppose this depends on whether the optimal parameter lies in the subspace that existing methods can find. ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Though the baselines mentioned in our paper should be a special case of TFG, the results for the highest MO energy guidance in Table 3 show that MPGD outperforms TFG slightly. We want to point out that the reason TFG could occasionally have slightly worse performance in practice is due to the beam search computation limit we currently pose. More specifically, we allow TFG to search at most six steps (for all hyper-parameters) and all other methods for seven steps (in their subspaces). For the MO energy task, the searched parameter for MPGD is that $\\bar{\\mu}=0.016$ (this is the only parameter that we need to search for MPGD), where the best (and last step) of TFG is that $\\bar{\\mu}=0.004$ (because it uses one step to double another parameter). If we allocate more computational budget for the beam search steps, TFG will outperform MPGD on this target (in fact, eight steps suffice). ", "page_idx": 39}, {"type": "table", "img_path": "N8YbGX98vc/tmp/010011dabb046641e186e3b22ba16850e6d35a61e71834ac7b8c28be33d5c25e.jpg", "table_caption": ["Table 1l: The parameter $(\\bar{\\rho},\\bar{\\mu},\\bar{\\gamma})$ selected by beam search strategy for all methods, tasks, and targets. The search space of each method can be found in Section 3.1. For the detailed semantics of each task, please refer to Appendix D. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "E.4  Tricks implemented in FreeDoM codebase ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In the codebase of FreeDoM23 , the schedule of guidance strength for different applications is different. For example, the guidance strength has a schedule coefficient $\\sqrt{\\bar{\\alpha_{t}}}$ for face generation, and the schedule for style transfer is complex and involves a correction term, the mean of gradients\u2019 norm, and a specific constant coefficient O.2. The paper does not mention this particular schedule, leaving the rationale for choosing these schedules unclear. We choose not to include the tricks and find that with our unified hyper-parameter searching strategy, the performance of FreeDoM is similar. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "E.5 Hardware and Software ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We run most of the experiments on clusters using NVIDIA A10Os. We implemented our experiments using PyTorch [49] and the HuggingFace library.24 Overall, we estimated that a total of 2,000 GPU hourswere consumed. ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We have both theoretically and empirically justified our contributions. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: See Sec. 6. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Sec. C. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: See Sec. D. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to acces this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: See supplementary file. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: See Sec. D. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our experiment is conducted on an extensively large scale, and existing works of the same line were not reported since the numbers are stable. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: See Sec. 5. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: See Sec. 6. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: It is not applicable to the concern of this paper. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have cited the related papers. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 46}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do not include new assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do not involve crowdsourcing and research with human subjects. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do not involve human subjects and studies. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}]