[{"type": "text", "text": "A Pairwise Pseudo-likelihood Approach for Matrix Completion with Informative Missingness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiangyuan Li\\* + Department of Statistics Texas A&M University College Station, TX 77843 jiangyuanli@tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Jiayi Wang\\* Department of Mathematical Sciences University of Texas at Dallas Richardson, TX 75080 jiayi.wang2@utdallas.edu ", "page_idx": 0}, {"type": "text", "text": "Raymond K. W. Wong Department of Statistics Texas A&M University College Station, TX 77843 raywong@tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Kwun Chuen Gary Chan   \nDepartment of Biostatistics   \nUniversity of Washington Seattle,WA 98195 kcgchan@uw.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While several recent matrix completion methods are developed to deal with nonuniform observation probabilities across matrix entries, very few allow the missingness to depend on the mostly unobserved matrix measurements, which is generally ill-posed. We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements. We propose a regularized pairwise pseudo-likelihood approach for matrix completion and prove that the proposed estimator can asymptotically recover the low-rank parameter matrix up to an identifiable equivalence class of a constant shift and scaling, at a near-optimal asymptotic convergence rate of the standard well-posed (non-informative missing) setting, while effectively mitigating the impact of informative missingness. The efficacy of our method is validated via numerical experiments, positioning it as a robust tool for matrix completion to mitigate data bias. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The goal of matrix completion is to recover a target matrix from its noisy and incomplete measurements. It is a modern high-dimensional missing data problem. Despite various significant advances made in the last two decades [e.g., 7, 17, 20, 18], many works on matrix completion still focus on the missing-at-random mechanism. Although such an assumption is doubtful in many real-life applications, there are very few options available for more general missing data mechanism, especially those with theoretical guarantees. This work aims to provide a principled and theoretically well-supported alternative method for missing-not-at-random settings, where missingness could depend on the measurements that are mostly unobserved. ", "page_idx": 0}, {"type": "text", "text": "A usual assumption to allow for succeeding matrix completion is that the unknown matrix is lowrank or approximately low-rank. The noiseless setting has been studied in [7] using nuclear norm minimization. The vast majority of existing theories on matrix completion assume that entries are revealed with a constant probability with respect to both entry location and measurement value. ", "page_idx": 0}, {"type": "text", "text": "Recent approaches to handling entries revealed with nonuniform probabilities, depending on the entry location, have shown the strength to improve matrix completion with solid theoretical guarantees [e.g., 31, 11, 28, 25, 23, 35]. These works aim to mitigate the effects due to the non-uniformity in observation probabilities [30]. When additional row/column attributes are available, it is also possible to use this additional information for handling the non-uniform missing [e.g., 24]. Although the non-uniform missing mechanism is quite fexible, it is fundamentally different from the missing-notat-random mechanism. The key difference is whether the missing probability depends on the possibly unobserved measurement, which we will highlight below. In a missing-not-at-random setting, the methods developed for the non-uniform missing mechanism could still be biased in matrix recovery. See Section 6 for a numerical example. The method we propose in this work not only deals with (a flexible subclass of) missing-not-at-random settings, but it is also applicable in the non-uniform missing settings mentioned above. ", "page_idx": 1}, {"type": "text", "text": "In the missing data literature, likelihood-based methods for missing-not-at-random settings commonly involve specifying a parametric distribution of the missing data mechanism. However, this assumption should be used with caution, as it is highly sensitive and may easily induce a misspecified model, resulting in biased estimation and inaccurate results. To circumvent such issues, it is preferable to adopt a missingness assumption as flexible and generally applicable as possible. This type of assumption, often referred to as an unspecified missing data mechanism [37], avoids explicitly specifying a parametric model. Instead of using the full likelihood for estimation, certain unspecified missing-not-at-random assumption allows for the derivation of a non-standard likelihood [22], which serves as the foundation for subsequent estimation. Such non-standard likelihood approaches have been used in regression analysis [32] and variable selection when confronted with informative missing [37]. One disadvantage of this approach is that not all the unknown parameters are estimable due to certain non-identification issues [16, 22]. ", "page_idx": 1}, {"type": "text", "text": "In this work, we extend the pairwise pseudo-likelihood approach [22] to matrix completion with a mild separable informative missingness assumption (see Assumption 2.1 in Section 2), which is very fexible and generally applicable. While not all the parameters are estimable, we can identify the dispersion-scaled matrix up to a constant shift without suffering from bias due to informative missingness. This shows great promise to be applied in practice, for example, in recommendation systems where the rankings of entries are of interest. ", "page_idx": 1}, {"type": "text", "text": "Apart from the informative missing mechanism, our matrix completion method is based on the exponential family model, which has received extensive attention within the matrix completion literature for its efficacy in modeling non-Gaussian data, particularly discrete data. Notably, researchers have investigated its application in specific scenarios such as one-bit matrix completion [1o] and multinomial matrix completion [4, 19]. The application of the exponential family model also extends to accommodating unbounded non-Gaussian observations, including Poisson matrix completion [8] and exponential family matrix completion [13, 21]. ", "page_idx": 1}, {"type": "text", "text": "Overall, the combination of the separable missing-not-at-random mechanism and the exponential family model allows the proposed method to be applicable in a wide range of settings. We summarize the major contributions of this work as follows. ", "page_idx": 1}, {"type": "text", "text": "1. We formulate the pairwise pseudo-likelihood approach for matrix completion under informative missingness and exponential family model. To the best of our knowledge, the pairwise pseudo-likelihood approach has never been adopted in the matrix completion setup before. As opposed to the classical applications of pairwise pseudo-likelihood that assume i.i.d. sampling, matrix completion problems exhibit a non-identical and high-dimensional sampling structure. 2. We investigate the identifiability issues of the crucial separable missingness structures (Assumption 2.1) which lies at the core of the pairwise pseudo-likelihood approach. 3. We provide a non-trivial convergence analysis of the proposed estimator up to an identifiable equivalence class. Such analysis involves a novel concentration analysis of matrix-valued $U$ -statistics where existing works on this type of concentration is sparse. ", "page_idx": 1}, {"type": "text", "text": "Related Work: To the best of our knowledge, we are one of the first works that consider the missing not at random (MNAR) setting in matrix completion and provide solid theoretical guarantees. [2] claims that they can deal with the MNAR setting. However, they assume selections and noise are independent conditioned on latent factors, as shown in their Assumption 2. On the contrary, our setting allows missingness to depend on noise. [15] also addresses informative missingness in matrix completion. However, they require additional covariate information to complete the matrix. Compared to the above two works, our setting is more general as we do not require independence between selections and noise given the true matrix, and we do not need additional covariate information. However, we do require that $\\epsilon_{i}$ is independent of $\\epsilon_{j}$ for $i\\neq j$ or $|i-j|>1$ , while [2] allows selection to be dependent among different entries. Regarding to the theoretical bounds, [2] requires additional technical conditions to develop finite sample error bounds, and their bound is point-wise, i.e., the bound is for a given location $i$ - [15] also requires additional conditions on the likelihood and restricted eigenvalues to obtain convergence. Our error bound is developed under relatively weak conditions and achieves the minimax convergence rate. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "$\\mathbf{A}_{\\star}=(A_{\\star,i j})_{i,j=1}^{m_{1},m_{2}}\\in\\mathbb{R}^{m_{1}\\times m_{2}}$ be the matrix of interest, which is related to the observation through a generalized linear model. More specifically, we posit that the measurements $Y_{i j}$ of the $(i,j)$ -th entry possesses a probability density/mass function of the exponential family form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nY_{i j}\\sim f_{i j}(y|\\mathbf{A}_{\\star},\\phi_{\\star}),\\qquad f_{i j}(y|\\mathbf{A}_{\\star},\\phi_{\\star}):=h(y;\\phi_{\\star})\\mathrm{exp}\\left(\\frac{A_{\\star,i j}y-G(A_{\\star,i j})}{\\phi_{\\star}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h:\\mathbb{R}\\to\\mathbb{R}^{+}$ and $G:\\mathbb{R}\\rightarrow\\mathbb{R}$ are the base measure function and log-partition function associated with the canonical representation, and $\\phi_{\\star}>0$ is the dispersion parameter. Note that this family of distributions covers a wide variety of popular distributions including Bernoulli, Gaussian, and Poisson distributions. For matrix completion problems, we do not have measurements from every single entry. Let $T_{i j}$ be the observation indicator variable of the $(i,j)$ -th entry, with value 1 if $Y_{i j}$ is observed and O otherwise. We assume that $\\{((Y_{i j},T_{i j}):i=1,\\ldots,m_{1};j=1,\\ldots,m_{2}\\}$ are independent. ", "page_idx": 2}, {"type": "text", "text": "Uniform-sampling-at-random (USR) mechanism is regarded as one of the simplest missing structures for matrix completion. Under USR, $\\mathrm{Pr}(T_{i j}=1|Y_{i j})$ is a constant across all $i,j$ , which implies that the observation indicator $T_{i j}$ is independent of the measurement $Y_{i j}$ . While this has been widely used to simplify theoretical analyses in many prior ground works [e.g., 7, 6, 17], USR is a strong assumption that can be violated in many applications. To address this issue, a few analyses and methods [e.g., 31, 11, 28, 18, 4, 25, 23, 35] have been developed based on the non-uniform missing structures, where $\\mathrm{Pr}(T_{i j}=1|Y_{i j})=t_{i j}$ for $0<t_{i j}\\le1$ . Here the observation probabilities are allowed to differ across $i,j$ , but the missingness remains independent of the measurement $Y_{i j}$ . In this paper, we relax this restriction and allow whether an entry is observed or not to depend on the corresponding possibly unobserved measurement, leading to a challenging missing-not-at-random (MNAR) setup. ", "page_idx": 2}, {"type": "text", "text": "Matrix completion under general MNAR is ill-posed, leading to non-identifiability of $\\mathbf{A}_{\\star}$ (even under standard low-rank assumption). Indeed, general MNAR is ill-posed [33] not only in matrix completion, but also in regression [28, 31] and statistical inference [26] in general. However, some additional structure imposed within the MNAR setting can ensure identifiability. To proceed, we make the following assumption, which corresponds to a fexible subclass of MNAR settings. This assumption makes it possible to identify $\\mathbf{A}_{\\star}$ up to some equivalence relations (see Section 3). ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. The observation probability is separable in the following sense: $\\mathrm{Pr}(T_{i j}=1|Y_{i j})=$ $t_{i j}s(Y_{i j})$ ,for some $t_{i j}\\in(0,1]$ and some non-negative function $s(\\cdot):\\mathbb{R}\\to\\mathbb{R}^{+}$ ", "page_idx": 2}, {"type": "text", "text": "As will be made clear later, the proposed technique does not require the knowledge of $s(\\cdot)$ and $\\{t_{i j}\\}$ A similar condition has been widely used in various regression problems [e.g., 22, 29, 37]. These works posit an i.i.d. setup with additional covariates, while our setup does not imply an identically distributed assumption across locations and has no covariates. Moreover, in our setup, it is not possible to observe replicates in the same location, while the i.i.d. setup generally allows replicates. Assumption 2.1 is fexible and widely applicable. Not only does it accomodate USR, it also includes non-uniform missing mechanism as a special case, where we can set $s(\\cdot)\\equiv1$ and leave $\\{t_{i j}\\}$ variable to account for the non-uniform missing. Obviously, as the observation probability is allowed to depend on possibly unobserved $Y_{i j}$ , it also includes many MNAR settings. ", "page_idx": 2}, {"type": "text", "text": "Clearly, we only have access to the observed data, i.e., $Y_{i j}$ conditional on $T_{i j}\\,=\\,1$ . To estimate $\\mathbf{A}_{\\star}$ , we first look at the observed data likelihood of the $(i,j)$ -th entry: $\\mathrm{Pr}(Y_{i j}|T_{i j}=1;{\\bf A},\\phi)$ for $\\mathbf{A}\\in\\mathbb{R}^{m_{1}\\times m_{2}}$ and $\\phi>0$ By the Bayes? Theorem and Assumption 2.1, ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}(Y_{i j}|T_{i j}=1;\\mathbf{A},\\boldsymbol{\\phi})=\\frac{\\mathrm{Pr}(T_{i j}=1|Y_{i j})f_{i j}(Y_{i j};\\mathbf{A},\\boldsymbol{\\phi})}{\\int\\mathrm{Pr}(T_{i j}=1|Y_{i j})f_{i j}(Y_{i j};\\mathbf{A},\\boldsymbol{\\phi})d Y_{i j}}}\\\\ &{=s(Y_{i j})\\frac{1}{\\int s(y)f_{i j}(y;\\mathbf{A},\\boldsymbol{\\phi})d y}f_{i j}(Y_{i j};\\mathbf{A},\\boldsymbol{\\phi})=s(Y_{i j})b_{i j}(\\mathbf{A},\\boldsymbol{\\phi})f_{i j}(Y_{i j}|\\mathbf{X};\\mathbf{A},\\boldsymbol{\\phi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{b_{i j}({\\bf A},\\phi)=1/\\int s(y)f_{i j}(y|{\\bf A},\\phi)d y}\\end{array}$ We see that the conditional likelihood involves unknown functions $s(\\cdot)$ and $b_{i j}(\\cdot)$ , which makes the estimation of $\\mathbf{A}_{\\star}$ difficult. To address this issue, we adopt a pseudo-likelihood approach [22] based on local ranks. ", "page_idx": 3}, {"type": "text", "text": "3 Pseudo-likelihood approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{E}=(e_{1},\\ldots,e_{n})\\subseteq\\{1,\\ldots,m_{1}\\}\\times\\{1,\\ldots,m_{2}\\}$ be a lexicographically ordered set of $n$ unique locations $\\{(i,j)\\,:\\,T_{i j}\\,=\\,1\\}$ .(Indeed, the specific choice of ordering does not matter.) Let the corresponding measurements be $\\widetilde{\\mathbf{Y}}=(\\widetilde{Y}_{1},\\ldots,\\widetilde{Y}_{n}):=(Y_{e_{1}},\\ldots,Y_{e_{n}})$ and the observation indicator be $\\widetilde{\\mathbf{T}}=(\\tilde{T}_{1},\\hdots,\\tilde{T}_{n}):=(T_{e_{1}},\\hdots,T_{e_{n}})$ . We also write $\\tilde{A}_{k}=\\tilde{A}_{e_{k}}$ for $k=1,\\dotsc,n$ We decompose the vector $\\widetilde{\\mathbf Y}$ into two vectors: the order statistics $\\widetilde{\\mathbf Y}_{(\\cdot)}\\,=\\,(\\widetilde{Y}_{(1)},\\dots,\\widetilde{Y}_{(n)})$ and the rank statistics $\\mathbf{R}=(R_{1},\\ldots,R_{n})$ . Precisely, $\\tilde{Y}_{(j)}$ is the $j$ -th smallest entry in $\\widetilde{\\mathbf Y}$ and $R_{k}$ is the rank of the $k$ -th entry in $\\widetilde{\\mathbf Y}$ . To motivate the proposed pseudolikelihood in (3), we first consider the conditional likelihood based on the full rank statistics given the observed data: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}(\\mathbf{R}|\\widetilde{\\mathbf{Y}}_{(\\cdot)},\\widetilde{\\mathbf{T}}=\\mathbf{1};\\mathbf{A},\\phi)=\\frac{\\prod_{k=1}^{n}s(\\widetilde{Y}_{k})t_{e_{k}}f_{e_{k}}(\\widetilde{Y}_{k};\\mathbf{A},\\phi)}{\\sum_{\\pi\\in\\Xi}\\prod_{k=1}^{n}s(\\widetilde{Y}_{\\pi(k)})t_{e_{k}}f_{e_{k}}(\\widetilde{Y}_{\\pi(k)};\\mathbf{A},\\phi)}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\prod_{k=1}^{n}\\exp(\\tilde{A}_{k}\\widetilde{Y}_{k}/\\phi)}{\\sum_{\\pi\\in\\Xi}\\prod_{k=1}^{n}\\exp(\\tilde{A}_{k}\\widetilde{Y}_{\\pi(k)}/\\phi)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Xi$ is the set of all one-to-one maps from $\\{1,\\ldots,n\\}$ to $\\{1,\\ldots,n\\}$ , i.e., permutations. We notice that (2) does not involve unknown components $s(\\cdot)$ and $t_{i j}$ due to the separable assumption (Assumption 2.1), and does not depend on the base measure $h(\\cdot)$ and the log-partition function $G(\\cdot)$ However, (2) is computationally infeasible due to the summation over all permutations. The proposed pairwise pseudo-likelihood consider local ranks for pairs of observations. For any $k$ and $k^{\\prime}$ let $\\mathbf{R}_{k k^{\\prime}}^{L}$ denote the local rank statistic of $\\tilde{Y}_{k}$ and $\\tilde{Y}_{k^{\\prime}}$ among the pair $(\\tilde{Y}_{k},\\tilde{Y}_{k^{\\prime}})$ . We denote $\\widetilde{\\mathbf{Y}}_{(k,k^{\\prime})}^{L}$ as the local order statistics $(\\operatorname*{min}\\{\\tilde{Y}_{k},\\tilde{Y}_{k^{\\prime}}\\},\\operatorname*{max}\\{\\tilde{Y}_{k},\\tilde{Y}_{k^{\\prime}}\\})$ . Instead of the full conditional probability (2), we consider the product of all possible combinations of the local rank conditional probability on observations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\prod_{k<k^{\\prime}}\\mathrm{Pr}(\\mathbf{R}_{k k^{\\prime}}^{L}=\\mathbf{r}_{k k^{\\prime}}^{L}|\\widetilde{\\mathbf{Y}}_{(k,k^{\\prime})}^{L},\\widetilde{T}_{k}=\\widetilde{T}_{k^{\\prime}}=1;\\mathbf{A},\\boldsymbol{\\phi})}\\\\ {\\displaystyle\\quad=\\prod_{k<k^{\\prime}}\\frac{\\exp{\\left(\\frac{\\tilde{A}_{k}\\widetilde{Y}_{k}+\\tilde{A}_{k^{\\prime}}\\widetilde{Y}_{k^{\\prime}}}{\\phi}\\right)}}{\\exp{\\left(\\frac{\\tilde{A}_{k}\\widetilde{Y}_{k}+\\tilde{A}_{k^{\\prime}}\\widetilde{Y}_{k^{\\prime}}}{\\phi}\\right)}+\\exp{\\left(\\frac{\\tilde{A}_{k}\\widetilde{Y}_{k^{\\prime}}+\\tilde{A}_{k}\\widetilde{Y}_{k^{\\prime}}}{\\phi}\\right)}}=\\prod_{k<k^{\\prime}}\\frac{1}{1+\\exp(-(\\widetilde{Y}_{k}-\\widetilde{Y}_{k^{\\prime}})(\\tilde{A}_{k}-\\tilde{A}_{k^{\\prime}})/\\phi)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similar to (2), this pairwise pseudo-likelihood (3) (of $\\mathbf{A}$ and $\\phi$ ) does not contain unknown functions and quantities. However, unlike (2), it does not involve all permutations and is therefore significantly easiertocompute. ", "page_idx": 3}, {"type": "text", "text": "The negative logarithm of the pairwise pseudo-likelihood reads ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{1\\leq k<k^{\\prime}\\leq n}\\log(1+R_{k k^{\\prime}}(\\phi^{-1}\\mathbf{A})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $R_{k k^{\\prime}}(\\phi^{-1}\\mathbf{A})\\;=\\;\\exp\\{-(\\tilde{Y}_{k}\\,-\\,\\tilde{Y}_{k^{\\prime}})(\\phi^{-1}\\tilde{A}_{k}\\,-\\,\\phi^{-1}\\tilde{A}_{k^{\\prime}})\\}\\;=\\;\\exp\\{-(\\tilde{Y}_{k}\\,-\\,\\tilde{Y}_{k^{\\prime}})(\\phi^{-1}\\tilde{A}_{k}\\,-\\,\\phi^{-1}\\tilde{A}_{k^{\\prime}})\\}\\;.$ $\\phi^{-1}\\tilde{A}_{k^{\\prime}})\\}$ . We notice two immediate issues with estimating $\\mathbf{A}_{\\star}$ (and $\\phi_{\\star.}$ ) via minimizing (4). ", "page_idx": 3}, {"type": "text", "text": "Scale Equivalence: The values of (4) evaluated at any two pairs $(\\mathbf{A}_{1},\\phi_{1})$ and $(\\mathbf{A}_{2},\\phi_{2})$ are the same when $\\phi_{1}^{-1}\\mathbf{A}_{1}=\\phi_{2}^{-1}\\mathbf{A}_{2}$ . Therefore, (4) does not have the ability to distinguish between these pairs. ", "page_idx": 3}, {"type": "text", "text": "In other words, if $\\phi_{\\star}>0$ is unknown, (4) would not be able to identify elements in the equivalence class of $\\mathbf{A}_{\\star}$ under equivalence relation: $\\mathbf{A}\\sim c_{1}\\mathbf{A}$ for any $c_{1}>0$ . Instead, we try to estimate the dispersion-scaled matrix $\\phi_{\\star}^{-1}\\mathbf{A}_{\\star}$ . Therefore, we consider ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(\\mathbf{A})=\\sum_{1\\leq k<k^{\\prime}\\leq n}\\log(1+R_{k k^{\\prime}}(\\mathbf{A})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, this does not solve all the identifiability issues, and, indeed, $\\ell$ cannot identify a shiftequivalence class described below. ", "page_idx": 4}, {"type": "text", "text": "Shift Equivalence: Let $\\mathbf{J}$ be a matrix with all entries being one. Consider $\\mathbf{A}+c_{2}\\mathbf{J}$ for any $c_{2}\\in\\mathbb{R}$ Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell({\\mathbf{A}}+c_{2}{\\mathbf{J}})=R_{k k^{\\prime}}(({\\mathbf{A}}+c_{2}{\\mathbf{J}}))=\\exp\\{-(\\tilde{Y}_{k}-\\tilde{Y}_{k^{\\prime}})(\\tilde{A}_{k}+c_{2}-\\tilde{A}_{k^{\\prime}}-c_{2})\\}}\\\\ {=\\exp\\{-(\\tilde{Y}_{k}-\\tilde{Y}_{k^{\\prime}})(\\tilde{A}_{k}-\\tilde{A}_{k^{\\prime}})\\}=\\ell({\\mathbf{A}}).\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Combining the scale and shift equivalence, we can only estimate $\\mathbf{A}_{\\star}$ up to an equivalence relation $\\mathbf{A}\\sim c_{1}\\mathbf{A}+c_{2}\\mathbf{J}$ for any $c_{1}>0$ and $c_{2}\\in\\mathbb{R}$ , which we will refer to as scale-shift equivalence. We remark that the scale-shift equivalence still allows the identification of much useful information from $\\mathbf{A}_{\\star}$ , such as ranking an arbitrary set of entries of $\\mathbf{A}_{\\star}$ . For example, in recommender system applications, one is mostly interested in the ranking within each row/column. Among the elements in the scale-shift equivalence class, we choose to estimate the following representer ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{A}_{\\star}=\\phi_{\\star}^{-1}\\mathbf{A}_{\\star}-\\frac{\\langle\\mathbf{J},\\phi_{\\star}^{-1}\\mathbf{A}_{\\star}\\rangle}{\\langle\\mathbf{J},\\mathbf{J}\\rangle}\\mathbf{J}=\\phi_{\\star}^{-1}\\mathbf{A}_{\\star}-\\frac{\\langle\\mathbf{J},\\phi_{\\star}^{-1}\\mathbf{A}_{\\star}\\rangle}{m_{1}m_{2}}\\mathbf{J}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "by imposing the constraint $\\langle\\mathbf{J},\\mathbf{A}\\rangle=0$ in the optimization. Here $\\begin{array}{r}{\\langle A,B\\rangle=\\sum_{i,j}A_{i j}B_{i j}}\\end{array}$ for any matrices $A,B\\in\\mathbb{R}^{m_{1}\\times m_{2}}$ ", "page_idx": 4}, {"type": "text", "text": "Overall, we propose the following penalized pairwise pseudo-likelihood estimator ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{A}}=\\operatorname*{argmin}_{\\left\\langle\\mathbf{J},\\mathbf{A}\\right\\rangle=0,\\left\\|\\mathbf{A}\\right\\|_{\\infty}\\leq a}\\ell(\\mathbf{A})+\\lambda\\left\\|\\mathbf{A}\\right\\|_{\\star},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lVert\\mathbf{A}\\rVert_{\\star}$ and $\\|\\mathbf{A}\\|_{\\infty}(=\\operatorname*{max}_{i,j}A_{i j})$ represent the nuclear norm and the entrywise max norm of a matrix A respectively, and $a,\\lambda\\geq0$ are tuning parameters. We also use $\\|\\mathbf{A}\\|_{F}$ to denote the Frobenius norm of a matrix A. Nuclear norm regularization has been commonly used to promote low-rankness in the estimation [25, 24, 14]. Since $\\ell$ is convex, this optimization is convex. The discussion of the optimization algorithm is given in Appendix C. One natural question is whether there would be further hidden identifiability issues beyond scale-shift equivalence. In Section 5, we will provide a finite-sample error bound of the proposed estimator (6) based on the pairwise pseudo-likelihood, which indicates convergence to $\\bar{A}_{\\star}$ , eliminating the possibility of additional identifiability issues. ", "page_idx": 4}, {"type": "text", "text": "4  Identifiability based on separable assumption ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One of the major difficulties associated with informative missing is non-identifiability. We first emphasize the non-identifiability for constant shift is not an artifact of the pseudo-likelihood approach. The root cause is the informative missingness (Assumption 2.1). Here is a simple univariate example inspired from [27] to illustrate this point. Suppose we observe from two data-generating models, whose observations are identical in distributions. ", "page_idx": 4}, {"type": "text", "text": "ModelI Y \\~ N(-1,1) with observation probability Pr(T = 1|Y = y) = 1\u03b2), then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(T_{1}=1,Y_{1}=y)=p_{\\mathcal{N}}(y+1)\\frac{\\exp(y)}{1+\\exp(y)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{\\mathcal{N}}(\\cdot)$ is the p.d.f. of standard normal distribution. ", "page_idx": 4}, {"type": "text", "text": "Model I: Y2 \\~ N(0, 1) with observation probabiltyPr( =1Y2 = 9) = exp(-1/2)(), then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{Pr}(T_{2}=1,Y_{2}=y)=p_{\\mathcal N}(y+1)\\exp(-1)\\exp(y+1)\\frac{\\exp(-y)}{1+\\exp(-y)}}\\\\ {\\displaystyle=p_{\\mathcal N}(y+1)\\frac{\\exp(y)}{1+\\exp(y)}=\\operatorname*{Pr}(T_{1}=1,Y_{1}=y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Extending to the matrix form, the observation probabilities of the following two models, where $\\mathbf{Y}_{1},\\mathbf{Y}_{2}\\;\\in\\;\\mathbb{R}^{m_{1}\\times m_{2}}$ , are exactly the same. Model I: $\\mathrm{vec}(\\mathbf{Y}_{1})\\,\\sim\\,{\\mathcal{N}}(-{\\bf1},\\breve{\\mathbf{I}})$ \uff0c $t_{1,i j}\\,=\\,1$ for any (i,i) and s1() = 1+oxp@) . Model II: $\\mathrm{vec}(\\mathbf{Y}_{2})\\sim{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})$ $t_{2,i j}=1$ for any $(i,j)$ and $s_{2}(y)=$ exp(-1/2)1c(-)\u00b7 ", "page_idx": 5}, {"type": "text", "text": "As such,under Assumption 2.1,we cannot identify the constant shift. We note that, low-rank assumption generally would not provide enough additional information to eliminate this identifiability issue, as constant shift corresponds to at most a rank-1 perturbation. ", "page_idx": 5}, {"type": "text", "text": "The identification of the dispersion parameter is a difficult task because of the fact that at most one observation is available for each entry. Interestingly, as we have shown in the Appendix (Theorem B.2), under Assumption 2.1, the identification of the dispersion parameter is actually feasible in Gaussian distributions with replicates. However, it is unclear whether the dispersion parameter can be identified in a typical matrix completion setup, which often does not allow replicates. That said, previous works on exponential family matrix completion [21, 13] assume the dispersion parameter is known, under which there would not be a related identifiability issue. ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical guarantee ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recall that $\\pmb{A}_{\\star}\\ \\in\\ \\mathbb{R}^{m_{1}\\times m_{2}}$ .We denote some convenient notation for dimensions, i.e., $m=$ $\\operatorname*{min}\\{m_{1},m_{2}\\},M\\,=\\,\\operatorname*{max}\\{m_{1},m_{2}\\},d\\,=\\,m_{1}+m_{2}$ .We use the notation $\\lesssim(\\gtrsim)$ to denote less (greater) than up to an absolute multiplicative constant. We write $a\\asymp b$ if $a\\lesssim b$ and $b\\gtrsim a$ . Furthermore, define $\\begin{array}{r}{\\pi_{L}=\\operatorname*{min}_{i\\in[m_{1}],j\\in[m_{2}]}\\operatorname*{Pr}(T_{i j}=1)}\\end{array}$ and $\\pi_{U}=\\operatorname*{max}_{i\\in[m_{1}],j\\in[m_{2}]}\\operatorname*{Pr}(T_{i j}=1)$ . We use $[n]$ to represent $\\{1,\\ldots,n\\}$ for integer $n$ . In this section, we derive the convergence of $\\|\\widehat{A}-\\bar{A}_{\\star}\\|_{F}$ Recall that $\\bar{A}_{\\star}$ , defined in (5), is the representer in the equivalence class of $A_{\\star}$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.1. The following conditions hold. ", "page_idx": 5}, {"type": "text", "text": "(C1) There exists an absolute constant $\\rho>0$ such that $\\pi_{U}/\\pi_{L}\\leq\\rho$ (C2) There exists a constant $B$ such that $\\|\\mathbf{Y}\\|_{\\infty}\\leq B$ almost surely. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "(C3) There exists some constant $\\kappa>0$ (where $\\kappa$ can depend on $\\|\\bar{A}_{\\star}\\|_{\\infty})$ such that $\\mathbb{E}(Z_{i j,i^{\\prime}j^{\\prime}}^{2})\\ge\\kappa$ for any $i,i^{\\prime}\\in[m_{1}],j,j^{\\prime}\\in[m_{2}]$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ_{i j,i^{\\prime}j^{\\prime}}=(Y_{i j}-Y_{i^{\\prime}j^{\\prime}})\\times\\frac{\\mathrm{exp}((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}})/2)}{1+\\mathrm{exp}((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}}))}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Condition (C1) is posited to avoid some specific entries being sampled with very low probability in a relative sense, where the trace-norm penalization fails to work [11, 31]. Note that both $\\pi_{U}$ and $\\pi_{L}$ are allowed to diminish to zero as $m_{1},m_{2}\\rightarrow\\infty$ , but Condition (C1) implies that their diminishing orders are the same. Condition (C2) is a technical assumption for analyzing the concentration inequalities of the involved U-statistics in pairwise pseudolikelihood. Note that this does not violate the parametric assumption on the distribution of $Y$ . For example, truncated normal distribution satisfies both. We leave the extension to a light-tail type of assumption for future work. Condition (C3) is a technical condition, and is used in deriving the (expected) Hessian of the loss function with respect to A (see (8) in Appendix A). Note that (expected) Hessian of the loss is often important for deriving the convergence rate and so it is reasonable that a related term shows up in our condition. Indeed, this assumption Here, we provide further discussion to show that it is indeed a mild condition. Intuitively, it posits a positive lower bound for an expectation of a squared random variable. This expectation is always non-negative and is zero only when $Z_{i j,i^{\\prime}j^{\\prime}}$ is exactly zero almost everywhere. For noisy matrix completion settings, this assumption is very mild because, when there are noises, this variable is not exactly zero almost surely. Next, we show that with the exponential family model, we can explicitly characterize $\\kappa$ . First note that when $\\left\\Vert\\bar{A}_{\\star}\\right\\Vert_{\\infty}\\leq a$ as assumed in Theorem 5.3 and $\\|\\mathbf{Y}\\|_{\\infty}\\leq B$ as in Condition (C2), we have $\\begin{array}{r}{\\mathbb{E}Z_{i j,i^{\\prime}j^{\\prime}}^{2}\\geq\\frac{\\exp\\left(\\!\\!\\langle4a B\\rangle\\!\\right)}{[1+\\exp(4a B)]^{2}}\\mathbb{E}\\left\\{Y_{i j}^{2}+Y_{i^{\\prime}j^{\\prime}}^{2}-2Y_{i j}Y_{i^{\\prime}j^{\\prime}}\\right\\}\\geq}\\end{array}$ Htep(4aB), [Var(Yyi) + ar(Yus)l. Recal the densty of Yig 1), one can derive Var(Yi) = $G^{\\prime\\prime}(\\mathbf{A}_{\\star,i j})\\phi$ , where $G^{\\prime\\prime}(\\cdot)$ is nonnegative, from the well-known variance formula for exponential family Therefore one can ake $\\begin{array}{r}{\\kappa=\\operatorname*{min}_{i,j}\\,\\frac{\\exp(4a B)}{[1+\\exp(4a B)]^{2}}2\\phi[G^{\\prime\\prime}(A_{\\star,i j})].}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.3. Assume $\\mathrm{rank}(\\bar{\\pmb{A}}_{\\star})\\,\\,\\,\\,\\leq\\,\\,\\,\\,r$ and $\\begin{array}{r l}{\\left\\|\\bar{A}_{\\star}\\right\\|_{\\infty}}&{{}\\leq\\;\\;\\;a}\\end{array}$ for some positive constants $r,a\\ >\\ 0,$ .under Assumptions 2.1 and 5.1, if we further assume $m\\pi_{U}\\ \\gtrsim\\ \\log(d^{2})$ and $\\lambda\\ \\asymp$ $B^{2}\\log(d)\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]$ , then with probability at least $1-6/d$ the following holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{m_{1}m_{2}}\\left\\|\\widehat{\\mathbf{A}}-\\bar{A}_{\\star}\\right\\|_{F}^{2}\\lesssim\\operatorname*{max}\\biggr\\{\\frac{B^{4}[\\log d]^{2}}{\\kappa^{2}}\\rho^{3}\\frac{M r}{m_{1}m_{2}\\pi_{L}},\\frac{B^{2}\\log(d)}{\\kappa}\\rho^{2}\\sqrt{\\frac{1}{m_{1}m_{2}\\pi_{L}}}\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Our result implies that the penalized pairwise pseudolikelihood approach can consistently estimate $\\bar{A}_{\\star}$ Note that the difference between $\\operatorname{rank}(\\bar{A}_{\\star})$ and $\\operatorname{rank}(A_{\\star})$ is at most 1. So a low-rank assumption on $A_{\\star}$ automatically translates to a low-rank assumption on $\\bar{A}_{\\star}$ . Most existing work present the upper bound concerning the number of observed entries $n$ and treat the matrix completion as a trace regression problem [e.g. 28, 18, 5, 25]. One can take $n$ as $m_{1}m_{2}\\pi_{L}$ in their bound to compare their results with ours. Similar to the bound established in [18], our bound has two components and matches with the rates in their upper bound (up to some constants and logarithmic factor). [17] and [28] show a bound that has the same order as the first term (up to some constants and logarithmic factor) with some additional assumptions. [17] adopts the uniform sampling and boundedness of the condition number for $\\|\\bar{A}_{\\star}\\|$ . [28] assumes that the sampling distribution follows a product distribution and the \u201cspikiness ratio\" (see $\\alpha_{s p}$ in [28] ) is bounded. Besides the above matrix completion methods that use the nuclear norm regularization, the estimators utilizing the max-norm regularization [e.g. 5, 35] establish the same bound as the second term(up to some constants and logarithmic factor) when they assume the max-norm of $\\bar{A}_{\\star}$ is bounded. While the aforementioned methods address various missing mechanisms, it is important to emphasize that none of them can handle MNAR setting, where the missingness may depend on the observations. However, our method can tackle such informative missingness. It is interesting to see that our error bound resembles the same convergence rate as [18] (minimax optimal rate) up to a logarithmic order, despite that our setup allows MNAR mechanism. ", "page_idx": 6}, {"type": "text", "text": "In terms of theoretical analysis, the most notable distinction between our estimator with other existing ones lies in the objective function. The pairwise pseudo-likelihood we employ imposes unique theoretical challenges. Firstly, the gradient and Hessian are no longer as straightforward as those in the the commonly used squared loss or negative log-likelihood loss (for exponential family). We carefully derive these two terms, expressing them as pairwise summations (see exact forms in Eq. (8) and Eq. (9). Secondly, the elements in these pairwise summations are not mutually independent, posing difficulties in establishing the concentration inequality to bound them. Indeed, we need to develop corresponding theoretical tools for tackling the corresponding matrix concentration of a matrix-valued U-statistics. To address this challenge, we leverage the grouping lemma (Lemma A.5) to decouple these summations into different groups where mutual independence holds within each group. To obtain the efficient grouping, the decoupling is applied to those observed entries. Additionally, while the trace regression model provides a convenient tool for analyzing the sampling distribution, it implicitly assumes \u201csampling with replacement\", i.e., every entry can be observed repeatedly. We adopt the framework of the Bernoulli model for the observation indicator to avoid the issue. However, theoretical analysis become more challenging. A conditional argument (see the conditional event $\\mathcal{E}$ in (10)) is developed to address the discrepancy between these two frameworks. In addition, Lemma A.6 is established to marginalize the conditional event. ", "page_idx": 6}, {"type": "text", "text": "Finally, we remark that, while pseudo-likelihood approaches have been applied in regression analysis [32] and variable selection [37] to deal with informative missingness, such analyses mainly focus on i.i.d. design and usually make direct restricted eigenvalue condition of the (high-dimensional) Hessian matrix. In our problem, the eigenvalue condition is related to the observation probabilities. As in typical analysis of matrix completion, one is interested in the dependence on these probabilities, as they are allowed to diminish as $m_{1},m_{2}\\,\\rightarrow\\,\\infty$ . As such, we also analyze the corresponding restricted eigenvalue bound, under the complicated grouping nature and identifiability issue. By adapting the techniques aforementioned, we provide a rigorous convergence result in non-i.i.d. design, which involves analyzing the concentration of a matrix-valued U-statistics (i.e., the Hessian matrix). This analysis distinguishes our work from a mere application of standard pseudo-likelihood theory, and the techniques used in the proof contribute to the field on their own merit. ", "page_idx": 6}, {"type": "text", "text": "6 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct the following simulation study to demonstrate the efficacy of the proposed method. We generate a $50\\times50$ matrix $\\mathbf{A}_{\\star}$ with rank $r=5$ . The observations $Y_{i j}$ are generated from a Gaussian distribution with mean $A_{\\star,i j}$ and variance $\\sigma^{2}$ independently. In our study, we have settings with different variances $\\sigma^{2}$ . The probability of each entry being observed is related to the value of the entry itself: $\\mathbb{P}(T_{i j}=1|Y_{i j})\\stackrel{.}{=}1/[1+\\exp(3Y_{i j})]$ . Since the observation probability is smaller for larger $Y_{i j}$ , there exists a distinctive distributional shift between the observed and unobserved entries, as shown in Figure 1 ", "page_idx": 7}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/2b999008ec0dff029feea7ab3894852fad68323ee4f67619b94fe3d6af85dd4a.jpg", "img_caption": ["Figure 1: Observation bias with variance $\\sigma^{2}=1$ "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/b7b6b0fbbcad1ad377e6d654f3abd6c2d86dcfc9cf77eb349d9c2ab27e2d6341.jpg", "img_caption": ["Figure 2: TRMSE with standard error for differentvariances $\\sigma^{2}=0.0,0.2,0.4,0.6,0.8,1.0$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We use the observed entries as training data and equally split the unobserved data as validation and test data. We compare our method with SoftImpute [26], CZ [5], MFW [35] and SNN [2]. The validation data is used for hyper-parameter tuning in each method. Since the objective function is convex in the proposed method, we only tune the regularization parameter $\\lambda$ , and fix the number of iterations as $T=100$ and step size $\\eta=1.0$ in Algorithm 1. We use the output of SoftImpute [26] with the same regularization parameter $\\lambda$ as a warm-up initialization to shorten the training time. For SoftImpute [26], CZ [5] and MFW [35], we tune the hyper-parameters involved in the optimization and regularization as suggested. As for SNN [2], we choose uniform weights and spectral threshold suggested in [12], and choose the number of neighbors between 1 and 2. Due to the identifiability issue, the validation data is also used to learn a shift and scale parameter (via a simple linear regression) for the proposed method, which is then used in reporting error metrics on test data. ", "page_idx": 7}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/fdc8fecf65853c717c75492a25dbe638fa37db0b5edeb52192dd8add3c17323f.jpg", "img_caption": ["Figure 3: The recovered entries are left skewed from other methods with $\\sigma^{2}=1$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Before getting into the error metric, a simple check on the bias of each method is via histograms. We pick one run with $\\sigma^{2}=1$ and plot the distribution of recovered entries without any transformation for each method, as shown in Figure 3. It shows that only the proposed method is able to mitigate the observational bias due to the underlying informative missing structure, and exhibits a symmetric distribution, while the distributions generated by other methods are left-skewed, due to the leftskewness of distribution of the observed entries. ", "page_idx": 7}, {"type": "table", "img_path": "ZGN8dOhpi6/tmp/529ab2f5bb30ee74a5d24847bcac6e5b1a3bb266bc7f80388e7b7853159acec6.jpg", "table_caption": ["Table 1: Computational time comparison with $\\sigma^{2}=1$ "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We added comparisons of the computational time regarding the setting in Figure 2 ( $\\left(\\sigma^{2}=1\\right)$ ).The computational times are listed in Table 1. While incorporating more complex missing mechanisms, our method and SNN also take the most time. One practical way to speed up the computation of our method is to use a stochastic version of Algorithm 1 (i.e., training in batches). The focus of this paper is more on the robust recovery when encountering informative missing, and less on the computational efficiency with the knowledge that it could be theoretically slower than other SVD-based methods. However, our method is still faster than SNN, where both methods consider more complex missing mechanisms. Given the promising statistical properties of the proposed method, a future direction is to develop scalable algorithms for the proposed estimator or its variants. ", "page_idx": 8}, {"type": "text", "text": "To further validate the effectiveness of the proposed method, we vary the variances $\\sigma^{2}$ in the simulation. This setting is designed to differentiate non-uniform missingness and informative missingness. When the variance is small, the informative missingness is less severe, and non-uniform missingness might be used to approximately describe the missing mechanism. When the variance is large, the observational probability is more affected by the outcome as in a typical informative missingness setting. We choose the variances $\\sigma^{2}\\,=\\,0.{\\dot{0}},0.2,0.4,0.6,0.8,1.0$ For each setting, we repeat the simulation 9 times and report the average test root mean squared errors (TRMSE) with standard errors, shown in Figure 2. We see that as the variance gets larger, there is a larger improvement in the proposed method with the design to account for informative missing over other methods. SNN [2] performs the worst when the variance is large, as it mainly borrows information on observed entries which introduces a substantial bias. It demonstrates the robustness of the proposed method in difficult settings where the missing structure is informative. ", "page_idx": 8}, {"type": "text", "text": "7 Real data application ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we use three data examples to illustrate the robust performance of the proposed method. These are the Tobacco Dataset [9], Coat Shopping Dataset [30] and Yahoo! Webscope Dataset'. These datasets have been used in prior works for the demonstration of matrix completion methods [e.g., 2, 35]. Due to space limitation, we refer the readers to Appendix C.2 for more detailed discussions of the datasets and our analyses. Following the details of the implementation in Section 6, we report the results in Table 2. For the Coat Shopping Dataset and Yahoo! Webscope Dataset, the evaluations are based on associated test sets from the original data sources. As for the Tobacco Dataset, following [2], the missing data are randomly generated 100 times according to cigarette sales. Here is a summary of the results. ", "page_idx": 8}, {"type": "text", "text": "Tabacco Dataset. As we can see from Table 2, our method only performs worse than SNN for this MNAR dataset, with significantly smaller TRMSE than the other three methods. Note that in this synthetic missing data, the way to generate missingness is adapted from the SNN paper. When one entry is missed in Tobacco dataset, the entries in the following period are also missed. This does not satisfy the assumption of our work. So it is not surprising to see our method perform sub-optimality. However, the performance of our method still remains strong. ", "page_idx": 8}, {"type": "table", "img_path": "ZGN8dOhpi6/tmp/ae08f2818e4f9a455292b6aa7429c62b129b2c7e21f15d6b7e14f60cbd7e968a.jpg", "table_caption": ["Table 2: Test root mean squared errors (TRMSE) for Tabacoo Dataset, Coat Shopping Dataset and Yahoo! Webscope Dataset. For Tabacoo Dataset, the average of TRMSE with standard errors (SE) in parentheses under 1o0 random missing data generations are presented. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Coat Shopping Dataset. As Table 2 shows, SNN performs much worse than the remaining methods for this dataset. MFW has the smallest TRMSE. Our method has smaller errors than SoftImpute and has comparable performance to CZ. ", "page_idx": 9}, {"type": "text", "text": "Yahoo! Webscope Dataset. Due to its large size and to simplify the computation, we conducted a selection procedure to reduce the size of the matrix. Please see details in Appendix C.2 about how to obtain the subset of the matrix. From Table 2, we can see that the two methods (SNN and our method) that are designed for MNAR have better performance than the remaining methods, and our method has the smallest TRMSE. ", "page_idx": 9}, {"type": "text", "text": "Overall, our method performs robustly well across all these three datasets. In our comparison, a few alternatives can perform very well in one example, but badly in another. For example, SNN has an excellent performance in the Tobacco Dataset while performing very poorly in the Coat Shopping Dataset. The robust performance of our method is appealing in practice, as the missing mechanism is often unknown. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we tackle the matrix completion problem where missingness could depend on the possibly unobserved measurements, constituting a challenging missing-not-at-random setting. The proposed method is developed under a fexible separable missingness assumption, which allows us to develop a pairwise pseudo-likelihood approach. Corresponding identification is investigated. We also provide a non-trivial convergence analysis, as well as some numerical experiments to illustrate the efficacy of the proposed estimation. Due to the fexibility in both the missing structure (separable missingness) and measurement model (exponential family model), the proposed technique would be useful in a wide range of applications. ", "page_idx": 9}, {"type": "text", "text": "The grouping nature of the proposed method poses an additional burden in computation, particularly when dealing with a large number of observed entries. For future works, we consider adapting the stochastic grouping idea to reduce the computational cost and exploring its application in large-scale recommender systems. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank the reviewers for their helpful comments and suggestions. Portions of this research were conducted with the advanced computing resources provided by Texas A&M High Performance Research Computing. The work of Jiayi Wang is partly supported by the National Science Foundation (DMS-2401272). The work of Raymond K. W. Wong is partly supported by the National Science Foundation (DMS-1711952 and CCF-1934904). The work of K. C. G. Chan is partly supported by the National Science Foundation (DMS-1711952). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Abadie, A., A. Diamond, and J. Hainmueller (2010). Synthetic control methods for comparative case studies: Estimating the effect of california's tobacco control program. Journal of the American statisticalAssociation105(490),493-505. ", "page_idx": 9}, {"type": "text", "text": "[2] Agarwal, A., M. Dahleh, D. Shah, and D. Shen (2023, 12-15 Jul). Causal matrix completion. In G. Neu and L. Rosasco (Eds.), Proceedings of Thirty Sixth Conference on Learning Theory, Volume 195 of Proceedings of Machine Learning Research,pp. 3821-3826. PMLR.   \n[3]  Buhlmann, P. and S. Van De Geer (2011). Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media.   \n[4]  Cai, T. and W.-X. Zhou (2013). A max-norm constrained minimization approach to 1-bit matrix completion. J. Mach. Learn. Res. 14(1), 3619-3647.   \n[5] Cai, T. T. and W.-X. Zhou (2016). Matrix completion via max-norm constrained optimization. Electronic Journal of Statistics 10(1), 1493-1525.   \n[6] Candes, E. J. and Y. Plan (2010). Matrix completion with noise. Proceedings of the IEEE 98(6), 925-936.   \n[7]  Candes, E. J. and B. Recht (2009). Exact matrix completion via convex optimization. Foundations of Computational mathematics 9(6), 717-772.   \n[8]  Cao, Y. and Y. Xie (2015). Poisson matrix recovery and completion. IEEE Transactions on Signal Processing 64(6), 1609-1620.   \n[9]  Council, T. T. and T. 1. (Washington (1997). The Tax Burden on Tobacco, Volume 32. Tobacco Institute.   \n[10] Davenport, M. A., Y. Plan, E. Van Den Berg, and M. Wootters (2014). 1-bit matrix completion. Information and Inference: A Journal of the IMA 3(3), 189-223.   \n[11]  Foygel, R., O. Shamir, N. Srebro, and R. R. Salakhutdinov (2011). Learning with the weighted trace-norm under arbitrary sampling distributions. Advances in neural information processing systems 24, 2133-2141.   \n[12] Gavish, M. and D. L. Donoho (2014). The optimal hard threshold for singular values is $4/\\sqrt{3}$ IEEE Transactions on Information Theory 60(8), 5040-5053.   \n[13]  Gunasekar, S., P. Ravikumar, and J. Ghosh (2014). Exponential family matrix completion under structural constraints. In International Conference on Machine Learning, pp. 1917-1925. PMLR.   \n[14] Hu, Z., F. Nie, R. Wang, and X. Li (2021). Low rank regularization: A review. Neural Networks 136, 218-232.   \n[15]  Jin, H., Y. Ma, and F. Jiang (2022). Matrix completion with covariate information and informative missingness. Journal of Machine Learning Research 23(180), 1-62.   \n[16]  Kalbfleisch, J. D. (1978). Likelihood methods and nonparametric tests. Journal of the American Statistical Association 73(361), 167-170.   \n[17]  Keshavan, R. H., A. Montanari, and S. Oh (2010). Matrix completion from a few entries. IEEE transactions on information theory 56(6), 2980-2998.   \n[18] Klopp, O. (2014). Noisy low-rank matrix completion with general sampling distribution. Bernoulli 20(1), 282-303.   \n[19]  Klopp, O., K. Lounici, and A. B. Tsybakov (2017). Robust matrix completion. Probability Theory and Related Fields 169, 523-564.   \n[20] Koltchinski, V., K. Lounici, and A. B. Tsybakov (2011). Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics 39(5), 2302-2329.   \n[21] Lafond, J. (2015). Low rank matrix completion with exponential family noise. In Conference on Learning Theory, pp. 1224-1243. PMLR.   \n[22] Liang, K.-Y and J. Qin (200). Regression analysis under non-standard situations: a pairwise pseudolikelihood approach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 62(4), 773-786.   \n[23] Ma, W. and G. H. Chen (2019). Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. Advances in neural information processing systems 32, 14871-14880.   \n[24] Mao, X., S. X. Chen, and R. K. W. Wong (2019). Matrix completion with covariate information. Journal of the American Statistical Association 114(525), 198-210.   \n[25] Mao, X., R. K. Wong, and S. X. Chen (2021). Matrix completion under low-rank missing mechanism. Statistica Sinica 31(4), 2005-2030.   \n[26]  Mazumder, R., T. Hastie, and R. Tibshirani (2010). Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research 1l, 2287-2322.   \n[27]  Miao, W., P. Ding, and Z. Geng (2016). Identifiability of normal and normal mixture models with nonignorable missing data. Journal of the American Statistical Association 111(516), 1673-1683.   \n[28]  Negahban, S. and M. J. Wainwright (2012). Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research 13(1), 1665-1697.   \n[29] Ning, Y, T. Zhao, and H. Liu (2017). A likelihood ratio framework for high-dimensional semiparametric regression. The Annals of Statistics 45(6), 2299-2327.   \n[30] Schnabel, T., A. Swaminathan, A. Singh, N. Chandak, and T. Joachims (2016). Recommendations as treatments: Debiasing learning and evaluation. In international conference on machine learning, Pp. 1670-1679. PMLR.   \n[31] Srebro, N. and R. R. Salakhutdinov (2010). Collaborative filtering in a non-uniform world: Learning with the weighted trace norm. Advances in neural information processing systems 23, 2056-2064.   \n[32] Tang, G., R. J. Little, and T. E. Raghunathan (2003). Analysis of multivariate missing data with nonignorable nonresponse. Biometrika 90(4), 747-764.   \n[33] Tang, N. and Y. Ju (2018). Statistical inference for nonignorable missing-data problems: a selective review. Statistical Theory and Related Fields 2(2), 105-133.   \n[34]  Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics 12, 389-434.   \n[35] Wang, J., R. K. W. Wong, X. Mao, and K. C. G. Chan (2021). Matrix completion with model-free weighting. In International Conference on Machine Learning, pp. 10927-10936. PMLR.   \n[36] Wang, J., R. K. W. Wong, and X. Zhang (2022). Low-rank covariance function estimation for multidimensional functional data. Journal of the American Statistical Association 117(538), 809-822.   \n[37] Zhao, J., Y. Yang, and Y. Ning (2018). Penalized pairwise pseudo likelihood for variable selection with nonignorable missing data. Statistica Sinica 28(4), 2125-2148. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "AProof of Theorem 5.3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We provide the proof of our theoretical results and discussion about identifiability and pseudolikelihood approach below. The proof follows the roadmap from [18] to construct the convergence, whereas the details differ because we deal with a matrix-valued U-statistic type estimator. We mainly rely on Lemma S.4. from [36] to decouple the dependence in U-statistic structure. See Lemma A.5 in Section A.4. ", "page_idx": 12}, {"type": "text", "text": "We start by rewriting the pairwise pseudo-likelihood as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\ell(\\mathbf{A})=\\sum_{1\\leq k<k^{\\prime}\\leq n}\\{\\psi(\\tilde{Y}_{k\\setminus k^{\\prime}}\\tilde{A}_{k\\setminus k^{\\prime}})-\\tilde{Y}_{k\\setminus k^{\\prime}}\\tilde{A}_{k\\setminus k^{\\prime}}\\}}\\\\ &{=\\displaystyle\\sum_{1\\leq i,i^{\\prime}\\leq m_{1}}T_{i j}T_{i^{\\prime}j^{\\prime}}\\{\\psi(Y_{i j\\setminus i^{\\prime}j^{\\prime}}A_{i j\\setminus i^{\\prime}j^{\\prime}})-Y_{i j\\setminus i^{\\prime}j^{\\prime}}A_{i j\\setminus i^{\\prime}j^{\\prime}}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\tilde{Y}_{k\\backslash k^{\\prime}}\\;:=\\;\\tilde{Y}_{k}\\,-\\,\\tilde{Y}_{k^{\\prime}}\\;=\\;Y_{i j}\\,-\\,Y_{i^{\\prime}j^{\\prime}}\\;=:\\;Y_{i j\\backslash i^{\\prime}j^{\\prime}}$ , and $\\psi(t)\\;=\\;\\log(1+\\exp(t))$ .Simply, we obtain that $\\psi^{\\prime}(t)\\,=\\,\\exp(t)/\\{1+\\exp(t)\\}$ \uff0c $\\psi^{\\prime\\prime}(t)\\,=\\,\\exp(t)/\\{(1+\\exp(t))^{2}\\}$ and $\\psi^{\\prime\\prime\\prime}(t)\\;=$ $\\exp(t)(1-\\exp(t))/\\{(1+\\exp(t))^{3}\\}$ ", "page_idx": 12}, {"type": "text", "text": "Therefore, the first and second order derivatives of $\\ell(A)$ are ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla\\ell(\\mathbf{A})=\\sum_{\\substack{1\\le i,i^{\\prime}\\le m_{1},1\\le j,j^{\\prime}\\le m_{2}}}T_{i^{\\prime}j^{\\prime}}T_{i j}\\times\\{(\\psi^{\\prime}(Y_{i j\\backslash i^{\\prime}j^{\\prime}}A_{i j\\backslash i^{\\prime}j^{\\prime}})-1)Y_{i j\\backslash i^{\\prime}j^{\\prime}}\\mathbf{E}_{i j\\backslash i^{\\prime}j^{\\prime}}\\},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell(\\mathbf{A})=\\sum_{1\\leq i,i^{\\prime}\\leq m_{1},\\leq j,j^{\\prime}\\leq m_{2}}T_{i^{\\prime}j^{\\prime}}T_{i j}\\times\\{\\psi^{\\prime\\prime}(Y_{i j\\backslash i^{\\prime}j^{\\prime}}A_{i j\\backslash i^{\\prime}j^{\\prime}})Y_{i j\\backslash i^{\\prime}j^{\\prime}}^{2}\\mathrm{vec}(\\mathbf{E}_{i j\\backslash i^{\\prime}j^{\\prime}})^{\\otimes2}\\},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathbf{E}_{i j\\backslash i^{\\prime}j^{\\prime}}=\\mathbf{E}_{i j}-\\mathbf{E}_{i^{\\prime}j^{\\prime}}$ \uff0c $\\mathbf{E}_{i j}\\in\\mathbb{R}^{m_{1}\\times m_{2}}$ is the canonical basis with value 1 at the $(i,j)$ -th entry and 0 elsewhere, $\\operatorname{vec}(\\mathbf{X})$ is the standard vectorization of matrix $\\mathbf{X}$ and $\\mathbf{v}^{\\otimes2}=\\mathbf{v}\\mathbf{v}^{\\top}$ for $\\mathbf{v}\\in\\mathbb{R}^{m_{1}m_{2}}$ ", "page_idx": 12}, {"type": "text", "text": "A.1  Lemmas about gradient ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\ell(\\bar{A}_{\\star})=-\\displaystyle\\sum_{1\\leq k<k^{\\prime}\\leq n}\\frac{R_{k k^{\\prime}}(\\bar{A}_{\\star})}{1+R_{k k^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{k}-\\tilde{Y}_{k^{\\prime}})({\\mathbf{E}}_{e_{k}}-{\\mathbf{E}}_{e_{k^{\\prime}}})}\\\\ &{\\qquad=-\\displaystyle\\sum_{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}\\frac{R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}{1+R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{i j}-\\tilde{Y}_{i^{\\prime}j^{\\prime}})({\\mathbf{E}}_{i j}-{\\mathbf{E}}_{i^{\\prime}j^{\\prime}})T_{i j}T_{i^{\\prime}j^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{=-\\displaystyle\\frac{1}{2}_{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}\\frac{R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}{1+R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{i j}-\\tilde{Y}_{i^{\\prime}j^{\\prime}})({\\mathbf{E}}_{i j}-{\\mathbf{E}}_{i^{\\prime}j^{\\prime}})T_{i j}T_{i^{\\prime}j^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(i,j)\\prec(i^{\\prime},j^{\\prime})$ means $(i,j)$ appears before $(i^{\\prime},j^{\\prime})$ with the same ordering rule within $\\mathcal{E}$ ,e.g., dictionary order. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 5.2. Note that ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{'}\\ell(\\bar{A}_{\\star})\\}=\\mathbb{E}\\left\\{\\begin{array}{l l}{\\displaystyle-\\sum_{\\scriptstyle(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}\\frac{R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}{1+R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{i j}-\\tilde{Y}_{i^{\\prime}j^{\\prime}})(\\mathbf{E}_{i j}-\\mathbf{E}_{i^{\\prime}j^{\\prime}})T_{i j}T_{i^{\\prime}j^{\\prime}}\\Bigg\\}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {\\quad-\\mathbb{E}\\left\\{\\begin{array}{l l}{\\displaystyle\\sum_{\\scriptstyle(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}\\mathbb{E}\\left\\{\\frac{R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}{1+R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{i j}-\\tilde{Y}_{i^{\\prime}j^{\\prime}})(\\mathbf{E}_{i j}-\\mathbf{E}_{i^{\\prime}j^{\\prime}})\\right\\}T_{i j}=T_{i^{\\prime}j^{\\prime}}=}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The proof of (-)(EE=T=1}0direcyfolows Theorem 4.1 from [29]. ", "page_idx": 13}, {"type": "text", "text": "To simplify the notation, we denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i j,i^{\\prime}j^{\\prime}}=-\\frac{{R}_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}{1+{R}_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{i j}-\\tilde{Y}_{i^{\\prime}j^{\\prime}})(\\mathbf{E}_{i j}-\\mathbf{E}_{i^{\\prime}j^{\\prime}})T_{i j}T_{i^{\\prime}j^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that $m=\\operatorname*{min}\\{m_{1},m_{2}\\},d=m_{1}+m_{2}$ \uff0c $M=\\operatorname*{max}\\{m_{1},m_{2}\\}$ \uff0c $|Y_{k}|\\le B$ almost surely. Lemma A.1. With the condition that $m\\pi_{U}\\gtrsim\\log(d^{2})$ .We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left\\{\\left\\|\\nabla\\ell(\\bar{\\cal A}_{\\star})\\right\\|\\gtrsim\\sqrt{B^{2}\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right\\}\\leq\\frac{3}{d}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Denote ", "page_idx": 13}, {"type": "equation", "text": "$$\nC_{i j,i^{\\prime}j^{\\prime}}=-\\frac{R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}{1+R_{i j,i^{\\prime}j^{\\prime}}(\\bar{A}_{\\star})}(\\tilde{Y}_{i j}-\\tilde{Y}_{i^{\\prime}j^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "First of all, we can verify that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}(C_{i j,i^{\\prime}j^{\\prime}}T_{i j}T_{i^{\\prime}j^{\\prime}})=\\mathbb{E}\\left\\{\\mathbb{E}[C_{i j,i^{\\prime}j^{\\prime}}T_{i j}T_{i^{\\prime}j^{\\prime}}\\mid T_{i j}=1,T_{i^{\\prime}j^{\\prime}}=1]\\right\\}=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Define the event ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal E}=\\{\\sum_{i,j}T_{i,j}=n{\\mathrm{~with~sampling~matrices~}}X_{1},\\ldots,X_{n}\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": ",where $\\pmb{X}_{k}=\\pmb{E}_{i_{k},j_{k}}$ for some index $(i_{k},j_{k})$ $k=1,\\hdots,n$ . Without loss of generality, we consider thecasewhen $n$ is even. Therefore, by Lemma A.5, we have the following decomposition. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\ell(\\mathbf{A})\\mid\\mathcal{E}=\\sum_{g=1}^{n-1}\\sum_{(k,k^{\\prime})\\in G_{g}}C_{k,k^{\\prime}}[\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $C_{k,k^{\\prime}}=C_{i_{k},j_{k},i_{k^{\\prime}},j_{k^{\\prime}}}$ Within every group $G_{g}$ , there is nrpeatd ix. Thfor, vy element in the group $G_{g}$ is independent of other elements in $G_{g}$ conditioned on $\\mathcal{E}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}(C_{k,k^{\\prime}}\\mid\\mathcal{E})=0,\\qquad\\forall k,k^{\\prime}=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, we use MatrixBestein's inequalitytoboud $\\begin{array}{r}{\\|\\sum_{(k,k^{\\prime})\\in G_{g}}C_{k,k^{\\prime}}[\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}]\\|}\\end{array}$ conditioned on the event $\\mathcal{E}$ ", "page_idx": 13}, {"type": "text", "text": "Take $S_{k,k^{\\prime}}=C_{k,k^{\\prime}}[\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}]$ and we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\underset{(k,k^{\\prime})\\in G_{s}}{\\sum}S_{k,k^{\\prime}}S_{k,k^{\\prime}}^{\\top}\\mid\\varepsilon\\right\\|}\\\\ &{=\\left\\|\\underset{(k,k^{\\prime})\\in G_{s}}{\\sum}C_{k,k^{\\prime}}^{2}[\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}][\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}]^{\\top}\\mid\\varepsilon\\right\\|}\\\\ &{\\quad\\leq B^{2}\\left\\|\\underset{(k,k^{\\prime})\\in G_{s}}{\\sum}[\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}][\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}]^{\\top}\\right\\|}\\\\ &{\\quad\\leq B^{2}\\left(\\left\\|\\underset{(k,k^{\\prime})\\in G_{s}}{\\sum}X_{k}X_{k}^{\\top}+X_{k^{\\prime}}X_{k^{\\prime}}^{\\top}\\right\\|+\\left\\|\\underset{(k,k^{\\prime})\\in G_{s}}{\\sum}X_{k}X_{k^{\\prime}}^{\\top}+X_{k^{\\prime}}X_{k}^{\\top}\\right\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Take $i_{k}$ and $j_{k}$ as the corresponding row index and column index for $X_{k}$ such that $\\pmb{X}_{k}=\\pmb{E}_{i_{k},j_{k}}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{\\left(k,k^{\\prime}\\right)\\in G_{g}}X_{k}X_{k}^{\\intercal}+X_{k^{\\prime}}X_{k^{\\prime}}^{\\intercal}\\right\\|=\\left\\|\\sum_{k=1}^{n}E_{i_{k},i_{k}}\\right\\|=1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The last inequality is due to the fact that the diagonal matrices have the operator norm 1. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{(k,k^{\\prime})\\in G_{g}}X_{k}X_{k^{\\prime}}^{\\intercal}+X_{k^{\\prime}}X_{k}^{\\intercal}\\right\\|\\leq\\sum_{(k,k^{\\prime})\\in G_{g}}\\|X_{k}X_{k^{\\prime}}^{\\intercal}+X_{k^{\\prime}}X_{k}^{\\intercal}\\|\\leq\\sum_{(k,k^{\\prime})\\in G_{g}}2\\mathbb{1}\\{j_{k}=j_{k^{\\prime}}\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\sum_{(k,k^{\\prime})\\in G_{g}}S_{k,k^{\\prime}}S_{k,k^{\\prime}}^{\\intercal}\\mid\\mathcal{E}\\right\\|\\leq B^{2}\\left[1+\\sum_{(k,k^{\\prime})\\in G_{g}}2\\mathbb{1}\\{j_{k}=j_{k^{\\prime}}\\}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\sum_{(k,k^{\\prime})\\in G_{g}}S_{k,k^{\\prime}}^{\\intercal}S_{k,k^{\\prime}}\\mid\\mathcal{E}\\right\\|\\leq B^{2}\\left[1+\\sum_{(k,k^{\\prime})\\in G_{g}}2\\mathbb{1}\\{i_{k}=i_{k^{\\prime}}\\}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Take ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\xi_{n,g}=\\operatorname*{max}\\{\\sum_{(k,k^{\\prime})\\in G_{g}}2\\mathbb{1}\\{j_{k}=j_{k^{\\prime}}\\},\\sum_{(k,k^{\\prime})\\in G_{g}}2\\mathbb{1}\\{i_{k}=i_{k^{\\prime}}\\}\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then by Theorem 6.1 from [34], we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\operatorname*{Pr}\\left\\{\\left\\|\\displaystyle\\sum_{(k,k^{\\prime})\\in\\mathcal{G}}\\mathbf{S}_{k,k^{\\prime}}\\right\\|\\geq c\\sqrt{B^{2}(1+\\xi_{n,g})[2\\log(d)+2\\log(m_{1}m_{2})]}\\mid\\mathcal{E}\\right\\}}\\\\ &{}&{\\leq\\!d\\exp\\{-[2\\log(m_{1}m_{2})+2\\log(d)]\\}=d\\left(\\frac{1}{(m_{1}m_{2})^{2}d^{2}}\\right)=\\frac{1}{(m_{1}m_{2})^{2}d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some constant $c>0$ ", "page_idx": 14}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\left\\|\\nabla\\ell(\\bar{\\cal A}_{\\star})\\right\\|\\,|\\,\\mathcal{E}\\le\\sum_{g=1}^{n-1}\\left\\|\\sum_{(k,k^{\\prime})\\in g}{\\bf S}_{k,k^{\\prime}}\\right\\|}\\end{array}$ . By applying the union bound over all the groups, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{x}_{\\operatorname{r}}\\left\\{\\left\\|\\nabla\\ell({\\bar{A}}_{\\star})\\right\\|\\geq c\\sum_{g=1}^{n-1}{\\sqrt{B^{2}(1+\\xi_{n,g})[2\\log(d)+2\\log(m_{1}m_{2})]}}\\ |\\ \\mathcal{E}\\right\\}\\leq n{\\frac{1}{(m_{1}m_{2})^{2}d}}\\leq{\\frac{1}{(m_{1}m_{2})^{2}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Marginalize the event $\\mathcal{E}$ and we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left\\{\\|\\nabla\\ell(\\bar{A}_{\\star})\\|\\ge c\\sum_{g=1}^{n-1}\\sqrt{B^{2}(1+\\xi_{n,g})[2\\log(d)+2\\log(m_{1}m_{2})]}\\right\\}\\le\\frac{1}{(m_{1}m_{2})d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{p=1}^{n-1}\\sqrt{B^{2}(1+\\xi_{n,g})[2\\log(d)+2\\log(m_{1}m_{2})]}}\\\\ &{\\le\\displaystyle\\sum_{g=1}^{n-1}\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})]}+\\sum_{g=1}^{n-1}\\sqrt{B^{2}\\xi_{n,g}[2\\log(d)+2\\log(m_{1}m_{2})]}}\\\\ &{\\le(n-1)\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})]}+\\sqrt{n-1}\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})](\\sum_{g=1}^{G}\\xi_{n,g})}}\\\\ &{\\le n\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})]}+\\sqrt{n}\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})](\\sum_{g=1}^{G}\\xi_{n,g})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\|\\nabla\\ell(\\bar{A}_{\\star})\\|\\geq c\\left[n\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})]}+\\sqrt{n}\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})](\\displaystyle\\sum_{g=1}^{G}\\xi_{n,g})}\\right]\\right.}\\\\ {\\left.\\leq\\frac{1}{(m_{1}m_{2})^{2}}\\sum_{g=1}^{G}\\xi_{n,g}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Lemma A6, we adopt the boud for $n$ and $\\textstyle\\sum_{g=1}^{n-1}\\xi_{n,g}$ and further obtan that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{v}_{\\mathrm{T}}\\left\\{\\left\\|\\nabla\\ell(\\bar{A}_{\\star})\\right\\|\\geq c\\sqrt{B^{2}[2\\log(d)+2\\log(m_{1}m_{2})]}\\left[3m_{1}m_{2}\\pi_{U}+\\sqrt{3m_{1}m_{2}\\pi_{U}}\\sqrt{2m_{1}m_{2}M\\pi_{U}^{2}}\\right]\\right\\}}\\\\ &{}&{\\leq\\frac{1}{(m_{1}m_{2})d}+\\exp(-m_{1}m_{2}\\pi_{L})+2M\\exp(-m_{1}m_{2}\\pi_{U}^{2}/2).}\\\\ &{}&{\\mathrm{Pr}\\left\\{\\left\\|\\nabla\\ell(\\bar{A}_{\\star})\\right\\|\\gtrsim\\sqrt{B^{2}[\\log(d)+\\log(m_{1}m_{2})]}\\left[m_{1}m_{2}\\pi_{U}(1+\\sqrt{M\\pi})\\right]\\right\\}}\\\\ &{}&{\\leq\\frac{1}{(m_{1}m_{2})d}+\\exp(-m_{1}m_{2}\\pi_{L})+2M\\exp(-m_{1}m_{2}\\pi_{U}^{2}/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With the condition that $m\\pi_{U}\\gtrsim\\log(d^{2})$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left\\{\\|\\nabla\\ell(\\bar{A}_{\\star})\\|\\gtrsim\\sqrt{B^{2}[\\log(d)+\\log(m_{1}m_{2})]}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right\\}\\leq\\frac{3}{d}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The conclusion follows by assimilating universal constants in $\\gtrsim$ ", "page_idx": 15}, {"type": "text", "text": "A.2Lemmas about Hessian ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that $\\psi(t)=\\log(1+\\exp(t))$ \uff0c $\\psi^{\\prime}(t)=\\mathrm{exp}(t)/\\{1\\!+\\!\\exp(t)\\}$ \uff0c $\\psi^{\\prime\\prime}(t)=\\exp(t)/\\{(1+\\exp(t))^{2}\\}$ and $\\psi^{\\prime\\prime\\prime}(t)=\\exp(t)(1-\\exp(t))/\\{(1+\\exp(t))^{3}\\}$ It is simple to verify $|\\psi^{\\prime}(t)|\\leq1,|\\psi^{\\prime\\prime}(t)|\\leq0.25$ and $|\\psi^{\\prime\\prime\\prime}(t)|\\leq0.1$ ", "page_idx": 15}, {"type": "text", "text": "The Hessian matrix reads ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}^{2}\\ell(\\mathbf{A})=\\displaystyle\\sum_{1\\leq k<k^{\\prime}\\leq n}\\{\\psi^{\\prime\\prime}(Y_{i j\\backslash i^{\\prime}j^{\\prime}}A_{i j\\backslash i^{\\prime}j^{\\prime}})Y_{i j\\backslash i^{\\prime}j^{\\prime}}^{2}\\mathrm{vec}(\\mathbf{E}_{i j\\backslash i^{\\prime}j^{\\prime}})^{\\otimes2}\\}}\\\\ &{=\\displaystyle\\sum_{1\\leq k<k^{\\prime}\\leq n}(Y_{k}-Y_{k^{\\prime}})^{2}\\frac{\\mathrm{exp}((\\tilde{Y}_{k}-\\tilde{Y}_{k^{\\prime}})(\\tilde{A}_{k}-\\tilde{A}_{k^{\\prime}}))}{(1+\\mathrm{exp}((\\tilde{Y}_{k}-\\tilde{Y}_{k^{\\prime}})(\\tilde{A}_{k}-\\tilde{A}_{k^{\\prime}})))^{2}}\\mathrm{vec}(\\mathbf{E}_{k}-\\mathbf{E}_{k^{\\prime}})^{\\otimes2}}\\\\ &{=\\displaystyle\\frac{1}{2}\\sum_{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}T_{i j}T_{i^{\\prime}j^{\\prime}}(Y_{i j}-Y_{i^{\\prime}j^{\\prime}})^{2}\\frac{\\mathrm{exp}((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}}))}{(1+\\mathrm{exp}((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}})))^{2}}\\mathrm{vec}(\\mathbf{E}_{i j}-\\mathbf{E}_{i})}\\\\ &{=\\displaystyle\\frac{1}{2}\\sum_{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}T_{i j}T_{i^{\\prime}j^{\\prime}}Z_{i j,i^{\\prime}j^{\\prime}}^{2}\\mathrm{vec}(\\mathbf{E}_{i j}-\\mathbf{E}_{i^{\\prime}j^{\\prime}})^{\\otimes2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\nZ_{i j,i^{\\prime}j^{\\prime}}=(Y_{i j}-Y_{i^{\\prime}j^{\\prime}})\\frac{\\mathrm{exp}((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}})/2)}{1+\\mathrm{exp}((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}}))}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. For any $\\textbf{U}\\in\\ \\mathbb{R}^{m_{1}\\times m_{2}}$ such that $\\langle\\mathbf{J},\\mathbf{U}\\rangle~=~0$ and assume that $\\mathbb{E}(Z_{i j,i^{\\prime}j^{\\prime}}^{2})\\ \\ge$ $\\kappa,\\forall(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]$ when $\\|\\mathbf{A}\\|_{\\infty}\\leq a$ for some constant $a>0$ we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\mathrm{vec}(\\mathbf{U})^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathrm{vec}(\\mathbf{U})\\geq\\kappa m_{1}m_{2}\\pi_{L}^{2}\\left\\|\\mathbf{U}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We derive the quadratic form first and take the expectation to obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\mathbf{u}^{\\mathsf{T}}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}=\\frac{1}{2}\\underset{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}{\\sum}\\mathbb{E}\\{T_{i j}T_{i^{\\prime}j^{\\prime}}Z_{i j,i^{\\prime}j^{\\prime}}^{2}(U_{i j}-U_{i^{\\prime}j^{\\prime}})^{2}\\}}\\\\ &{\\phantom{\\sum}\\geq\\frac{1}{2}\\underset{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}{\\sum}\\mathbb{E}\\{T_{i j}T_{i^{\\prime}j^{\\prime}}(U_{i j}-U_{i^{\\prime}j^{\\prime}})^{2}\\}}\\\\ &{\\phantom{\\sum}\\ge\\frac{1}{2}\\kappa\\pi_{L}^{2}\\qquad\\underset{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}{\\sum}(U_{i j}-U_{i^{\\prime}j^{\\prime}})^{2}}\\\\ &{=\\frac{1}{2}\\kappa\\pi_{L}^{2}\\qquad\\underset{(i,j),(i^{\\prime},j^{\\prime})\\in[m_{1}]\\times[m_{2}]}{\\sum}U_{i j}^{2}+U_{i^{\\prime}j^{\\prime}}^{2}-2U_{i j}U_{i^{\\prime}j^{\\prime}}}\\\\ &{=\\kappa m_{1}m_{2}\\pi_{L}^{2}\\ensuremath{\\left\\|[\\mathbf{0}]]\\right\\|}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now start to lower bound $\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}$ . We consider the following constraint set ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{C}=\\bigg\\{\\mathbf{U}\\in\\mathbb{R}^{m_{1}\\times m_{2}}\\;\\bigg|\\;\\langle\\mathbf{J},\\mathbf{U}\\rangle=0\\bigg\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Still, we derive the argument by conditioning on the event $\\mathcal{E}$ defined in (10). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{vec}(\\mathbf{U})^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathrm{vec}(\\mathbf{U})\\mid\\mathcal{E}=\\frac{1}{2}\\sum_{k,k^{\\prime}=1,\\ldots,n}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i_{k},j_{k})$ is the entry where $E_{i_{k},j_{k}}=1$ ", "page_idx": 16}, {"type": "text", "text": "By Lemma A.5, WLOG, we assume $n$ is even. We decompose the summation into ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{vec}(\\mathbf{U})^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathrm{vec}(\\mathbf{U})\\mid\\mathcal{E}=\\sum_{g=1}^{n-1}\\sum_{(k,k^{\\prime})\\in G_{g}}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where within every group $G_{g}$ , there is no repeated index. Denote ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Sigma_{g}=\\sum_{(k,k^{\\prime})\\in G_{g}}\\varepsilon_{k,k^{\\prime}}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}(X_{k}-X_{k^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $\\varepsilon_{k,k^{\\prime}}$ are independent Radamacher variables. For convenience, we denote $\\mathbf{u}=\\operatorname{vec}(\\mathbf{U})\\in$ $\\mathbb{R}^{m_{1}m_{2}}$ forany $\\mathbf{U}\\in\\mathbf{\\bar{R}}^{m_{1}\\times m_{2}}$ ", "page_idx": 16}, {"type": "text", "text": "Lemma A.3. For all $\\mathbf{U}\\in{\\mathcal{C}}$ , the following holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}-\\mathbb{E}\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}\\Big\\rvert\\gtrsim B^{2}\\lvert\\lvert U\\rvert\\rvert_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]+B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{2}\\pi_{U})^{2}\\Big\\rvert\\ln(\\frac{1}{M}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probabilityat least $1-3/d.$ ", "page_idx": 16}, {"type": "text", "text": "Proof. Denote ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{T}:=\\{U\\in\\mathcal{C}:\\|U\\|_{*}\\leq T\\}.\\qquad\\qquad}\\\\ {W_{T}=\\underset{\\mathbf{U}\\in\\mathcal{C}(T)}{\\operatorname*{sup}}\\,\\left|\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}-\\mathbb{E}\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $v=\\sqrt{m\\pi_{U}/\\pi_{L}}$ . By Lemma A.4, we have with probability at least $1-1/d$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}-\\mathbb{E}\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}|\\gtrsim B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we focus on the case when $\\|\\pmb{U}\\|_{*}\\geq v$ . We will show that the probability of the following bad event is small ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}-\\mathbb{E}\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}|\\gtrsim B^{2}\\|U\\|_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi}\\right],\\|U\\|_{*}\\geq v\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We use a standard peeling argument. For $l\\in\\mathbb N$ set ", "page_idx": 17}, {"type": "equation", "text": "$$\nS_{l}=\\left\\{\\mathbf{U}\\in{\\mathcal{C}}(r):\\alpha^{l}\\nu\\leq\\|U\\|_{*}\\leq\\alpha^{l+1}\\nu\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For each $T>\\nu$ , define the following event ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}-\\mathbb{E}\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}|\\gtrsim B^{2}\\|U\\|_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bylemma4 withpbabilt alleh $\\begin{array}{r}{(m_{1}m_{2})\\exp\\left\\{\\frac{-T^{2}\\pi_{L}}{m\\pi_{U}}\\log(d^{3})\\right\\}}\\end{array}$ we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{T}\\geq B^{2}T\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(\\mathcal{B}_{l})\\leq\\left(m_{1}m_{2}\\right)\\exp\\left\\{-\\frac{(\\alpha^{2l}v^{2})\\pi_{L}}{m\\pi_{U}}\\log(d^{3})\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the union bound, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\operatorname*{Pr}(\\mathcal{B})\\leq\\sum_{l=0}^{\\infty}\\operatorname*{Pr}(\\mathcal{B}_{l})\\leq\\sum_{l=1}^{\\infty}(m_{1}m_{2})\\exp\\left\\{\\frac{-(\\alpha^{2l}v^{2})\\pi_{L}}{m\\pi_{U}}\\log(d^{3})\\right\\}+1/d}\\\\ {\\quad}&{\\leq\\displaystyle\\sum_{l=1}^{\\infty}(m_{1}m_{2})\\exp\\left\\{\\frac{-(v^{2})\\pi_{L}}{m\\pi_{U}}\\log(d^{3})2l\\log(\\alpha)\\right\\}+1/d}\\\\ {\\quad}&{\\leq\\displaystyle\\int_{l=1}^{\\infty}(m_{1}m_{2})\\exp\\left\\{\\frac{-(v^{2})\\pi_{L}}{m\\pi_{U}}\\log(d^{3})2l\\log(\\alpha)\\right\\}+1/d}\\\\ {\\quad}&{\\leq(m_{1}m_{2})\\exp\\left\\{-2\\log(\\alpha)\\log(d^{3})\\right\\}+1/d\\leq2/d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combine two cases, and the conclusion follows. ", "page_idx": 17}, {"type": "text", "text": "We now make up the concentration property on $W_{T}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. Conditioned on event $\\mathcal{E}$ ,For any $T>\\nu$ wedenote ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{C}(T):=\\left\\{\\mathbf{U}\\in\\mathcal{C}(r):\\|U\\|_{*}\\leq T\\right\\}\\quad a n d\\quad W_{T}:=\\operatorname*{sup}_{\\mathbf{U}\\in\\mathcal{C}(T)}\\left|\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}-\\mathbb{E}\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}\\right|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $T\\leq\\sqrt{m\\pi_{U}/\\pi_{L}}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(W_{T}\\gtrsim B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}\\right)\\leq1/d.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $T\\geq\\sqrt{m\\pi_{U}/\\pi_{L}},$ By taking $\\begin{array}{r}{t=\\frac{T\\sqrt{3\\log d}\\sqrt{M\\pi_{U}}}{m_{1}m_{2}\\pi_{U}}}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(W_{T}\\gtrsim B^{2}T\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right)\\leq(m_{1}m_{2})\\exp\\left\\{-\\frac{-T^{2}\\pi_{L}}{m\\pi_{U}}\\log(d^{3})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Still, without generality, we focus on the case when $n$ is even. Recall that conditioned on event $\\mathcal{E}$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal V}_{T}=\\operatorname*{sup}_{{\\bf U}\\in{\\cal C}(r,T)}\\left\\vert{\\bf u}^{\\top}\\nabla^{2}\\ell({\\bf A}){\\bf u}-{\\mathbb E}{\\bf u}^{\\top}\\nabla^{2}\\ell({\\bf A}){\\bf u}\\right\\vert}\\ ~}\\\\ {~~}\\\\ {{\\displaystyle~~~=\\operatorname*{sup}_{{\\bf U}\\in{\\cal C}(r,T)}\\left\\vert\\sum_{g}\\sum_{(k,k^{\\prime})\\in{\\cal G}_{g}}Z_{i_{k},j_{k},i_{k^{\\prime}},j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}-{\\mathbb E}\\sum_{g}\\sum_{(k,k^{\\prime})\\in{\\cal G}_{g}}Z_{i_{k},j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}\\right\\vert}\\ ~}}\\\\ {~~}\\\\ {{\\displaystyle~~~~\\leq\\sum_{g}\\operatorname*{sup}_{{\\bf U}\\in{\\cal C}(r,T)}\\left\\vert\\sum_{(k,k^{\\prime})\\in{\\cal G}_{g}}Z_{i_{k},j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}-{\\mathbb E}\\sum_{(k,k^{\\prime})\\in{\\cal G}_{g}}Z_{k,k^{\\prime}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}\\right\\vert,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\zeta_{g,T}=\\operatorname*{sup}_{\\mathbf{U}\\in\\mathcal{C}(r,T)}\\left|\\sum_{(k,k^{\\prime})\\in G_{g}}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}-\\mathbb{E}\\sum_{(k,k^{\\prime})\\in G_{g}}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}\\right|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that conditioned on the event $\\mathcal{E}$ within each grouping $G_{g}$ everyterm $Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-$ $U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}$ is independent of the others. The standard symmetrization trick still applies here, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}Z_{g,T}\\mid\\mathcal{E}\\leq2\\mathbb{E}\\operatorname*{sup}_{\\mathbf{U}\\in\\mathcal{C}(T)}\\left\\vert\\sum_{(k,k^{\\prime})\\in G_{g}}\\varepsilon_{k,k^{\\prime}}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}\\right\\vert.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $|Z_{i_{k}i_{k^{\\prime}}}|\\,\\le\\,2B$ and $\\|\\mathbf{U}\\|_{\\infty}\\,=\\,1$ \uff0c $Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}\\,-\\,U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}\\,\\le\\,16B^{2}$ for every $(k,k^{\\prime})$ \uff0c Therefore, $\\phi(u)=u^{2}$ \uff0c $|\\phi(u)-\\phi(v)|\\le|u+v||u-v|\\le8B|u-v|$ . The contraction inequality yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}Z_{\\delta_{r}}\\left[\\xi\\leq2\\!\\!\\!\\begin{array}{c}{\\operatorname*{sup}}\\\\ {0\\!\\!\\!\\!\\!\\operatorname*{sup}\\bigg|_{(k,k^{\\prime})\\in G_{r}}\\frac{\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k^{\\prime},i^{\\prime},i^{\\prime}\\nu}^{2}(U_{i,k}-U_{i^{\\prime},k^{\\prime}})}{\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k^{\\prime},i^{\\prime}\\nu}^{2}(U_{i,k}-U_{i^{\\prime},k^{\\prime}})}\\bigg|^{2}\\right]}\\\\ &{\\leq168B\\frac{\\operatorname*{sup}}{168c(c^{2})}\\bigg|\\sum_{k^{\\prime}\\in G_{r}}\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k,i^{\\prime},i^{\\prime}\\nu}(U_{i+h}-U_{i^{\\prime},k^{\\prime}})\\bigg|}\\\\ &{\\leq16B B\\left(\\frac{1}{16\\pi^{4}}\\bigg|\\sum_{k^{\\prime}\\in G_{r}}\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k,i^{\\prime},i^{\\prime}\\nu}(\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}},\\mathbf{U})\\bigg|\\;\\big|\\;\\xi\\right)}\\\\ &{\\leq16B B\\left(\\frac{1}{16\\pi^{4}}\\bigg|\\sum_{k^{\\prime}\\in G_{r}}\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k,i^{\\prime},i^{\\prime}\\nu}(\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}},\\mathbf{U}),\\mathbf{U}\\bigg|\\;\\big|\\;\\xi\\right)}\\\\ &{\\leq16B B^{\\prime}\\left(\\frac{1}{16\\pi^{4}}\\bigg|\\sum_{k^{\\prime}\\in G_{r}}\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k^{\\prime},i^{\\prime}\\nu}(\\mathbf{X}_{k}-\\mathbf{X}_{k^{\\prime}}),\\mathbf{U}\\bigg|\\;\\big|\\;\\xi\\right)}\\\\ &{\\leq16B B^{\\prime}\\left(\\left\\|\\sqrt{\\sum_{k^{\\prime}\\in G_{r}}\\varepsilon_{\\delta_{r},k^{\\prime}}Z_{i,k^{\\prime},i^{\\prime}\\nu}(\\mathbf{X}_{k}-\\mathbf{X}_{k^{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Take $\\gamma_{k,k^{\\prime}}(U)=Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}-\\mathbb{E}Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}$ Note that $|Z_{i_{k}j_{k},i_{k^{\\prime}}j_{k^{\\prime}}}^{2}(U_{i_{k}j_{k}}-U_{i_{k^{\\prime}}j_{k^{\\prime}}})^{2}|\\leq16B^{2}.$ ", "page_idx": 18}, {"type": "text", "text": "Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\gamma_{k,k}(U)=0}\\\\ {\\operatorname*{sup}_{(k,k^{\\prime})}\\operatorname*{sup}_{U\\in\\mathcal{C}(T)}\\gamma_{k,k^{\\prime}}(U)/(32B^{2})\\le1}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Massart's concentration inequality (e.g., Theorem 14.2 in [3]). We have that conditioning on the event $\\mathcal{E}$ ,forany $t>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(\\left|\\underset{U\\in\\mathcal{C}(T)}{\\operatorname*{sup}}\\frac{1}{n/2}\\sum_{\\stackrel{\\scriptstyle(k,k^{\\prime})\\in\\cal G_{g}}{(k,k^{\\prime})\\in\\cal G_{g}}}\\gamma_{k,k^{\\prime}}(U)/(32B^{2})\\right|>\\mathbb{E}\\left|\\underset{U\\in\\mathcal{C}(T)}{\\operatorname*{sup}}\\frac{1}{n/2}\\sum_{\\stackrel{\\scriptstyle(k,k^{\\prime})\\in\\cal G_{g}}{(k,k^{\\prime})\\in\\cal G_{g}}}\\gamma_{k,k^{\\prime}}(U)/(32B^{2})\\right|+t\\right)}\\\\ &{\\qquad\\qquad\\leq\\exp(-(n/2)t^{2}/8)}\\\\ &{\\qquad\\qquad\\operatorname*{Pr}\\left(\\left|\\underset{U\\in\\mathcal{C}(T)}{\\operatorname*{sup}}\\frac{1}{n/2}\\sum_{\\stackrel{\\scriptstyle(k,k^{\\prime})\\in\\cal G_{g}}{(k,k^{\\prime})\\in\\cal G_{g}}}\\gamma_{k,k^{\\prime}}(U)\\right|>\\mathbb{E}\\left|\\underset{U\\in\\mathcal{C}(T)}{\\operatorname*{sup}}\\frac{1}{n/2}\\sum_{\\stackrel{\\scriptstyle(k,k^{\\prime})\\in\\cal G_{g}}{(k,k^{\\prime})\\in\\cal G_{g}}}\\gamma_{k,k^{\\prime}}(U)\\right|+32B^{2}t\\right)\\leq\\exp(-(n/2)t^{2}/8)}\\\\ &{\\operatorname*{Pr}\\left(Z_{g,T}>\\mathbb{E}Z_{g,T}+16n B^{2}t\\right)\\leq\\exp(-n t^{2}/16).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Apply a union bound, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(W_{T}>\\sum_{g=1}^{n-1}\\mathbb{E}Z_{g,T}+16n(n-1)B^{2}t\\mid\\mathcal{E}\\right)\\leq(n-1)\\exp(-n t^{2}/16).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we marginalize the event $\\mathcal{E}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Pr}\\left(W_{T}>\\displaystyle\\sum_{g=1}^{n-1}\\mathbb{E}Z_{g,T}+16n^{2}B^{2}t\\right)\\leq\\mathbb{E}[n\\exp(-n t^{2}/16)]\\leq(m_{1}m_{2})\\mathbb{E}[\\exp(-n t^{2}/16)]}\\\\ &{}&{\\leq(m_{1}m_{2})\\exp(-\\mathbb{E}(n)t^{2}/16)\\leq(m_{1}m_{2})\\exp(-m_{1}m_{2}\\pi_{L}t^{2}/16).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we considering bounding $\\textstyle\\sum_{g=1}^{n-1}\\mathbb{E}Z_{g,T}$ . Note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{g=1}^{n-1}\\mathbb{E}Z_{g,T}\\leq16B T\\mathbb{E}\\left[\\mathbb{E}\\left(\\sum_{g=1}^{n-1}\\|\\Sigma_{g}\\|\\mid\\mathcal{E}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By using a similar argument in Lemma A.1, we are able to show that for any $x>0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left\\{\\sum_{g=1}^{n-1}\\|\\Sigma_{g}\\|\\gtrsim x\\sqrt{B^{2}\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right\\}\\leq\\frac{3}{d}\\exp\\{-x\\log(d^{2})\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{g=1}^{n-1}\\mathbb{E}Z_{g,T}\\lesssim16B^{2}T\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the result from Lemma A.6, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left(W_{T}\\gtrsim B^{2}T\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]+(m_{1}m_{2}\\pi_{U})^{2}B^{2}t\\right)\\leq(m_{1}m_{2})\\exp(-m_{1}m_{2}\\pi_{L}t^{2}/16)\\leq C_{1}\\exp(-m_{1}m_{2}\\pi_{U}t^{2}/16),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $T\\leq\\sqrt{m\\pi_{U}/\\pi_{L}}$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(W_{T}\\gtrsim B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}\\right)\\leq1/d.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When T \u2265 \u221am\u03c0u /\u03c0TL, By taking t = T\u221a3logd\u221aMu, ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(W_{T}\\gtrsim B^{2}T\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right)\\leq(m_{1}m_{2})\\exp\\left\\{\\frac{-T^{2}\\pi_{L}}{m\\pi_{U}}\\log(d^{3})\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.3Proof of Theorem 5.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. It follows from the definition of the estimator $\\widehat{\\bf A}$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\widehat{\\mathbf{A}})+\\lambda\\left\\|\\widehat{\\mathbf{A}}\\right\\|_{\\star}\\leq\\ell(\\bar{A}_{\\star})+\\lambda\\left\\|\\bar{A}_{\\star}\\right\\|_{\\star},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "equivalently ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\widehat{\\mathbf{A}})-\\ell(\\bar{A}_{\\star})\\leq\\lambda\\left(\\left\\lVert\\bar{A}_{\\star}\\right\\rVert_{\\star}-\\left\\lVert\\widehat{\\mathbf{A}}\\right\\rVert_{\\star}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle\\nabla\\ell(\\bar{A}_{\\star}),\\mathrm{vec}(\\widehat{\\mathbf{A}}-\\bar{A}_{\\star})\\rangle+\\mathrm{vec}(\\widehat{\\mathbf{A}}-\\bar{A}_{\\star})^{\\top}\\nabla^{2}\\ell(\\widetilde{\\mathbf{A}})\\mathrm{vec}(\\widehat{\\mathbf{A}}-\\bar{A}_{\\star})\\leq\\lambda\\left(\\|\\bar{A}_{\\star}\\|_{\\star}-\\left\\|\\widehat{\\mathbf{A}}\\right\\|_{\\star}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\widetilde{\\mathbf{A}}=t\\bar{A}_{\\star}+(1-t)\\widehat{\\mathbf{A}}$ for some $t\\in[0,1]$ ", "page_idx": 19}, {"type": "text", "text": "Let's denote $\\Delta=\\widehat{\\mathbf{A}}-\\bar{A}_{\\star}$ ", "page_idx": 19}, {"type": "text", "text": "From Lemma A.3 and Lemma A.2, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{7^{2}\\ell(\\mathbf{A})\\mathbf{u}\\gtrsim\\mathbb{E}\\left\\{\\mathbf{u}^{\\top}\\nabla^{2}\\ell(\\mathbf{A})\\mathbf{u}\\right\\}-B^{2}\\|U\\|_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]-B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{2}\\pi_{U})}\\\\ {\\gtrsim\\kappa m_{1}m_{2}\\pi_{L}^{2}\\|U\\|_{F}^{2}-B^{2}\\|U\\|_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]-B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{2}\\pi_{U})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at most $1-3/d$ ", "page_idx": 20}, {"type": "text", "text": "Therefore with probability at most $1-3/d$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{:}m_{1}m_{2}\\pi_{L}^{2}\\|\\Delta\\|_{F}^{2}\\lesssim B^{2}\\|\\Delta\\|_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]+B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}}\\\\ &{}&{+\\mathrm{vec}(\\widehat{\\mathbf{A}}-\\bar{A}_{\\star})^{\\top}\\nabla^{2}\\ell(\\widetilde{\\mathbf{A}})\\mathrm{vec}(\\widehat{\\mathbf{A}}-\\bar{A}_{\\star})}\\\\ &{}&{\\lesssim B^{2}\\|\\Delta\\|_{*}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]+B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}}\\\\ &{}&{+\\lambda\\left(\\|\\bar{A}_{\\star}\\|_{\\star}-\\left\\|\\widehat{\\mathbf{A}}\\right\\|_{\\star}\\right)+\\left\\|\\nabla\\ell(\\bar{A}_{\\star})\\right\\|\\|\\Delta\\|_{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\{\\mathbf{u}_{k}\\,\\in\\,\\mathbb{R}^{m_{1}}\\}$ and $\\{\\mathbf{v}_{k}\\in\\mathbb{R}^{m_{2}}\\}$ be the left and right singular vectors of $\\bar{A}_{\\star}$ respectively. For any matrix $\\mathbf{A}\\in\\mathbb{R}^{m_{1}\\times m_{2}}$ , we let $\\mathrm{row}(\\mathbf{A})\\subseteq\\mathbb{R}^{m_{2}}$ and $\\operatorname{col}(\\mathbf{A})\\subseteq\\mathbb{R}^{m_{1}}$ denote its row space and column space respectively. Let the column and row span of $\\bar{A}_{\\star}$ be $\\mathcal{U}_{\\star}=\\mathrm{col}(\\bar{\\mathbf{A}}_{\\star})=\\mathrm{span}\\bar{\\{}\\mathbf{u}_{k}\\}$ and $\\mathcal{V}_{\\star}=\\mathrm{row}(\\bar{A}_{\\star})\\stackrel{\\star}{=}\\mathrm{span}\\{\\dot{\\mathbf{v}}_{k}\\}$ respectively. Define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{M}:=\\{\\mathbf{A}:\\mathrm{row}(\\mathbf{A})\\subseteq\\mathcal{V}_{\\star},\\mathrm{col}(\\mathbf{A})\\subseteq\\mathcal{U}_{\\star}\\},}\\\\ &{\\quad\\overline{{\\mathcal{M}}}^{\\perp}:=\\{\\mathbf{A}:\\mathrm{row}(\\mathbf{A})\\subseteq\\mathcal{V}_{\\star}^{\\perp},\\mathrm{col}(\\mathbf{A})\\subseteq\\mathcal{U}_{\\star}^{\\perp}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is easy to see that ${\\mathcal{M}}\\subseteq{\\overline{{\\mathcal{M}}}}$ , but ${\\mathcal{M}}\\neq{\\overline{{\\mathcal{M}}}}$ . The subspace compatibility of $\\overline{{\\mathcal{M}}}$ is upper bounded by $\\sqrt{2r}$ i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{A}\\in\\overline{{\\mathcal{M}}}\\setminus\\{0\\}}\\frac{\\|\\mathbf{A}\\|_{\\star}}{\\|\\mathbf{A}\\|_{F}}\\leq\\sqrt{2r}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widehat{\\mathbf{A}}\\right\\|_{*}=\\left\\|\\bar{A}_{*}+\\Delta_{\\overline{{\\mathcal{M}}}}+\\Delta_{\\overline{{\\mathcal{M}}}^{\\perp}}\\right\\|_{*}\\geq\\left\\|\\bar{A}_{*}+\\Delta_{\\overline{{\\mathcal{M}}}^{\\perp}}\\right\\|_{*}-\\left\\|\\Delta_{\\overline{{\\mathcal{M}}}}\\right\\|_{*}=\\left\\|\\bar{A}_{*}\\right\\|_{*}+\\left\\|\\Delta_{\\overline{{\\mathcal{M}}}^{\\perp}}\\right\\|_{*}-\\left\\|\\Delta_{\\overline{{\\mathcal{M}}}}\\right\\|_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By choosing $\\lambda\\gtrsim2(\\left\\|\\nabla\\ell(\\bar{\\cal A}_{\\star})\\right\\|+B^{2}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right])$ , we have ", "page_idx": 20}, {"type": "text", "text": "km1m2\u03c0l\u25b31 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim\\left(\\|\\nabla\\ell(\\bar{A}_{\\star})\\|+B^{2}\\sqrt{\\log(d)}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right)(\\|\\Delta_{\\overline{{\\mathcal{M}}}^{\\bot}}\\|_{\\star}+\\|\\Delta_{\\overline{{\\mathcal{M}}}}\\|_{\\star})}\\\\ &{\\quad+\\lambda(\\|\\Delta_{\\overline{{\\mathcal{M}}}}\\|_{\\star}-\\|\\Delta_{\\overline{{\\mathcal{M}}}^{\\bot}}\\|_{\\star})+B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}}\\\\ &{\\lesssim3\\lambda\\,\\|\\Delta_{\\overline{{\\mathcal{M}}}}\\|_{\\star}+B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}\\leq3\\lambda\\sqrt{2r}\\|\\Delta\\|_{F}+B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(r)}\\end{array}\n$$nm2\u03c0L)-1/2 ", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we could derive ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Delta\\|_{F}^{2}\\lesssim\\frac{3\\lambda\\sqrt{2r}}{\\kappa m_{1}m_{2}\\pi_{L}^{2}}\\|\\Delta\\|_{F}+\\frac{B^{2}\\log(d)(m_{1}m_{2}\\pi_{U})^{2}(m_{1}m_{2}\\pi_{L})^{-1/2}}{\\kappa m_{1}m_{2}\\pi_{L}^{2}}}\\\\ {\\frac{1}{m_{1}m_{2}}\\|\\Delta\\|_{F}\\lesssim\\operatorname*{max}\\left\\{\\frac{\\lambda^{2}r}{m_{1}m_{2}(\\kappa m_{1}m_{2}\\pi_{L}^{2})^{2}},\\frac{B^{2}\\log(d)}{\\kappa}\\left(\\frac{\\pi_{U}}{\\pi_{L}}\\right)^{2}\\sqrt{\\frac{1}{m_{1}m_{2}\\pi_{L}}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Due to Lemma A.1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left\\{\\|\\nabla\\ell(\\mathbf{A})\\|\\gtrsim\\sqrt{B^{2}[\\log(d)+\\log(m_{1}m_{2})]}\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right]\\right\\}\\leq\\frac{3}{d}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we can take $\\lambda\\asymp(B^{2}\\log(d)\\left[m_{1}m_{2}\\pi_{U}\\sqrt{M\\pi_{U}}\\right])$ , and with probability at least $1-6/d$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{m_{1}m_{2}}\\|\\boldsymbol{\\Delta}\\|_{F}^{2}\\lesssim\\operatorname*{max}\\left\\{\\frac{B^{4}[\\log d]^{2}}{\\kappa^{2}}\\left(\\frac{\\pi_{U}}{\\pi_{L}}\\right)^{3}\\frac{M r}{m_{1}m_{2}\\pi_{L}},\\frac{B^{2}\\log(d)}{\\kappa}\\left(\\frac{\\pi_{U}}{\\pi_{L}}\\right)^{2}\\sqrt{\\frac{1}{m_{1}m_{2}\\pi_{L}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A.4 Auxiliary lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma A.5. For any collection of individual index pairs $\\{(j,j^{\\prime}):1\\leq j<j^{\\prime}\\leq n\\}$ ", "page_idx": 21}, {"type": "text", "text": "(a) (From Lemma S.4. in [36]) When n is even, we can decompose it into $(n-1)$ groupssuch that within eachgroup,thereare $n/2$ pairs and norepeated individuals.   \n$(b)$ When n is odd, we can decompose it into n groups such that within each group, there are $(n-1)/2$ pairsandnorepeatedindividuals. ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof for part (a) is done in [36]. ", "page_idx": 21}, {"type": "text", "text": "For part (b), when $n$ is odd, we consider an extra index $n+1$ and add all the pairs $\\{(j,n+1):1\\leq$ $j\\leq n\\}$ to the original collection. For the new collection of individual index pairs $\\{(j,j^{\\prime}):1\\leq j<$ $j^{\\prime}\\leq n+1\\}$ , since $n+1$ is even, we can apply part (a) and get $n$ groups such that within each group, there are $(\\bar{n}+1)/2$ pairs and no repeated individuals. Therefore, every index appears in each group exactly once. In each group, we remove the pair with $n+1$ in it. We now obtain $n$ groups such that within each group, there are $(n-1)/2$ pairs and no repeated individuals for the collection of individual index pairs $\\{(j,j^{\\prime}):1\\leq j<j^{\\prime}\\leq n\\}$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Recall that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Z_{i j,i^{\\prime}j^{\\prime}}=(Y_{i j}-Y_{i^{\\prime}j^{\\prime}})^{2}\\frac{\\exp((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}})/2)}{1+\\exp((Y_{i j}-Y_{i^{\\prime}j^{\\prime}})(A_{i j}-A_{i^{\\prime}j^{\\prime}}))},}}\\\\ &{}&{\\Sigma_{g}=\\sum_{(i j,i^{\\prime}j^{\\prime})\\in g}\\varepsilon_{i j,i^{\\prime}j^{\\prime}}T_{i j}T_{i^{\\prime}j^{\\prime}}Z_{i j,i^{\\prime}j^{\\prime}}({\\bf E}_{i j}-{\\bf E}_{i^{\\prime}j^{\\prime}}),\\quad\\quad}\\\\ &{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any collection of non-overlap index pairs. ", "page_idx": 21}, {"type": "text", "text": "We analyze the upper bound for $\\mathbb{E}\\left\\|\\Sigma_{g}\\right\\|$ to prove Corollary 5.3. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.6. Take $\\begin{array}{r}{n=\\sum_{i,j}T_{i,j}}\\end{array}$ and $\\xi_{n,g}$ defined in (11). We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Pr}\\,\\left(n\\geq e(\\displaystyle\\sum_{i,j}\\pi_{i,j})\\right)\\leq\\exp(-m_{1}m_{2}\\pi_{L}).}\\\\ &{}&{\\mathrm{Pr}\\,\\left(\\displaystyle\\sum_{g=1}^{n-1}\\xi_{n,g}>2m_{1}m_{2}M\\pi_{U}^{2}\\right)\\leq2M\\exp\\left\\{-m_{1}m_{2}\\pi_{U}^{2}/2\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Note that $T_{i,j}$ are independent Bernoulli random variables and $\\begin{array}{r}{\\mathbb{E}n=\\sum_{i,j}\\pi_{i,j}}\\end{array}$ .We apply Chernoff's inequality and obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(n\\geq t\\right)\\leq\\exp\\{-\\sum_{i,j}\\pi_{i,j}\\}\\left(\\frac{e(\\sum_{i,j}\\pi_{i,j})}{t}\\right)^{t},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $t>\\mathbb{E}n$ . Take $\\begin{array}{r}{t=e(\\sum_{i,j}\\pi_{i,j})}\\end{array}$ and we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(n\\geq e(\\sum_{i,j}\\pi_{i,j})\\right)\\leq\\exp\\{-\\sum_{i,j}\\pi_{i,j}\\}\\leq\\exp(-m_{1}m_{2}\\pi_{L}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{g=1}^{n-1}\\xi_{n,g}=\\operatorname*{max}\\left\\{\\sum_{k\\neq k^{\\prime}}^{n}\\mathbb{1}\\{i_{k}=i_{k^{\\prime}}\\},\\sum_{k\\neq k^{\\prime}}^{n}\\mathbb{1}\\{j_{k}=j_{k^{\\prime}}\\}\\right\\}}\\\\ {=\\operatorname*{max}\\left\\{\\displaystyle\\sum_{i=1}^{m_{1}}\\sum_{j\\neq j^{\\prime}}^{m_{2}}T_{i j}T_{i j^{\\prime}},\\displaystyle\\sum_{j=1}^{m_{2}}\\sum_{i\\neq i^{\\prime}}^{m_{1}}T_{i j}T_{i j^{\\prime}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We coside bounding $\\begin{array}{r}{\\sum_{i=1}^{n}\\sum_{j\\neq j^{\\prime}}^{m_{2}}T_{i,j}T_{i,j^{\\prime}}}\\end{array}$ $j$ even, and by Lemma A.5, we can decompose ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m_{1}}\\sum_{j\\neq j^{\\prime}}^{m_{2}}T_{i,j}T_{i,j^{\\prime}}=\\sum_{g=1}^{m_{2}-1}(\\sum_{i=1}^{m_{1}}\\sum_{(j,j^{\\prime})\\in G_{g}^{\\prime}}T_{i j}T_{i j^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Within every group $G_{g}^{\\prime}$ , every pair $T_{i,j}T_{i,j^{\\prime}}$ is independent of others. Then we apply Bernstein inequality o bound $\\begin{array}{r}{\\sum_{i=1}^{\\tilde{m}_{1}}\\sum_{(j,j^{\\prime})\\in G_{g}^{\\prime}}T_{i,j}T_{i,j^{\\prime}}}\\end{array}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{y}_{\\mathrm{r}}\\left(\\displaystyle\\sum_{i=1}^{m_{1}}\\sum_{(j,j^{\\prime})\\in G_{g}^{\\prime}}T_{i j}T_{i j^{\\prime}}-\\displaystyle\\sum_{i=1}^{m_{1}}\\sum_{(j,j^{\\prime})\\in G_{g}^{\\prime}}\\pi_{i,j}\\pi_{i,j^{\\prime}}\\geq t\\right)\\leq\\exp\\left\\{\\frac{-t^{2}/2}{\\sum_{i=1}^{m_{1}}\\sum_{(j,j^{\\prime})\\in G_{g}^{\\prime}}\\mathbb{E}T_{i j}^{2}T_{i j^{\\prime}}^{2}+t}\\right\\}}\\\\ {\\leq\\exp\\left\\{\\frac{-t^{2}/2}{\\pi_{U}^{2}m_{1}m_{2}/2+t}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Take $t=m_{1}m_{2}\\pi_{U}^{2}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{i=1}^{m_{1}}\\sum_{(j,j^{\\prime})\\in G_{g}^{\\prime}}T_{i j}T_{i j^{\\prime}}\\ge2m_{1}m_{2}\\pi_{U}^{2}\\right)\\le\\exp\\left\\{-m_{1}m_{2}\\pi_{U}^{2}/3\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Take a union bound over $g=1,\\dots,m_{2}-1$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Pr}\\left(\\displaystyle\\sum_{i=1}^{m_{1}}\\sum_{j\\neq j^{\\prime}}^{m_{2}}T_{i j}T_{i j^{\\prime}}>2m_{1}m_{2}(m_{2}-1)\\pi_{U}^{2}\\right)\\leq m_{2}\\exp\\left\\{-m_{1}m_{2}\\pi_{U}^{2}/3\\right\\}.}\\\\ &{}&{\\mathrm{Pr}\\left(\\displaystyle\\sum_{i=1}^{m_{1}}\\sum_{j\\neq j^{\\prime}}^{m_{2}}T_{i j}T_{i j^{\\prime}}>2m_{1}m_{2}M\\pi_{U}^{2}\\right)\\leq M\\exp\\left\\{-m_{1}m_{2}\\pi_{U}^{2}/3\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With the same argument, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{j=1}^{m_{2}}\\sum_{i\\neq i^{\\prime}}^{m_{1}}T_{i j}T_{i j^{\\prime}}>2m_{1}m_{2}M\\pi_{U}^{2}\\right)\\leq M\\exp\\left\\{-m_{1}m_{2}\\pi_{U}^{2}/3\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{g=1}^{n-1}\\xi_{n,g}>2m_{1}m_{2}M\\pi_{U}^{2}\\right)\\le2M\\exp\\left\\{-m_{1}m_{2}\\pi_{U}^{2}/3\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B  Identifiability of dispersion parameter in Gaussian distributions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Assume $Y_{i j}\\sim\\mathcal{N}(\\mu_{i j},\\sigma^{2})$ with missing mechanism: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(T_{i j}=1|Y_{i j}=y)=c_{i j}s(y),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $c_{i j},s(\\cdot)\\in[0,1]$ ", "page_idx": 22}, {"type": "text", "text": "Lemma B.1. Assume that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{y\\to\\infty}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)}=b_{i j}\\;o r\\;\\operatorname*{lim}_{y\\to-\\infty}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)}=b_{i j}^{\\prime},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $b_{i j},b_{i j}^{\\prime}$ can not be both $\\boldsymbol{O}$ or $\\infty$ simultaneously. ", "page_idx": 22}, {"type": "text", "text": "f $\\sigma\\ne\\sigma^{\\prime}$ ,thenfor any $(c_{i j},c_{i j}^{\\prime},s(y),s^{\\prime}(y))$ at least one of the following two statements holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{y\\rightarrow+\\infty}{\\operatorname*{lim}}\\,\\frac{\\phi\\,\\left(\\frac{y-\\mu_{i j}}{\\sigma}\\right)}{\\phi\\,\\left(\\frac{y-\\mu_{i j}^{\\prime}}{\\sigma^{\\prime}}\\right)}\\frac{c_{i j}s\\left(y\\right)}{c_{i j}^{\\prime}s^{\\prime}\\left(y\\right)}=+\\infty\\,o r\\,0}\\\\ {\\underset{y\\rightarrow-\\infty}{\\operatorname*{lim}}\\,\\frac{\\phi\\,\\left(\\frac{y-\\mu_{i j}}{\\sigma}\\right)}{\\phi\\,\\left(\\frac{y-\\mu_{i j}^{\\prime}}{\\sigma^{\\prime}}\\right)}\\frac{c_{i j}s\\left(y\\right)}{c_{i j}^{\\prime}s^{\\prime}\\left(y\\right)}=-\\infty\\,o r\\,0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We observe that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\phi\\left(\\frac{y-\\mu_{i j}}{\\sigma}\\right)}{\\phi\\left(\\frac{y-\\mu_{i j}^{\\prime}}{\\sigma^{\\prime}}\\right)}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)}=\\exp\\left\\{\\frac{\\left(\\sigma^{2}-\\sigma^{\\prime2}\\right)y^{2}}{2\\sigma^{2}\\sigma^{\\prime2}}+\\frac{\\left(\\sigma^{\\prime2}\\mu-\\sigma^{2}\\mu^{\\prime}\\right)y}{\\sigma^{2}\\sigma^{\\prime2}}+\\frac{\\sigma^{2}\\mu^{\\prime2}-\\sigma^{\\prime2}\\mu^{2}}{2\\sigma^{2}\\sigma^{\\prime2}}\\right\\}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With our assumption assume $\\begin{array}{r}{\\operatorname*{lim}_{y\\to\\infty}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)}=b_{i j}}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{y\\to-\\infty}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)}=b_{i j}^{\\prime}.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "When $b_{i j}\\in(0,\\infty)$ ,if $\\sigma>\\sigma^{\\prime}$ , when $y\\rightarrow\\infty$ , (14) converges to $\\infty$ .. If $\\sigma<\\sigma^{\\prime}$ , when $y\\rightarrow\\infty$ (14) converges to 0. ", "page_idx": 23}, {"type": "text", "text": "When $b_{i j}=0$ and $b_{i j}^{\\prime}\\neq0$ ,if $\\sigma<\\sigma^{\\prime}$ , when $y\\rightarrow\\infty$ , (14) converges to 0 is still rue. If $\\sigma>\\sigma^{\\prime}$ , when $y\\rightarrow-\\infty$ ,(15) converges to $\\infty$ . as $b_{i j}^{\\prime}\\neq0$ ", "page_idx": 23}, {"type": "text", "text": "When $b_{i j}=\\infty$ and $b_{i j}^{\\prime}\\neq\\infty$ , if $\\sigma>\\sigma^{\\prime}$ , when $y\\rightarrow\\infty$ , (14) converges to $\\infty$ . till holds. If $\\sigma<\\sigma^{\\prime}$ \uff0c when $y\\rightarrow-\\infty$ , (15) converges to $0$ , as $b_{i j}^{\\prime}\\neq\\infty$ \uff1a \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Theorem B.2. Assume at most one of $\\begin{array}{r}{\\operatorname*{lim}_{y\\to-\\infty}s(y)\\,=\\,0}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{y\\to\\infty}s(y)\\,=\\,0}\\end{array}$ is true, $\\sigma^{2}$ is identifiable. ", "page_idx": 23}, {"type": "text", "text": "Proof. Proof by contradiction. Suppose that there are two sets of parameters satisfying the same observed distribution: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\sigma}\\phi\\left(\\frac{y-\\mu_{i j}}{\\sigma}\\right)c_{i j}s(y)=\\frac{1}{\\sigma^{\\prime}}\\phi\\left(\\frac{y-\\mu_{i j}^{\\prime}}{\\sigma^{\\prime}}\\right)c_{i j}^{\\prime}s^{\\prime}(y).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\phi\\left(\\frac{y-\\mu_{i j}}{\\sigma}\\right)}{\\phi\\left(\\frac{y-\\mu_{i j}^{\\prime}}{\\sigma^{\\prime}}\\right)}\\frac{c_{i j}s(y)}{c_{i j}^{\\prime}s^{\\prime}(y)}=\\frac{\\sigma}{\\sigma^{\\prime}}\\in(0,\\infty).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "However, if $\\sigma\\ne\\sigma^{\\prime}$ , by Lemma B.1, we know that the left-side will converge to 0 or $\\infty$ ,which violates the above equation. Thus, we must have $\\sigma=\\sigma^{\\prime}$ $\\sigma^{2}$ is identifiable. ", "page_idx": 23}, {"type": "text", "text": "C  Algorithm and experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Note that the objective function and the constraint set are both convex, and the constraint set is a closed convex set. So (6) is a convex optimization problem. To deal with the constraint on $\\pmb{A}$ one can use the Alternating Direction Method of Multipliers (ADMM) to tackle it. However, the comuptation of ADMM can be slow in practice. We propose a practically more efficient algorithm based on the idea of proximal gradient descent with an additional projection as detailed in Algorithm 1. The code is publically available on $\\mathrm{GitHub}^{2}$ : In the algorithm, POCS is the projection onto the intersection of two convex sets $\\{\\mathbf{A}\\,:\\,\\langle\\mathbf{J},\\mathbf{A}\\rangle\\,=\\,0\\}$ and $\\{\\mathbf{A}:\\|\\mathbf{A}\\|_{\\infty}\\leq\\,a\\}$ (see Algorithm 2). The notation $\\mathcal{S}_{\\lambda}(\\cdot)$ is the soft-thresholding operator defined by $\\begin{array}{r}{S_{\\lambda}(\\mathbf{A})\\,=\\,\\mathbf{U}\\mathbf{D}_{\\lambda}\\mathbf{V}^{\\top}}\\end{array}$ ,_ where ${\\bf D}_{\\lambda}=\\mathrm{diag}[(d_{1}-\\lambda)_{+},\\dots,(d_{r}-\\lambda)_{+}]$ with $t_{+}=\\operatorname*{max}(t,0)$ , and $\\mathbf{U}\\,\\mathrm{diag}[d_{1},\\ldots,d_{r}]\\,\\mathbf{V}^{\\top}$ is the singular value decomposition of A. ", "page_idx": 23}, {"type": "table", "img_path": "ZGN8dOhpi6/tmp/a6062f29fd6d0e868bee013c5823e26fe7567f0e751d397c6eb13f0ec715613e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ZGN8dOhpi6/tmp/6c9b2f9ea0c2a3203143c95d32e02a031260951a23040b1b991c2b947823cfc0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Due to space constraints, we present the test mean absolute errors (TMAE) curve (Figure 4) for simulation setting in Section 6. Note that the trend is consistent on both TRMSE and TMAE with multiple runs. ", "page_idx": 24}, {"type": "text", "text": "C.1  More simulation: missing on small entries ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We conduct more simulation studies in higher dimensions with a different missing mechanism where the observation probability reads ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(T_{i j}=1|Y_{i j})=\\frac{1}{1+\\exp(-3(Y_{i j}-2))},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which means larger potential observations imply higher observation probabilities, as shown in Figure 5. We generate a $100\\times100$ matrix $\\mathbf{A}_{\\star}$ with rank $r=5$ . The observations $Y_{i j}$ are generated from a Gaussian distribution with mean $A_{\\star,i j}$ and variance 1 independently. Note that compared with experiments shown in Section 6, the dimension is larger and the observation probability is flipped. ", "page_idx": 24}, {"type": "text", "text": "As shown in Figure 6, other methods suffer from a severe observation bias, and the recovered entries are right-skewed, whereas the distribution of true entries is symmetric. Our method alleviates the observation bias and recovers the symmetric pattern of the distribution on recovered entries. The test root mean squared errors (TRMSE) and test mean absolute errors (TMAE) of recovered entries are reported in Table 3. The experiments are repeated with 9 runs, and the standard deviation of both metrics is included. Our method shows a significant advantage when the observation bias persists. ", "page_idx": 24}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/35d00f96404470ac07686c73250341589101183e252d2ddb5f59c0fc16a4d4ea.jpg", "img_caption": ["Figure 4: TMAE with standard error for different variances $\\sigma^{2}=0.0,0.2,0.4,0.6,0.8,1.0$ "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/452f5a09c1ee364d119fc33c8887a13bf9951e80da377eb38bab4315748522ba.jpg", "img_caption": ["Figure 5: Distribution of observed and unobserved entries, implying the existence of observation bias. "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ZGN8dOhpi6/tmp/f82a40540d448d375ffe8468bedadbb71f4f9bb5f1c541b7c1458df2035acf54.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/9c002b281f624dde253a9a190a71a1af102f160263e1c6b24586ee77e6d1a87d.jpg", "img_caption": ["Table 3: Test root mean squared errors (TRMSE) and test mean absolute errors (TMAE) with standard deviations. ", "Figure 6: The recovered entries are right skewed from other methods. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.2   Real data application: more details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we use three data examples to illustrate the robust performance of the proposed method. Similar to the implementation procedures in Section 6, we equally separate half of the testing data points and use them as the validation dataset to tune the hyper parameters for all the methods mentioned in Section 6 and learn the shift and scale parameters for the proposed method. Then we used the remaining half of the test data points to evaluate their performance. ", "page_idx": 25}, {"type": "text", "text": "C.2.1  Tobacco Dataset ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This dataset is available in Table 11 in [9] and has been widely studied for synthetic control methods [e.g. 1]. From Table 11 in [9], we can obtain the Tax-Paid Per Capita Sales in Number of Packs for 51 states across the US from year 1950 to year 2014. California implemented a large-scale tobacco control program in 1988 and we consider it as the \u201c\"treated\" state. We take the remaining 50 states as the control states. For the detailed background, we refer readers to [1] and [2]. ", "page_idx": 26}, {"type": "text", "text": "We collect data from year 1970 to year 2000 and restrict our focus to the 50 control states, which results in a 50 by 31 matrix. Following the same experimental setup in [2], we generate MNAR data. We introduce \u201cinterventions\" to a subsets of states in 1989 based on their change in the mean of cigarette sales during 1989-2000 versus that during 1970-1988. See details of the intervention probabilities in Section 6.3 in [2]. As long as an intervention is adopted for state $i$ , all sales under control after 1988 are unobserved, i.e. $T_{i,j}=0$ for $j>19$ .And we take $Y_{i,j}$ \uff0c $j>19$ as the test data points. ", "page_idx": 26}, {"type": "text", "text": "Table 2 provides the results for five methods under 100 randomizations on the intervention based on the intervention probability for every state. As we can see, our method only performs worse than SNN for this MNAR dataset, with significantly smaller TRMSE than other three methods. Note that in order to compare the errors, we need to perform a transformation on our estimated matrix. For this study, the untransformed data can also provide valuable information about the trend of sales change for every state during this 30 years. For example, as illustrated in Figure 7, using untransformed estimated result from our method, we are able to capture the overall trend of the sales change for state KS across 30 years. Our method is able to capture the increasing trend of sales for state KS after year 1988. ", "page_idx": 26}, {"type": "image", "img_path": "ZGN8dOhpi6/tmp/7777b0cd63ec18e3992b259eb6d3cc1de650d9d6efd495e780394f2bf85a93b1.jpg", "img_caption": ["Figure 7: The first plot shows the observed sales for State KS across 30 years. The second plot is the untransformed estimated sales by our method from 1970 to 2000. The rest four plots are the estimated sales from SNN, SoftImpute, CZ and MFW from 1970 to 2000 for state KS. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "C.2.2 Coat Shopping Dataset ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This dataset is available at https : //www . cs . cornell .edu/\\~schnabts/mnar/. It contains ratings from 290 Turkers on an inventory of 300 items [30]. Training set contains 6960 self-selected ratings and the test set consists of 4640 entries. This dataset has been used as an illustration for the nonuniform missingness [e.g. 30, 35]. ", "page_idx": 26}, {"type": "text", "text": "As Table 2 shows, SNN performs a lot worse than the remaining methods for this dataset. MFW has the smallest TRMSE. Our method has smaller errors than SoftImpute and has comparable performance toCz. ", "page_idx": 26}, {"type": "text", "text": "C.2.3 Yahoo! Webscope Dataset ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This dataset is available at https : //webscope .sandbox. yahoo. com/catalog php?datatype $=$ $\\scriptstyle{\\mathtt{r}}\\&\\mathtt{d i d}=3$ , which contains ratings from 15,400 users on 1000 songs. The IDs for users and songs are randomly assigned. The training set includes approximately 300,000 ratings from these 15,400 users. The ratings from the training set are collected during normal use of Yahoo! Music services for each user. The test set was constructed by surveying the first 5,400 users. And each surveyed-user provides ratings for exactly 10 additional songs. ", "page_idx": 26}, {"type": "text", "text": "Due to its large size and to simplify the computation, we conducted a selection procedure to reduce the size of the matrix. First, we focus on the users with ID 1-50, 5401-5550 and songs with ID 1-250, which results in a matrix where we have 50 surveyed users and 150 unserveyed users. Next, we construct the training (test) matrix from the original training (test) dataset with selected user IDs and selected song IDs. Then, we remove those users and songs who have no single observation in the training matrix. In the end, we obtain a matrix with 199 users and 219 songs. ", "page_idx": 27}, {"type": "text", "text": "From Table 2, we can see that the two methods (SNN and our method) that are designed for MNAR have better performance than the remaining methods, and our method has the smallest TRMSE. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main contributions are adequately described in the abstract and introduction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The limitation of this work such as identifiability issue is discussed in Section 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The theoretical assumptions and proofs are provided in Section 5 and Appendix A as a main contribution of this work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide a complete description of experiments in Section 6&7 and Appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The data used in the paper has been cited and linked properly (Section 7). The implementation detail has been described in detail in Appendix C and the code will be made available on GitHub after acceptance. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All the training and test details can be found in Section 6&7 and Appendix C. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The error bars and statistical significance has been shown and discussed in Section 6 and Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The experiments performed in the paper don't require intense computer resources, and can be reproduced on a personal computer. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work abides by the NeurIPs Code of Ethics in every respect. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The assets (data) used in the paper are open for non-commercial use by academics and have been properly credited in Section 7. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}]