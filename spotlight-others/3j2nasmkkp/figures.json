[{"figure_path": "3j2nasmKkP/figures/figures_3_1.jpg", "caption": "Figure 1: Definition of Node-to-Cluster Attention (N2C-Attn). N2C-Attn perceives the graph as interconnected node sets instead of coarsening each cluster into a single node. It integrates multiple kernel learning methods into the kernelized attention framework to facilitate message propagation among node clusters, simultaneously capturing both the node-level and cluster-level information.", "description": "This figure illustrates the core idea of the Node-to-Cluster Attention (N2C-Attn) mechanism.  Instead of the traditional approach of compressing each cluster into a single embedding, N2C-Attn treats the graph as a network of interconnected node sets.  It uses a bi-level attention mechanism that considers both node-level and cluster-level information to effectively transfer information between clusters.  The figure visually depicts this process, showing how N2C-Attn integrates multiple kernel learning to capture hierarchical graph structure and preserves node-level details.", "section": "3 Node-to-Cluster Attention"}, {"figure_path": "3j2nasmKkP/figures/figures_4_1.jpg", "caption": "Figure 2: An efficient implementation of N2C-Attn-T with the message-passing framework. |NP| denotes the number of clusters and |EP| denotes the number of edges between clusters. The computation can be decomposed into 4 steps: 1) aggregation of node-level keys and values within each cluster, 2) computation of gate on each edge with the cluster-level kernel, 3) message propagation among clusters, 4) dot product of aggregated value with the node-level query of each cluster.", "description": "This figure illustrates an efficient implementation of the Node-to-Cluster Attention with Tensor Product of Kernels (N2C-Attn-T) using a message-passing framework. It breaks down the computation into four steps, visualizing the process of aggregating node-level information, calculating cluster-wise similarity, propagating messages among clusters, and finally, combining aggregated information with node-level queries.", "section": "3.2 Efficient Implementation of Node-to-Cluster Attention"}, {"figure_path": "3j2nasmKkP/figures/figures_6_1.jpg", "caption": "Figure 3: Architecture of Cluster-wise Graph Transformer (Cluster-GT), which can be decomposed into three main modules: 1) a node-wise convolution module with GNN, 2) a graph partition module with Metis, and 3) a cluster-wise interaction module with N2C-Attn.", "description": "This figure illustrates the architecture of the Cluster-wise Graph Transformer (Cluster-GT) model. It shows a workflow diagram with three main modules: 1) a pre-processing module that uses Metis for graph partitioning and positional encoding; 2) a node-wise convolution module using a Graph Neural Network (GNN); 3) a cluster-wise interaction module using the Node-to-Cluster Attention (N2C-Attn) mechanism. The output of the model is a graph-level embedding.", "section": "4 Cluster-wise Graph Transformer"}, {"figure_path": "3j2nasmKkP/figures/figures_7_1.jpg", "caption": "Figure 4: Visualization of \u03b1 (weight of the cluster-level kernel) during the training process. N2C-Attn learns to pay more attention to cluster-level information in social networks than in bioinformatics.", "description": "This figure visualizes how the weight assigned to the cluster-level kernel (\u03b1) changes during the training process of the N2C-Attn model.  It shows that the model dynamically adjusts the balance between node-level and cluster-level information.  For social networks, \u03b1 tends to be higher (more attention to cluster-level information), while for bioinformatics datasets, \u03b1 is lower (more balanced attention). This indicates N2C-Attn adapts its attention strategy based on the dataset's characteristics.", "section": "5.3 Visualization of \u03b1 in N2C-Attn-L"}, {"figure_path": "3j2nasmKkP/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of different attention strategies. We restrict the attention module in Cluster-GT to focus on different granularities. N2C-Attn-T and N2C-Attn-L represent schemes that integrate information at both the node and cluster granularities. Cluster-Level-Attn focuses solely on cluster-level information, i.e., \u03b1 = 1, while Node-Level-Attn focuses solely on node-level information, i.e., \u03b1 = 0. We provide a detailed description of the methods compared here in subsection F.3.", "description": "This figure compares the performance of four different attention mechanisms on four different datasets.  The x-axis represents the datasets (IMDB-BINARY, IMDB-MULTI, PROTEINS, D&D), and the y-axis represents the accuracy.  Each bar represents the accuracy achieved by a different attention mechanism: GCN (baseline), N2C-Attn-T, N2C-Attn-L, Cluster-Level-Attn, and Node-Level-Attn.  The variations in accuracy across the different methods highlight the impact of integrating node and cluster-level information in the attention mechanism.", "section": "5.4 Necessity of Combining Cluster-level and Node-level Information"}]