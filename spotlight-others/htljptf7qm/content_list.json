[{"type": "text", "text": "Noisy Label Learning with Instance-Dependent Outliers: Identifiability via Crowd Wisdom ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tri Nguyen\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shahana Ibrahim ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of EECS Oregon State University Corvallis, Oregon, USA nguyetr9@oregonstate.edu ", "page_idx": 0}, {"type": "text", "text": "Department of ECE   \nUniversity of Central Florida Orlando, Florida, USA   \nshahana.ibrahim@ucf.edu   \nXiao Fu   \nSchool of EECS   \nOregon State University   \nCorvallis, Oregon, USA   \nxiao.fu@oregonstate.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The generation of label noise is often modeled as a process involving a probability transition matrix (also interpreted as the annotator confusion matrix) imposed onto the label distribution. Under this model, learning the \u201cground-truth classifier\u201d\u2014 i.e., the classifier that can be learned if no noise was present\u2014and the confusion matrix boils down to a model identification problem. Prior works along this line demonstrated appealing empirical performance, yet identifiability of the model was mostly established by assuming an instance-invariant confusion matrix. Having an (occasionally) instance-dependent confusion matrix across data samples is apparently more realistic, but inevitably introduces outliers to the model. Our interest lies in confusion matrix-based noisy label learning with such outliers taken into consideration. We begin with pointing out that under the model of interest, using labels produced by only one annotator is fundamentally insufficient to detect the outliers or identify the ground-truth classifier. Then, we prove that by employing a crowdsourcing strategy involving multiple annotators, a carefully designed loss function can establish the desired model identifiability under reasonable conditions. Our development builds upon a link between the noisy label model and a columncorrupted matrix factorization mode\u2014based on which we show that crowdsourced annotations distinguish nominal data and instance-dependent outliers using a lowdimensional subspace. Experiments show that our learning scheme substantially improves outlier detection and the classifier\u2019s testing accuracy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks can easily overfti to noisy labels due to its excessive expressiveness [1,2]. Many strategies have been proposed to avoid negative impacts of label noise when training neural classifiers; see, e.g., noisy label flitering [3\u20138], robust loss design [9\u201313], and noise generation modeling (or loss correction) [14\u201322]. In the last genre, a noisy class label is modeled as a realization of a categorical distribution, which is the ground-truth label distribution being distorted by a probability transition matrix. The transition matrix is interpreted as the \u201cconfusion matrix\u201d [9,14,18] that can effectively model the annotators\u2019 expertise level and the difficulty of annotating each class/sample, and thus is considered intuitive. Under this model, learning the \u201clabel noise-free\u201d target neural classifier boils down to identifying the confusion matrix. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The confusion matrix-based models have proven quite useful in practice\u2014algorithms developed in this line of work often exhibits appealing empirical performance; see, e.g., [9,14\u201317,19\u201321,23\u201331]. In addition, these models admit interesting statistical and algebraic structures, leading to plausible results on identifiability of the confusion matrix and/or the \u201cground-truth classifier\u201d\u2014i.e., the classifier that can be learned if no noisy annotations were present. However, most of the aforementioned early works considered an instance-invariant confusion matrix\u2014i.e., a confusion matrix is not affected by sample features, but only classes\u2014for analytical and computational simplicity. Considering instancedependent confusion models is more realistic, as the sample characteristics, e.g., lightening and resolution of an image, affect the annotation accuracy [32]. The existence of such (at least occasionally occurred) instance-dependent noisy labels inevitably introduces outliers to the instance-invariant confusion models, leading to performance degradation. In general, learning under instance-dependent confusion matrices is heavily ill-posed. Hence, various problem-specific structures were exploited to add regularization terms and constraints; see, e.g., [27,28,33\u201343]. Nonetheless, unlike the instanceinvariant confusion matrix case, identifiability guarantees of the target classifier have been largely under-studied. The lack of theoretical understanding also affects algorithm design\u2014many approaches in this domain had to resort to somewhat ad-hoc treatments with multi-stage training procedures, often involving nontrivial pre- and post-processing; see [27,28,33\u201335,37,39,40]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. To advance understanding, we consider a model where instance-dependent confusion matrices occur occasionally across the samples, and the rest of data share a common nominal confusion matrix. This way, the instance-dependent noisy labels can be regarded as outliers. The model is motivated by the fact that only a proportion of all instances may have a labeling difficulty that significantly deviates from the general population [36,44,45]. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "(i) Identifiability via Crowdsourcing. We first show that using the sparsity prior on outliers is insufficient to identify the model of interest, if only one annotator is present. To circumvent this challenge, we propose to employ a crowd of annotators\u2014which is the common practice in data labeling [14, 18, 20, 31]. We show that, by incorporating a carefully designed column sparsity constraint to a coupled cross-entropy (CCE) loss from crowdsourcing [20, 31] to integrate the annotators\u2019 outputs, the outliers can be provably identified. Consequently, the ground-truth classifier can be learned with generalization guarantees. ", "page_idx": 1}, {"type": "text", "text": "(ii) End-to-end One-Stage Implementation. Our approach features a one-stage continuous optimization-based implementation. To be specific, our proposed learning loss allows to approximate the column-sparsity constraint using a smoothed nonconvex $\\ell_{p}$ (where $0<p\\leq1]$ ) function based regularization (see [46]). This way, the overall loss is differentiable and can be readily tackled by off-the-shelf optimizers. This is in contrast to many existing methods that involve multiple stages (e.g., [27,28,33\u201335,37,39,40])\u2014and is arguably easier to implement. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the proposed method over a number real datasets that are annotated by machine and human annotators under various conditions and observed nontrivial improvements of testing accuracy. ", "page_idx": 1}, {"type": "text", "text": "Notation. The notations are summarized in the supplementary material in Sec. A. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The Confusion Model and Learning Goal. Consider $N$ data items $\\{{\\pmb x}_{n}\\}_{n=1}^{N}$ from $K$ classes. Here, $\\pmb{x}_{n}\\in\\mathbb{R}^{D}$ represents the feature vector of the nth data item. Let $\\{y_{n}\\}_{n=1}^{N}$ be the set of ground-truth labels, where $y_{n}\\in[K]$ . Assume that $y_{n}$ \u2019s are unobserved. Instead, we observe the \u201cnoisy\u201d version $\\{\\widehat{y}_{n}\\}_{n=1}^{N}$ . The label $\\bar{\\widehat{y}_{n}}\\in[K]$ is noisy due to various reasons, e.g., the lack of expertise of the annotator. In this setting, our goal is to learn a performance-guaranteed classifier using the data items $\\{{\\pmb x}_{n}\\}_{n=1}^{N}$ , and noisy labels $\\{\\widetilde{\\widehat{y}}_{n}\\}_{n=1}^{N}$ . We consider the following expression of $\\mathsf{P r}(\\widehat{y}_{n}^{-}=k|\\pmb{x}_{n})$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathsf{P r}(\\widehat{y}_{n}=k|x_{n})=\\sum_{k^{\\prime}=1}^{K}\\mathsf{P r}(\\widehat{y}_{n}=k|y_{n}=k^{\\prime},x_{n})\\mathsf{P r}(y_{n}=k^{\\prime}|x_{n}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that $\\mathsf{P r}(y_{n}=k|\\pmb{x}_{n})$ is the ground-truth label distribution given the sample ${\\pmb x}_{n}$ . This is also the distribution that the target classifier wishes to learn from. We represent the ground-truth classifier using a function $\\pmb{f}^{\\natural}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{K}$ such that $[{\\pmb f}^{\\natural}({\\pmb x}_{n})]_{k}\\triangleq\\mathsf{P r}(y_{n}=k|{\\pmb x}_{n})$ . We call $\\pmb{f}^{\\sharp}$ the ground-truth classifier as it is the function that we aim to learn\u2014and it can be learned under ideal conditions (e.g., when $N=\\infty$ and the learner is a universal approximator), given that no noise is present. We define $T^{\\natural}:\\mathbb{R}^{D}\\to\\mathbb{R}^{K\\times K}$ such that $[\\pmb{T}^{\\sharp}(\\pmb{x}_{n})]_{k,k^{\\prime}}\\triangleq\\mathsf{P r}(\\widehat{y}_{n}=k|y_{n}=k^{\\prime},\\pmb{x}_{n}),\\forall\\pmb{x}_{n}$ . That is, $\\pmb{T}^{\\sharp}(\\pmb{x}_{n})$ is the label transition matrix or confusion matrix of sample ${\\boldsymbol{x}}_{n}$ . Let $[\\pmb{g}_{n}^{\\natural}]_{k}\\triangleq\\mathsf{P r}(\\widehat{y}_{n}=k|\\pmb{x}_{n})$ . Accordingly, the noisy label generation process for $x_{n}\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\mathcal{D}_{\\pmb{x}}$ is modeled as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\pmb g}_{n}^{\\sharp}={\\pmb T}^{\\sharp}({\\pmb x}_{n}){\\pmb f}^{\\sharp}({\\pmb x}_{n}),}}\\\\ {{\\widehat{y}_{n}\\sim\\mathsf{c a t e g o r i c a l}({\\pmb g}_{n}^{\\sharp}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where categorica $|(g)$ denotes the $K$ -dimensional categorical distribution. Per the physical meaning of ${\\pmb T}^{\\sharp}({\\pmb x}_{n})$ and ${\\pmb f}^{\\natural}({\\pmb x}_{n})$ , we have $\\mathbf{1}^{\\top}\\mathbf{T}^{\\natural}(\\mathbf{x}_{n})=\\mathbf{1}^{\\top},T^{\\natural}(\\mathbf{x}_{n})\\geq\\mathbf{0}$ and $\\mathbf{1}^{\\top}f^{\\natural}(x_{n})={\\dot{1}}$ , $\\pmb{f}^{\\natural}(\\pmb{x}_{n})\\geq\\mathbf{0}$ , for all $n$ . Under this model, the main goal is to learn $\\pmb{f}^{\\sharp}$ from $\\{{\\pmb x}_{n}\\}_{n=1}^{N}$ and $\\{\\widehat{y}_{n}\\}_{n=1}^{N}$ . ", "page_idx": 2}, {"type": "text", "text": "Identifiability under Instance-Invariant ${\\pmb T}^{\\sharp}({\\pmb x})$ . From (2a), it becomes apparent that learning $f^{\\natural}$ is not a straightforward task. Even if $\\pmb{g}_{n}^{\\natural}$ is observed (which is not), it is hard to identify ${\\pmb f}^{\\sharp}({\\pmb x}_{n})$ or $\\pmb{T}^{\\sharp}(\\pmb{x}_{n})$ from the product $g_{n}^{\\natural}=T^{\\natural}(x_{n})f^{\\natural}(x_{n})$ . To tackle the identifiability challenge, a popular approach is to simplify (2) by assuming that all instances have the same confusion matrix, i.e., $\\pmb{\\hat{T}}^{\\sharp}(\\pmb{x}_{n})=A^{\\sharp}$ with a certain $\\mathbf{\\dot{A}}^{\\sharp}\\in\\mathbb{R}^{K\\times\\mathbf{\\check{K}}}$ for all $n$ [16,18,19,21,47,48]. Under this assumption, many used the \u201closs correction\u201d based formulation, e.g., [9,16,19,21,48]. The loss correction idea modifies the training loss of the classifier by taking the confusion matrix into consideration, e.g., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\pmb{A}\\in\\mathcal{A},\\pmb{f}\\in\\mathcal{F}}\\ -\\ \\frac1N\\sum_{n=1}^{N}\\sum_{k=1}^{K}[\\widehat{\\pmb{y}}_{n}]_{k}\\log[\\pmb{A}\\pmb{f}(\\pmb{x}_{n})]_{k},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the objective is a modified cross-entropy (CE) loss. The \u201cnoise correction\u201d term $\\pmb{A}$ and $\\boldsymbol{\\textbf{\\textit{f}}}$ are used to learn $A^{\\natural}$ and $\\pmb{f}^{\\natural}$ , respectively, $\\widehat{\\pmb{y}}_{n}\\in\\bar{\\{0,1\\}}^{K}$ denotes the one-hot encoding of the noisy label ${\\widehat{y}}_{n}$ , and $\\boldsymbol{\\mathcal{A}}$ and $\\mathcal{F}$ denote appropriate c o nstraint sets\u2014i.e., $\\begin{array}{r}{\\mathcal{A}=\\{A\\in\\mathbb{R}^{K\\times K}\\mid\\mathbf{1}^{\\top}\\!\\!A=\\mathbf{1}^{\\top},}\\end{array}$ ${\\bf{\\cal A}}\\geq{\\bf0}\\}$ and $\\mathcal{F}$ is a certain deep neural network function class whose outputs reside in the probability simplex. Note that the objective is often used together with regularization and additional constraints of $\\pmb{A}$ for various purposes; see, e.g., [19, 21, 30]. When $N\\,\\rightarrow\\,\\infty$ , the CE term enforces the learned $\\pmb{A}$ and $\\pmb{f}$ to satisfy $g_{n}^{\\sharp}\\,=\\,A f(x_{n})$ for all $n$ [19]. Note that $\\widetilde{\\boldsymbol{G}}=[\\pmb{g}_{1}^{\\natural},\\dots,\\pmb{g}_{N}^{\\natural}]$ can be expressed as $\\widetilde G=A^{\\natural}\\left[f^{\\natural}(x_{1}),\\dots,f^{\\natural}(x_{N})\\right]=A^{\\natural}F^{\\natural}$ , where $F^{\\natural}\\,=\\,[f^{\\natural}(x_{1}),\\dots,f^{\\natural}(x_{N})]$ . Consequently, Eq. (3) can be understood as learning $\\pmb{A}$ and $\\pmb{F}$ such that $\\widetilde{G}\\,\\approx\\,A F$ . As both $A^{\\natural}$ and $F^{\\natural}$ are nonnegative matrices (due to their physical meaning), the ide ntifiability of $F^{\\natural}$ can be connected to uniqueness of the nonnegative matrix factorization (NMF) model $\\widetilde{G}=\\dot{A}^{\\sharp}F^{\\sharp}$ ; see [19,21,45]. ", "page_idx": 2}, {"type": "text", "text": "These are interesting developments, yet the key assumption $\\pmb{T}^{\\natural}(\\pmb{x}_{n})=\\pmb{A}^{\\natural}$ for all $n$ appears to be overly stringent. As mentioned, it makes sense to believe that at least a proportion of samples have instance-dependent $\\pmb{T}^{\\natural}(\\pmb{x}_{n})$ \u2019s [36,44,45,49]. Not considering such samples may cause performance degradation. ", "page_idx": 2}, {"type": "text", "text": "Instance-Dependent Confusion-Induced Outliers. When the instance-dependent confusion happens sparingly instead of overwhelmingly, we can re-express $\\pmb{T}^{\\natural}(\\pmb{x}_{n})$ using the following decomposition: ", "page_idx": 2}, {"type": "equation", "text": "$$\nT^{\\natural}(x_{n})=A^{\\natural}+E^{\\natural}(x_{n}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{A}^{\\natural}\\in\\mathbb{R}^{K\\times K}$ represents an instance-independent (class-dependent) confusion matrix\u2014which is the nominal confusion matrix that reflects the general annotation difficulty of the dataset. The term ${\\pmb E}^{\\sharp}({\\pmb x}_{n})$ represents the instance-dependent \u201cperturbation\u201d. For many $n$ \u2019s, $\\pmb{{\\cal E}}^{\\natural}(\\pmb{{x}}_{n})=\\mathbf{0}$ . When $E^{\\natural}(x_{n})\\neq0$ , we have $(A^{5}+E^{5}(\\pmb{x}_{n}))\\bar{\\pmb{f}}^{\\natural}(\\pmb{x}_{n})\\ \\stackrel{=}{=}A^{5}\\pmb{f}^{\\natural}(\\pmb{x}_{n})+\\pmb{e}_{n}^{\\natural}$ , where $e_{n}^{\\natural}\\,=\\,E^{\\natural}(x_{n})f^{\\natural}(x_{n})$ . Using (4), the model in (2a) can be expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{g}_{n}^{\\sharp}=\\left\\{\\pmb{A}^{\\sharp}\\pmb{f}^{\\sharp}(\\pmb{x}_{n})+\\pmb{e}_{n}^{\\sharp},\\right.\\quad\\forall n\\in\\mathbb{Z}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{T}$ and $\\mathcal{T}^{\\mathrm{c}}$ represent the instance index set where $\\pmb{{\\cal E}}^{\\natural}(\\pmb{{x}}_{n})\\neq\\mathbf{0}$ and its complement, respectively.   \nIn other words, $\\mathcal{T}$ is the outlier index set. ", "page_idx": 2}, {"type": "text", "text": "A remark is that from this point on we will ignore the structure $e_{n}^{\\natural}=E^{\\natural}(x_{n})f^{\\natural}(x_{n})$ of the outliers. Disregarding the structure comes with some losses. For example, this way, our method is not able to learn ${\\pmb E}^{\\sharp}({\\pmb x}_{n})$ that could be of interest. Nonetheless, considering ${\\pmb E}^{\\sharp}({\\pmb x}_{n})$ renders extra modeling and computational burdens. Our treatment simplifies the subsequent analytical and computational developments, particularly, algorithm design. In addition, other types of outliers and anomalies can also be handled by the proposed approach due to the unstructured treatment. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Challenge of Outlier Detection. Under the model in (5), the natural idea is to first identify $\\mathcal{T}$ and remove the impacts of outliers. Our first attempt is to explicitly model $e_{n}^{\\natural}$ and modify (3) as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{minimize}_{A\\in\\mathcal{A},\\,\\{e_{n}\\in\\mathcal{E}\\},\\,f\\in\\mathcal{F}}\\,-\\,\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{k=1}^{K}[\\widehat{y}_{n}]_{k}\\log\\left[A f(x_{n})+e_{n}\\right]_{k},}\\\\ {\\mathrm{subject\\,to~}\\displaystyle\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\|e_{n}\\|_{2}>0\\right\\}\\leq C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{E}\\,=\\,\\{e\\,\\in\\,\\mathbb{R}^{K}\\,\\mid\\,{\\bf1}^{\\top}e\\,=\\,0\\}$ is the constraint set for $e_{n}$ \u2019s (due to the probability simplex constraints on the model parameters in (5)) and $C$ is a scalar that is an estimation of $|{\\mathcal{T}}|$ . The idea is to use the prior knowledge that $e_{n}$ does not occur overwhelmingly as constraint. The hope is that the solution of (6) can detect $\\mathcal{T}$ , thereby enabling accurate estimation of $A^{\\natural}$ and $f^{\\natural}$ . However, the following fact reveals a conflicting insight: ", "page_idx": 3}, {"type": "text", "text": "Fact 2.1. Assume that all $f\\in\\mathcal F$ are universal function approximators, that $\\mathsf{r a n k}(A^{\\natural})=K$ , and that $N\\rightarrow\\infty$ . Suppose that $\\mathcal{T}\\neq\\emptyset$ . Optimal solutions of Problem (6) can attain the minimal value of the objective function and satisfy the sparsity constraint without detecting any outliers. One such trivial solution $(A^{\\star},f^{\\star},e_{n}^{\\star})$ is $\\mathbf{\\ddot{A}}^{\\star}={\\dot{I}},f^{\\star}({\\dot{\\mathbf{x}}})=A^{\\natural}f^{\\natural}(\\mathbf{x}_{n})+e_{n}^{\\natural}$ , and $e_{n}^{\\star}=\\mathbf{0}$ for all $n$ . ", "page_idx": 3}, {"type": "text", "text": "This fact is easy to show (see Appendix F), yet it highlights a somewhat unexpected issue in confusion matrix-based noisy label learning: If only one nominal confusion matrix $A^{\\natural}$ is present (i.e., only one annotator is employed), it does not suffice to recover the ground-truth $\\pmb{f}^{\\natural}$ when there exists instancedependent outliers (or any other types of outliers) under the model in (5)\u2014under the condition that $\\pmb{f}$ is a universal function approximator. The practical implication is undesirable: as deep neural networks are powerful function approximators and are usually very expressive, Fact 2.1 means that the learned neural networks easily overfti to outliers, even if the sparsity prior on outliers is explicitly used in the loss function. ", "page_idx": 3}, {"type": "text", "text": "3 Proposed Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Intuition: Exploiting Crowd-induced Subspace. To get around the issues illustrated in Fact 2.1, our idea is to \u201ccreate\u201d a low-dimensional subspace where the nominal data reside while the outliers are likely far away from. To this end, we propose a crowdsourcing approach. Consider the scenario where $M$ annotators label the dataset $\\{x_{n}\\}_{n=1}^{N^{\\mathrm{~\\,~}}}$ . The noisy label provided by an annotator $m$ for the data item ${\\pmb x}_{n}$ is denoted as $\\widehat{y}_{n}^{(m)}\\in[K]$ and follows the generation model as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{y}_{n}^{(m)}\\sim\\mathsf{c a t e g o r i c a l}(g_{n}^{\\natural(m)}),\\ g_{n}^{\\natural(m)}=A_{m}^{\\natural}f^{\\natural}(x_{n})+e_{n}^{\\natural(m)},\\forall m,n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{A}_{m}^{\\natural}$ is the annotator $m$ \u2019s confusion matrix and $e_{n}^{\\natural(m)}$ is the outlier term induced by annotator $m$ \u2019s instance-dependent confusion. ", "page_idx": 3}, {"type": "text", "text": "Putting together, we have the following expression: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G^{\\sharp}=\\left[\\begin{array}{c c c c}{g_{1}^{\\sharp(1)}}&{\\ldots}&{g_{N}^{\\sharp(1)}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {g_{1}^{\\sharp(M)}}&{\\ldots}&{g_{N}^{\\sharp(M)}}\\end{array}\\right]=\\left[\\begin{array}{c c c c}{A_{1}^{\\sharp}f^{\\sharp}(x_{1})+e_{1}^{\\sharp(1)}}&{\\ldots}&{A_{1}^{\\sharp}f^{\\sharp}(x_{N})+e_{N}^{\\sharp(1)}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {A_{M}^{\\sharp}f^{\\sharp}(x_{1})+e_{1}^{\\sharp(M)}}&{\\ldots}&{A_{M}^{\\sharp}f^{\\sharp}(x_{N})+e_{N}^{\\sharp(M)}}\\end{array}\\right]}\\\\ &{\\qquad\\Longleftrightarrow G^{\\sharp}=W^{\\sharp}F^{\\sharp}+E^{\\sharp},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $F^{\\natural}$ is defined as before, and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{W}^{\\sharp}=[(\\pmb{A}_{1}^{\\sharp})^{\\top},\\dots,(\\pmb{A}_{M}^{\\sharp})^{\\top}]^{\\top},\\quad\\pmb{e}_{n}^{\\sharp}=[(\\pmb{e}_{n}^{\\sharp(1)})^{\\top},\\dots,(\\pmb{e}_{n}^{\\sharp(M)})^{\\top}]^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Denote $\\mathcal{T}^{(m)}$ as the index set where e\u266en(m)\u0338= 0. Then I = I(1) \u222a. . . \u222aI(M) stands for the nonzero column support of $E^{\\natural}$ . ", "page_idx": 3}, {"type": "text", "text": "From (7), one can see that now the \u201cdata columns\u201d, i.e., the columns of the matrix $G^{\\natural}$ , live in a highdimensional space, i.e., $\\mathbb{R}^{M K}$ . However, the nominal data part $W^{\\natural}F^{\\natural}$ resides in a $K$ -dimensional subspace range $(\\boldsymbol{W}^{\\natural})$ where $K\\ll M K$ , if multiple annotators are employed. More importantly, $e_{j}^{\\natural}$ for $j\\in\\mathcal{T}$ is an $M K$ -dimensional vector and is unlikely to be inside range $(\\boldsymbol{W^{\\natural}})$ . With this geometry, it is much more likely that one can separate the outliers from the nominal data. ", "page_idx": 4}, {"type": "text", "text": "Next, we will use the above intuition to build a learning loss. We will take into consideration of practical aspects, e.g., missing annotations and finite dataset size $N$ . ", "page_idx": 4}, {"type": "text", "text": "Proposed Identification Criterion $\\&$ Analysis. In this section, we connect our proposed idea to a practically convenient, end-to-end identification criterion and provide identifiability guarantees for the desired model parameters. We consider the following empirical risk minimization under the crowdsourcing model in (6): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{\\{A_{m}\\in\\mathcal{A}\\},\\{e_{n}^{(m)}\\in\\mathcal{E}\\},f\\in\\mathcal{F}}{\\mathrm{minimize~}}\\mathsf{L}_{\\mathrm{ce}}\\triangleq-\\frac{1}{S}\\sum_{(m,n)\\in\\mathcal{S}}\\sum_{k=1}^{K}[\\widehat{y}_{n}^{(m)}]_{k}\\log\\left[A_{m}f(\\boldsymbol{x}_{n})+\\boldsymbol{e}_{n}^{(m)}\\right]_{k},}\\\\ &{\\displaystyle\\qquad\\qquad\\mathrm{subject~to~}\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|\\boldsymbol{e}_{n}^{(m)}\\|_{2}>0\\right\\}\\leq C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\cal S\\subseteq[M]\\times[N]$ denotes the set of observed noisy labels indexed by $(m,n)$ with $S=|S|$ (note that all data items may not be labeled by an annotator), $\\widehat{\\pmb{y}}_{n}^{(m)}$ is the one-hot encoding of the annotatorprovided label $\\widehat{y}_{n}^{(m)}$ , and $C$ is an estimate of $|{\\mathcal{T}}|$ . Here, the constraint sets ${\\mathcal{F}},A$ , and $\\mathcal{E}$ are as defined before. If $e_{n}^{(m)}$ is not considered, the objective function (9a) is sometimes referred to as coupled cross-entropy minimization (CCEM) in the end-to-end crowdsourcing literature [20, 30, 31, 50]. CCEM has not been used together with the outlier detection constraint (9b)\u2014and the theoretical characterizations of the constrained formulation is unknown. ", "page_idx": 4}, {"type": "text", "text": "We will use the following notations in our analysis. We use $\\mathcal{T}\\subseteq[N]$ to denote index set of the outliers, i.e., ${\\mathcal{T}}=\\left\\{n\\mid e_{n}^{\\sharp}\\neq\\mathbf{0}\\right\\}$ . We also consider that the function class in our learning problem, i.e., $\\mathcal{F}$ , has a complexity measure $\\mathcal{R}_{\\mathcal{F}}$ . In particular, we adopt the so-called spectral-complexity upper bound of the function class $\\mathcal{F}$ [51]; see Lemma C.4 in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "We first establish that the criterion (9) recovers the ground-truth ${g_{n}^{\\sharp}}^{(m)}$ \u2019s with a reasonable accuracy. Specifically, we hope to characterize the average estimation accuracy of g(nm)\u2019s whereg(nm $\\widehat{\\pmb{g}}_{n}^{(m)}\\,=$ $\\hat{A}_{m}\\hat{f}(x_{n})+\\hat{e}_{n}^{(m)}$ and $\\widehat{A}_{m}$ \u2019s, e(nm)\u2019s andf  are estimated via solving (9): ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. Assume that the observed index pairs $(m,n)\\in\\mathcal{S}$ are sampled uniformly at random with replacement. Also assume $f^{\\natural}\\in{\\mathcal{F}}$ , $|\\mathcal{Z}|\\le C\\le N/2$ , and that each $[{\\pmb A}_{m}{\\pmb f}({\\pmb x}_{n})+{\\pmb e}_{n}^{(m)}]_{k}$ , $\\forall\\mathbf{A}_{m}\\in$ $\\mathcal{A},\\forall e_{n}^{(m)}\\in\\mathcal{E},\\forall\\pmb{f}\\in\\mathcal{F}$ are lower bounded by $(1/\\beta)$ for a certain $\\beta>0$ . Then, with probability greater than $1-\\delta$ , the following holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{N M}\\sum_{n=1}^{N}\\sum_{m=1}^{M}\\|g_{n}^{\\natural(m)}-\\widehat{g}_{n}^{(m)}\\|_{2}^{2}\\leq\\epsilon_{g}(S),\\quad\\epsilon_{g}(S)=\\mathcal{O}\\left(\\beta\\Re s\\log S/\\sqrt{s}+\\log\\left(\\beta\\right)\\sqrt{\\log(1/\\delta)}/\\sqrt{s}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Re_{S}^{2}\\,=\\,K^{2}\\log{(K)}+\\kappa\\|\\mathbf{{\\boldsymbol{X}}}\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{F}}/s+C M K\\log{\\sqrt{N M}}$ and $X\\,=\\,[{\\pmb x}_{n_{1}},\\ldots,{\\pmb x}_{n_{S}}]$ are the annotated samples, in which $(m_{s},n_{s})\\in\\mathcal{S}$ for $s=1,\\ldots,S$ . ", "page_idx": 4}, {"type": "text", "text": "The proof is in Appendix $\\mathbf{C}$ , which uses a similar idea as [31, Proposition 1] but takes into consideration of the outliers. Lemma 3.1 reveals that the criterion (9) essentially recovers the complete matrix $G^{\\natural}$ [cf. Eq, (7)] from the noisy and incomplete observations of its entries, when $S/c$ is sufficiently large. To proceed, we will need a suite of assumptions and definitions: ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. The outliers satisfy $e_{i}^{\\sharp}\\not\\in\\mathsf{r a n g e}(W^{\\sharp}),\\forall i\\in\\mathcal{Z},\\,\\mathsf{w h e r e}\\,\\mathsf{r a n k}(W^{\\sharp})=K.$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 presents the key condition to disassociate instance-dependent and instanceindependent confusions. As discussed after Eq. (8), having a larger $M$ would decrease the possibility that any $e_{n}^{\\natural}$ belongs to range $(\\boldsymbol{W^{\\natural}})$ . We also characterize the \u201cquantity\u201d that $e_{n}^{\\natural}$ is perturbed from range $(W^{\\natural})$ in the following definition: ", "page_idx": 4}, {"type": "image", "img_path": "HTLJptF7qM/tmp/057786aad8b89b4d98be91224230ca700d348950ba9756fc09c7de6025a986dc.jpg", "img_caption": ["Figure 1: An illustration of outliers and nominal data items w.r.t. range $(W^{\\natural})$ : (left) when $M=1$ , $e^{\\natural}\\in{\\mathsf{r a n g e}}(W^{\\natural})$ , (right) while $M>1$ , it is highly likely that $e^{\\natural}\\neq{\\mathsf{r a n g e}}(W^{\\natural})$ . In addition, the measure $\\kappa(e)$ is larger for outliers that are farther from range $(\\boldsymbol{W}^{\\natural})$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Definition 3.3. The outlier impact score $\\kappa(e_{i}^{\\natural})$ of data $i\\in\\mathcal{Z}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa(e_{i}^{\\natural})\\triangleq\\operatorname*{min}_{\\substack{\\underline{{V}}\\in\\mathcal{V};\\,|\\mathcal{L}|=N-2|\\mathcal{I}|}}\\left(\\|W^{\\natural}F^{\\natural}(\\cdot,\\mathcal{L})-W H\\|_{\\mathrm{F}}^{2}+\\|W^{\\natural}f_{i}^{\\natural}+e_{i}^{\\natural}-W h\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{W}=\\left\\{W=[A_{1}^{\\top},\\ldots,A_{M}^{\\top}]^{\\top}\\,|\\,A_{m}\\in\\mathcal{A}\\right\\}$ . ", "page_idx": 5}, {"type": "text", "text": "A larger score $\\kappa(e_{i}^{\\natural})$ implies it is easier for our criterion to distinguish the outliers from the nominal data. Fig. 1 illustrates the geometry that we rely on to detect outliers. One can notice that when $M=1$ , we always have $\\kappa(e_{i}^{\\sharp})=0$ , as the outlier satisfies $e_{i}^{\\natural}\\in{\\mathsf{r a n g e}}(W^{\\natural})$ . ", "page_idx": 5}, {"type": "text", "text": "In addition, we consider the following assumptions to assist identifying $A^{\\natural}$ and $\\pmb{f}^{\\natural}$ : ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.4 (Class Specialists and Anchor Points). Assume that the following conditions hold: ", "page_idx": 5}, {"type": "text", "text": "a. There exists a near-class specialist for each class $k$ , i.e., $\\forall k\\;\\;\\in\\;\\;[K],\\exists m_{k}\\;\\;\\in$ $[M]$ such that $\\|A_{m_{k}}^{\\natural}(k,:)-\\overline{{e}}_{k}^{\\top}\\|\\leq\\xi_{1}$ , where $\\overline{{e}}_{k}$ is a unit vector.   \nb. There exists a near-anchor point sample for each class $k$ in the dataset, i.e., $\\forall k\\in[K],\\exists n_{k}\\in$ $\\mathcal{Z}^{c}$ such that $\\|\\pmb{f}^{\\natural}(\\pmb{x}_{n_{k}})-\\overline{{e}}_{k}^{\\top}\\|\\leq\\xi_{2}$ , where $\\overline{{e}}_{k}$ is a unit vector. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.4.a. is sometimes used in the crowdsourcing literature (e.g., [52]) to characterize the expertise of annotators. Assumption 3.4.b. is often seen in loss correction based noisy label learning; see, e.g., [16,21,27]. Under Assumptions 3.2 and 3.4, we have the following result: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Identifiability and Generalization). Let $(\\{\\hat{A}_{m}\\},\\,\\{\\hat{e}_{n}^{(m)}\\},\\,\\hat{f})$ be any optimal solution of (9) with ${\\widehat{\\boldsymbol{\\mathcal{Z}}}}=\\left\\{{\\boldsymbol{n}}\\in[N]\\;|\\;{\\widehat{e}}_{n}=[({\\widehat{e}}_{n}^{(1)})^{\\top},\\ldots,({\\widehat{e}}_{n}^{(M)})^{\\top}]^{\\top}\\neq\\mathbf{0}\\right\\}$ . Suppose that the conditions in Lemma 3.1 holds, that we set $C=|{\\mathcal{T}}|$ , and that Assumption 3.4 holds with $\\xi_{1},\\xi_{2}\\le1/K$ . Denote $\\sigma$ as the upper bound $\\sigma_{\\operatorname*{max}}(A_{m}^{\\natural})\\leq\\sigma$ , $\\forall m,$ . In addition, assume that $\\forall\\mathcal{L}\\subset\\mathcal{T}^{c},|\\mathcal{L}|=N-2\\left|\\mathcal{T}^{c}\\right|$ , $\\mathsf{r a n k}(F^{\\natural}(:,\\mathcal{L}))=K$ . Then, for some $\\alpha>0$ , and $S>S_{0}$ where $S_{0}$ as the smallest integer such that $\\kappa(e_{i}^{\\natural})\\geq\\epsilon_{g}(S_{0})$ , $\\forall i\\in{\\mathcal{T}}$ , the following result holds with probability greater than $1-2/S-K/T^{\\alpha}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{{\\pmb x}\\sim\\mathcal{D}_{\\pmb x}}{\\mathbb{E}}\\left[\\!\\operatorname*{min}_{\\mathbf{I}}\\|\\widehat{{\\pmb f}}({\\pmb x})-\\mathbf{I}\\mathbf{I}^{\\top}{\\pmb f}^{\\sharp}({\\pmb x})\\|_{2}^{2}\\right]\\le K(\\eta+\\xi_{1}+\\xi_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{H}}\\|\\widehat{A}_{m}-A_{m}^{\\natural}\\Pi\\|_{\\mathrm{F}}^{2}=K\\sigma^{2}(\\eta+\\xi_{1}+\\xi_{2}),\\,\\forall m,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta^{2}=\\mathcal{O}\\left(\\beta M T^{\\alpha}/\\sqrt{S}\\left(\\sqrt{M\\log S}+(\\|X\\|\\mathcal{R}_{\\mathcal{F}})^{0.25}\\right)\\right)$ , $\\mathbf{\\delta}\\pi$ a permutation matrix, $\\mathbf{\\deltaX}$ is defined as before, and $T=N-|Z|$ . In addition, we have exact outlier detection, i.e., $\\widehat{\\mathcal{Z}}=\\mathcal{Z}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix D. We should mention that we set $C=|{\\mathcal{T}}|$ for notation simplicity. With a notation-wise slightly more complicated definition of $\\mathcal{L}$ , the same proof holds under $C\\geq|{\\mathcal{Z}}|$ , which leads to $T=N-C$ and $\\mathcal{T}\\subseteq\\widehat{\\mathcal{T}}$ . That is, over-estimated $C$ still enables identifying $\\mathcal{T}$ with the price of discarding some nominal samples. While the criterion (9) offers the desired identifiability, it requires the presence of class-specialist annotators and the anchor data points (c.f. Assumption 3.4), which are considered relatively restrictive [19,31]. In [19], the CE loss was combined with a simplex volume minimization-based regularization to establish identifiability of $\\pmb{f}^{\\sharp}$ under more relaxed conditions, namely, the sufficiently scattered condition (SSC) from the NMF literature [53]. We show that this is also viable in the presence of outliers: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6 (Enhanced Identifiability). Let $(\\{\\hat{A}_{m}\\},\\,\\{\\hat{e}_{n}^{(m)}\\},\\,\\hat{f})$ be any optimal solution of (9) with $\\ensuremath{\\widehat{W}}\\triangleq[\\ensuremath{\\widehat{A}}_{1}^{\\top},\\dots,\\ensuremath{\\widehat{A}}_{M}^{\\top}]^{\\top}$ admitting the minimum volume of conv $\\{\\widehat{W}\\}$ (i.e., the simplex spanned by its columns) over all possible optimal solutions. Assume that we set $C=|Z|$ and that $F^{\\natural}(:,\\mathcal{Z}^{c})$ satisfies the SSC. Under Assumptions 3.2 and the same conditions used in Lemma 3.1, the following result holds with probability greater than $1-\\delta$ , when $S$ grows to infinity: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim\\mathcal{D}_{x}}{\\mathbb{E}}\\left[\\|\\widehat{f}(x)-\\Pi^{\\top}f^{\\natural}(x)\\|_{2}^{2}\\right]=\\mathcal{O}(|\\mathbb{Z}^{c}|^{-5/8}\\left(2\\|X(:,\\mathbb{Z}^{c})\\|_{\\mathbb{F}}\\mathcal{R}_{\\mathcal{F}}\\right)^{\\frac{1}{4}}+\\sqrt{\\log(1/\\delta)/|\\mathbb{Z}^{c}|}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In addition, we have $\\|\\widehat{A}_{m}-A_{m}^{\\natural}\\Pi\\|_{\\mathrm{F}}=0,\\forall m\\,a n d\\,\\widehat{\\mathcal{Z}}=\\mathcal{Z}$ . ", "page_idx": 6}, {"type": "text", "text": "The proof is relegated to Appendix E, which also holds for $C\\geq|{\\mathcal{T}}|$ with slight modifications as discussed before. Theorem 3.6 shows that the target classifier can be accurately estimated in the presence of instance-dependent outliers, without needing anchor samples or class specialists. ", "page_idx": 6}, {"type": "text", "text": "Implementation. Our learning losses allows relatively easy implementation and optimization. We consider the following regularized criterion: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\b{A}_{m}\\in\\mathcal{A},\\b{e}_{n}^{(m)}\\in\\mathcal{E},\\b{f}\\in\\mathcal{F}}{\\mathrm{minimize}}\\,\\mathsf{L}_{\\mathsf{c e}}+\\mu_{1}\\mathsf{L}_{\\mathsf{o u t l i e r}}+\\mu_{2}\\mathsf{L}_{\\mathsf{v o l}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that we use $\\begin{array}{r}{\\mathsf{L}_{\\mathsf{o u t l i e r}}=\\sum_{n=1}^{N}(\\sum_{m=1}^{M}\\|e_{n}^{(m)}\\|_{2}^{2}\\!+\\!\\zeta)^{\\frac{p}{2}}}\\end{array}$ where $\\zeta>0$ and $0<p\\le1$ to approximate the column-sparsity const raint. The $\\ell_{2}/\\ell_{p}$ nonconvex mixed quasi-norm has proven to be a very effective approximator for column sparsity [54]. In addition, as the function is differentiable, it offers an optimization-friendly surrogate. ", "page_idx": 6}, {"type": "text", "text": "For the minimum-volume loss, we use $\\mathsf{L}_{\\mathsf{v o l}}\\,=\\,-\\log\\operatorname*{det}(\\boldsymbol{F}\\boldsymbol{F}^{\\top})$ , where ${\\pmb F}=[{\\pmb f}({\\pmb x}_{1}),\\dots,{\\pmb f}({\\pmb x}_{N})]$ . Note that the term encourages maximizing the volume of $\\operatorname{conv}\\{F\\}$ , which in turn minimizes the volume of $\\mathrm{conv}\\{W\\}$ under the model ${\\tilde{G}}\\,=\\,W F$ . Therefore, the volume of $\\mathrm{conv}\\{W\\}$ can be minimized via using either $\\log\\operatorname*{det}(W^{\\top}W)$ or $-\\log\\operatorname*{det}(\\boldsymbol{F}\\boldsymbol{F}^{\\top})$ as the regularizer. The reason why we choose the latter is because minimizing $\\log\\operatorname*{det}(W^{\\top}W)$ is ill-defined\u2014the term encourages a rank-deficient $W$ instead of a small-volume full-rank $W$ . The remedy in the literature is to use $\\log\\operatorname*{det}(\\boldsymbol{W}^{\\top}\\boldsymbol{W}+\\gamma\\boldsymbol{I})$ with $\\gamma>0$ [54] or adding structural constraints (e.g., diagonal dominance) to $W$ [19], but these require more parameter tuning and more prior knowledge. ", "page_idx": 6}, {"type": "text", "text": "The constraints on $\\pmb{A}_{m}$ \u2019s can be handled by adding softmax to the columns. The function class $\\mathcal{F}$ can be a designated deep neural network model (e.g., ResNet), where the outputs are also constrained by softmax. The constraint on $e_{n}^{(m)}$ can be handled by parameterizing each $\\bar{e}_{n}^{m}=\\widetilde{e}_{n}^{(m)}\\!-\\!(1^{\\top}\\widetilde{e}_{n}^{(m)})1/\\bar{K}$ , where $\\widetilde{e}_{n}^{(m)}$ is trainable weight vector. More details of the implementation   are provid ed in Appendix G.1. We name our approach as Crowdsourcing-based Outlier-robust criterion and INstancedependence-aware deep neural Network learning, abbreviated as COINNet. ", "page_idx": 6}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our development is related to works in transition matrix-based [14\u201319,21,23,27\u201329,33,47,55,56] and sample selection-based [28, 33, 35, 37, 39] noisy label learning, confusion identification in crowdsourcing [14,18,57], end-to-end crowdsourcing [20,30,31,49] (with some paying particular attention to incomplete annotations [58, 59]), and identifiability of NMF [53, 60]\u2014see detailed discussions of the related work in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "A particular connection to highlight is the usage of multiple annotators as a key enabler of our identifiability guarantees. This result (implicitly) aligns with some recently proposed approaches that advocate for the use of multiple labels in learning under label noise [45,49]. Specifically, the work by [45] established the identifiability of instance-independent label noise using the consensus of at least three similar data items with the same true label. Similarly, the work by [49] leveraged Kruskal\u2019s identifiability-based arguments to recommend using at least three noisy labels to establish the identifiability of the instance-dependent noise transition matrix. These works, particularly [49], were more concerned with the theoretical limitations other than realizable methods. They did not connect the existence of repeated noisy labels to crowdsourcing, but used clusterability properties of datasets to support their arguments. In this work, we advocate diversity in crowd wisdom\u2014labels from multiple annotators\u2014based on an intuitive geometric characteristic of the model, which leads to clearly realizable implementations. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Baselines. The proposed method is compared with a number of existing baselines in noisy label learning. We choose several end-to-end crowdsourcing methods, namely, GeoCrowdNet(F)&(W) [31], TraceReg [30], CrowdLayer [20], and Max-MIG [61]. We also employ four different instancedependent noisy learning approach, namely, MEIDTM [36], PTD [27], and BLTM [33]\u2014which all use a single annotator. In addition, we present the result of VolMinNet [19] that uses the instanceindependent confusion model and the volume minimization regularization. We also use two noiserobust loss function-based approaches, namely GCE [10] and Reweight [21]. For baselines that were developed for single annotator cases, we train them using the labels after majority voting. Note that all the confusion matrix-based noise correction methods (including ours) inherently have a permutation ambiguity (cf. the \u03a0 term in Theorems 3.5 and 3.6). Hence, we report the highest classification accuracy attained among all possible permutation matrices for every method. In practice, $\\mathbf{\\delta}\\pi$ can be removed by additional annotator inspection on several anchor samples, but we skipped these steps to keep the evaluation simple. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experiments with Machine Annotations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset. We consider the CIFAR-10 [62] and the STL-10 datasets [63]\u2014see Appendix G for details. ", "page_idx": 7}, {"type": "text", "text": "Experiment Setup. To generate multiple noisy labels, we use $M=3$ machine annotators for each dataset. In order to simulate a wide range of annotation behaviors, we employ different classification and clustering methods such as $k$ -nearest neighbors (kNN), logistic regression, and convolutional neural networks. Each machine annotator is trained by randomly choosing a subset of the training data. We control the labeling accuracies of these machine annotators by varying the size of the dataset and the number of epochs during their training phase. This results in annotators with different labeling qualities, with their average individual noise rates around $20\\%$ (good), $40\\%$ (medium), and $70\\%$ (low). This way, we set up the following three cases: $(i)$ High Noise: Three machine annotators are with low, medium, and good quality, respectively. (i) Medium Noise: Two machine annotators are with medium quality and one machine annotator with good quality. $(i)$ Low Noise: One machine annotators is with medium quality and two machine annotators with good quality. More details are in Appendix G.2. ", "page_idx": 7}, {"type": "text", "text": "Neural Network and Optimizer Settings. We use ResNet-34 and ResNet-9 architectures [64] as the backbone to run all methods on CIFAR-10 and STL-10 datasets, respectively. For our proposed approach COINNet, we fix $\\zeta=10^{-10},p=0.4$ , and $\\mu_{1}=\\mu_{2}=0.01$ . Adam [65] is used as the optimizer with weight decay of $10^{-4}$ , learning rate of 0.01, and batch size of 512. ", "page_idx": 7}, {"type": "text", "text": "More details and ablation studies with the hyperparameters are provided in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Results. Table 1 presents the average testing accuracy of the methods. One can see that our approach COINNet consistently outperforms all baselines in different scenarios under test. Notably, the performance gap between COINNet and the second-best method is more significant in the high noise regime. For example, in the CIFAR-10 high noise scenario, COINNet shows around $4\\%$ improvement over the second-best performing baseline GeoCrowdNet(F). Another key observation is that the instance-dependent modeling based baselines such as MEIDTM, PTD, and BLTM, exhibit much degraded performance in high-noise scenarios, possibly due to their multi-stage procedures that accumulate errors easily under such circumstances. In addition, our approach outperforms the instance-independent confusion-based baselines, such as VolMinNet, Reweight and GeoCrowdNet(W), by nontrivial margins, showing that modeling instance-dependent outliers is indeed beneficial. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments Using Real Annotations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we use three datasets to evaluate the algorithms. ", "page_idx": 7}, {"type": "text", "text": "CIFAR-10N. The first dataset that we use is the CIFAR-10N dataset [66]. The data has $N=60,000$ samples from $K=10$ classes and the samples were labeled by $M=3$ anonymous real annotators. The error rates of the 3 annotators are $17.23\\%$ , $18.12\\%$ , and $17.64\\%$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "LabelMe. We also test the algorithms over the LabelMe dataset [67,68]. The dataset has $N=2,688$ samples from $K=8$ classes, and $M=59$ anonymous real annotators were involved in labeling the data. The average error rate is $25.95\\%$ . ", "page_idx": 7}, {"type": "table", "img_path": "HTLJptF7qM/tmp/0327a3c8e599b61e8f2e66c9b0b8bbacdc4551fe9a9b8d0ee82a5bdff3ba1a58.jpg", "table_caption": ["Table 1: Average classification accuracy using machine annotators in CIFAR-10 and STL-10 datasets under different labeling scenarios. Bold black represents the best and blue represents the second best. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "HTLJptF7qM/tmp/4565ef86c51cbedb2e94a293c029f3daf942fd96b53effed78a62c6a397f1686.jpg", "table_caption": ["Table 2: Average classification accuracy on CIFAR-10N, LabelMe, and ImageNet-15N datasets, labeled by human annotators. Bold black represents the best and blue represents the second best. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ImageNet-15N. In addition to existing datasets, we also acquire noisy annotations by asking AMT workers to annotate some images from ImageNet. We select $K=15$ classes and submit randomly chosen images to AMT for labeling. Eventually we collect annotations for $N=2$ , 514 images from $M=100$ anonymous real annotators, which serve as our training set. The average error rate of the annotators is $42.68\\%$ . The validation and testing sets have 1,462 and 13,157 images, respectively. We release the code and our acquired noisy annotations at https://github.com/ductri/COINNet. ", "page_idx": 8}, {"type": "text", "text": "Settings. For CIFAR-10N, we employ the ResNet-34 architecture to serve as $\\boldsymbol{\\textbf{\\textit{f}}}$ . For the LabelMe and ImageNet-15N datasets, we employ similar settings as in [20]. Specifically, as the training sets are small, the pretrained VGG-16 [69] and CLIP [70] models are used to first extract image embeddings for the experiments on LabelMe and ImageNet-15N, respectively. The embeddings are then fed to $\\pmb{f}$ , which is a fully connected neural network with one hidden layer and 128 hidden ReLU units. The same encoders and architecture choices are employed for the all methods under test. For our approach COINNet, we set $\\mu_{1}=\\mu_{2}=0.1$ for LabelMe, and $\\mu_{1}=\\mu_{2}=0.01$ for CIFAR-10N and ImageNet-15N. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 2 presents the average testing accuracy of different methods on the three datasets. The proposed approach, COINNet, shows promising results in all cases, clearly outperforming the baselines by a noticeable edge. This is consistent with the machine annotator experiments. ", "page_idx": 8}, {"type": "text", "text": "Fig. 2 demonstrates the outlier identification results using the CIFAR-10N dataset. Here, we define the outlier indicator as $\\begin{array}{r}{s_{n}=\\sum_{m=1}^{M}\\|\\widehat{\\pmb{e}}_{n}^{(m)}\\|_{2},\\forall n}\\end{array}$ , where $\\widehat{e}_{n}^{(m)}$ are the learned instance-dependent perturbations in the model (5). The histogram plot of these scores in Fig. 2 clearly shows that our method does output two types of samples, i.e., nominal samples and outliers, based on these perturbations. In the figure, the images on the middle and the right are examples from the low and high outlier indicator value regimes, respectively. The images with high $s_{n}$ values show more instance-dependent confusion characteristics (such as background noise and blurring) compared to those in the middle. Overall, these results indicate the effectiveness of our outlier-based model in real-world settings. ", "page_idx": 8}, {"type": "image", "img_path": "HTLJptF7qM/tmp/3ee78d20a189e4f8003daa5b7461a93ffee37e47b2bcff8eff068a3bdef7f75c.jpg", "img_caption": ["Figure 2: Histogram of the learned outlier indicator values $\\begin{array}{r}{s_{n}=\\sum_{m=1}^{M}\\|\\widehat{\\boldsymbol{e}}_{n}^{(m)}\\|_{2}^{2}}\\end{array}$ over all training images in the CIFAR-10N dataset (left), and examples with low (middle) and high (right) $s_{n}$ \u2019s \u2014see more examples in Appendix H. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "HTLJptF7qM/tmp/2af23fb4776bc9eb896b4b169c870071faefd63b3cb00b0ee5cc4ce1ebad10db.jpg", "img_caption": ["Figure 3: Some examples from ImageNet-15N with low (top) and high (bottom) $s_{n}$ \u2019s "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Fig. 3 shows some examples selected from high- $^{S}n$ and low- $s_{n}$ regimes from the results output by COINNet in the ImageNet-15N experiment. Similar as before, the images in the first row that have lower $s_{n}$ scores are visually much easier to recognize. The images in the second row that have about 5 times higher $s_{n}$ scores are apparently more visually confusing. ", "page_idx": 9}, {"type": "text", "text": "More experiments can be found in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we considered the noisy label learning problem under a confusion matrix-based model. We developed theory and algorithms in the presence of instance-dependent outliers. Our study revealed that relying solely on single-annotator labels is insufficient for effective outlier detection under the model of interest. We further demonstrated that a crowdsourcing approach, leveraging multiple annotators and a sparsity-constrained loss function, can successfully detect outliers and identify the desired, label noise-free learning system under reasonable conditions. Our analyses and design feature a one-stage differentiable training loss that is optimization-friendly. Empirical results underscore the plausibility of our model and the effectiveness of our proposed method, showing noticeable performance improvements over existing baselines. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our work explores a noise transition matrix-based model and treats instance-dependent noisy labels as outliers. The key assumption is that the outliers do not occur overwhelmingly. The assumption is useful, but also could be debatable. In principle, all labels could be generated in an instance-dependent manner. More sophisticated models are needed to deal with this more general case. In addition, our treatment is outlier structure-agnostic. The upshot is that this treatment can also help exclude the negative impacts of other types of outliers. However, the downside is that the structural prior knowledge of the outliers is not fully exploited. If the outlier-generating function could be learned, it can help detect \u201cdifficult samples\u201d before sending for annotation, which would potentially save resources and reduce annotation errors. We leave this direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work was supported in part by the National Science Foundation (NSF) under Project IIS-2007836. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Devansh Arpit, Stanis\u0142aw Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In Proceedings of International Conference on Machine Learning, page 233\u2013242, 2017.   \n[2] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In Proceedings of International Conference on Learning Representations, 2016.   \n[3] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In Proceedings of International Conference on Machine Learning, volume 80, pages 2304\u20132313, 2018.   \n[4] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In Proceedings of International Conference on Machine Learning, volume 80, pages 4334\u20134343, 2018.   \n[5] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In Proceedings of International Conference on Machine Learning, pages 7164\u20137173, 2019.   \n[6] Eran Malach and Shai Shalev-Shwartz. Decoupling \"when to update\" from \"how to update\". In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[7] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems, page 8536\u20138546, 2018.   \n[8] Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit memorization effect in learning with noisy labels. In Proceedings of International Conference on Machine Learning, volume 119, pages 10789\u201310798, 2020.   \n[9] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In Advances in Neural Information Processing Systems, volume 26, 2013.   \n[10] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[11] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 322\u2013330, 2019.   \n[12] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Curriculum loss: Robust learning and generalization against label corruption. In Proceedings of International Conference on Learning Representations, 2019.   \n[13] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. LDMI: A novel information-theoretic loss function for training deep nets robust to label noise. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[14] Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics, pages 20\u201328, 1979.   \n[15] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping, 2015.   \n[16] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1944\u20131952, 2017.   \n[17] Jacob Whitehill, Ting fan Wu, Jacob Bergsma, Javier R. Movellan, and Paul L. Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems, volume 22, pages 2035\u20132043. 2009.   \n[18] Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I. Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. Journal of Machine Learning Research, 17(102):1\u201344, 2016.   \n[19] Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end label-noise learning without anchor points. In Proceedings of International Conference on Machine Learning, pages 6403\u20136413, 2021.   \n[20] Filipe Rodrigues and Francisco Pereira. Deep learning from crowds. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018.   \n[21] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38:447\u2013461, 2016.   \n[22] Tri Nguyen, Shahana Ibrahim, and Xiao Fu. Deep clustering with incomplete noisy pairwise annotations: A geometric regularization approach. In International Conference on Machine Learning, pages 25980\u201326007. PMLR, 2023.   \n[23] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 6226\u20136236, 2020.   \n[24] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy labels via total variation regularization. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 12501\u201312512, 2021.   \n[25] Kiran K Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh. Robustness of conditional gans to noisy labels. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[26] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, 2019.   \n[27] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instancedependent label noise. In Advances in Neural Information Processing Systems, volume 33, pages 7597\u20137610, 2020.   \n[28] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance and label-dependent label noise. In Proceedings of International Conference on Machine Learning, volume 119, pages 1789\u20131799, 2020.   \n[29] David Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In Advances in Neural Information Processing Systems, volume 24, pages 1953\u20131961, 2011a.   \n[30] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C. Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11236\u2013 11245, 2019.   \n[31] Shahana Ibrahim, Tri Nguyen, and Xiao Fu. Deep learning from crowdsourced labels: Coupled cross-entropy minimization, identifiability, and regularization. In Proceedings of International Conference on Learning Representations, 2023.   \n[32] Kristen Grauman and Bastian Leibe. Visual Object Recognition. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2011.   \n[33] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating instance-dependent Bayes-label transition matrix using a deep neural network. In Proceedings of International Conference on Machine Learning, 2021.   \n[34] Yu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instancedependent label-noise learning under a structural causal model, 2021.   \n[35] Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instancedependent label noise, 2021.   \n[36] D. Cheng, T. Liu, Y. Ning, N. Wang, B. Han, G. Niu, X. Gao, and M. Sugiyama. Instancedependent label-noise learning with manifold-regularized transition matrix estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16609\u2013 16618, jun 2022.   \n[37] Shikun Li, Xiaobo Xia, Jiankang Deng, Shiming Ge, and Tongliang Liu. Transferring annotator- and instance-dependent transition matrix for learning from crowds. arXiv preprint arXiv:2306.03116, 2023.   \n[38] Qizhou Wang, Bo Han, Tongliang Liu, Gang Niu, Jian Yang, and Chen Gong. Tackling instancedependent label noise via a universal probabilistic model. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):10183\u201310191, May 2021.   \n[39] Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent label noise: A sample sieve approach. In Proceedings of International Conference on Learning Representations, 2021.   \n[40] Zhimeng Jiang, Kaixiong Zhou, Zirui Liu, Li Li, Rui Chen, Soo-Hyun Choi, and Xia Hu. An information fusion approach to learning with instance-dependent label noise. In Proceedings of International Conference on Learning Representations, 2022.   \n[41] Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, and Chao Chen. Learning with feature-dependent label noise: A progressive approach. In Proceedings of International Conference on Learning Representations, 2021.   \n[42] Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga Cicarrelli, Frederik Barkhof, and Daniel Alexander. Disentangling human error from ground truth in segmentation of medical images. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15750\u201315762. Curran Associates, Inc., 2020.   \n[43] Hui Guo, Boyu Wang, and Grace Yi. Label correction of crowdsourced noisy annotations with an instance-dependent noise transition model. In Advances in Neural Information Processing Systems, 2023.   \n[44] Qizhou Wang, Jiangchao Yao, Chen Gong, Tongliang Liu, Mingming Gong, Hongxia Yang, and Bo Han. Learning with group noise. Proceedings of the AAAI Conference on Artificial Intelligence, 35:10192\u201310200, 05 2021.   \n[45] Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when learning with noisy labels. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 12912\u201312923, 2021.   \n[46] Xiao Fu, Wing-Kin Ma, Jos\u00e9 Bioucas-Dias, and Tsung-Han Chan. Semiblind hyperspectral unmixing in the presence of spectral library mismatches. arXiv preprint arXiv:1507.01661, 2015.   \n[47] Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In Advances in neural information processing systems, volume 25, pages 692\u2013700, 2012.   \n[48] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary labels. In Proceedings of the European conference on computer vision (ECCV), pages 68\u201383, 2018.   \n[49] Yang Liu, Hao Cheng, and Kun Zhang. Identifiability of label noise transition matrix. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 21475\u201321496, 2023.   \n[50] Zhendong Chu, Jing Ma, and Hongning Wang. Learning from crowds by modeling common confusions. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.   \n[51] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[52] Shahana Ibrahim, Xiao Fu, Nikos Kargas, and Kejun Huang. Crowdsourcing via pairwise co-occurrences: Identifiability and algorithms. In Advances in Neural Information Processing Systems, volume 32, pages 7847\u20137857, 2019.   \n[53] K. Huang, N. Sidiropoulos, and A. Swami. Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition. IEEE Trans. Signal Process., 62(1):211\u2013224, 2014.   \n[54] Xiao Fu, Kejun Huang, Bo Yang, Wing-Kin Ma, and Nicholas D Sidiropoulos. Robust volume minimization-based matrix factorization for remote sensing and document clustering. IEEE Trans. Signal Process., 64(23):6254\u20136268, 2016.   \n[55] Rion Snow, Brendan O\u2019Connor, Daniel Jurafsky, and Andrew Y Ng. Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254\u2013263, 2008.   \n[56] Peter Welinder, Steve Branson, Pietro Perona, and Serge J Belongie. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems, pages 2424\u20132432, 2010.   \n[57] Panagiotis A Traganitis, Alba Pages-Zamora, and Georgios B Giannakis. Blind multiclass ensemble classification. IEEE Trans. Signal Process., 66(18):4737\u20134752, 2018.   \n[58] Hansong Zhang, Shikun Li, Dan Zeng, Chenggang Yan, and Shiming Ge. Coupled confusion correction: Learning from crowds with sparse annotations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16732\u201316740, 2024.   \n[59] Shikun Li, Tongliang Liu, Jiyong Tan, Dan Zeng, and Shiming Ge. Trustable co-label learning from multiple noisy annotators. IEEE Transactions on Multimedia, 25:1045\u20131057, 2021.   \n[60] Nicolas Gillis. Nonnegative Matrix Factorization. Society for Industrial and Applied Mathematics, 2020.   \n[61] Peng Cao, Yilun Xu, Yuqing Kong, and Yizhou Wang. Max-Mig: An information theoretic approach for joint learning from crowds. In Proceedings of International Conference on Learning Representations, 2019.   \n[62] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.   \n[63] Adam Coates, Andrew $\\mathrm{Ng}$ , and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of International Conference on Artificial Intelligence and Statistics, volume 15, pages 215\u2013223, 2011.   \n[64] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[65] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of International Conference on Learning Representations, ICLR, 2015.   \n[66] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In International Conference on Learning Representations, 2022.   \n[67] Filipe Rodrigues, Mariana Lourenco, Bernardete Ribeiro, and Francisco C. Pereira. Learning supervised topic models for classification and regression from crowds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2409\u20132422, 2017.   \n[68] Bryan C. Russell, Antonio Torralba, Kevin P. Murphy, and William T. Freeman. Labelme: A database and web-based tool for image annotation. International Journal of Computer Vision, 77:157\u2013173, 2007.   \n[69] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[71] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, pages 1\u201319, 2022.   \n[72] Dana Angluin and Philip Laird. Learning from noisy examples. Mach. Learn., 2(4):343\u2013370, apr 1988.   \n[73] Javed A. Aslam and Scott E. Decatur. On the sample complexity of noise-tolerant learning. Inf. Process. Lett., 57(4):189\u2013195, feb 1996.   \n[74] Michael Kearns. Efficient noise-tolerant learning from statistical queries. J. ACM, 45(6):983\u20131006, nov 1998.   \n[75] Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: Consistency and maximal denoising. In Shai Shalev-Shwartz and Ingo Steinwart, editors, Proceedings of the 26th Annual Conference on Learning Theory, volume 30, pages 489\u2013511, 2013.   \n[76] Antonin Berthon, Bo Han, Gang Niu, Tongliang Liu, and Masashi Sugiyama. Confidence scores make instance-dependent label-noise learning possible. In Proceedings of International Conference on Machine Learning, volume 139, pages 825\u2013836, 2021.   \n[77] Mark A. Davenport, Yaniv Plan, Ewout van den Berg, and Mary Wootters. 1-bit matrix completion. Information and Inference: A Journal of the IMA, 3(3):189\u2013223, 2014.   \n[78] M S Pinsker. The information stability of gaussian random variables and processes (in Russian). Dokl. Akad. Nauk SSSR, 133:28\u201330, 1960.   \n[79] A.A. Fedotov, P. Harremoes, and F. Topsoe. Refinements of pinsker\u2019s inequality. IEEE Transactions on Information Theory, 49(6):1491\u20131498, 2003.   \n[80] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.   \n[81] Andreas Maurer. A vector-contraction inequality for rademacher complexities. ArXiv, abs/1605.00251, 2016.   \n[82] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, page 210\u2013268. Cambridge University Press, 2012.   \n[83] G. Raskutti, Martin Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over $\\ell_{q}$ -balls. IEEE Transactions on Information Theory, 57:6976 \u2013 6994, 11 2011.   \n[84] X. Fu, W.-K. Ma, K. Huang, and N. D. Sidiropoulos. Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain. IEEE Trans. Signal Process., 63(9):2306\u20132320, May 2015.   \n[85] Xiao Fu, Kejun Huang, and Nicholas D Sidiropoulos. On identifiability of nonnegative matrix factorization. IEEE Signal Process. Lett., 25(3):328\u2013332, 2018.   \n[86] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369\u2013386, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Material of \u201c Noisy Label Learning with Instance-Dependent Outliers: Identifiability via Crowd Wisdom\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The notations used in the paper are summarized in Table 3. ", "page_idx": 15}, {"type": "table", "img_path": "HTLJptF7qM/tmp/a479b3f2cc8732492790bad8d2f43412b9a0f2f6983a02383af8985bea6be1a3.jpg", "table_caption": ["Table 3: Definitions of the notations. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we briefly review some of the existing works that is relevant to our proposed method. Noisy label learning has been a prominent research focus within the machine learning community for many years\u2014see a survey of these methods in [71]. Among the numerous approaches proposed, those based on label transition matrix-based models (cf. the model in (2)) have been proven effective and several variants of the model have been introduced; see, e.g., [14\u201319,21,23,27\u201329,33,47,52,55,56]. A broad classification of them is of instance-independent models and instance-dependent models. ", "page_idx": 15}, {"type": "text", "text": "Instance-independent Model-based Methods. One of the earliest instance-independent label noise models (i.e., assuming $T^{\\natural}({\\pmb x}_{n})={\\pmb A}^{\\natural},\\forall n$ in (2)) is the random classification model (RCN) [72\u201374], where true labels are filpped with a fixed noise rate independently at random. To extend this modeling to more general scenarios, class-conditioned model (CCN) was proposed [9], where the noise rates are assumed to be dependent on the true-class the data item belongs. CCN model has led to the design of several statistically consistent classifiers, as the label transition matrix acts as a correction term for the loss associated with the noisy labels [21,24,26,75]. To provably learn the CCN model, several notions are exploited, e.g., assuming the presence of anchor data items [21], exploiting the clusterability of data items [45], and minimizing the geometric volume of the transition matrix [19]. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Instance-dependent Model-based Approaches. Instance-dependent label noise models are naturally more plausible. In these models, each data item has a label transition matrix dependent on the data\u2019s attributes (cf. Eq. (2)). Recently, a number of methods studied such models by using various simplified representations/parameterizations of the transition matrix $\\pmb{T}^{\\natural}(\\pmb{x}_{n})$ , e.g., imposing structural assumptions on label transition matrices [27,41,43], bounding noise rates [28], modeling the Bayes label transition matrix or posterior transition matrix instead of the true label transition matrix [33,37,40], leveraging additional forms of supervision such as confidence scores [76], and crafting specific regularization terms [36,39,42]. To be specific, the work in [27] assumed that the label transition matrix is a convex combination of several instance-independent transition matrices and the combination coefficients depend on the instances. The work in [28] assumed a known bounded noise rate and proposed an instance selection-based strategy. The method proposed in [39] also advocated an instance selection scheme, accompanied with a regularization term to combat instance-dependent label noise. In the work [36], the simplification relies on the assumption that many data items that lie close to each other in the feature space have similar label transition matrices. These interesting developments showed empirical progresses, but the theoretical understanding has been limited. Particularly, the aforementioned identifiability issue was not resolved. In addition, most approaches resort to multi-stage training strategies which may be practically inefficient due to potential error propagation. For example, the approach in [33] first performs instance selection, then estimates the (Bayes-label) transition matrices, and finally estimates the classifier, in a three-stage training scheme. Similar multi-stage approaches are employed in [27,28,36]. ", "page_idx": 16}, {"type": "text", "text": "Our approach is also related to sample-selection-based approaches in noisy label learning, e.g., clean sample selection-based methods [28,33,35,37,39]. However, most of them treat sample selection as a pre-processing step based on strict prior assumptions (e.g., known noise rates) and then estimate the transition matrix using these selected samples. In contrast, our proposed method and identifiability analysis offer a more principled approach by integrating sample selection with the learning of the classifier and transition matrices in an end-to-end fashion. ", "page_idx": 16}, {"type": "text", "text": "C Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 3.1 is re-stated below: ", "page_idx": 16}, {"type": "text", "text": "Lemma C.1. Assume that the observed index pairs $(m,n)\\in\\mathcal{S}$ are sampled uniformly at random with replacement. Also assume $f^{\\natural}\\in{\\mathcal{F}}$ , $|\\mathcal{Z}|\\le C\\le N/2$ , and that each $[{\\pmb A}_{m}{\\pmb f}({\\pmb x}_{n})+{\\pmb e}_{n}^{(m)}]_{k}$ , $\\forall\\mathbf{A}_{m}\\in$ $\\mathcal{A},\\forall e_{n}^{(m)}\\in\\mathcal{E},\\forall\\pmb{f}\\in\\mathcal{F}$ are lower bounded by $(1/\\beta)$ for a certain $\\beta>0$ . Then, with probability greater than $1-\\delta$ , the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{N M}\\sum_{n=1}^{N}\\sum_{m=1}^{M}\\|g_{n}^{\\natural(m)}-\\widehat g_{n}^{(m)}\\|_{2}^{2}\\leq\\epsilon_{g}(S),\\quad\\epsilon_{g}(S)=\\mathcal{O}\\left(\\beta\\Re s\\log S/\\sqrt{s}+\\log\\left(\\beta\\right)\\sqrt{\\log(1/\\delta)}/\\sqrt{s}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Re_{S}^{2}\\,=\\,K^{2}\\log{(K)}+\\ K\\|X\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{F}}/s+C M K\\log{\\sqrt{N M}}$ and $X\\,=\\,[{\\pmb x}_{n_{1}},\\ldots,{\\pmb x}_{n_{S}}]$ are the annotated samples, in which $(m_{s},n_{s})\\in\\mathcal{S}$ for $s=1,\\ldots,S$ . ", "page_idx": 16}, {"type": "text", "text": "The proof is essentially viewing the ${\\pmb g}_{n}^{\\natural(m)}$ estimation problem as a quantized matrix completion problem with missing blocks. The key ideas and steps all follow the proof of the end-to-end crowdsourcing work [31, Proposition 1] with the additional consideration of e\u266en(m). This addition is conceptually not hard to come up with, but the notations can be involved. Thus, we still derive the proof in detail. ", "page_idx": 16}, {"type": "text", "text": "Proof: Consider the cross entropy-based learning criterion given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{A_{m}\\in\\mathcal{A},\\,f\\in\\mathcal{F},e_{n}^{(m)}\\in\\mathcal{E}}{\\mathrm{minimize}}\\,\\frac{1}{S}\\sum_{(m,n)\\in\\mathcal{S}}\\mathsf{C E}(A_{m}f(x_{n})+e_{n}^{(m)},\\widehat{y}_{n}^{(m)}),}\\\\ &{\\quad\\quad\\mathrm{~subject\\to~}\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|e_{n}^{(m)}\\|_{2}>0\\right\\}\\leq C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $S=|S|$ , and $\\begin{array}{r}{\\mathsf{C E}(\\pmb{x},y)=-\\sum_{k=1}^{K}\\mathbb{1}[y=k]\\log(\\pmb{x}(k)).}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Let us denote the objective function in (11a) as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{L}_{\\mathcal{S}}(G;\\widehat{\\mathcal{Y}})\\triangleq\\frac{1}{S}\\sum_{(m,n)\\in\\mathcal{S}}\\mathsf C\\mathsf E(g_{n}^{(m)},\\widehat{y}_{n}^{(m)}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where g (nm)is given by g(nm $\\pmb{g}_{n}^{(m)}=\\pmb{A}_{m}\\pmb{f}(\\pmb{x}_{n})+\\pmb{e}_{n}^{(m)}$ , $\\boldsymbol{G}$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{G}=\\left[\\begin{array}{c c c c}{g_{1}^{(1)}}&{\\hdots}&{g_{N}^{(1)}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {g_{1}^{(M)}}&{\\hdots}&{g_{N}^{(M)}}\\end{array}\\right]=\\left[\\begin{array}{c c c c}{A_{1}f(x_{1})+e_{1}^{(1)}}&{\\hdots}&{A_{1}f(x_{N})+e_{N}^{(1)}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {A_{M}f(x_{1})+e_{1}^{(M)}}&{\\hdots}&{A_{M}f(x_{N})+e_{N}^{(M)}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $\\widehat{\\boldsymbol{y}}$ denotes the set of observed noisy labels, i.e., $\\{\\widehat{y}_{n}^{(m)}\\}_{(m,n)\\in S}$ . Let $\\{\\widehat{A}_{m}\\}$ , $\\{\\widehat{e}_{n}^{(m)}\\}$ and $\\widehat{\\pmb f}$ denote opti mal solutions of the learning criterion (11). Let u s also define the fol lowing: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{G}\\triangleq\\underset{G\\in\\mathcal{G}}{\\arg\\operatorname*{min}}\\,\\mathsf{L}_{\\mathcal{S}}(G;\\widehat{\\mathcal{V}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{G}\\triangleq\\{G\\in\\mathbb{R}^{M K\\times N}\\mid g_{n}^{(m)}=A_{m}f(x_{n})+e_{n}^{(m)},f\\in\\mathcal{F},e_{n}^{(m)}\\in\\mathcal{E},\\|E\\|_{0}\\leq C,A_{m}\\in\\mathcal{A}\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{E}=\\{e\\in\\mathbb{R}^{K}\\;|\\;{\\bf1}^{\\top}e=0\\},\\;\\|E\\|_{0}=\\displaystyle\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|e_{n}^{(m)}\\|_{2}>0\\right\\},}\\\\ {\\displaystyle A=\\{A\\in\\mathbb{R}^{K\\times K}\\;|\\;{\\bf1}^{\\top}A={\\bf1}^{\\top},\\;A\\geq{\\bf0}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $G^{\\natural}$ represents the ground-truth such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{G}^{\\sharp}=\\left[\\begin{array}{c c c c}{g_{1}^{\\sharp(1)}}&{\\cdot\\cdot\\cdot}&{g_{N}^{\\sharp(1)}}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {g_{1}^{\\sharp(M)}}&{\\cdot\\cdot}&{g_{N}^{\\sharp(M)}}\\end{array}\\right]=\\left[\\begin{array}{c c c c}{A_{1}^{\\sharp}f^{\\sharp}(x_{1})+e_{1}^{\\sharp(1)}}&{\\cdot\\cdot\\cdot}&{A_{1}^{\\sharp}f^{\\sharp}(x_{N})+e_{N}^{\\sharp(1)}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {A_{M}^{\\sharp}f^{\\sharp}(x_{1})+e_{1}^{\\sharp(M)}}&{\\cdot\\cdot\\cdot}&{A_{M}^{\\sharp}f^{\\sharp}(x_{N})+e_{N}^{\\sharp(M)}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We aim to bound $\\|G^{\\sharp}-{\\widehat{G}}\\|_{\\mathrm{F}}$ . Towards this goal, we adopt the proof strategy based on the viewpoint of matrix completion as employed in [31,77]. ", "page_idx": 17}, {"type": "text", "text": "Let us consider that we have $S$ number of observations $\\omega_{1},\\ldots,\\omega_{S}$ such that $\\omega_{s}=(m_{s},n_{s},\\widehat{y}_{n_{s}}^{(m_{s})})$ , where each $(m_{s},n_{s})$ is sampled uniformly at random (with replacement) from $[M]\\times[N]$ ,  i. e., the saraem gpelinnegr apterod baasbility for each $(m,n)$ is given by $\\textstyle\\pi_{m,n}={\\frac{1}{N M}}$ . In addition, the noisy observations ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{y}_{n_{s}}^{(m_{s})}\\sim\\mathsf{c a t e g o r i c a l}(\\pmb{g}_{n_{s}}^{\\sharp(m_{s})}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\mathcal{D}_{\\omega}$ denote the joint probability distribution from where the observations $\\omega_{s}$ are sampled. Then we define the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{L}_{\\mathcal{D}}(G,\\widehat{\\mathcal{V}})\\triangleq\\mathbb{E}_{\\omega\\sim\\mathcal{D}_{\\omega}}[\\mathsf{C E}(g_{n}^{(m)},\\widehat{y}_{n}^{(m)})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We consider the following set of relations: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{L}_{\\mathcal{D}}(\\widehat{G},\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{D}}(G^{\\sharp},\\widehat{\\mathcal{Y}})=\\mathsf{L}_{\\mathcal{D}}(\\widehat{G},\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{S}}(\\widehat{G};\\widehat{\\mathcal{Y}})+\\mathsf{L}_{\\mathcal{S}}(G^{\\sharp};\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{D}}(G^{\\sharp},\\widehat{\\mathcal{Y}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathsf{L}_{\\mathcal{S}}(\\widehat{G};\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{S}}(G^{\\sharp};\\widehat{\\mathcal{Y}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathsf{L}_{\\mathcal{D}}(\\widehat{G},\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{S}}(\\widehat{G};\\widehat{\\mathcal{Y}})+\\mathsf{L}_{\\mathcal{S}}(G^{\\sharp};\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{D}}(G^{\\sharp},\\widehat{\\mathcal{Y}})}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\operatorname*{sup}_{G\\in\\mathcal{G}}\\left|\\mathsf{L}_{\\mathcal{D}}(G,\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{S}}(G;\\widehat{\\mathcal{Y}})\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality is obtained since $\\hat{G}$ is the optimal solution as given by (13) and hence satisfies $\\operatorname{L}_{\\mathcal{S}}(\\widehat{G};\\widehat{\\mathcal{P}})\\leq\\operatorname{L}_{\\mathcal{S}}(G^{\\sharp};\\widehat{\\mathcal{P}})$ . The last inequality is from the given assumption that $G^{\\sharp}\\in{\\mathcal{G}}$ (This is satisfied since it is given that $f^{\\natural}\\in{\\mathcal{F}}$ and $\\|E^{\\natural}\\|_{0}\\leq C)$ . ", "page_idx": 18}, {"type": "text", "text": "Next, we consider the following set of relations: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle\\overline{{S}}(\\overline{{A}},\\overline{{y}})\\rangle-\\mathrm{L}_{\\infty}(\\overline{{A}}^{*})=\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}(\\overline{{A}}(\\overline{{B}}_{n}^{*},\\overline{{y}}),\\overline{{\\boldsymbol x}}_{n}^{*})-\\mathrm{C}_{4}(\\overline{{A}}(\\overline{{B}}_{n}^{*},\\overline{{y}}),\\overline{{\\boldsymbol x}}_{n}^{*})}\\\\ &{=\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty};}\\\\ &{=\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty};}\\\\ &{=\\frac{\\lambda_{1}}{\\sum_{j=1}^{N}}\\sum_{\\overline{{\\pi}}_{i},\\overline{{\\boldsymbol x}}_{n}}\\Biggl[-\\frac{\\lambda_{j}}{\\sum_{l=1}^{N}}(\\overline{{\\boldsymbol x}}_{l}^{(i)}-\\boldsymbol x_{l}^{(i)}\\mathrm{L}_{\\infty}^{(i)})+\\frac{\\lambda_{1}}{\\sum_{l=1}^{N}}(\\overline{{\\boldsymbol y}}_{l}^{(i)}-\\boldsymbol x_{l}^{(i)}\\mathrm{L}_{\\infty}^{(j)})}\\\\ &{=\\frac{\\lambda_{1}}{N\\overline{{M}}_{\\infty}}\\sum_{\\overline{{\\pi}}_{i},\\overline{{\\boldsymbol x}}_{n}^{(i)}}\\Biggl(-\\frac{\\lambda_{j}}{N\\overline{{U}}_{n}^{(i)}}\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}^{(i)}\\sum_{l=1}^{N}\\sum_{\\overline{{\\pi}}_{i},\\overline{{\\boldsymbol x}}_{n}^{(i)}}\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}^{(i)}}\\\\ &{=\\frac{1}{N\\overline{{M}}_{\\infty}}\\sum_{\\overline{{\\pi}}_{i},\\overline{{\\boldsymbol x}}_{n}^{(i)}}\\Biggl(\\frac{\\lambda_{j}}{N\\overline{{U}}_{n}^{(i)}}-\\frac{\\lambda_{j}}{N\\overline{{U}}_{n}^{(i)}}\\mathrm{L}_{\\infty}\\mathrm{L}_{\\infty}^{(i)}\\Biggr)+\\frac{\\lambda_{1}}{\\sum_{l=1}^{N}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we applied Pinsker\u2019s inequality [78, 79] for the first inequality and the last inequality uses the fact that $\\|\\pmb{x}\\|_{1}\\;\\geq\\;\\|\\pmb{x}\\|_{2}$ for any vector $\\textbf{\\em x}\\in\\ \\mathbb{R}^{K}$ having $|[\\pmb{x}]_{k}|\\;\\le\\;1,\\forall k\\;\\in\\;[K]$ . Here $\\mathsf{D}_{\\mathsf{K L}}(g_{1},g_{2})$ denotes the Kullback\u2013Leibler divergence between any $g_{1},g_{2}\\in\\varDelta_{K}$ , i.e., ${\\mathsf{D}}_{\\mathsf{K L}}(g_{1},g_{2})=$ kK=1[g1]k log [[gg21]]kk]. ", "page_idx": 18}, {"type": "text", "text": "Combining (15) with (16), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{N M}\\sum_{n=1}^{N}\\sum_{m=1}^{M}\\|g_{n}^{\\natural(m)}-\\widehat g_{n}^{(m)}\\|_{2}^{2}\\leq4\\operatorname*{sup}_{G\\in\\mathcal G}\\left|\\mathsf{L}_{\\mathcal D}(G,\\widehat\\mathcal{\\hat{y}})-\\mathsf{L}_{\\mathcal S}(G;\\widehat\\mathcal{\\hat{y}})\\right|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we proceed to characterize the R.H.S. of (17). To achieve this, we invoke the following theorem derived from Theorem 26.5 in [80]: ", "page_idx": 18}, {"type": "text", "text": "Theorem C.2. [80, Theorem 26.5] Assume that $\\forall(m,n,\\widehat{y}_{n}^{(m)})$ y(nm)) and \u2200G \u2208 G, we have than (g(nm), y (nm))| \u2264 zmax. Then for any G \u2208 G, the followi n g holds with probability greater ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big|\\mathsf{L}_{\\mathcal{D}}(G,\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{S}}(G;\\widehat{\\mathcal{Y}})\\Big|\\le2\\Re(\\ell\\circ\\mathcal{G}\\circ\\mathcal{S})+4z_{\\operatorname*{max}}\\sqrt{\\frac{2\\log(4/\\delta)}{S}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\ell\\circ\\mathcal{G}\\circ\\mathcal{S}$ denotes the set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ell\\circ\\mathcal{G}\\circ S\\triangleq\\left\\{\\left(\\mathsf{C E}(g_{n_{1}}^{(m_{1})},\\widehat{y}_{n_{1}}^{(m_{1})}),\\dots,\\mathsf{C E}(g_{n_{S}}^{(m_{S})},\\widehat{y}_{n_{S}}^{(m_{S})})\\right)\\ |\\ G\\in\\mathcal{G}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\Re(\\mathcal{X})$ denotes the empirical Rademacher complexity of the set $\\mathcal{X}$ . ", "page_idx": 18}, {"type": "text", "text": "To apply Theorem C.2, we need to characterize the terms $z_{\\mathrm{max}}$ and $\\Re(\\ell\\circ\\mathcal{G}\\circ S)$ . The term $z_{\\mathrm{max}}$ can be characterized as below: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ z_{\\operatorname*{max}}=\\operatorname*{max}_{[g]_{k}>\\frac{1}{\\beta}}\\mathbb{C}(g,y)\\le\\operatorname*{max}_{[g]_{k}>\\frac{1}{\\beta}}-\\sum_{k=1}^{K}\\mathbb{1}[y=k]\\log[g]_{k}\\le\\operatorname*{max}_{[g]_{k}>\\frac{1}{\\beta}}-\\log[g]_{k}=\\log(\\beta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that in the above relation (19), we utilize the given assumption that each $[\\pmb{g}]_{k}\\ge1/\\beta,\\forall\\pmb{g}\\in\\mathcal{G}$ . To characterize the Rademacher complexity $\\Re(\\ell\\circ\\mathcal{G}\\circ S)$ , we consider its definition as follows [80]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Re(\\ell\\circ\\mathcal{G}\\circ S)\\triangleq\\frac{1}{S}\\mathbb{E}\\left[\\operatorname*{sup}_{G\\in\\mathcal{G}}\\sum_{s=1}^{S}\\sigma_{s}\\mathsf{C E}(g_{n_{s}}^{(m_{s})},\\widehat{y}_{n_{s}}^{(m_{s})})\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where expectation is w.r.t. the independent Rademacher random variables $\\sigma_{s}\\in\\{-1,1\\}$ . Since ${\\pmb g}_{n}^{(m)}$ are vectors, we utilize the following result to upper bound $\\Re(\\ell\\circ\\mathcal{G}\\circ S)$ : ", "page_idx": 19}, {"type": "text", "text": "Lemma C.3. [81] Assume that the function $\\mathsf{C E}(\\cdot,y),\\forall y$ has the Lipschitz constant $L$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\underset{G\\in\\mathcal{G}}{s u p}\\sum_{s=1}^{S}\\sigma_{s}\\mathsf{C E}(g_{n_{s}}^{(m_{s})},\\widehat{y}_{n_{s}}^{(m_{s})})\\right]\\leq\\sqrt{2}L\\mathbb{E}\\left[\\underset{G\\in\\mathcal{G}}{s u p}\\sum_{s=1}^{S}\\sum_{k=1}^{K}\\sigma_{s k}[g_{n_{s}}^{(m_{s})}]_{k}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\sigma_{s k}$ is an independent (doubly indexed) Rademacher random variable and the expectations are taken w.r.t. the Rademacher random variables. ", "page_idx": 19}, {"type": "text", "text": "To apply Lemma C.3, let us define a vector z \u225c [g(n1m1)]1, [g(n1m1)]2, . . . , [g(nmSS)]K\u22121, [g(nmSS)]K \u2208 $\\mathbb{R}^{S K}$ and the set $\\mathcal{Z}\\circ S\\triangleq\\{z=\\left([g_{n_{1}}^{(m_{1})}]_{1},[g_{n_{1}}^{(m_{1})}]_{2},\\dots,[g_{n_{S}}^{(m_{S})}]_{K-1},[g_{n_{S}}^{(m_{S})}]_{K}\\right)\\ |\\ G\\in\\mathcal{G}\\}.\\ \\mathrm{Using}$ these definitions, let us apply Lemma C.3 in (20) and get the following relation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re(\\ell\\circ\\mathcal{G}\\circ S)\\leq\\displaystyle\\frac{\\sqrt{2}\\beta}{S}\\mathbb{E}\\left[\\operatorname*{sup}_{G\\in\\mathcal{G}}\\sum_{s=1}^{S}\\sigma_{s k}[g_{n_{s}}^{(m_{s})}]_{k}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\sqrt{2}\\beta}{S}\\mathbb{E}\\left[\\operatorname*{sup}_{z\\in\\mathcal{L}\\circ S}\\sum_{i}^{S K}\\sigma_{i}[z]_{i}\\right]}\\\\ &{\\qquad\\qquad=\\sqrt{2}\\beta K\\displaystyle\\frac{1}{S K}\\mathbb{E}\\left[\\operatorname*{sup}_{z\\in\\mathcal{L}\\circ S}\\sum_{i=1}^{S K}\\sigma_{i}[z]_{i}\\right]=\\sqrt{2}\\beta K\\Re(\\mathcal{Z}\\circ S),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\beta$ is an upper bound of the Lipschitz constant of the cross entropy loss function ${\\mathsf{C E}}(\\pmb{g},y)=$ $\\begin{array}{r}{-\\sum_{k=1}^{K}\\mathbb{1}[y=k]\\log[g]_{k}}\\end{array}$ when $\\pmb{g}\\in\\Delta_{K}$ with $[{\\pmb x}]_{k}>(1/\\beta).\\,\\forall k$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we will characterize $\\Re({\\mathcal{Z}}\\circ S)$ using the covering number of the set $\\mathcal{Z}\\circ S$ . To define the covering number, let us first consider the concept of $\\epsilon$ -net covering of a set. The $\\epsilon$ -net covering of a set $\\mathcal{X}$ (denoted as $\\overline{{\\mathcal{X}}}_{\\epsilon})$ ) is defined as the finite subset of $\\mathcal{X}$ (i.e., $\\overline{{\\mathcal{X}}}_{\\epsilon}\\subseteq\\mathcal{X})$ such that for any $\\pmb{x}\\in\\mathcal{X}$ , there exists an $\\overline{{\\mathbf{\\boldsymbol{x}}}}\\in\\overline{{\\mathcal{X}}}_{\\epsilon}$ satisfying $\\|{\\pmb x}-{\\overline{{\\pmb x}}}\\|_{2}\\leq\\epsilon$ [82]. The smallest cardinality of the $\\epsilon$ -nets of $\\mathcal{X}$ is known as the covering number of $\\mathcal{X}$ , which is denoted as $\\overline{{\\mathsf{N}}}(\\epsilon,\\mathcal{X})$ . ", "page_idx": 19}, {"type": "text", "text": "Let us consider a pair of vectors $z_{S},\\overline{{z}}_{S}\\in\\mathcal{Z}\\circ S$ as below: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{\\mathcal{S}}=\\Big([g_{n_{1}}^{(m_{1})}]_{1},[g_{n_{1}}^{(m_{1})}]_{2},\\dots,[g_{n_{\\mathcal{S}}}^{(m_{\\mathcal{S}})}]_{K-1},[g_{n_{\\mathcal{S}}}^{(m_{\\mathcal{S}})}]_{K}\\Big)\\in\\mathbb{R}^{S K},\\;g_{n_{s}}^{(m_{s})}=A_{m_{s}}f(x_{n_{s}})+e_{n_{s}}^{(m_{s})},}\\\\ {\\overline{{z}}_{\\mathcal{S}}=\\Big([\\overline{{g}}_{n_{1}}^{(m_{1})}]_{1},[\\overline{{g}}_{n_{1}}^{(m_{1})}]_{2},\\dots,[\\overline{{g}}_{n_{\\mathcal{S}}}^{(m_{\\mathcal{S}})}]_{K-1},[\\overline{{g}}_{n_{\\mathcal{S}}}^{(m_{\\mathcal{S}})}]_{K}\\Big)\\in\\mathbb{R}^{S K},\\;\\overline{{g}}_{n_{s}}^{(m_{s})}=\\overline{{A}}_{m_{s}}\\overline{{f}}(x_{n_{s}})+\\overline{{e}}_{n_{s}}^{(m_{s})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\overline{{A}}_{m_{s}}$ belongs to the $\\epsilon$ -net covering of $\\boldsymbol{\\mathcal{A}}$ , $\\overline{{f}}$ belongs to the $\\epsilon$ -net covering of $\\mathcal{F}$ , and $\\overline{{e}}_{n_{s}}^{(m_{s})}$ is the appropriate sub-vector belonging to the $\\epsilon$ -net covering of $\\mathcal{E}_{0}$ . ", "page_idx": 19}, {"type": "text", "text": "Let us also consider the following definitions: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{S}=([A_{m_{1}}f(x_{n_{1}})]_{1},[A_{m_{1}}f(x_{n_{1}})]_{2},\\dots,[A_{m_{S}}f(x_{n_{S}})]_{K-1},[A_{m_{S}}f(\\pmb{x}_{n_{S}})]_{K})\\in\\mathbb{R}^{S K},}\\\\ &{\\overline{{u}}_{S}=\\big([\\overline{{A}}_{m_{1}}\\overline{{f}}(x_{n_{1}})]_{1},[\\overline{{A}}_{m_{1}}\\overline{{f}}(x_{n_{1}})]_{2},\\dots,[\\overline{{A}}_{m_{S}}\\overline{{f}}(x_{n_{S}})]_{K-1},[\\overline{{A}}_{m_{S}}\\overline{{f}}(\\pmb{x}_{n_{S}})]_{K}\\big)\\in\\mathbb{R}^{S K},}\\\\ &{e_{S}=\\Big([e_{n_{1}}^{(m_{1})}]_{1},[e_{n_{1}}^{(m_{1})}]_{2},\\dots,[e_{n_{S}}^{(m_{S})}]_{K-1},[e_{n_{S}}^{(m_{S})}]_{K}\\Big)\\in\\mathbb{R}^{S K},}\\\\ &{\\overline{{e}}_{S}=\\Big([\\overline{{e}}_{n_{1}}^{(m_{1})}]_{1},[\\overline{{e}}_{n_{1}}^{(m_{1})}]_{2},\\dots,[\\overline{{e}}_{n_{S}}^{(m_{S})}]_{K-1},[\\overline{{e}}_{n_{S}}^{(m_{S})}]_{K}\\Big)\\in\\mathbb{R}^{S K},}\\\\ &{e_{N M}=\\Big([e_{1}^{(1)}]_{1},[e_{1}^{(1)}]_{2},\\dots,[e_{N}^{(M)}]_{K-1},[e_{N}^{(M)}]_{K}\\Big)\\in\\mathbb{R}^{N M K},}\\\\ &{\\overline{{e}}_{N M}=\\Big([\\overline{{e}}_{1}^{(1)}]_{1},[\\overline{{e}}_{1}^{(1)}]_{2},\\dots,[\\overline{{e}}_{N}^{(M)}]_{K\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we have the following relation due to triangle inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{z}_{S}-\\overline{{\\boldsymbol{z}}}_{S}\\|_{2}\\leq\\|\\pmb{u}_{S}-\\overline{{\\pmb{u}}}_{S}\\|_{2}+\\|\\pmb{e}_{S}-\\overline{{\\pmb{e}}}_{S}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, consider the second term on the R.H.S. of (22): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|e_{S}-\\overline{{e}}_{S}\\|_{2}^{2}=\\sum_{s=1}^{S}\\|e_{n_{s}}^{(m_{s})}-\\overline{{e}}_{n_{s}}^{(m_{s})}\\|_{2}^{2}\\leq S\\|e_{N\\mathcal{M}}-\\overline{{e}}_{\\mathcal{N}\\mathcal{M}}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We also have the following relations for the first term on the R.H.S. of (22) : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|u_{s}-\\overline{{u}}_{s}\\|^{2}=\\displaystyle\\sum_{s=1}^{S}\\|A_{m_{s}}f(x_{n_{s}})-\\overline{{A_{m_{s}}}}\\,\\overline{{f}}(x_{n_{s}})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{s=1}^{S}\\|A_{m_{s}}f(x_{n_{s}})-\\overline{{A_{m_{s}}}}\\,f(x_{n_{s}})+\\overline{{A_{m_{s}}}}\\,f(x_{n_{s}})-\\overline{{A_{m_{s}}}}\\,\\overline{{f}}(x_{n_{s}})\\|_{2}^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{s=1}^{S}\\left(\\|A_{m_{s}}-\\overline{{A_{m_{s}}}}\\|_{\\mathbb{F}}\\|f(x_{n_{s}})\\|_{2}+\\|\\overline{{A_{m_{s}}}}\\|_{\\mathbb{F}}\\|f(x_{n_{s}})-\\overline{{f}}(x_{n_{s}})\\|\\right)^{2}}\\\\ &{\\qquad\\leq2\\displaystyle\\sum_{s=1}^{S}\\|A_{m_{s}}-\\overline{{A_{m_{s}}}}\\|_{\\mathbb{F}}^{2}\\|f(x_{n_{s}})\\|_{2}^{2}+2\\displaystyle\\sum_{s=1}^{S}\\|\\overline{{A_{m_{s}}}}\\|_{\\mathbb{F}}^{2}\\|f(x_{n_{s}})-\\overline{{f}}(x_{n_{s}})\\|^{2}}\\\\ &{\\qquad\\leq2\\displaystyle\\sum_{s=1}^{S}\\|A_{m_{s}}-\\overline{{A_{m_{s}}}}\\|_{\\mathbb{F}}^{2}+2K\\displaystyle\\sum_{s=1}^{S}\\|f(x_{n_{s}})-\\overline{{f}}(x_{n_{s}})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the above set of relations, the first inequality is by triangle inequality. The second inequality uses that fact that $(a+b)^{2}\\leq2a^{2}+2b^{2}$ . The third inequality uses the fact that the Frobenius norm of $\\overline{{\\mathbf{A}}}_{m}(\\pmb{x}_{n})$ \u2019s are bounded by $\\sqrt{K}$ and the $\\ell_{2}$ norm of $\\pmb{f}(\\pmb{x}_{n})$ is bounded by 1. Hence, combining (22)-(24), we get that in order to obtain an $\\varepsilon$ -net covering for the set $\\mathcal{Z}\\circ S$ (i.e., $\\|z-\\overline{{z}}\\|_{2}\\leq\\varepsilon)$ , we only need to show that there exists a $\\frac{\\varepsilon}{4\\sqrt{K}}$ -net covering for ${\\mathcal{F}}\\circ S$ , an $\\frac{\\varepsilon}{2\\sqrt{S}}$ -net covering for ${\\mathcal{E}}_{{\\mathcal{N M}}}$ , $\\forall m$ , and a4\u221a\u03b5S -net covering for each A\u2019s. Here we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr F}\\circ{\\mathscr S}=\\{[{\\pmb f}({\\pmb x}_{1}),\\dots,{\\pmb f}({\\pmb x}_{S})]\\in\\mathbb R^{K\\times S}\\ |\\ {\\pmb f}\\in{\\mathscr F},(m,n)\\in{\\mathscr S}\\},}\\\\ &{{\\mathscr E}_{N\\mathscr M}=\\{[{e}_{1}^{(1)\\top},\\dots,{e}_{N}^{(M)\\top}]^{\\top}\\in\\mathbb R^{K N M}\\ |\\ {e}_{n}^{(m)}\\in{\\mathscr E},\\|{\\pmb E}\\|_{0}\\leq C,\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{\\|E\\|_{0}\\,=\\,\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|e_{n}^{(m)}\\|_{2}>0\\right\\}}\\end{array}$ . Note that the full rank matrix $\\pmb{A}_{m}\\,\\in\\,\\mathbb{R}^{K\\times K}$ can be represented as a $K^{2}$ -dimensional vector whose Euclidean norm is bounded by $\\sqrt{K}$ . Hence, the cardinality of the $\\frac{\\varepsilon}{4\\sqrt{S}}$ -net covering for $A_{m}\\in\\mathbb{R}^{K\\times K}$ is at most $\\left(\\frac{8K\\sqrt{S K}}{\\varepsilon}\\right)^{K^{2}}$ [80]. We consider the covering number corresponding to the function class ${\\mathcal{F}}\\circ S$ . Towards this, we invoke the following lemma: ", "page_idx": 20}, {"type": "text", "text": "Lemma C.4. [51, Theorem $3.3J$ Let fixed nonlinearities $\\left(\\sigma_{1},\\ldots,\\sigma_{L}\\right)$ and reference matrices $(M_{1},\\dots,M_{L})$ be given, where $\\sigma_{i}$ is $\\rho_{i}$ -Lipschitz and $\\sigma_{i}(0)~=~0$ . Let spectral norm bounds $\\left(s_{1},\\ldots,s_{L}\\right),$ , and matrix (2,1)-norm bounds $b_{1},\\ldots,b_{L}$ be given. Let $\\mathbf{\\Delta}x_{1},\\dots,\\mathbf{\\Delta}x_{S}$ be the $S$ number of data items. Let h belongs to the following function class: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}\\triangleq\\{h:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{K}\\mid h(x)=\\sigma_{L}(C_{1}\\sigma_{L-1}(C_{L-1}\\ldots\\sigma_{1}(C_{1}(x)\\ldots)),\\forall x\\in\\mathbb{R}^{D}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{i}\\in\\mathbb{R}^{d_{i}\\times d_{i-1}}$ with $d_{0}=D$ and $d_{L}=K$ . Let $\\mathcal{H}\\circ S$ denote the family of matrices obtained by evaluating the data items with all choices of network $\\mathcal{H}$ : ", "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{H}\\circ S\\triangleq\\{[h(x_{1}),\\dots,h(x_{S})]\\mid\\mathcal{C}=(C_{1},\\dots,C_{L}),\\|C_{i}\\|_{2}\\leq s_{i},\\|C_{i}-M_{i}\\|_{2,1}\\leq b_{i},h\\in\\mathcal{H}\\},}\\end{array}$ Then for any $\\epsilon>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log\\overline{{\\mathsf{N}}}(\\epsilon,\\mathcal{H}\\circ\\mathcal{S})\\leq\\frac{\\|\\pmb{X}\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{H}}}{\\epsilon^{2}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{X\\ =\\ [x_{1},\\ldots,x_{S}],\\ \\mathcal{R}_{\\mathcal{H}}\\ =\\ \\log(2H^{2})\\left(\\prod_{j=1}^{L}s_{j}^{2}\\rho_{j}^{2}\\right)\\left(\\sum_{L}^{i}(b_{i}/s_{i})^{2/3}\\right)^{3}}\\end{array}$ is called as the spectral-complexity upper bound of neural network function class $\\mathcal{H}$ and $H=\\operatorname*{max}_{\\ell}d_{\\ell}$ . ", "page_idx": 21}, {"type": "text", "text": "Using Lemma C.4, we get the cardinality of the $\\frac{\\varepsilon}{8\\sqrt{S K}}$ -net covering for $\\mathcal{F}\\circ\\mathcal{N}$ as below: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overline{{\\mathsf{N}}}\\left(\\frac{\\varepsilon}{8\\sqrt{S K}},\\mathcal{F}\\circ S\\right)\\leq\\exp\\left(\\frac{16K\\|X_{S}\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{F}}}{\\varepsilon^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{X}_{\\mathcal{S}}=[\\pmb{x}_{1},\\dots,\\pmb{x}_{S}]\\in\\mathbb{R}^{d\\times S}}\\end{array}$ and the parameter $\\mathcal{R}_{\\mathcal{F}}$ the spectral-complexity upper bound of $\\mathcal{F}$ . ", "page_idx": 21}, {"type": "text", "text": "Next, we proceed to get the cardinality of the $\\frac{\\varepsilon}{2\\sqrt{S}}$ -net covering for ${\\mathcal{E}}_{{\\mathcal{N M}}}$ . Here, ${\\mathcal{E}}_{{\\mathcal{N M}}}$ is a set of sparse vectors with the number of non-zero elements upper-bounded by $C M K$ , i.e., for any $e_{\\mathcal{N M}}\\in\\mathcal{E}_{\\mathcal{N M}}$ , $\\lVert e_{\\mathcal{N M}}\\rVert_{0}\\leq C M K$ . To get the covering number of a set of sparse vectors, we invoke the following result: ", "page_idx": 21}, {"type": "text", "text": "Lemma C.5. [83] Let $\\mathcal{X}=\\{z\\in\\mathbb{R}^{N}|\\|z\\|_{2}\\leq q,\\|z\\|_{0}\\leq C\\}$ . Suppose that $C\\leq\\frac{N}{2}$ . Then, there exists an $\\epsilon$ -net covering denoted as $\\overline{{\\mathcal{X}}}_{\\epsilon}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overline{{\\mathsf{N}}}(\\epsilon,\\mathcal{X})\\leq\\left(\\frac{e N}{C}\\frac{q}{\\epsilon}\\right)^{C},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where e is the Euler\u2019s number. ", "page_idx": 21}, {"type": "text", "text": "Applying Lemma C.5, we get the cardinality of $\\frac{\\varepsilon}{2\\sqrt{S}}$ -net covering of of $\\mathcal{E}_{\\mathcal{N M}}$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathsf{N}}}\\left(\\frac{\\varepsilon}{2\\sqrt{S}},\\varepsilon_{N\\mathcal{M}}\\right)\\leq\\left(\\frac{e N M K}{C M K}\\frac{4C M\\sqrt{S}}{\\varepsilon}\\right)^{C M K}}\\\\ &{\\qquad\\qquad=\\left(\\frac{4e N M\\sqrt{S}}{\\varepsilon}\\right)^{C M K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that in (25), we used the result that $\\|e_{\\mathcal{N}\\mathcal{M}}\\|_{2}\\leq\\sqrt{2C M}\\leq2C M,\\forall e_{\\mathcal{N}\\mathcal{M}}\\in\\mathcal{E}_{\\mathcal{N}\\mathcal{M}}.$ ", "page_idx": 21}, {"type": "text", "text": "Using the covering number results, the cardinality of the $\\varepsilon$ -net covering of set $\\mathcal{Z}\\circ S$ is bounded by the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\overline{{\\mathsf{N}}}(\\varepsilon,\\mathcal{Z}\\circ\\mathcal{S})\\leq\\exp\\left(\\frac{16K\\|X_{S}\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{F}}}{\\varepsilon^{2}}\\right)\\times\\left(\\frac{4e N M\\sqrt{S}}{\\varepsilon}\\right)^{C M K}\\times\\left(\\frac{8K\\sqrt{S K}}{\\varepsilon}\\right)^{K^{2}}}\\\\ &{}&{=\\exp\\left(\\frac{16K\\|X_{S}\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{F}}}{\\varepsilon^{2}}+C M K\\log\\frac{4e N M\\sqrt{S}}{\\varepsilon}+K^{2}\\log\\left(\\frac{8K\\sqrt{S K}}{\\varepsilon}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now that we have characterized $\\overline{{\\mathsf{N}}}(\\varepsilon,\\mathcal{Z}\\circ S)$ , we invoke the Dudley entropy chaining technique lemma to obtain the Rademacher complexity $\\Re(\\mathcal{Z}\\circ S)$ [80]: ", "page_idx": 21}, {"type": "text", "text": "Lemma C.6. Consider set $A\\subseteq\\mathbb{R}^{m}$ , let $\\begin{array}{r}{c=\\operatorname*{min}_{\\overline{{\\alpha}}}\\operatorname*{max}_{\\pmb{a}\\in A}{\\|\\overline{{\\alpha}}-\\pmb{a}\\|}}\\end{array}$ . Then for any integer $T>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Re(A)\\leq\\frac{c2^{-T}}{\\sqrt{m}}+\\frac{6c}{m}\\sum_{t=1}^{T}2^{-t}\\sqrt{\\log\\overline{{\\mathsf{N}}}(c2^{-t},A)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|z\\|\\leq{\\sqrt{S}},\\quad\\forall z\\in{\\mathcal{Z}}\\circ S,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "applying Lemma C.6 on set $\\mathcal{Z}\\circ S$ gives ", "page_idx": 22}, {"type": "text", "text": "R(Z \u25e6S) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{\\sqrt{S}2^{-T}}{\\sqrt{S K}}+\\frac{6\\sqrt{S}}{S K}\\frac{T}{t=1}2^{-t}\\sqrt{\\log(\\sqrt{S}2^{-t},Z\\circ S)}}\\\\ &{\\leq\\frac{2^{-T}}{\\sqrt{K}}+\\frac{6}{K\\sqrt{S}}\\frac{\\sum}{t=1}^{2}\\sqrt{\\frac{16K\\|X_{\\delta}\\|_{\\tilde{F}}^{2}\\mathcal{B}_{F}}{S A^{-t}}+C M K\\log\\frac{4e N M}{2^{-t}}+K^{2}\\log\\left(\\frac{8K\\sqrt{K}}{2^{-t}}\\right)}}\\\\ &{\\leq\\frac{2^{-T}}{\\sqrt{K}}+\\frac{6}{K\\sqrt{S}}\\frac{T}{t=1}\\sqrt{\\frac{16K\\|X_{\\delta}\\|_{\\tilde{F}}^{2}\\mathcal{B}_{F}}{S}+4^{-t}\\left(C M K\\log\\frac{4e N M}{2^{-t}}+K^{2}\\log\\left(\\frac{8K\\sqrt{K}}{2^{-t}}\\right)\\right)}}\\\\ &{\\leq\\frac{2^{-T}}{\\sqrt{K}}+\\frac{6}{K\\sqrt{S}}\\frac{T}{t=1}\\sqrt{\\frac{16K\\|X_{\\delta}\\|_{\\tilde{F}}^{2}\\mathcal{B}_{F}}{S}+C M K\\log\\left(4e N M\\right)+K^{2}\\log\\left(8K\\sqrt{K}\\right)}}\\\\ &{=\\frac{2^{-T}}{\\sqrt{K}}+\\frac{6T}{K\\sqrt{S}}\\sqrt{\\frac{16K\\|X_{\\delta}\\|_{\\tilde{F}}^{2}\\mathcal{B}_{F}}{S}+C M K\\log\\left(4e N M\\right)+K^{2}\\log\\left(8K\\sqrt{K}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Choosing $T=\\log S/(2\\log2)$ , we get the bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re(\\mathcal{Z}\\circ\\mathcal{S})\\leq\\frac{1}{\\sqrt{S K}}}\\\\ &{\\,+\\,\\frac{6\\log S}{2K\\sqrt{S}\\log2}\\sqrt{\\frac{16K\\|X_{S}\\|_{\\mathrm{F}}^{2}\\mathcal{R}_{\\mathcal{F}}}{S}+C M K\\log\\left(4e N M\\right)+K^{2}\\log\\left(8K\\sqrt{K}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the upper bound of $\\Re({\\mathcal{Z}}\\circ S)$ given by (27) with the upper bound of $\\Re(\\ell\\circ\\mathcal{G}\\circ S)$ as given by (21) and with the results in (18) and (19), we get that with probability greater than $1-\\delta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Big|\\mathsf{L}_{\\mathcal{D}}(G,\\widehat{\\mathcal{Y}})-\\mathsf{L}_{\\mathcal{S}}(G;\\widehat{\\mathcal{Y}})\\Big|\\le2\\sqrt{2}\\beta K\\Re(\\mathcal{Z}\\circ S)+4\\log(\\beta)\\sqrt{\\frac{2\\log(4/\\delta)}{S}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\Re_{S}(\\mathcal{Z}\\circ S)$ is upper bounded by (27). ", "page_idx": 22}, {"type": "text", "text": "The above relation, combined with (16), implies that with probability greater than $1-2\\delta$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac1{N M}\\sum_{n=1}^{N}\\sum_{m=1}^{M}\\|g_{n}^{\\tt t}(m)-\\widehat g_{n}^{(m)}\\|_{2}^{2}\\le4\\sqrt{2}\\beta K\\Re(\\mathcal{Z}\\circ\\mathcal{S})+8\\log(\\beta)\\sqrt{\\frac{2\\log(4/\\delta)}{S}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\Re({\\mathcal{Z}}\\circ S)$ is upper-bounded in (27). ", "page_idx": 22}, {"type": "text", "text": "D Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To obtain the results in Theorem 3.5, we first show that the criterion correctly distinguishes the instance-dependent and instance-independent samples, i.e., $\\widehat{\\mathcal{Z}}\\,=\\,\\mathcal{Z}$ where we define $\\widehat{\\mathcal{I}}\\:\\triangleq$ $\\{i\\in[N]\\mid{\\widehat{e}}_{i}\\neq\\mathbf{\\bar{0}}\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Recall $\\widehat{A}_{m}\\,{}^{\\circ}\\mathrm{s},\\widehat{e}_{n}^{(m)}$ \u2019s and $\\hat{\\pmb f}$ are defined as in Theorem 3.5, and let $\\boldsymbol{\\widehat{e}}_{n}=[(\\boldsymbol{\\widehat{e}}_{n}^{(1)})^{\\top},\\dots,(\\boldsymbol{\\widehat{e}}_{n}^{(M)})^{\\top}]^{\\top}$ . We first show that $\\widehat{\\mathcal{T}}^{c}\\subseteq\\mathcal{Z}^{c}$ . From Lemma 3.1, the following holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|W^{\\natural}F^{\\natural}+E^{\\natural}-\\widehat W\\widehat F-\\widehat E\\|_{\\mathrm{F}}\\leq\\epsilon_{g}(S)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then the inequality also holds for any submatrix of the left hand side matrix. In particular, for $i\\in\\mathcal{Z}$ and $S>S_{0}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(S_{0})>\\epsilon_{g}(S)}\\\\ &{\\quad\\quad\\geq\\|W^{\\sharp}F^{\\sharp}(\\cdot,\\{i\\}\\cap{\\mathcal T}^{c}\\cap\\widehat{\\mathcal Z}^{c})+{\\cal E}^{\\sharp}(\\cdot,\\{i\\}\\cap{\\mathcal T}^{c}\\cap\\widehat{\\mathcal Z}^{c})-\\widehat{W}\\widehat{F}(\\cdot,\\{i\\}\\cap{\\mathcal T}^{c}\\cap\\widehat{\\mathcal Z}^{c})-\\widehat{E}(\\cdot,\\{i\\}\\cap{\\mathcal T}^{c})}\\\\ &{\\quad\\quad=\\|W^{\\sharp}[f_{i}^{\\sharp},F^{\\sharp}(\\cdot,\\mathcal{Z}^{c}\\cap\\widehat{\\mathcal Z}^{c})]+[e_{i}^{\\sharp},\\mathbf{0},\\ldots,\\mathbf{0}]-\\widehat{W}[\\widehat{f}_{i},\\widehat{F}(\\cdot,\\mathcal{Z}^{c}\\cap\\widehat{\\mathcal Z}^{c})]-[\\widehat{e}_{i},\\mathbf{0},\\ldots,\\mathbf{0}]\\|_{\\mathrm{F}}}\\\\ &{\\quad\\geq\\|W^{\\sharp}[f_{i}^{\\sharp},F^{\\sharp}(\\cdot,\\mathcal{Z}^{c}\\cap\\widehat{\\mathcal Z}^{c})]+[e_{i}^{\\sharp},\\mathbf{0},\\ldots,\\mathbf{0}]-\\widehat{W}[\\widehat{f}_{i},\\widehat{F}(\\cdot,\\mathcal{Z}^{c}\\cap\\widehat{\\mathcal Z}^{c})]\\|_{\\mathrm{F}}-\\|[\\widehat{e}_{i},\\mathbf{0},\\ldots,\\mathbf{0}]\\|_{\\mathrm{F}}}\\\\ &{\\quad\\geq\\kappa(e_{i}^{\\sharp})-\\|[\\widehat{e}_{i},\\mathbf{0},\\ldots,\\mathbf{0}]\\|_{\\mathrm{F}}\\quad(\\mathrm{by~Definition~}3.3)}\\\\ &{\\quad\\geq\\epsilon_{g}(S_{0})-\\|[\\widehat{e}_{i},\\mathbf{0},\\ldots,\\mathbf{0}]\\|_{\\mathrm{F}}\\quad(\\mathrm{by~the~assumption~in~Theorem~}3.5).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, $\\widehat{e}_{i}\\;\\neq\\;{\\bf0}$ and that concludes $\\widehat{\\mathcal{T}}^{c}\\ \\subseteq\\ \\mathcal{T}$ . On the other hand, since $\\widehat{e}_{n}^{(m)}$ \u2019s minimizes $\\begin{array}{r}{\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|\\widehat{e}_{n}^{(m)}\\|_{2}>0\\right\\}}\\end{array}$ , it holds that $\\mathcal{T}^{c}\\subseteq\\widehat{\\mathcal{Z}}^{c}$ . This implies that $\\mathcal{Z}^{c}=\\widehat{\\mathcal{Z}}^{c}$ must hold. ", "page_idx": 23}, {"type": "text", "text": "With Ic =Ic, we have the following relation from (30): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{M N}\\|W^{\\dagger}F^{\\dagger}(\\cdot,\\mathcal{Z}^{c})-\\widehat{W}\\widehat{F}(\\cdot,\\mathcal{Z}^{c})\\|_{\\mathrm{F}}^{2}\\leq4\\sqrt{2}\\beta K\\Re(\\mathcal{Z}\\circ\\mathcal{S})+8\\log(\\beta)\\sqrt{\\frac{2\\log(4/\\delta)}{S}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To proceed, we invoke a result from the work [31] considering Assumption 3.4 satisfied, and the set   \n$\\mathcal{Z}^{c}$ is uniformly chosen from $[N]$ , with probability at least $1-\\frac{2}{S}-\\frac{2\\bar{K}}{N^{\\alpha}}$ ,   \n$\\underset{\\gamma\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{min}_{\\Pi}\\lVert\\hat{f}(x)-\\Pi f^{(\\natural}(x)\\rVert_{2}^{2}\\right]\\leq\\frac{c_{3}(\\zeta+\\sqrt{T K\\kappa})^{2}}{T(1-\\sqrt{K}\\kappa)^{2}}+64T^{-5/8}(2\\lVert X\\rVert_{\\mathrm{F}}\\mathcal{R}_{\\mathcal{F}})^{0.25}+16\\sqrt{\\frac{2\\log(4S)}{N}}\\right]$   \nwhere $\\begin{array}{r l}&{\\zeta^{2}=4\\sqrt{2}\\beta K\\Re(\\mathcal{L}\\circ S)+8\\log(\\beta)\\sqrt{\\frac{2\\log(4S)}{S}}=\\mathcal{O}\\left(\\frac{\\beta\\Re_{S}\\log S}{\\sqrt{S}}+\\log\\left(\\beta\\right)\\frac{\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{S}}\\right),}\\\\ &{\\varphi^{2}=M T^{\\alpha}\\left(4\\sqrt{2}\\beta K\\Re(\\mathcal{L}\\circ S)+8\\log(\\beta)\\sqrt{\\frac{2\\log(4S)}{S}}+4/S\\right)}\\\\ &{\\kappa=\\varphi+\\xi_{1}+\\xi_{2}+\\sqrt{K}\\xi_{1}\\xi_{2},}\\\\ &{X=\\left[x_{n},\\cdots,x_{n}\\right],\\quad\\mathrm{where~}(m_{s},n_{s})\\in\\mathcal{S},}\\\\ &{\\Re_{S}^{2}=K^{2}\\log\\left(K\\right)+\\frac{K\\left\\Vert X\\right\\Vert_{F}^{2}\\mathcal{R}_{F}}{S}+C M K\\log\\sqrt{N M},}\\\\ &{\\quad T=N-\\left\\vert\\mathcal{L}\\right\\vert.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "E Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The proof of Theorem 3.6 follows similar steps in finite case presented in Theorem 3.5, except that we have the exact recovery of $\\hat{G}$ when $S\\to\\infty$ . We repeat the derivation here for completeness. ", "page_idx": 23}, {"type": "text", "text": "We first show that the criterion correctly distinguishes the instance-dependent and instanceindependent labels. ", "page_idx": 23}, {"type": "text", "text": "Let $\\hat{A}_{m}$ \u2019s , e (nm)\u2019s and f ) be an optimal solutions as state in Theorem E. From Lemma (3.1), the follo wing holds as the number of observations grows infinitely large: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{g}}_{n}^{(m)}=\\widehat{A}_{m}\\widehat{f}(\\pmb{x}_{n})+\\widehat{\\pmb{e}}_{n}^{(m)}=\\pmb{g}_{n}^{\\sharp}(m)=A_{m}^{\\sharp}f^{\\sharp}(\\pmb{x}_{n})+e_{n}^{\\sharp(m)},\\forall n,m.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consider the following relation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\boldsymbol{G}^{\\sharp}=\\left[\\begin{array}{c c c c}{g_{1}^{\\sharp(1)}}&{\\cdot\\cdot\\cdot}&{g_{N}^{\\sharp(1)}}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {g_{1}^{\\sharp(M)}}&{\\cdot\\cdot}&{g_{N}^{\\sharp(M)}}\\end{array}\\right]=\\left[\\begin{array}{c c c c}{A_{1}^{\\sharp}f^{\\sharp}(x_{1})+e_{1}^{\\sharp(1)}}&{\\cdot\\cdot\\cdot}&{A_{1}^{\\sharp}f^{\\sharp}(x_{N})+e_{N}^{\\sharp(1)}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {A_{M}^{\\sharp}f^{\\sharp}(x_{1})+e_{1}^{\\sharp(M)}}&{\\cdot\\cdot\\cdot}&{A_{M}^{\\sharp}f^{\\sharp}(x_{N})+e_{N}^{\\sharp(M)}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, each column of the matrix $G^{\\natural}$ is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{[G^{\\sharp}]_{n}=}&{\\underbrace{\\left[\\begin{array}{c}{A_{1}^{\\sharp}}\\\\ {\\vdots}\\\\ {A_{M}^{\\sharp}}\\end{array}\\right]}_{W^{\\sharp}\\in\\mathbb{R}^{M K\\times K}}f^{\\sharp}({\\pmb x}_{n})+\\underbrace{\\left[\\begin{array}{c}{e_{n}^{\\sharp(1)}}\\\\ {\\vdots}\\\\ {e_{n}^{\\sharp(M)}}\\end{array}\\right]}_{e_{n}^{\\sharp}\\in\\mathbb{R}^{M K}}}\\\\ {\\implies[G^{\\sharp}]_{n}=W^{\\sharp}f^{\\sharp}({\\pmb x}_{n})+e_{n}^{\\sharp},\\forall n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, we can also define ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{W}}=\\left[\\begin{array}{c}{\\widehat{\\pmb{A}}_{1}}\\\\ {\\vdots}\\\\ {\\dot{\\widehat{A}}_{M}}\\end{array}\\right],\\;\\widehat{\\pmb{e}}_{n}=\\left[\\begin{array}{c}{\\widehat{\\pmb{e}}_{n}^{(1)}}\\\\ {\\vdots}\\\\ {\\widehat{\\pmb{e}}_{n}^{(M)}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From (32), we get that ", "page_idx": 24}, {"type": "equation", "text": "$$\n[G^{\\natural}]_{n}=\\widehat{W}\\widehat{f}({\\pmb x}_{n})+\\widehat{{\\pmb e}}_{n},\\forall n.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We will use contradiction-based argument towards this. Let us start by considering the case where there exists $i^{\\star}\\in\\mathcal{T}^{c}$ as well as $i^{\\star}\\in\\widehat{\\mathcal{Z}}^{c}$ holds. Then, combining (34) and (35) we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{W}\\widehat{f}(x_{i^{\\star}})=W^{\\natural}f^{\\natural}(x_{i^{\\star}}),\\forall i^{\\star}\\in\\mathcal{T}^{c}\\cap\\widehat{\\mathcal{T}}^{c}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can rewrite the relation (36) as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{W}\\widehat{F}(:,\\mathcal{Z}^{c}\\cap\\widehat{\\mathcal{Z}}^{c})=W^{\\natural}F^{\\natural}(:,\\mathcal{Z}^{c}\\cap\\widehat{\\mathcal{Z}}^{c}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since any submatrix formed by $N-2|Z^{c}|$ columns of $F^{\\natural}=[{\\pmb f}({\\pmb x}_{1}),\\dots,{\\pmb f}({\\pmb x}_{N})]$ has rank $K$ and that ran $:(\\pmb{W}^{\\sharp})=K$ , we get that there exists a nonsingular matrix $Q\\in\\mathbb{R}^{K\\times K}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\widehat{\\pmb{W}}}={\\pmb{W}}^{\\natural}{\\pmb{Q}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we consider the case where there exists $i^{\\star}\\in\\mathcal{T}$ and $i^{\\star}\\in\\widehat{\\mathcal{Z}}^{c}$ . Then, again from (34), (35) we get that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{W}\\widehat{f}(x_{i^{\\star}})=W^{\\natural}f^{\\natural}(x_{i^{\\star}})+e_{i^{\\star}}^{\\natural}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "However, by applying the relation (37) in (38), we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W^{\\natural}Q\\widehat{f}(x_{i^{\\star}})=W^{\\natural}f^{\\natural}(x_{i^{\\star}})+e_{i^{\\star}}^{\\natural}}\\\\ {\\implies W^{\\natural}Q\\widehat{f}(x_{i^{\\star}})-W^{\\natural}f^{\\natural}(x_{i^{\\star}})=e_{i^{\\star}}^{\\natural}}\\\\ {\\implies W^{\\natural}q=e_{i^{\\star}}^{\\natural},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\pmb q=Q\\hat{\\pmb f}(\\pmb x_{i^{\\star}})-\\pmb f^{\\natural}(\\pmb x_{i^{\\star}})$ is a nonzero vector since it is assumed that $e_{i^{\\star}}^{\\natural}\\neq\\mathbf{0}$ . The above relation implies that $e_{i^{\\star}}^{\\natural}\\in\\mathrm{range}(W^{\\natural})$ must hold which is a contradiction to the assumption that $e_{i^{\\star}}^{\\natural}\\notin\\mathrm{range}(W^{\\natural})$ when $i^{\\star}\\in\\mathcal{T}$ . Hence, the relation (38) is feasible only if $e_{i^{\\star}}^{\\natural}=\\mathbf{0}$ . It means that if there exists $i^{\\star}\\in\\widehat{\\mathcal{Z}}^{c}$ , we should have $i^{\\star}\\in\\mathcal{T}^{c}$ , leading to the conclusion that $\\widehat{\\mathcal{T}}^{c}\\subseteq\\mathcal{Z}^{c}$ . ", "page_idx": 24}, {"type": "text", "text": "On the other hand, if $\\widehat{\\mathcal{Z}}^{c}\\subset\\mathcal{Z}^{c}$ holds, then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|\\widehat{e}_{n}^{(m)}\\|_{2}>0\\right\\}>\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|e_{n}^{\\natural(m)}\\|_{2}>0\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\widehat{e}_{n}^{(m)}$ \u2019s are the optimal solution of Problem (9) with minimal $\\begin{array}{r}{\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|\\widehat{\\boldsymbol{e}}_{n}^{(m)}\\|_{2}>0\\right\\}}\\end{array}$ , the following should hold ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|\\widehat{e}_{n}^{(m)}\\|_{2}>0\\right\\}\\leq\\sum_{n=1}^{N}\\mathbb{1}\\left\\{\\sum_{m=1}^{M}\\|e_{n}^{\\natural(m)}\\|_{2}>0\\right\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is a contradiction. This implies that $\\mathcal{I}=\\widehat{\\mathcal{I}}$ must hold. ", "page_idx": 25}, {"type": "text", "text": "Since $\\mathcal{I}=\\widehat{\\mathcal{Z}}$ holds, we have the following relation from (32): ", "page_idx": 25}, {"type": "equation", "text": "$$\nG^{\\natural}(:,\\mathcal{Z}^{c})=\\widehat{G}(:,\\mathcal{Z}^{c}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we utilized the following notations: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{G}(:,\\mathbb{Z}^{c})=\\widehat{W}\\widehat{F}(:,\\mathbb{Z}^{c}),}\\\\ &{G^{\\sharp}(:,\\mathbb{Z}^{c})=W^{\\sharp}F^{\\sharp}(:,\\mathbb{Z}^{c}),}\\\\ &{\\qquad\\widehat{g}_{i}=[g_{i}^{(1)\\top},\\dots,g_{i}^{(M)\\top}]^{\\top},i\\in\\mathbb{Z}^{c},}\\\\ &{W^{\\sharp}=[A_{1}^{\\top_{\\sharp}},\\dots,A_{M}^{\\top_{\\sharp}}]^{\\top}}\\\\ &{\\widehat{W}=[\\widehat{A_{1}},\\dots,\\widehat{A_{M}^{\\top}}]^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we connect the $\\operatorname*{det}(\\boldsymbol{W}^{\\top}\\boldsymbol{W})$ term to our optimal solutions. This part is similar to the classical proofs in [84,85], with proper modifications. Consider the following result extracted from the proof of [84, Theorem 1]: ", "page_idx": 25}, {"type": "text", "text": "Lemma E.1. Suppose a matrix $\\boldsymbol{Y}\\in\\mathbb{R}^{K\\times J}$ satisfies $\\mathbf{\\nabla}Y\\geq\\mathbf{0}$ , $\\mathbf{1}^{\\top}\\mathbf{Y}=\\mathbf{1}^{\\top}$ , $\\mathsf{r a n k}(\\pmb{Y})=K$ , and SSC. Then, for any $\\widehat{Y}=Q Y$ satisfying $\\widehat{\\boldsymbol{Y}}\\geq\\mathbf{0}$ , $\\mathbf{1}^{\\top}\\widehat{\\mathbf{Y}}=\\mathbf{1}^{\\top}$ , the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\operatorname*{det}(Q)|\\leq1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The equality holds only if $Q$ is a permutation matrix. ", "page_idx": 25}, {"type": "text", "text": "Since $\\widehat{W}$ is the optimal solution of the criterion given by Theorem 3.6, the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\widehat{\\pmb{W}}^{\\top}\\widehat{\\pmb{W}})\\leq\\operatorname*{det}(\\pmb{W}^{\\sharp\\top}\\pmb{W}^{\\sharp}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We also have $W^{\\natural}$ and $F^{\\natural}(:,\\mathcal{Z}^{c})$ are of rank $K$ . Hence, from (39), we get that $\\widehat{W}$ and $\\widehat{\\pmb{F}}(:,\\!\\mathcal{Z}^{c})$ satisfies $\\widehat{\\pmb{W}}=\\pmb{W}^{\\natural}\\pmb{Q}^{-1}$ , $\\widehat{\\pmb{F}}(:,\\mathcal{T}^{c})=Q\\boldsymbol{F}^{\\natural}(:,\\mathcal{T}^{c})$ , for a certain invertible matrix $Q$ . Let us assume that $Q$ is not a permutation matrix, Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}(\\boldsymbol{W}^{\\flat\\intercal}\\boldsymbol{W}^{\\flat})=\\operatorname*{det}(\\boldsymbol{Q}^{\\intercal}\\widehat{\\boldsymbol{W}}^{\\intercal}\\widehat{\\boldsymbol{W}}\\boldsymbol{Q})}\\\\ &{\\qquad\\qquad\\qquad=|\\operatorname*{det}(\\boldsymbol{Q})|^{2}\\operatorname*{det}(\\widehat{\\boldsymbol{W}}^{\\intercal}\\widehat{\\boldsymbol{W}})}\\\\ &{\\qquad\\qquad\\qquad<\\operatorname*{det}(\\widehat{\\boldsymbol{W}}^{\\intercal}\\widehat{\\boldsymbol{W}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality is from Lemma E.1 using the SSC condition on $F^{\\natural}(:,\\mathcal{Z}^{c})$ . Note that the result is a contradiction from (40). Hence, $Q$ must be a permutation matrix. This implies that the optimal solution $\\widehat{W}$ and $\\widehat{\\mathbf{F}}(:,\\!\\mathcal{L}^{c})$ satisfies the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widehat{W}=W^{\\natural}\\Pi,}}\\\\ {{\\widehat{F}(:,\\mathcal{T}^{c})=\\Pi^{\\top}F^{\\natural}(:,\\mathcal{T}^{c}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "F Proof of Fact 2.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For $\\mathsf{r a n k}(A^{\\natural})=K$ , when $N\\rightarrow\\infty$ , the objective in (6) seeks the optimal solutions $A^{\\star},f^{\\star}$ and $\\boldsymbol{e}_{n}^{\\star}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\nA^{\\sharp}f^{\\sharp}({\\pmb x})+e_{n}^{\\sharp}=A^{\\star}f^{\\star}({\\pmb x}_{n})+e_{n}^{\\star},\\quad n\\in[N].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This can be seen from Lemma 3.1 with $M=1$ . One solution that satisfies the equality (42) and the sparsity constraint in (6) is $A^{\\star}=I,f^{\\star}({\\pmb x})=A^{\\natural}f^{\\natural}(\\pmb x_{n})+e_{n}^{\\natural}$ , and $e_{n}^{\\star}=\\mathbf{0}$ , which can always hold when $\\pmb{f}$ is a universal function approximator. ", "page_idx": 25}, {"type": "table", "img_path": "HTLJptF7qM/tmp/761531cd26104f8991446686f39bb4043b678e09ffa1006b4844dae5fea387c7.jpg", "table_caption": ["Table 4: Noise levels and methods used to train machine annotators "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "HTLJptF7qM/tmp/fd1aecfd1cd93c487c3d1da0b6379e3424d9e590a2b69e5c77b2274a0a527fe7.jpg", "table_caption": ["Table 5: Average classification accuracy on CIFAR-10 using synthetic annotators over 3 random trials. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G More Details on Experiment Settings and Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "G.1 Implementation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We train with batch size of 512, number of epochs 200, Adam optimizer with learning rate of 0.01 and learning rate scheduler OneCycleLR [86]. We initialize $\\pmb{A}_{m}$ using an identity matrix going through softmax and $e_{m}=\\mathbf{0}$ for all $m\\in[M]$ . We also apply standard data augmentation including random cropping, random filpping, when training on CIFAR-10, CIFAR-10N, and STL-10. The experiment results are averaged over 3-5 random trials. All runs have been conducted using either Nvidia A40 or Nvidia DGX H100 GPU. Each run of COINNet on CIFAR-10 takes about 60 minutes using Nvidia A40, and 30 minutes using Nvidia DGX H100 GPU. ", "page_idx": 26}, {"type": "text", "text": "G.2 Machine Annotations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For machine annotator experiments, we consider the CIFAR-10 [62] and the STL-10 datasets [63]. The CIFAR-10 dataset consists of 60, 000 labeled color images of animals, vehicles and so on, each having a size of $32\\times32$ and belonging $K=10$ different classes. The images are split into training and testing sets with size 50,000 and 10,000, respectively. The STL-10 dataset consists of 13,000 labeled images from 10 different classes, similar to CIFAR-10. Of these, 5,000 images are designated for training and 8,000 for testing. ", "page_idx": 26}, {"type": "text", "text": "Table 4 shows the details regarding the individual label noise rates and training methods in our experiments with machine annotators. ", "page_idx": 26}, {"type": "text", "text": "G.3 Real Annotations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For real annotation experiments, we use 2 popular public datasets: CIFAR-10N [66] and LabelMe [67, 68], and construct the ImageNet-15N dataset with a higher number of annotators. All three datasets are labeled by anonymous annotators from the AMT platform. The CIFAR-10N is a \u201cnoisy\" version of the popular CIFAR-10 dataset, containing 50,000 images for training, and a separate ", "page_idx": 26}, {"type": "image", "img_path": "HTLJptF7qM/tmp/7f23963bc6d2a5dc63869152725bd413520595c2c7668d7f4b3baf1e2d92f2c8.jpg", "img_caption": ["Figure 4: Performance of COINNet on CIFAR-10 with synthetic labels against different number of annotators; left: $\\tau=0.2,\\eta=0.1.$ , right: $\\tau=0.4,\\eta=0.1$ . "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "10,000 images for testing. We randomly split the training set into 47,500 and 2,500 to use as train and validation set for all methods. Every image in CIFAR-10N has 3 labels. ", "page_idx": 27}, {"type": "text", "text": "The LabelMe dataset comprises 2,688 images from eight distinct classes: highway, inside city, tall building, street, forest, coast, mountain, and open country. Out of the available images, 1,000 have been annotated by 59 AMT workers. In total, approximately 2,547 image annotations were collected, with labeling accuracy ranging from $0\\%$ to $100\\%$ and an average accuracy of $69.20\\%$ . To enhance the training dataset, standard augmentation techniques such as rescaling, cropping, and horizontal flips were applied, resulting in a training set of 10,000 images annotated by the same 59 workers\u2014see more details in [67]. The validation set comprises 500 images, while the remaining 1,188 images are reserved for testing. ", "page_idx": 27}, {"type": "text", "text": "We select 15 classes from the ImageNet dataset with the intention of including easy-to-confuse classes. These classes are: dog, leopard, sports_car, tiger_cat, airship, aircraft_carrier, trailer_truck, orange, penguin, lemon, soccer_ball, airliner, freight_car, container_ship, passenger_car. We collect annotations for $N\\,=\\,2,514$ images from $M\\,=\\,100$ anonymous real annotators. These serve as our training set. The average error rate of the annotators is $42.68\\%$ . The validation and testing sets have 1,462 and 13,157 images, respectively. We release the noisy annotations at https: //github.com/ductri/COINNet. ", "page_idx": 27}, {"type": "text", "text": "H More Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.1 Experiments with Synthetic Annotators ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Noisy Label Generation. Here, we consider labels corrupted by synthetically generated annotator confusions. To generate the confusion matrices, we utilize the following strategy. We control the instance-independent label noise rate using a parameter $\\tau\\in(0,1)$ . By employing this parameter, we construct the ground-truth confusion matrix $A^{\\natural}$ , where the diagonal entries of $A^{\\natural}$ are set as $[\\pmb{A}^{\\natural}]_{k,k}\\,=\\,1\\,-\\,\\tau,\\,\\stackrel{\\textstyle\\cdot}{\\forall k}$ , and the off-diagonal entries are set as $\\begin{array}{r}{[A^{\\natural}]_{k,j}\\,=\\,\\frac{\\tau}{K-1},\\forall k\\,\\neq\\,j}\\end{array}$ . First, we randomly select $1-\\eta$ fraction of the data samples and generate their labels using the instanceindependent confusion matrix $A^{\\natural}$ . For the remaining $\\eta$ percent of the samples, we adopt an instancedependent noise generation process, by following the strategy outlined in [27]. Specifically, for such \u201coutliers\", the label confusions vary depending on the features of such instances. ", "page_idx": 27}, {"type": "table", "img_path": "HTLJptF7qM/tmp/bb47f49c213f517952d8ca84acd5e7b116089c3a8828753de2a5f5fdb1fd884a.jpg", "table_caption": ["Table 6: Average classification accuracy on CIFAR-10 and STL-10 datasets using machine annotators; results are averaged over 3 random trials. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "HTLJptF7qM/tmp/0ced86aa988e717cd38fad54fd87aabf513b759034f0d3692357bc0bc643eef0.jpg", "table_caption": ["Table 7: Average classification accuracy on CIFAR-10 using synthetic annotators; results are averaged over 3 random trials. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "HTLJptF7qM/tmp/70a279489057527320167e41f9f1736b03bcf4d310a866db37b41f25cd49df75.jpg", "table_caption": ["Table 8: Average classification accuracy over 3 random trials on real datasets. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "We generate synthetic noisy labels for CIFAR-10 dataset with $M=3$ . CIFAR-10 contains 50,000 images of size $32\\times32$ as training set, and 10,000 images as test set. We keep the test set untouched while randomly splitting the train set into 2 parts (47,500; 2,500) to serve as training and validation set. We use the same neural network architecture and the optimization settings as used in machine annotator case. The results are presented in Table 5. One can observe that our approach consistently perform well in all scenarios. ", "page_idx": 28}, {"type": "text", "text": "To demonstrate the effectiveness of the advocated crowdsourcing approach in outlier detection, we vary the number of annotators $M$ and present the average outlier detection rate and the testing accuracy, where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{~m~Rate}=\\frac{\\lvert\\mathcal{Z}\\cap\\{i\\ \\rvert\\ \\lVert\\widehat{e}_{i}\\rVert_{2}\\in\\mathrm{top}\\ \\lvert\\mathcal{Z}\\rvert\\ \\mathrm{values~among~all}\\ \\lVert\\widehat{e}_{n}\\rVert_{2},n\\in\\left[N\\right]\\}\\rvert}{\\lvert\\mathcal{Z}\\rvert}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We observe that increasing the number of annotators $M$ shows improvement over both outlier detection and the final accuracy score as shown in Fig. 4. ", "page_idx": 28}, {"type": "text", "text": "H.2 Ablation Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Effect of regularization terms. Tables 6, 7 and 8 show the effectiveness of having both the proposed regularization terms (i.e., sparsity and volume). ", "page_idx": 28}, {"type": "text", "text": "Effect of missing annotations. Table 9 and Table 10 show results under different levels of missing annotations under various parameters settings. Table 11 and Table 12 present results where only a single label is available for each image. In all four cases, the proposed method demonstrates superior robustness against the negative effect of missing annotations relative to MaxMIG and GeoCrowdNet(F), two competitive baselines. ", "page_idx": 28}, {"type": "text", "text": "Effect of initialization. In Table 13, we test the performance of our approach using different initialization strategies. In particular, we conduct the machine annotator experiment on CIFAR-10 using the following initialization strategies: the confusion matrices are (i) initialized using identity matrices, and (ii) initialized by the GeoCrowdNet(F) method. We also include the performance of GeoCrowdNet(F) for reference. We observe a slight improvement (around $0.1{-}0.4\\%$ ) in accuracy when using initialization from GeoCrowdNet(F) with the cost of training GeoCrowdNet(F) for 10 epochs. ", "page_idx": 28}, {"type": "text", "text": "Table 9: Average classification accuracy v.s. missing rate on CIFAR-10 using synthetic annotators;   \n$M=3,\\tau=0.2,\\eta=0.3$ ; results are averaged over 3 random trials. ", "page_idx": 29}, {"type": "table", "img_path": "HTLJptF7qM/tmp/1e3b1116ea136872a4788b1f810f559bfa0d014308b1167b63d3f7cc4d302e97.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "HTLJptF7qM/tmp/2823bb0e652a938eaf32d47a4803a9f36f73fa62fd85c54578e1cfdd3f8355c4.jpg", "table_caption": ["Table 10: Average classification accuracy v.s. missing rate on CIFAR-10 using synthetic annotators; $M=3,\\tau=0.2,\\eta=0.5$ ; results are averaged over 3 random trials. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "HTLJptF7qM/tmp/3c18fecbad0c4169a72de6911c7294e2b83237dabf2c276fa274703372aa1d13.jpg", "img_caption": ["Figure 5: Some examples from the CIFAR-10N dataset learned with low outlier scores $s_{n}$ \u2019s. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "H.3 Outlier Detection in CIFAR-10N Dataset ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Following the result in Fig. 2, we show more examples of nominal data items and outlier data items as separated by our approach in Fig. 5 and Fig. 6, respectively. Clearly, more instance-varying confusing characteristics are present in the latter case. ", "page_idx": 29}, {"type": "text", "text": "Table 11: Average classification accuracy in the scenario where each image of CIFAR-10 is labeled by only one randomly chosen annotator from $M=3$ synthetic annotators; results are averaged over 3 random trials. ", "page_idx": 30}, {"type": "table", "img_path": "HTLJptF7qM/tmp/bde3dba3bcb5168a82c4994f357dad8b361558ba3533e452fc3742757cea6a2d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 12: Average classification accuracy in the scenario where each image of CIFAR-10 is labeled by only one randomly chosen annotator from $M=3$ machine annotators; results are averaged over 3 random trials\u2014see the annotation generation settings as in Sec. 5.1. ", "page_idx": 30}, {"type": "table", "img_path": "HTLJptF7qM/tmp/22eec4d53209a1166c9ac90a97595d24d6056d8ff85eef02967a8281adcbb642.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 13: Average classification accuracy with different initializations for the confusion matrices. We use machine annotations with the same setting as described in Sec. 5.1 in the manuscript. Results are averaged over 3 random trials. ", "page_idx": 30}, {"type": "table", "img_path": "HTLJptF7qM/tmp/599d36fe026bdb9a1e48849f493e267c93519c77a8ee6ddafea317c94a0a0594.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "HTLJptF7qM/tmp/c49a8aefbd0365cd639105245a1782a88ac6ca6bfe4103fd383ebd78d4fdd9ad.jpg", "img_caption": ["Figure 6: Some examples from the CIFAR-10N dataset learned with high outlier scores $s_{n}$ \u2019s "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our claims have been discussed and supported by our analysis in Section 2 and 3, particularly by Fact 2.1, Theorem 3.5 and Theorem 3.6 with empirical evidence in Section 5 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have discussed the limitations at the end of Section 6 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have presented all assumptions we have used in Section 3. All proof are presented in the Appendix, particularly in Appendix D, Appendix E, Appendix F. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have described in details experiment settings as well as the implementation techniques in Section 3, Section 5 and Appendix G. We also include our code in this submission as supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have included code, including instruction how to run in this submission. It is as almost the same code that we use to to produce all experiment results, except removing all identity related information. All data we used are publicly accessible. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We describe these in Appendix G ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All experiments are repeated at least 3 times. All numbers are accompanied with standard deviation, as described in Section 5. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have included these information in Appendix G. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We have made to our best effort to remove all identity-related information in both the main paper supplementary material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work tackles the noisy labels problem which is ubiquitous, especially in the big data era, and hence bring positive impact to society. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We do not see any potential harm in publishing data and trained models of the proposed method. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have cited all datasets used in Section 5. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: There is no new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: There is no human subjects or crowdsourcing experiments conducted in the work. All datasets are from published sources. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: As mentioned above, our work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]