[{"figure_path": "AqcPvWwktK/tables/tables_6_1.jpg", "caption": "Table 1: Summary of the dataset statistics", "description": "This table presents a summary of the characteristics of five datasets used in the paper's experiments. For each dataset, it lists the number of training samples, the number of testing samples, the number of classes (labels), and the average number of positive classes per sample.  These statistics are essential for understanding the scale and complexity of the multi-label learning tasks.", "section": "4 Experiments"}, {"figure_path": "AqcPvWwktK/tables/tables_7_1.jpg", "caption": "Table 2: Experimental results on images datasets. The best results are highlighted in boldface.", "description": "This table presents the performance of different multi-label learning methods on three image datasets (VOC, COCO, AWA).  The results are shown for various data proportions (5%, 10%, 15%, 20% of labeled data), and five evaluation metrics are used: Micro-F1, Macro-F1, mAP, Hamming Loss, and One Loss.  The best performing method for each metric and data proportion is highlighted in bold.", "section": "4.2 Results"}, {"figure_path": "AqcPvWwktK/tables/tables_7_2.jpg", "caption": "Table 2: Experimental results on images datasets. The best results are highlighted in boldface.", "description": "This table presents the performance comparison of different multi-label learning methods on image datasets (VOC, COCO, AWA).  The metrics used are Micro-F1, Macro-F1, mAP, Hamming Loss, and One-Loss.  Results are shown for different proportions of labeled data (\u03c0 = 5%, 10%, 15%, 20%). The best-performing method for each metric and data proportion is highlighted in bold.", "section": "4.2 Results"}, {"figure_path": "AqcPvWwktK/tables/tables_8_1.jpg", "caption": "Table 2: Experimental results on images datasets. The best results are highlighted in boldface.", "description": "This table presents the performance comparison of different multi-label learning (MLL) methods on image datasets (VOC, COCO, AWA). The results are shown for different proportions of labeled data (\u03c0 = 5%, 10%, 15%, 20%). The metrics used for evaluation include Micro-F1, Macro-F1, mAP, Hamming Loss, and One Loss.  The table compares S2ML2-BBAM (the proposed method) with several baseline methods (SoftMatch, FlatMatch, MIME, DRML, CAP).  The best results for each metric and data proportion are highlighted in boldface.", "section": "4.2 Results"}, {"figure_path": "AqcPvWwktK/tables/tables_16_1.jpg", "caption": "Table 2: Experimental results on images datasets. The best results are highlighted in boldface.", "description": "This table presents the experimental results of the proposed S2ML2-BBAM model and several baseline methods on three image datasets: VOC, COCO, and AWA.  The results are broken down by the percentage of labeled data used (\u03c0 = 5%, 10%, 15%, 20%) and evaluated using five metrics: Micro-F1, Macro-F1, mAP, Hamming Loss, and One Loss.  The best performing model for each metric and dataset is highlighted in boldface, showcasing the relative performance of S2ML2-BBAM compared to existing methods. ", "section": "4.2 Results"}, {"figure_path": "AqcPvWwktK/tables/tables_16_2.jpg", "caption": "Table 5: Time cost (second, s) of each training epoch on VOC and COCO.", "description": "This table shows the training time in seconds for each epoch for different methods on the VOC and COCO datasets.  It compares the efficiency of S2ML2-BBAM against several baseline methods (SoftMatch, FlatMatch, DRML, CAP). The results show that S2ML2-BBAM is competitive in terms of time efficiency with other SSMLL methods and even faster than SSL methods.", "section": "4.4 Parameter Evaluation"}]