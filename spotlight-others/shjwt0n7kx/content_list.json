[{"type": "text", "text": "Doob\u2019s Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuanqi Du\u22171 Michael Plainer\u22172,3,4,5 Rob Brekelmans\u22176 Chenru Duan 7,8 ", "page_idx": 0}, {"type": "text", "text": "Frank No\u00e9 4,9,10 Carla P. Gomes 1 Al\u00e1n Aspuru-Guzik 6,11 Kirill Neklyudov 12,13 ", "page_idx": 0}, {"type": "text", "text": "1Cornell University 2Zuse School ELIZA 3Technische Universit\u00e4t Berlin 4Freie Universit\u00e4t Berlin 5Berlin Institute for the Foundations of Learning and Data 6Vector Institute 7Massachusetts Institute of Technology 8Deep Principle, Inc. 9Rice University 10Microsoft Research AI4Science 11University of Toronto 12Universit\u00e9 de Montr\u00e9al 13Mila Quebec AI Institute ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob\u2019s $h$ -transform. However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob\u2019s $h$ -transform as an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conditioning a stochastic process to obey a particular endpoint distribution, satisfy desired terminal conditions, or observe a rare event is a problem with a long history (Schr\u00f6dinger, 1932; Doob, 1957) and wide-ranging applications from generative modeling (De Bortoli et al., 2021; Chen et al., 2021a; Liu et al., 2022, 2023c; Somnath et al., 2023) to molecular simulation (Anderson, 2007; Wu et al., 2022; Plainer et al., 2023; Holdijk et al., 2023), drug discovery (Kirmizialtin et al., 2012, 2015; Dickson, 2018), and materials science (Xi et al., 2013; Selli et al., 2016; Sharma et al., 2016). ", "page_idx": 0}, {"type": "text", "text": "Transition Path Sampling. In this work, we take a particular interest in the problem of transition path sampling (TPS) in computational chemistry (Dellago et al., 2002; Weinan and Vanden-Eijnden, 2010), which attempts to describe how molecules transition between local energy minima or metastable states under random fluctuations or the influence of external forces. Understanding such transitions has numerous applications for combustion, catalysis, battery, material design, and protein folding (Zeng et al., 2020; Klucznik et al., 2024; Blau et al., 2021; No\u00e9 et al., 2009; Escobedo et al., 2009). ", "page_idx": 0}, {"type": "text", "text": "Figure 1: Given reference dynamics, transition path sampling seeks to capture the conditional or posterior distribution over paths which reach a terminal set $x_{T}\\in\\boldsymbol{B}$ . However, simulating the reference dynamics (blue) can be wasteful since we rarely obtain paths (orange) which reach (the vicinity of) the terminal set $\\boldsymbol{\\mathbf{\\rho}}_{\\perp}$ . This is a major challenge for techniques based on importance sampling or Monte Carlo estimation, even when adding a control term to the reference dynamics. By contrast, our approach optimizes a tractable variational distribution over transition paths with a parameterization which satisfies the initial and terminal conditions by design. ", "page_idx": 1}, {"type": "text", "text": "While the TPS problem is often framed as finding the \u2018most probable path\u2019 transitioning between states (D\u00fcrr and Bach, 1978; Vanden-Eijnden and Heymann, 2008), we build upon connections between TPS and Doob\u2019s $h$ -transform (Das et al., 2019, 2021, 2022; Koehl and Orland, 2022; Singh and Limmer, 2023) and seek to match the full posterior distribution over conditioned processes. ", "page_idx": 1}, {"type": "text", "text": "Doob\u2019s $h$ -Transform. For Brownian motion diffusion processes, conditioning is known to be achieved by Doob\u2019s $h$ -transform (Doob, 1957; S\u00e4rkk\u00e4 and Solin, 2019). However, solving this problem amounts to estimating rare event probabilities or matching a complex target distribution. Approaches which involve simulation of trajectories to construct Monte Carlo expectations or importance sampling estimators (Papaspiliopoulos and Roberts, 2012; Schauer et al., 2017; Yan et al., 2022; Holdijk et al., 2023) can be extremely inefficient if the target event is rare or endpoint distribution is difficult to match. Recent methods based on score matching (Heng et al., 2021) or nonlinear Feynman-Kac formula (Chopin et al., 2023) still require simulation during optimization. ", "page_idx": 1}, {"type": "text", "text": "Variational Formulation of Doob\u2019s $h$ -Transform. In this work, we propose a variational formulation of Doob\u2019s $h$ -transform as the solution to an optimization on the space of paths of probability distributions. We focus on solving for the Doob transform conditioning on a particular terminal point, which is natural in the TPS setting (see Fig. 1). Taking inspiration from recent bridge matching methods (Peluchetti, 2021, 2023; Liu et al., 2022; Lipman et al., 2022; Shi et al., 2023; Liu et al., 2023a), we propose a parameterization with the following attractive features. ", "page_idx": 1}, {"type": "text", "text": "Every Sample Matters. In contrast to most existing approaches, our training method is simulation-free, thereby avoiding computationally wasteful simulation methods to estimate rare-event probabilities and inefficient importance or rejection sampling. We thus refer to our approach as being sample-efficient. ", "page_idx": 1}, {"type": "text", "text": "2. Optimization over Sampling. We propose an expressive variational family of approximations to the conditioned process, which are tractable to sample and can be optimized using neural networks with end-to-end backpropagation.   \n3. Problem-Informed Parameterization. Our parameterization enforces the boundary conditions by design, thereby reducing the search space for optimization and efficiently making use of the conditioning information. ", "page_idx": 1}, {"type": "text", "text": "We begin by linking the problem of transition path sampling to the Doob\u2019s $h$ -transform and recalling background results in Sec. 2. We present our variational formulation in Sec. 3.1 and detail our optimization algorithm throughout Sec. 3.2. We demonstrate the ability of our approach to achieve comparable performance to Markov Chain Monte Carlo (MCMC) methods with notably improved efficiency on synthetic, and real-world molecular simulation tasks in Sec. 5. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Transition Path Sampling ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider a forward or reference stochastic process with states $x_{t}$ and the density of transition probability $\\rho_{t+d t}(y|x_{t}=x):=\\rho(x_{t+d t.}=y\\,\\backslash\\,x_{t}=x)$ . Starting from an initial point $x_{0}=A$ , the probability density of a discrete-time path is given as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\rho(x_{T},\\ldots,x_{d t}\\,|\\,x_{0}=A)=\\prod_{t=d t}^{T-d t}\\rho(x_{t+d t}\\,|\\,x_{t})\\cdot\\rho(x_{d t}\\,|\\,x_{0}=A).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The problem of rare event sampling aims to condition this reference stochastic process on some event at time $T$ , for example, that the final state belongs to a particular set $x_{T}\\in\\boldsymbol{B}$ . We are interested in ", "page_idx": 1}, {"type": "text", "text": "sampling from the entire transition path, namely the posterior distribution over intermediate states ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho(x_{T-d t},\\ldots,x_{d t}\\,|\\,x_{0}=A,x_{T}\\in\\mathcal{B})=\\frac{\\rho(x_{T}\\in\\mathcal{B},x_{T-d t}\\cdot\\ldots,x_{d t}\\,|\\,x_{0}=A)}{\\rho(x_{T}\\in\\mathcal{B}\\,|\\,x_{0}=A)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moving to continuous time, we focus on the transition path sampling problem in the case where the reference process is given by a Brownian motion. In particular, we are motivated by applications in computational chemistry (Dellago et al., 2002; Weinan and Vanden-Eijnden, 2010), where the reference process is given by molecular dynamics following either overdamped Langevin dynamics, ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x_{t}=-\\big(\\gamma M\\big)^{-1}\\nabla_{x}U\\big(x_{t}\\big)\\cdot d t+\\big(\\gamma M\\big)^{-1/2}\\sqrt{2k_{B}\\mathcal{T}}\\cdot d W_{t}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "or the second-order Langevin dynamics with spatial coordinates $\\bar{x}_{t}$ and velocities $\\bar{v}_{t}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\bar{x}_{t}\\,=\\bar{v}_{t}\\cdot d t\\,,\\qquad d\\bar{v}_{t}\\,=\\,\\Big(-M^{-1}\\nabla_{x}U(\\bar{x}_{t})-\\gamma\\bar{v}_{t}\\Big)\\cdot d t+M^{-1/2}\\sqrt{2\\gamma k_{B}\\mathcal{T}}\\cdot d W_{t}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a potential energy function $U$ , where $W_{t}$ denotes the Wiener process. Note that $k_{B}\\tau$ is the Boltzman constant times temperature, $M$ is the mass matrix, and $\\gamma$ is the friction coefficient. ", "page_idx": 2}, {"type": "text", "text": "2.2 Doob\u2019s $h$ -transform ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Doob\u2019s $h$ -transform addresses the question of conditioning a reference Brownian motion to satisfy a terminal condition such as $x_{T}\\in\\boldsymbol{B}$ , thereby providing an avenue to solve the transition path sampling problem described above. Without loss of generality, and to provide a unified treatment of the dynamics in (3)\u2013(4), we consider the forward or reference stochastic differential equation (SDE), ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x_{t}=b_{t}(x_{t})\\cdot d t+\\Xi_{t}\\,d W_{t}\\,,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;x_{0}\\sim\\rho_{0}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with drift vector field $b_{t}:\\mathbb{R}^{N}\\to\\mathbb{R}^{N}$ and diffusion coefficient matrix $\\Xi_{t}\\in\\mathbb{R}^{N\\times N}$ such that $G_{t}:=$ $\\scriptstyle{\\frac{1}{2}}\\Xi_{t}\\Xi_{t}^{T}$ is positive definite.2 We denote the induced path measure as $\\mathbb{P}_{0:T}^{\\mathrm{ref}}\\in\\mathcal{P}(\\mathcal{C}([0,T]\\to\\mathbb{R}^{N})]$ ), i.e. a measure over continuous functions from time to $\\mathbb{R}^{N}$ . ", "page_idx": 2}, {"type": "text", "text": "Remarkably, Doob\u2019s $h$ -transform (Doob, 1957; S\u00e4rkk\u00e4 and Solin, 2019, Sec. 7.5) shows that conditioning the reference process (5) on $x_{T}\\in\\boldsymbol{B}$ results in another Brownian motion process. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1. [Jamison (1975, Thm. 2)] Let $h_{\\mathcal{B}}(x,t)\\;:=\\;\\rho_{T}(x_{T}\\;\\in\\;\\mathcal{B}\\,|\\,x_{t}\\;=\\;x)$ denote the conditional transition probability of the reference process in (5). Then, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{0:T}^{*}:\\quad\\quad d x_{t|T}=\\Big(b_{t}(x_{t|T})+2G_{t}\\nabla_{x}\\log h_{B}(x_{t|T},t)\\Big)\\cdot d t+\\Xi_{t}\\ d W_{t}\\qquad x_{0}\\sim\\rho_{0}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we use $x_{t|T}$ to denote a conditional process. The SDE in (6) is associated with the following transition probabilities for $s<t<T$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho_{t}(y\\,\\vert\\,x_{s}=x,x_{T}\\in\\mathcal{B})=\\frac{h_{B}(y,t)}{h_{B}(x,s)}\\rho_{t}(y\\,\\vert\\,x_{s}=x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that all of our subsequent results hold for the case when $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is a point-mass, with the only change being that the $h$ -function becomes a density, $h_{B}(x,t)=\\rho_{T}(B\\,|\\,x_{t}\\,\\stackrel{.}{=}x)$ . ", "page_idx": 2}, {"type": "text", "text": "See App. A.1 for proof, and note that (7) is simply an application of Bayes rule $\\rho_{t}(y\\,|\\,x_{s}=x,x_{T}\\in$ $\\beta)=\\dot{\\rho_{T}}(x_{T}\\in\\mathcal{B}]x_{t}=y)\\rho_{t}(y\\,|\\,x_{s}=x)/\\rho_{T}(x_{T}^{\\star}\\in\\mathcal{B}|\\dot{x_{s}}^{\\star}=x)$ with the unconditioned or reference transition probability as the prior. Furthermore, the conditioned transition probabilities in (7) allow us to directly construct the transition path (2). Using Bayes rule, we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho(x_{T-d t},\\ldots,x_{d t}\\,|\\,x_{0}=A,x_{T}\\in\\mathcal{B})=\\frac{h_{B}(x_{T-d t},T-d t)}{h_{B}(A,0)}\\rho(x_{T-d t}\\cdot\\,.\\,.\\,,x_{d t}\\,|\\,x_{0}=A)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "after telescoping cancellation of $h$ -functions and rewriting the denominator in (2) as $h_{B}(A,0)$ Thus, we can solve the TPS problem by exactly solving for the $h$ -function and simulating the SDE in (6). ", "page_idx": 2}, {"type": "text", "text": "Finally, the $h$ -process and temporal marginals $\\rho_{t}(x|x_{0}=A,x_{T}\\in B)$ of the conditioned process satisfy the following forward and backward Kolmogorov equations, which will be useful in deriving our variational objectives in the next section. Note, we use $\\langle\\nabla_{x},\\cdot\\rangle\\,=\\,\\mathrm{div}(\\bullet)$ for the divergence operator, and we use $\\rho_{t|0,T}$ to indicate the dependence on both $x_{0}=A$ (via the initial condition of (8a)) and $x_{T}\\in B$ (via the $h$ -transform $h_{B}$ ). See App. A.1 for the proof. ", "page_idx": 2}, {"type": "text", "text": "2Note that the second-order dynamics in (4) can be represented using ", "text_level": 1, "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=\\left[\\!\\!\\begin{array}{c}{\\bar{x}_{t}}\\\\ {\\bar{v}_{t}}\\end{array}\\!\\!\\right],\\quad\\quad b_{t}(x_{t})=\\left[\\!\\!\\begin{array}{c c}{\\bar{v}_{t}}\\\\ {-M^{-1}\\nabla_{x}U(\\bar{x}_{t})-\\gamma\\bar{v}_{t}}\\end{array}\\!\\!\\right],\\quad\\quad G_{t}=\\left[\\!\\!\\begin{array}{c c}{0}&{0}\\\\ {0}&{M^{-1/2}\\sqrt{2\\gamma k_{B}T}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proposition 2. The following PDEs are obeyed by (a) the density of the conditioned process $\\rho_{t\\mid0,T}(x):=\\rho_{t}(x\\mid x_{0}=A,x_{T}\\in B)$ and $(b)$ the $h$ -function $h_{B}(x,t)$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial\\rho_{t|0,T}(x)}{\\partial t}+\\bigl\\langle\\nabla_{x},\\rho_{t|0,T}(x)\\bigl(b_{t}(x)+2G_{t}\\nabla_{x}\\log h_{B}(x,t)\\bigr)\\bigr\\rangle-\\displaystyle\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}\\rho_{t|0,T}(x)=0\\,,}\\\\ {\\displaystyle\\frac{\\partial h_{B}(x,t)}{\\partial t}+\\bigl\\langle\\nabla_{x}h_{B}(x,t),b_{t}(x)\\bigr\\rangle+\\displaystyle\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}h_{B}(x,t)=0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Reparameterizing (8b) in terms of $s_{B}(x,t):=\\log h_{B}(x,t),$ , we can also write ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{B}(x,t)}{\\partial t}+\\langle\\nabla s_{B}(x,t),G_{t}\\nabla s_{B}(x,t)\\rangle+\\langle\\nabla s_{B}(x,t),b_{t}(x)\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}s_{B}(x,t)=0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first present a novel variational objective whose minimum corresponds to the Doob $h$ -transform in Sec. 3.1, and then propose an efficient parameterization to solve for the $h$ -transform in Sec. 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Doob\u2019s Lagrangian ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider reference dynamics given in the form of either (3) or (4), with known drift $b_{t}$ or energy $U$ We will restrict our attention to conditioning on a terminal rare event of reaching a given endpoint $x_{T}=B$ , along with an initial point $x_{0}=A$ . We approach solving for Doob\u2019s $h$ -transform via a least action principle where, in the following theorem, we define a Lagrangian action whose minimization yields the optimal $q_{t|0,T}^{*}(x)=\\rho_{t|0,T}(\\bar{x})$ and $v_{t|0,T}^{*}(x)=\\nabla_{x}\\log h_{B}(x,t)$ from Prop. 1 and 2. ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal S}=\\operatorname*{min}_{q,v}\\int_{0}^{T}d t\\;\\int d x\\;q_{t|0,T}(x)\\bigl<v_{t|0,T}(x),G_{t}\\;v_{t|0,T}(x)\\bigr>\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial q_{t|0,T}(x)}{\\partial t}=-\\Big\\langle\\nabla_{x},q_{t|0,T}(x)\\big(b_{t}(x)+2G_{t}\\;v_{t|0,T}(x)\\big)\\Big\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}q_{t|0,T}(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{0}(x)=\\delta(x-A),\\qquad q_{T}(x)=\\delta(x-B)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This objective will form the basis for our computational approach, with proof of Thm. 1 deferred to App. A.2. We proceed briefly to contextualize our variational objective and highlight several optimization challenges which will be solved by our proposed parameterization in Sec. 3.2. ", "page_idx": 3}, {"type": "text", "text": "Unconstrained Dual Objective. Introducing Lagrange multipliers to enforce the constraints in (9b)\u2013(9c) and eliminating $v_{t|0,T}$ , we obtain an alternative, unconstrained version of (9a). ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. The Lagrangian objective in Thm. 1 which solves Doob\u2019s $h$ -transform is equivalent to S ${\\mathit{\\Sigma}}^{5}=\\operatorname*{min}_{q_{t}\\mid0,T}\\operatorname*{max}_{\\delta}\\,s_{B}(B,T)-s_{B}(A,0)-\\int_{0}^{T}\\!d t\\int\\!d x\\,q_{t\\mid0,T}\\bigg(\\frac{\\partial s_{B}}{\\partial t}+\\langle\\nabla s_{B},G_{t}\\nabla s_{B}\\rangle+\\langle\\nabla s_{B},b_{t}\\rangle+\\langle\\nabla,G_{t}\\nabla s_{B}\\rangle\\bigg)$ $i f q_{t\\vert0,T}$ satisfies (9c). Note $v_{t|0,T}(x)=\\nabla_{x}s_{B}(x,t),$ , with $s_{B}^{*}(x,t)=\\log h_{B}(x,t)$ at optimality. 3 ", "page_idx": 3}, {"type": "text", "text": "This objective is similar to the objectives optimized by Action Matching methods (Neklyudov et al., 2023, 2024). Notably, the objective in Cor. 1 is expressed directly in terms of the (log) of the $h$ -function for fixed conditioning information $x_{T}=B$ . We also note that the Hamilton Jacobi-style quantity, whose expectation appears in the final term, is zero at optimality in (8c) of Prop. 2. ", "page_idx": 3}, {"type": "text", "text": "Path Measure Perspective. We next relate our variational objective in Thm. 1 to a KL divergence optimization over path measures. Let $\\mathbb{P}_{0:T}^{\\mathrm{ref}}$ denote the law of the reference SDE in (5) with fixed $\\mathbb{P}_{0}^{\\mathrm{ref}}=\\delta(x_{0}-A)$ . Let $\\mathbb{Q}_{0:T}^{v}$ denote the law of a controlled process similar to (6), but with a variational $v_{t|0,T}$ in place of $\\nabla_{x}\\log{h_{B}}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{Q}_{0:T}^{v}:\\qquad d x_{t}=\\left(b_{t}(x_{t|T})+2G_{t}\\;v_{t|0,T}(x_{t|T})\\right)\\cdot d t+\\Xi_{t}\\;d W_{t}\\,,\\qquad x_{0}=A.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the density $q_{t|0,T}$ of $\\mathbb{Q}_{0:T}^{v}$ evolve according to the Fokker-Planck equation in (9b) (S\u00e4rkk\u00e4 and Solin, 2019, Sec. 5.2) . Using the Girsanov Theorem, the objective in (9a) can then be viewed as a KL divergence minimization over path measures $\\mathbb{Q}_{0:T}^{v}$ which satisfy the boundary constraints. ", "page_idx": 4}, {"type": "text", "text": "Corollary 2. The following Schr\u00f6dinger Bridge (SB) problem ", "page_idx": 4}, {"type": "equation", "text": "$$\nS:=\\operatorname*{min}_{\\mathbb{Q}_{0:T}^{v}\\;s.t.\\;\\mathbb{Q}_{0}^{v}=\\delta_{A},\\mathbb{Q}_{T}^{v}=\\delta_{B}}D_{K L}[\\mathbb{Q}_{0:T}^{v}:\\mathbb{P}_{0:T}^{r e f}]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "yields the path measure $\\mathbb{P}_{0:T}^{*}$ associated with the SDE in (6) as its unique minimizing argument. The temporal marginals of $\\mathbb{P}_{0:T}^{*}$ are equal to those which optimize the Lagrangian objective in Thm. 1. ", "page_idx": 4}, {"type": "text", "text": "Our Lagrangian action minimization thus corresponds to the solution of an SB problem (Schr\u00f6dinger, 1932; L\u00e9onard, 2014) with Dirac delta functions as the endpoint measures. Our objective in (9a) particularly resembles optimal control formulations of SB (Chen et al., 2016, 2021b, Prob. 4.4, 5.3). While it is well-known that the Doob $h$ -transform (and large deviation theory more generally) plays a role in the solution to SB problems (Jamison, 1975; L\u00e9onard, 2014), our interest in the transition path sampling problem leads to specific computational decisions below. See Sec. 4 for further discussion. ", "page_idx": 4}, {"type": "text", "text": "Challenges of Optimizing (9a). We highlight several distinctive features of our problem which inform the development of new computational methods in Sec. 3.2. ", "page_idx": 4}, {"type": "text", "text": "1. First, we perform optimization over the first argument of the KL divergence in (11), indicating that we need to be able to efficiently sample from the conditioned process in (10) or $q_{t|0,T}$ in (9). This appears challenging due to the nonlinearity of both the reference and variational drifts, bt and vt|0,T .   \n2. For a given $q_{t|0,T}$ , it can be difficult to solve for $v_{t|0,T}$ which satisfies the FokkerPlanck equation in (9b) or $\\nabla s$ which solves the inner optimization in Cor. 1.   \n3. Finally, we would like to strictly enforce the boundary constraints on $q_{t|0,T}$ or $\\mathbb{Q}_{0:T}^{v}$ to avoid simulating or wasting computation on trajectories for which $i_{T}\\neq B$ . ", "page_idx": 4}, {"type": "text", "text": "In fact, our parameterization of $q_{t|0,T}$ in Sec. 3.2 will completely avoid simulation of the SDE in (10) during training (Challenge 1), provide analytic solutions for $v_{t|0,T}$ satisfying (9b) with a given $q_{t|0,T}$ (Challenge 2), and exactly enforce the boundary constraints (Challenge 3). ", "page_idx": 4}, {"type": "text", "text": "3.2 Computational Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now propose a family of Gaussian (mixture) path parameterizations $q_{t|0,T}$ which overcome the computational challenges posed in the previous section, while still maintaining expressivity. We present all aspects of our proposed method in the context of the first-order dynamics (3) in Sec. 3.2.1, before presenting extensions to mixture paths and the second-order setting (4) in Sec. 3.2.2\u20133.2.3. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 First-Order Dynamics and General Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Tractable Drift $v_{t|0,T}$ for Variational Doob Objective. We begin by considering a modification of the Fokker-Planck constraint in (9b), with all drift terms absorbed into a single vector field $u_{t|0,T}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial q_{t|0,T}(x)}{\\partial t}=-\\Big\\langle\\nabla_{x},q_{t|0,T}(x)\\;u_{t|0,T}(x)\\Big\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}q_{t|0,T}(x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For arbitrary $q_{t|0,T}$ , solving for any $u_{t|0,T}(x)$ satisfying (12) can be a difficult optimization problem, whose solution is not unique without some cost-minimizing assumption (Neklyudov et al., 2023). ", "page_idx": 4}, {"type": "text", "text": "To sidestep this optimization, and address Challenge 2, we restrict attention to variational families of qt|0,T \u2208Q where it is analytically tractable to calculate a vector field ut(|q0,,\u03b8T) which satisfies (12). We first consider the family of Gaussian paths $\\mathcal{Q}_{G}$ , in similar fashion to (conditional) flow matching methods (Lipman et al., 2022; Tong et al., 2023; Liu et al., 2023a), with proof in App. B. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2: Sampling Trajectories def get_drift(xt, t): Evaluate \u00b5t|0,T , $\\boldsymbol{\\mu}_{t\\mid0,T}^{(\\theta)},\\boldsymbol{\\Sigma}_{t\\mid0,T}^{(\\theta)}$ \u03a3t|0,T at t return drift $u_{t|0,T}^{(q,\\theta)}(x_{t})$ using (13) Sample initial state $x_{0}\\sim\\mathcal{N}(A,\\sigma_{\\operatorname*{min}}^{2})$ return SDESolve( $\\scriptstyle x_{0}$ , get_drift, $T$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Input: Reference drift $b_{t}$ , diffusion matrix $G_{t}$ Conditioning endpoints   \nwhile not converged do Sample $t\\sim\\mathcal{U}(0,T)$ Sample $x_{t}\\sim q_{t\\mid0,T}^{(\\theta)}$ using (15)   \nCCaallccuullaattee $u_{t|0,T}^{(q,\\theta)}(x_{t})$ uussiinngg , (14) $v_{t|0,T}^{(q,\\theta)}(x_{t})$ $u_{t|0,T}^{(q,\\theta)}(x_{t})$ $b_{t}(x_{t})$ Calculate $\\mathcal{L}=\\langle v_{t|0,T}^{(q,\\theta)}(x_{t}),G_{t}\\;v_{t|0,T}^{(q,\\theta)}(x_{t})\\rangle$ (Thm. 1) Update $\\theta\\gets\\mathrm{optimizer}(\\theta,\\nabla_{\\theta}\\mathcal{L})$   \nend while   \nreturn $\\theta$ ", "page_idx": 5}, {"type": "text", "text": "Algorithms for training with a single Gaussian path (Alg. 1) and sampling or generating transition paths at test time (Alg. 2). Note that we sample from the marginals $q_{t|0,T}$ during training, but generate paths by simulating the SDE (10). ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. For the family of endpoint-conditioned marginals $q_{t\\mid0,T}(x)=\\mathcal{N}(x\\mid\\mu_{t\\mid0,T},\\Sigma_{t\\mid0,T}),$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{t|0,T}^{(q,\\theta)}(x):=\\frac{\\partial\\mu_{t|0,T}}{\\partial t}+\\left[\\frac{1}{2}\\frac{\\partial\\Sigma_{t|0,T}}{\\partial t}\\Sigma_{t|0,T}^{-1}-G_{t}\\,\\Sigma_{t|0,T}^{-1}\\right]\\left(x-\\mu_{t|0,T}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "satisfies the Fokker-Planck equation (12) for $q_{t|0,T}$ and diffusion coefficients $\\begin{array}{r}{G_{t}=\\frac12\\Xi_{t}\\Xi_{t}^{T}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Given ut|0,T corresponding to $q_{t}|0\\sqrt{T}$ , we can simply solve for the $v_{t|0,T}$ satisfying the Fokker-Planck equation in (9b) in our variational Doob objective (Thm. 1). Since $G_{t}$ was assumed to be invertible and the base drift $b_{t}$ is known, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{t|0,T}^{(q,\\theta)}(x)=\\frac{1}{2}(G_{t})^{-1}\\Big(u_{t|0,T}^{(q,\\theta)}(x)-b_{t}(x)\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We may now evaluate terms involving $v_{t|0,T}$ in our Lagrangian objective in (9) using (14) directly, without spending effort to solve an inner minimization over $v_{t|0,T}$ (thus addressing Challenge 2). ", "page_idx": 5}, {"type": "text", "text": "aO gpitviemni $q_{t|0,T}$ osvaet,i sofyuir nvga rBiaotiuonndaal rDy oCobo nosbtjreacitinvtes. i nG i(v9ea)n  rtehde uacbeisl ittoy  ta os ienvgalleu aotpet $v_{t|0,T}^{(q,\\theta)}$ t ifoonr $q_{t|0,T}\\in\\mathcal{Q}_{G}$ over the marginals $q_{t|0,T}$ of a conditioned process which satisfies the boundary conditions (9c). ", "page_idx": 5}, {"type": "text", "text": "We consider parameterizing the mean $\\mu_{t|0,T}$ and covariance $\\Sigma_{t|0,T}$ of our Gaussian path $q_{t|0,T}$ diag({\u03c3t2|0,T ,d}dD=1). We parameterize a neural network NNET\u03b8 : [0, T] \u00d7 RD \u00d7 RD \u2192RD \u00d7 RD which inputs time $t$ and boundary conditions $x_{0}=A,x_{T}=B$ , and outputs vectors of mean perturbations and per-dimension variances. Finally, using index notation to separate the output, we construct ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle x_{t|0,T}=\\mu_{t|0,T}^{(\\theta)}+\\Sigma_{t|0,T}^{(\\theta)}\\epsilon,\\quad\\mathrm{where}\\quad\\epsilon\\sim\\mathcal{N}(0,\\mathbb{I}_{D}).}\\\\ {\\displaystyle\\mu_{t|0,T}^{(\\theta)}=\\bigg(1-\\displaystyle\\frac{t}{T}\\bigg)A+\\displaystyle\\frac{t}{T}\\,B+\\displaystyle\\frac{t}{T}\\bigg(1-\\displaystyle\\frac{t}{T}\\bigg)\\mathrm{NNET}_{\\theta}\\big(t,A,B\\big)_{[:D]}}\\\\ {\\displaystyle\\Sigma_{t|0,T}^{(\\theta)}=\\displaystyle\\frac{t}{T}\\bigg(1-\\displaystyle\\frac{t}{T}\\bigg)\\mathrm{diag}\\big(\\mathrm{NNET}_{\\theta}(t,A,B)_{[D:]}\\big)+\\sigma_{\\mathrm{min}}^{2}\\mathbb{I}_{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Crucially, our Gaussian parameterization addresses Challenge 1, in that we can easily draw samples $x_{t|0,T}\\,\\sim\\,q_{t|0,T}$ from our variational conditioned process (9b) without simulating the corresponding SDE with nonlinear drift (10). Further, the coefficients in (15b) and (15c) ensure that, as $t\\,\\rightarrow\\,0$ or $t\\,\\rightarrow\\,T$ , our parameterization satisfies the (smoothed) boundary conditions by design (Challenge 3). Although we add $\\sigma_{\\mathrm{min}}^{2}$ to ensure invertibilty of $\\Sigma_{t|0,T}$ (see (13)), we preserve $q_{0}(x_{0})=\\mathcal{N}(x_{0}\\,|\\,A,\\sigma_{\\mathrm{min}}^{2}\\mathbb{I}_{D})\\approx\\delta(x_{0}-A)$ and $q_{T}(x_{T})=\\mathcal{N}(x_{T}\\,|\\,B,\\sigma_{\\operatorname*{min}}^{2}\\mathbb{I}_{D})\\approx\\delta(x_{T}-B)$ . ", "page_idx": 5}, {"type": "text", "text": "Reparameterization Gradients. Having shown that our parameterization satisfies the constraints (9b)-(9c) by design, we can finally optimize our variational Doob objective with respect to $q_{t|0,T}\\in$ $\\mathcal{Q}_{G}$ using the reparameterization trick (Kingma and Welling, 2013; Rezende et al., 2014). In particular, for the expectation at each $t$ in (9a), we rewrite ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\theta}\\mathbb{E}_{q_{t[0,T}^{(\\theta)}}\\big[\\Big\\langle v_{t[0,T}^{(q,\\theta)}(x),G_{t}\\,v_{t[0,T}^{(q,\\theta)}(x)\\Big\\rangle\\Big]=\\mathbb{E}_{N(\\epsilon[0,\\mathbb{I}_{D})}\\left[\\nabla_{\\theta}\\Big\\langle v_{t[0,T}^{(q,\\theta)}\\big(g(t,\\epsilon;\\theta)\\big),G_{t}\\,v_{t[0,T}^{(q,\\theta)}\\big(g(t,\\epsilon;\\theta)\\big)\\right\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$x=g(t,\\epsilon;\\theta)$ is the mapping in (15) and $v_{t|0,T}^{(q,\\theta)}$ depends on $\\theta$ via $\\boldsymbol{\\mu}_{t\\mid0,T}^{(\\theta)},\\boldsymbol{\\Sigma}_{t\\mid0,T}^{(\\theta)}$ in (13)\u2013(14). ", "page_idx": 5}, {"type": "text", "text": "Full Training Algorithm. In practice, we sample a batch of times $\\{t_{i}\\}_{i=1}^{M}$ uniformly from the interval $t\\in[0,T]$ . For each time point, we approximate the gradient using a single-sample estimate of the expectation above (or (9)), which yields a simulation-free training procedure. The full training algorithm is outlined in Alg. 1. ", "page_idx": 6}, {"type": "text", "text": "Sampling of Trajectories. While we sample directly from qt(|\u03b80),T d uring training, we can sample full trajectories which obey this sequence of marginals at test time (Alg. 2). In particular, we simulate SDE trajectories with drift ut(|q0,,\u03b8T) (x) and diffusion coefficient Gt using an appropriate solver. Note that this generation scheme sidesteps computationally expensive evaluation of the force field or base drift $b_{t}(\\bar{x_{t}})$ . We visualize example sampling trajectories in Fig. 2. ", "page_idx": 6}, {"type": "text", "text": "3.2.2 Second-Order Dynamics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To handle the case of the second-order dynamics in (4), we can adapt our recipe from the previous section with minimal modifications by extending the state space $x\\in\\mathbb{R}^{D}$ to include velocities $\\bar{v}$ , with $\\boldsymbol{x}=(\\bar{\\boldsymbol{x}},\\bar{\\boldsymbol{v}})\\in\\mathbb{R}^{2D}$ . However, note that the dynamics in (4) are no longer stochastic in the spatial coordinates $\\textstyle{\\bar{x}}$ . To ensure invertibility of $G_{t}$ and existence of the $h$ -transform, we add a small nonzero diffusion coefficient in the coordinate space $\\textstyle{\\bar{x}}$ , so that the reference process in Eq. (5) is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t}=\\left[\\!\\!\\begin{array}{c c}{\\bar{x}_{t}}\\\\ {\\bar{v}_{t}}\\end{array}\\!\\!\\right],\\quad b_{t}(x_{t})=\\left[\\!\\!\\begin{array}{c c}{\\bar{v}_{t}}&{\\Xi_{t}=\\left[\\!\\!\\begin{array}{c c}{\\xi_{\\operatorname*{min}}\\mathbb{I}_{D}}&{0}\\\\ {0}&{M^{-1/2}\\sqrt{2\\gamma k_{B}\\mathcal{T}}\\right].}\\end{array}\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "All steps in our algorithm proceed in similar fashion to Sec. 3.2.1. We now parameterize $q_{t|0,T}(\\bar{x},\\bar{v})$ using $\\mathbf{NNET}_{\\theta}:[0,T]\\times\\mathbb{R}^{2D}\\times\\mathbb{R}^{2D}\\rightarrow\\mathbb{R}^{2D}\\times\\mathbb{R}^{2D}$ , which outputs mean perturbations and perdimension variances to calculate $\\mu_{t\\vert0,T}^{\\bar{x}},\\mu_{t\\vert0,T}^{\\bar{v}}$ and $\\Sigma_{t|0,T}^{\\bar{x}},\\Sigma_{t|0,T}^{\\bar{v}}$ and sample $(\\bar{x},\\bar{v})$ , as in (15). Note that we parameterize $\\Sigma_{t|0,T}^{\\bar{x}},\\Sigma_{t|0,T}^{\\bar{v}}$ separately, matching the block diagonal form of (16). We calculate $v_{t|0,T}^{(q)}(\\bar{x},\\bar{v})\\,:=\\,\\bar{[v_{t|0,T}^{\\bar{x}(q)},v_{t|0,T}^{\\bar{v}(q)}]}$ from $u_{t|0,T}^{(q)}(\\bar{x},\\bar{v})\\,=\\,[u_{t|0,T}^{\\bar{x}(q)},u_{t|0,T}^{\\bar{v}(q)}]$ as in (13)\u2013(14), with $G_{t}^{-1}\\,=\\,(\\frac{1}{2}\\dot{\\Xi}_{t}\\Xi_{t}^{T})^{-1}$ given by (16). The Lagrangian objective in (9) minimizes the norm of the concatenated vector $v_{t|0,T}^{(q)}(\\bar{x},\\bar{v})$ , which depends on the reference drift $b_{t}(\\bar{x},\\bar{v})$ in (16). ", "page_idx": 6}, {"type": "text", "text": "3.2.3 Gaussian Mixture Paths ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Note that the true Doob $h$ -transform may not yield marginal distributions which are unimodal cGoanudsistiioannes da psr ionc etshsee s,p rweve inoousw  seexctteinodn .o urT op airnacmreetaesrei ztahtieo ne xtpo remsisxitvuirteys  ooff  oGuaru svsairainast,i $q_{t|0,T}\\in\\dot{\\mathcal{Q}_{\\mathrm{MoG}}^{K}}$ Given a set of mixture weights $\\pmb{w}:=\\{\\boldsymbol{w}^{k}\\}_{k=1}^{K}$ and component Gaussian paths $\\{q_{t|0,T}^{k}\\}_{k=1}^{K}$ , the following identity allows us to obtain the drift $u_{t|0,T}^{(q,\\theta)}$ of the corresponding mixture distribution $q_{t|0,T}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. Given a set of processes $q_{t\\mid0,T}^{k}(x)$ and mixtures weights $w^{k}$ , the vector field satisfying the Fokker-Planck equation in (12) for the mixture $\\begin{array}{r}{q_{t|0,T}(x)=\\sum_{k}w^{k}q_{t|0,T}^{k}(x)}\\end{array}$ is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nu_{t|0,T}^{(q,\\theta)}(x)=\\sum_{k=1}^{K}\\frac{w^{k}q_{t|0,T}^{k}(x)}{\\sum_{j=1}^{K}w^{j}q_{t|0,T}^{j}(x)}u_{t|0,T}^{(q,k)}(x)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $u_{t|0,T}^{(q,k)}(x)$ satisfies the Fokker-Planck equation in (12) for $q_{t\\mid0,T}^{k}(x)$ . This identity holds for both first-order dynamics in spatial coordinates only or second-order dynamics in . ", "page_idx": 6}, {"type": "text", "text": "Finally, we can calculate $v_{t|0,T}^{(q,\\theta)}(x)$ by comparing $u_{t|0,T}^{(q,\\theta)}(x)$ for the mixture of Gaussian path $q_{t|0,T}\\in$ $\\mathcal{Q}_{\\mathrm{MoG}}^{K}$ to the reference drift $\\boldsymbol{b}_{t}(\\boldsymbol{x})$ as in (14), and proceed to minimize its norm as in (9). In practice, we use Gumbel softmax reparamerization gradients (Maddison et al., 2016; Jang et al., 2017) to optimize the mixture weights $\\{w^{k}\\}_{k=1}^{K}$ alongside the neural network parameters $\\bar{\\{\\theta^{k}\\}}_{k=1}^{K}$ for each Gaussian component $\\{\\boldsymbol{\\mu}_{t|0,T}^{(\\theta)},\\boldsymbol{\\Sigma}_{t|0,T}^{(\\theta)}\\}_{k=1}^{K}$ and either first- or second-order dynamics. ", "page_idx": 6}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "(Aligned) Schr\u00f6dinger Bridge Matching Methods. Many existing \u2018bridge matching\u2019 approaches (Shi et al., 2023; Peluchetti, 2021, 2023; Liu et al., 2022; Lipman et al., 2022; Liu et al., 2023b) for SB and generative modeling rely on convenient properties of Brownian bridges and would require calculating $h$ -transforms to simulate bridges for general reference processes. Our conditional ", "page_idx": 6}, {"type": "text", "text": "Gaussian path parameterization is similar to Liu et al. (2023a); Neklyudov et al. (2024), where analytic bridges are not available for SB problems with nonlinear reference drift or general costs. ", "page_idx": 7}, {"type": "text", "text": "Somnath et al. (2023); Liu et al. (2023b) attempt to solve the SB problem given access to aligned data $x_{0},x_{T}\\sim q_{0,T}^{\\mathrm{data}}$ assumed to be drawn from an optimal coupling. While the method in Somnath et al. (2023) involves approximating an $h$ -transform, their goal is to obtain an unconditioned vector field $v_{t}$ to simulate a Markov process. However, De Bortoli et al. (2023) use Doob\u2019s $h_{-}$ -transform to argue the learned Markov process will not preserve the empirical coupling unless q0d,atTa i s the optimal coupling for the SB problem, and show that an \u2018augmented\u2019 $v_{0,t}$ which conditions on $x_{0}$ can correct this issue. ", "page_idx": 7}, {"type": "text", "text": "After training on a dataset of $x_{0},x_{T}\\,\\sim\\,q_{0,T}^{\\mathrm{data}}$ pairs using our method, we could consider using an (augmented) bridge matching objective (Shi et al., 2023; De Bortoli et al., 2023) to distill our learned $v_{t|0,T}^{(q)}$ into a vector field $v_{t}$ or $v_{0,t}$ which does not condition on the endpoint. Our use of a Gaussian path parameterization with samples from a fixed endpoint coupling and no Markovization step corresponds to a simplified version of the conditional optimal control step in Liu et al. (2023a). ", "page_idx": 7}, {"type": "text", "text": "Transition Path Sampling. We refer to the surveys of Dellago et al. (2002); Weinan and VandenEijnden (2010); Bolhuis and Swenson (2021) for an overview of the TPS problem. Least action principles for TPS have a long history, building upon the Freidlin-Wentzell (Freidlin and Wentzell, 1998) and Onsager-Machlup (Onsager and Machlup, 1953; D\u00fcrr and Bach, 1978) Lagrangian functionals in the zero-noise limit and finite-noise cases. In particular, the Onsager-Machlup functional relates maximum a posteriori estimators or \u2018most probable (conditioned) paths\u2019 to the minimizers of an action functional similar to Thm. 1, where example algorithms include (Vanden-Eijnden and Heymann, 2008; Sheppard et al., 2008). By contrast, our approach targets the entire posterior over transition paths using an expressive variational family. While Lu et al. (2017) provide analysis for the Gaussian family, we draw connections with Doob\u2019s $h$ -transform and extend to mixtures of Gaussians. ", "page_idx": 7}, {"type": "text", "text": "Shooting methods are among the most popular for sampling the posterior of transition paths. From a path that satisfies the boundary conditions (obtained, e.g., using high-temperature simulations), shooting picks points and directions to propose alterations, then simulates new trajectories and accepts or rejects using Metropolis-Hastings (MH) (Juraszek and Bolhuis, 2008; Borrero and Dellago, 2016; Jung et al., 2017; Falkner et al., 2023; Jung et al., 2023). While the MCMC corrections yield theoretical guarantees, shooting methods involve expensive molecular dynamics (MD) simulations and need to balance high rejection rates with large changes in trajectories. One-way shooting methods sample paths efficiently but yield highly correlated samples. Two-way shooting methods, which we compare to in Sec. 5, are more expensive but typically sample diverse paths faster. Recent machine learning approaches (e.g. Plainer et al. (2023); Leli\u00e8vre et al. (2023)) aim to reduce the need for MD. ", "page_idx": 7}, {"type": "text", "text": "Finally, various related methods rely on iterative simulation of SDE in (10) during training to learn the control drift term. Yan et al. (2022); Holdijk et al. (2023) are motivated from the perspective of stochastic optimal control, while Das et al. (2021); Rose et al. (2021) develop actor-critic methods using closely-related ideas from soft reinforcement learning. The variational method in Das et al. (2019) optimizes the rate function quantifying the probability of the rare events, while Singh and Limmer (2023) solves the Kolmogorov backward equation to learn the Doob\u2019s $h$ -transform. However, all of these methods may be inefficient if the desired terminal state is sampled infrequently. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We investigate the capabilities of our approach across a variety of different settings. We first illustrate features of our method on toy potentials before continuing to real-world molecular systems, including a commonly-used benchmark system, alanine dipeptide, and a small protein, Chignolin. The code behind our method is available at https://github.com/plainerman/variational-doob. Before diving into the experiments, we introduce the evaluation procedure and baseline methods. ", "page_idx": 7}, {"type": "text", "text": "Evaluation metrics. In our evaluation, we emphasize two key quantities: accuracy and efficiency. Efficiency is evaluated by the number of calls to the potential energy function, which requires extensive computation and dominates the runtime of larger molecules. For accuracy, we evaluate the log-likelihood of each sampled path and the maximum energy point (saddle point/transition state) along each sampled path. A good method samples many probable paths (i.e., high log-likelihood) and an accurate transition state (i.e., small maximum energy). See App. D for further details. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We compare our approach against the MCMC-based two-way shooting method with uniform point selection with variable or fixed length trajectories. We found that two-way shooting ", "page_idx": 7}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/cd49022737b27f92297fa662001f56e9c248bb85e2f43d1c51d151e4b1d67a4d.jpg", "img_caption": ["Figure 2: Comparing path histograms and trajectories of TPS using fixed-length two-way shooting and comparing it with our variational approach. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/6a628a81ff6babd3ef9b593cfd8665df7009c8502d874b9c2ff293956577fc61.jpg", "img_caption": ["Figure 3: Illustration of the expressivity of unimodal Gaussian versus mixture of Gaussian paths on a symmetric potential with two transition path modes. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ShJWT0n7kX/tmp/ff32579e19b9e0559fa86ec9e141b1590364c3703bf75c66d324f1be50601089.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: Transition path sampling experiment for M\u00fcller-Brown potential. We report the number of potential evaluations needed to sample 1,000 paths, as well as the maximum energy and the likelihood of each path (including mean and standard deviation). The methods marked with \u2018variable\u2019 use a variable length setting. MinMax energy reports the lowest maximum energy across all paths (i.e., energy of lowest transition state). ", "page_idx": 8}, {"type": "text", "text": "produced the most diverse path ensembles among possible baselines, although the acceptance probability can be relatively low for systems dominated by diffusive dynamics (Brotzakis and Bolhuis, 2016) and might be improved by better shooting point selection. This baseline gives theoretical guarantees about the ensemble and thus can be considered as a proxy for the ground truth. In that sense, our goal is not to beat two-way shooting but to approximate it with fewer potential evaluations. ", "page_idx": 8}, {"type": "text", "text": "5.1 Synthetic Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "M\u00fcller-Brown Potential. The M\u00fcller-Brown potential is a popular benchmark to study transition path sampling between metastable states. It consists of three local minima, and we aim to sample transition paths connecting state $A$ and state $B$ with a circular state definition. In Fig. 2, we visualize the potential and the sampled paths and can see that the same ensemble is sampled for both our method and two-way shooting. Our method exhibits a slightly reduced variance for unlikely transitions. In Table 1, we can observe that MCMC-based methods require many potential evaluations to achieve a good result, which comes from the low acceptance rate (especially when fixing the lengths of trajectories). Our method requires fewer energy evaluations (1 million vs. 1 billion) while finding paths with similar energy and likelihood. Note that the likelihood for variable approaches has been omitted, as it is governed by the number of steps in the trajectory and cannot be compared directly. ", "page_idx": 8}, {"type": "text", "text": "Gaussian Mixture. We further consider a potential in which the states are separated by a symmetric high-energy barrier that allows for two distinct reaction channels. In Fig. 3, we observe that a single Gaussian path cannot model a system with multiple modes of transition paths. Nevertheless, this issue can be resolved using a mixture of Gaussian paths, with slightly increased computational cost. ", "page_idx": 8}, {"type": "text", "text": "The Case for Neural Networks. According to our empirical study, the neural network parameterization of the Gaussian distribution statistics $\\mu_{t\\,|\\,0,T},\\Sigma_{t\\,|\\,0,T}$ is an invaluable part of our framework. As an ablation, we consider parameterizing $\\mu_{t\\,|\\,0,T},\\Sigma_{t\\,|\\,0,T}$ as piecewise linear splines whose intermediate points are updated using the same gradient-based optimizer as used for neural network training. In App. D.3, we report results comparing the W1 distance of learned marginals using neural network versus spline parameterizations, observing that splines yield inferior results even after an order of magnitude more potential function evaluations. We thus conclude that spline parameterizations are not competitive for learning transition paths and continue to focus on our neural-network approach. ", "page_idx": 8}, {"type": "text", "text": "5.2 Second-order Dynamics and Molecular Systems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experiment Setup. We evaluate our methods on real-world high-dimensional molecular systems governed by the second-order dynamics (4): alanine dipeptide and Chignolin. Alanine dipeptide is a well-studied system of 22 atoms (66 total degrees of freedom), where the molecule can be described by two collective variables (CV): the dihedral angles $\\phi$ , $\\psi$ . Chignolin is a larger system consisting of 10 residues with 138 atoms (414 total degrees of freedom) that cannot be summarized as easily. We use an AMBER14 force field (Maier et al., 2015) implemented in OpenMM (Eastman et al., 2017) but use DMFF (Wang et al., 2023) to backpropagate through the energy evaluations. ", "page_idx": 8}, {"type": "table", "img_path": "ShJWT0n7kX/tmp/caef6d44261fa81d6239e61cb2540e0c3f49e88b70e916774fb359ecc630ab62.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 2: Transition path sampling for alanine dipeptide. For MCMC methods, we compare different state definitions of $A,B$ : \u2018CV\u2019 uses $\\phi,\\psi$ angles. \u2018Exact\u2019 uses a very small threshold of aligned root-mean-square deviation (RMSD) around reference states $A,B$ (as in Ours). \u2018Relaxed\u2019 uses a larger threshold of RMSD around $A,B$ . The method marked with a \\* only samples 100 paths due to computational limitations, while others sample 1,000. Fields with $\\mathrm{N/A}$ are intractable as a single trajectory requires more than 1 billion potential evaluations. ", "page_idx": 9}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/78c473a8d9de54f8518344cfee869d52a1da61a4a0a8310d4a125504c573974c.jpg", "img_caption": ["Figure 4: Transition path for the protein Chignolin. The energy plot a transition path in which the protein folds in $T=1$ , $000f s$ , and passes a high energy barrier at $460f s$ with about $3,000\\,\\bar{\\frac{k J}{m o l}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Alanine Dipeptide. In Table 2, we report results for four variants of our models, which either predict Cartesian coordinates or internal coordinates in the form of bond lengths and dihedral angles (compare App. D.4), either with or without Gaussian mixture. For our method, operating in internal coordinates takes more iterations to converge but yields better results compared to Cartesian coordinates, where the internal coordinates are distributed similarly to Gaussians and our network does need not learn equivariances (Du et al., 2022). Similarly, Gaussian mixture paths perform slightly better than a single Gaussian path due to the additional expressiveness. We note that paths sampled with Gaussian mixture exhibit a larger variance in max energy as they represent multiple reaction channels. ", "page_idx": 9}, {"type": "text", "text": "We find that prior-informed definitions of the desired initial and target states (i.e., CV) are necessary for MCMC to work efficiently with fixed-length trajectories. Finding these CVs in practice is challenging and only possible in this instance because the molecule is small and well-studied. For the larger system size in Table 2, it becomes intractable to use MCMC to connect precise states $A,B$ (\u2018exact\u2019) instead of larger regions (\u2018relaxed\u2019), even with a single trajectory. Variable length MCMC with relaxed endpoint conditions with CV perform well on this task, but our method is competitive using fewer evaluations and more strict boundary conditions. Fixed-length MCMC, even with prior-informed knowledge, can only find 100 trajectories while needing 50 times more potential evaluations compared to variable length. ", "page_idx": 9}, {"type": "text", "text": "Chignolin. The folding dynamics of Chignolin already pose a challenge and have not yet been well-studied compared to alanine dipeptide. We illustrate the qualitative experimental results for this system in Fig. 4. Operating in Cartesian space, our model samples a feasible transition within $25.6\\mathrm{M}$ potential energy evaluation calls and a transition with a duration of $T=1p s$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion, Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose an efficient computational framework for transition path sampling with Brownian dynamics. We formulate the transition path sampling problem by using Doob\u2019s $h$ -transform to condition a reference stochastic process, and propose a variational formulation for efficient optimization. Specifically, we propose a simulation-free training objective and model parameterization that imposes boundary conditions as hard constraints. We compare our method with MCMC-based baselines and show comparable accuracy with lower computational costs on both synthetic datasets and real-world molecular systems. Our method is currently limited by rigidly defining states A and B to be a point mass with Gaussian noise instead of any arbitrary set. Finally, our method might be improved by accommodating variable length paths. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Juno Nam and Soojung Yang for spotting the unphysical steric barrier in the original pair of initial and target states and Guan-Horng Liu, Maurice Weiler, Hannes St\u00e4rk, and Yanze Wang for helpful discussions. The work of Yuanqi Du and Carla P. Gomes was supported by the Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship, a Schmidt Futures program; the National Science Foundation (NSF), the Air Force Office of Scientific Research (AFOSR); the Department of Energy; and the Toyota Research Institute (TRI). The work of Michael Plainer was supported by the Konrad Zuse School of Excellence in Learning and Intelligent Systems (ELIZA) through the DAAD program Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the German Ministry of Education and Research and by Max Planck Society. The work of Kirill Neklyudov was supported by IVADO. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Anderson, J. B. (2007). Predicting rare events in molecular dynamics. Advances in Chemical Physics, 91:381\u2013431.   \nBatatia, I., Benner, P., Chiang, Y., Elena, A. M., Kov\u00e1cs, D. P., Riebesell, J., Advincula, X. R., Asta, M., Baldwin, W. J., Bernstein, N., et al. (2023). A foundation model for atomistic materials chemistry. arXiv preprint arXiv:2401.00096.   \nBlau, S. M., Patel, H. D., Spotte-Smith, E. W. C., Xie, X., Dwaraknath, S., and Persson, K. A. (2021). A chemically consistent graph architecture for massive reaction networks applied to solid-electrolyte interphase formation. Chemical science, 12(13):4931\u20134939.   \nBolhuis, P. G. and Swenson, D. W. H. (2021). Transition path sampling as markov chain monte carlo of trajectories: Recent algorithms, software, applications, and future outlook. Advanced Theory and Simulations, 4(4).   \nBorrero, E. and Dellago, C. (2016). Avoiding traps in trajectory space: Metadynamics enhanced transition path sampling. The European Physical Journal Special Topics, 225(8-9):1609\u20131620.   \nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs.   \nBrotzakis, Z. F. and Bolhuis, P. G. (2016). A one-way shooting algorithm for transition path sampling of asymmetric barriers. The Journal of Chemical Physics, 145(16):164112.   \nCastellan, G. W. (1983). Physical Chemistry. Addison-Wesley, Reading, Mass, 3rd ed edition.   \nChen, T., Liu, G.-H., and Theodorou, E. (2021a). Likelihood training of schr\u00f6dinger bridge using forward-backward sdes theory. In International Conference on Learning Representations.   \nChen, Y., Georgiou, T. T., and Pavon, M. (2016). On the relation between optimal transport and schr\u00f6dinger bridges: A stochastic control viewpoint. Journal of Optimization Theory and Applications, 169:671\u2013691.   \nChen, Y., Georgiou, T. T., and Pavon, M. (2021b). Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schrodinger bridge. Siam Review, 63(2):249\u2013313.   \nChopin, N., Fulop, A., Heng, J., and Thiery, A. H. (2023). Computational doob h-transforms for online flitering of discretely observed diffusions. In International Conference on Machine Learning, pages 5904\u20135923. PMLR.   \nDas, A., Kuznets-Speck, B., and Limmer, D. T. (2022). Direct evaluation of rare events in active matter from variational path sampling. Physical Review Letters, 128(2):028005.   \nDas, A., Limmer, D. T., and Limmer, D. T. (2019). Variational control forces for enhanced sampling of nonequilibrium molecular dynamics simulations. The Journal of chemical physics, 151(24).   \nDas, A., Rose, D. C., Garrahan, J. P., and Limmer, D. T. (2021). Reinforcement learning of rare diffusive dynamics. The Journal of Chemical Physics, 155(13).   \nDe Bortoli, V., Liu, G.-H., Chen, T., Theodorou, E. A., and Nie, W. (2023). Augmented bridge matching. arXiv preprint arXiv:2311.06978.   \nDe Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709.   \nDellago, C., Bolhuis, P. G., and Geissler, P. L. (2002). Transition path sampling. Advances in chemical physics, 123:1\u201378.   \nDickson, A. (2018). Mapping the ligand binding landscape. Biophysical Journal, 115(9):1707\u20131719.   \nDoob, J. L. (1957). Conditional brownian motion and the boundary limits of harmonic functions. Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, 85:431\u2013458.   \nDu, W., Zhang, H., Du, Y., Meng, Q., Chen, W., Zheng, N., Shao, B., and Liu, T.-Y. (2022). Se (3) equivariant graph neural networks with complete local frames. In International Conference on Machine Learning, pages 5583\u20135608. PMLR.   \nD\u00fcrr, D. and Bach, A. (1978). The onsager-machlup function as lagrangian for the most probable path of a diffusion process. Communications in Mathematical Physics, 60:153\u2013170.   \nEastman, P., Swails, J., Chodera, J. D., McGibbon, R. T., Zhao, Y., Beauchamp, K. A., Wang, L.-P., Simmonett, A. C., Harrigan, M. P., Stern, C. D., Wiewiora, R. P., Brooks, B. R., and Pande, V. S. (2017). OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. PLOS Computational Biology, 13(7):e1005659.   \nEscobedo, F. A., Borrero, E. E., and Araque, J. C. (2009). Transition path sampling and forward flux sampling. applications to biological systems. Journal of Physics: Condensed Matter, 21(33):333101.   \nFalkner, S., Coretti, A., Romano, S., Geissler, P., and Dellago, C. (2023). Conditioning normalizing flows for rare event sampling. arXiv preprint arXiv:2207.14530.   \nFlamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. (2021). Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1\u20138.   \nFreidlin, M. and Wentzell, A. (1998). Random perturbations of Dynamical Systems. Springer.   \nGabri\u00e9, M., Rotskoff, G. M., and Vanden-Eijnden, E. (2022). Adaptive monte carlo augmented with normalizing flows. Proceedings of the National Academy of Sciences, 119(10):e2109420119.   \nHeng, J., De Bortoli, V., Doucet, A., and Thornton, J. (2021). Simulating diffusion bridges with score matching. arXiv preprint arXiv:2111.07243.   \nHoldijk, L., Du, Y., Hooft, F., Jaini, P., Ensing, B., and Welling, M. (2023). Stochastic optimal control for collective variable free sampling of molecular transition paths. Advances in Neural Information Processing Systems, 36.   \nJamison, B. (1975). The markov processes of schr\u00f6dinger. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 32(4):323\u2013331.   \nJang, E., Gu, S., and Poole, B. (2017). Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations (ICLR 2017). OpenReview. net.   \nJung, H., Covino, R., Arjun, A., Leitold, C., Dellago, C., Bolhuis, P. G., and Hummer, G. (2023). Machine-guided path sampling to discover mechanisms of molecular self-organization. Nature Computational Science, 3(4):334\u2013345.   \nJung, H., ichi Okazaki, K., and Hummer, G. (2017). Transition path sampling of rare events by shooting from the top. The Journal of Chemical Physics, 147(15).   \nJuraszek, J. and Bolhuis, P. G. (2008). Rate constant and reaction coordinate of trp-cage folding in explicit water. Biophysical Journal, 95(9):4246\u20134257.   \nKingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. International Conference on Learning Representations.   \nKirmizialtin, S., Johnson, K. A., and Elber, R. (2015). Enzyme selectivity of HIV reverse transcriptase: Conformations, ligands, and free energy partition. The Journal of Physical Chemistry B, 119(35):11513\u201311526.   \nKirmizialtin, S., Nguyen, V., Johnson, K. A., and Elber, R. (2012). How conformational dynamics of DNA polymerase select correct substrates: Experiments and simulations. Structure, 20(4):618\u2013627.   \nKlucznik, T., Syntrivanis, L.-D., Bas\u00b4, S., Mikulak-Klucznik, B., Moskal, M., Szymkuc\u00b4, S., Mlynarski, J., Gadina, L., Beker, W., Burke, M. D., et al. (2024). Computational prediction of complex cationic rearrangement outcomes. Nature, 625(7995):508\u2013515.   \nKoehl, P. and Orland, H. (2022). Sampling constrained stochastic trajectories using brownian bridges. The Journal of Chemical Physics, 157(5).   \nLeli\u00e8vre, T., Robin, G., Sekkat, I., Stoltz, G., and Cardoso, G. V. (2023). Generative methods for sampling transition paths in molecular dynamics. ESAIM: Proceedings and Surveys, 73:238\u2013256.   \nL\u00e9onard, C. (2014). A survey of the schr\u00f6dinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems-A, 34(4):1533\u20131574.   \nLipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. (2022). Flow matching for generative modeling. International Conference on Learning Representations.   \nLiu, G.-H., Lipman, Y., Nickel, M., Karrer, B., Theodorou, E., and Chen, R. T. (2023a). Generalized schr\u00f6dinger bridge matching. In The Twelfth International Conference on Learning Representations.   \nLiu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E., Nie, W., and Anandkumar, A. (2023b). $\\mathrm{I}^{2}\\mathrm{sb}$ : Image-to-image schr\u00f6dinger bridge. arXiv preprint arXiv:2302.05872.   \nLiu, X., Wu, L., Ye, M., and Liu, Q. (2022). Let us build bridges: Understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699.   \nLiu, X., Wu, L., Ye, M., and qiang liu (2023c). Learning diffusion bridges on constrained domains. In The Eleventh International Conference on Learning Representations.   \nLu, Y., Stuart, A., and Weber, H. (2017). Gaussian approximations for transition paths in brownian dynamics. SIAM Journal on Mathematical Analysis, 49(4):3005\u20133047.   \nMaddison, C. J., Mnih, A., and Teh, Y. W. (2016). The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations.   \nMaier, J. A., Martinez, C., Kasavajhala, K., Wickstrom, L., Hauser, K. E., and Simmerling, C. (2015). ff14sb: improving the accuracy of protein side chain and backbone parameters from ff99sb. Journal of chemical theory and computation, 11(8):3696\u20133713.   \nMehdi, S., Smith, Z., Herron, L., Zou, Z., and Tiwary, P. (2024). Enhanced sampling with machine learning. Annual Review of Physical Chemistry, 75.   \nNeklyudov, K., Brekelmans, R., Severo, D., and Makhzani, A. (2023). Action matching: Learning stochastic dynamics from samples. In International Conference on Machine Learning.   \nNeklyudov, K., Brekelmans, R., Tong, A., Atanackovic, L., Liu, Q., and Makhzani, A. (2024). A computational framework for solving wasserstein lagrangian flows. International Conference on Machine Learning.   \nNo\u00e9, F., Olsson, S., K\u00f6hler, J., and Wu, H. (2019). Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457):eaaw1147.   \nNo\u00e9, F., Sch\u00fctte, C., Vanden-Eijnden, E., Reich, L., and Weikl, T. R. (2009). Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations. Proceedings of the National Academy of Sciences, 106(45):19011\u201319016.   \nOnsager, L. and Machlup, S. (1953). Fluctuations and irreversible processes. Physical Review, 91(6):1505.   \nPapaspiliopoulos, O. and Roberts, G. (2012). Importance sampling techniques for estimation of diffusion models. Statistical methods for stochastic differential equations, 124:311\u2013340.   \nPeluchetti, S. (2021). Non-denoising forward-time diffusions.   \nPeluchetti, S. (2023). Diffusion bridge mixture transports, Schr\u00f6dinger bridge problems and generative modeling. arXiv preprint arXiv:2304.00917.   \nPlainer, M., St\u00e4rk, H., Bunne, C., and G\u00fcnnemann, S. (2023). Transition path sampling with boltzmann generator-based MCMC moves. In Generative AI and Biology Workshop.   \nRezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR.   \nRose, D. C., Mair, J. F., and Garrahan, J. P. (2021). A reinforcement learning approach to rare trajectory sampling. New Journal of Physics, 23(1):013013.   \nRotskoff, G. M. (2024). Sampling thermodynamic ensembles of molecular systems with generative neural networks: Will integrating physics-based models close the generalization gap? Current Opinion in Solid State and Materials Science, 30:101158.   \nS\u00e4rkk\u00e4, S. and Solin, A. (2019). Applied stochastic differential equations, volume 10. Cambridge University Press.   \nSchauer, M., van der Meulen, F., and van Zanten, H. (2017). Guided proposals for simulating multi-dimensional diffusion bridges. Bernoulli, 23(4A).   \nSchr\u00f6dinger, E. (1932). Sur la th\u00e9orie relativiste de l\u2019\u00e9lectron et l\u2019interpr\u00e9tation de la m\u00e9canique quantique. In Annales de l\u2019institut Henri Poincar\u00e9, volume 2, pages 269\u2013310.   \nSelli, D., Boulfelfel, S. E., Schapotschnikow, P., Donadio, D., and Leoni, S. (2016). Hierarchical thermoelectrics: crystal grain boundaries as scalable phonon scatterers. Nanoscale, 8(6):3729\u2013 3738.   \nSharma, N. D., Singh, J., Vijay, A., Samanta, K., Dogra, S., and Bandyopadhyay, A. K. (2016). Pressure-induced structural transition trends in nanocrystalline rare-earth sesquioxides: A raman investigation. The Journal of Physical Chemistry C, 120(21):11679\u201311689.   \nSheppard, D., Terrell, R., and Henkelman, G. (2008). Optimization methods for finding minimum energy paths. The Journal of chemical physics, 128(13).   \nShi, Y., De Bortoli, V., Campbell, A., and Doucet, A. (2023). Diffusion schr\u00f6dinger bridge matching. arXiv preprint arXiv:2303.16852.   \nShoghi, N., Kolluru, A., Kitchin, J. R., Ulissi, Z. W., Zitnick, C. L., and Wood, B. M. (2023). From molecules to materials: Pre-training large generalizable models for atomic property prediction. In The Twelfth International Conference on Learning Representations.   \nSidky, H., Chen, W., and Ferguson, A. L. (2020). Molecular latent space simulators. Chemical Science, 11(35):9459\u20139467.   \nSingh, A. N. and Limmer, D. T. (2023). Variational deep learning of equilibrium transition path ensembles. The Journal of Chemical Physics, 159(2).   \nSmith, J. S., Isayev, O., and Roitberg, A. E. (2017). Ani-1: an extensible neural network potential with dft accuracy at force field computational cost. Chemical science, 8(4):3192\u20133203.   \nSomnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R., Krause, A., and Bunne, C. (2023). Aligned diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, pages 1985\u20131995. PMLR.   \nTong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. (2023). Conditional flow matching: Simulation-free dynamic optimal transport. arXiv preprint arXiv:2302.00482, 2(3).   \nVanden-Eijnden, E. and Heymann, M. (2008). The geometric minimum action method for computing minimum energy paths. The Journal of chemical physics, 128(6).   \nWang, H., Zhang, L., Han, J., and Weinan, E. (2018). Deepmd-kit: A deep learning package for manybody potential energy representation and molecular dynamics. Computer Physics Communications, 228:178\u2013184.   \nWang, X., Li, J., Yang, L., Chen, F., Wang, Y., Chang, J., Chen, J., Feng, W., Zhang, L., and Yu, K. (2023). DMFF: An open-source automatic differentiable platform for molecular force field development and molecular dynamics simulation. Journal of Chemical Theory and Computation, 19(17):5897\u20135909.   \nWeinan, E. and Vanden-Eijnden, E. (2010). Transition-path theory and path-finding algorithms for the study of rare events. Annual review of physical chemistry, 61(2010):391\u2013420.   \nWu, L., Gong, C., Liu, X., Ye, M., and Liu, Q. (2022). Diffusion-based molecule generation with informative prior bridges. Advances in Neural Information Processing Systems, 35:36533\u201336545.   \nXi, L., Shah, M., and Trout, B. L. (2013). Hopping of water in a glassy polymer studied via transition path sampling and likelihood maximization. The Journal of Physical Chemistry B, 117(13):3634\u20133647.   \nYan, J., Touchette, H., and Rotskoff, G. M. (2022). Learning nonequilibrium control forces to characterize dynamical phase transitions. Physical Review E, 105(2):024115.   \nZeng, J., Cao, L., Xu, M., Zhu, T., and Zhang, J. Z. (2020). Complex reaction processes in combustion unraveled by neural network-based molecular dynamics simulation. Nature communications, 11(1):5713.   \nZhang, D., Bi, H., Dai, F.-Z., Jiang, W., Zhang, L., and Wang, H. (2022). Dpa-1: Pretraining of attention-based deep potential model for molecular simulation. arXiv preprint arXiv:2208.08236. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proofs from Sec. 2.2 (Doob\u2019s $h$ -Transform Background) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 1. [Jamison (1975, Thm. 2)] Let $h_{\\mathcal{B}}(x,t)\\;:=\\;\\rho_{T}(x_{T}\\;\\in\\;\\mathcal{B}\\,|\\,x_{t}\\;=\\;x)$ denote the conditional transition probability of the reference process in (5). Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}_{0:T}^{*}:\\quad\\quad d x_{t|T}=\\Big(b_{t}(x_{t|T})+2G_{t}\\nabla_{x}\\log h_{B}(x_{t|T},t)\\Big)\\cdot d t+\\Xi_{t}\\ d W_{t}\\qquad x_{0}\\sim\\rho_{0}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we use $x_{t|T}$ to denote a conditional process. The SDE in (6) is associated with the following transition probabilities for $s<t<T$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho_{t}(y\\,\\vert\\,x_{s}=x,x_{T}\\in\\mathcal{B})=\\frac{h_{B}(y,t)}{h_{B}(x,s)}\\rho_{t}(y\\,\\vert\\,x_{s}=x),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that all of our subsequent results hold for the case when $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is a point-mass, with the only change being that the $h$ -function becomes a density, $h_{B}(x,t)=\\rho_{T}(B\\,|\\,x_{t}\\,\\stackrel{.}{=}x)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. See Jamison (1975) for a simple proof based on Ito\u2019s Lemma, assuming smoothness and strict positivity of $h$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proposition 2. The following PDEs are obeyed by (a) the density of the conditioned process $\\bar{\\rho_{t|0,T}}\\bar{(x)}:=\\rho_{t}(x\\,|\\,x_{0}\\stackrel{.}{=}A,x_{T}\\in\\mathcal{B})$ and $(b)$ the $h$ -function $h_{B}(x,t)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial\\rho_{t|0,T}(x)}{\\partial t}+\\bigl\\langle\\nabla_{x}\\,\\rho_{t|0,T}(x)\\bigl(b_{t}(x)+2G_{t}\\nabla_{x}\\log h_{B}(x,t)\\bigr)\\bigr\\rangle-\\displaystyle\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}\\rho_{t|0,T}(x)=0\\,,}\\\\ {\\displaystyle\\frac{\\partial h_{B}(x,t)}{\\partial t}+\\bigl\\langle\\nabla_{x}h_{B}(x,t),b_{t}(x)\\bigr\\rangle+\\displaystyle\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}h_{B}(x,t)=0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Reparameterizing (8b) in terms of $s_{B}(x,t):=\\log h_{B}(x,t),$ , we can also write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{B}(x,t)}{\\partial t}+\\langle\\nabla s_{B}(x,t),G_{t}\\nabla s_{B}(x,t)\\rangle+\\langle\\nabla s_{B}(x,t),b_{t}(x)\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}s_{B}(x,t)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $p(x_{t+s}=y\\,|\\,x_{t}=x)$ denote the transition probability of a reference diffusion process ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial s}p(x_{t+s}=y\\mid x_{t}=x)=-\\big\\langle\\nabla_{y},p(x_{t+s}=y\\mid x_{t}=x)b_{t+s}(y)\\big\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}p(x_{t+s}=y\\mid x_{t}=x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(G_{t})_{i j}=\\textstyle{\\frac{1}{2}}\\Xi_{t+s}\\Xi_{t+s}^{T}$ . ", "page_idx": 15}, {"type": "text", "text": "Now we condition the process on the end-point value $x_{T}\\in\\boldsymbol{B}$ , and we get another kernel, i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\np(x_{t+s}=y\\,|\\,x_{t}=x,x_{T}\\in\\mathcal{B})=\\frac{p(x_{T}\\in\\mathcal{B}\\,|\\,x_{t+s}=y)}{p(x_{T}\\in\\mathcal{B}\\,|\\,x_{t}=x)}p(x_{t+s}=y\\,|\\,x_{t}=x)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We let $h_{\\mathcal{B}}(x,t)=p(x_{T}\\in\\mathcal{B}\\,|\\,x_{t}=x)$ denote the conditional probability over the desired endpoint condition given $x_{t}=x$ . According to laws of conditional probability, we can describe how $h_{B}(x,t)$ changes in time using the unconditioned transition probability ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underbrace{p(x_{T}\\in\\mathcal{B}\\,|\\,x_{t}=x)}_{h.(x,t)}=\\int d y\\,\\underbrace{p(x_{T}\\in\\mathcal{B}\\,|\\,x_{t+s}=y)}_{h.(y,t+s)}p(x_{t+s}=y\\,|\\,x_{t}=x)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we take the derivative $\\frac{\\partial}{\\partial s}$ on both sides, and we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n0=\\int d y\\,\\left[p(x_{t+s}=y\\,|\\,x_{t}=x)\\frac{\\partial h_{B}(y,t+s)}{\\partial s}+\\frac{\\partial p(x_{t+s}=y\\,|\\,x_{t}=x)}{\\partial s}h_{B}(y,t+s)\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the FP equation for the transition probability and integrating by parts, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n:=\\int d y\\,p(x_{t+s}=y\\,|\\,x_{t}=x)\\left[\\frac{\\partial h_{B}(y,t+s)}{\\partial s}+\\left\\langle\\nabla_{y}h_{B}(y,t+s),b_{t}(y)\\right\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}h_{B}(y,t+s)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that this holds $\\forall x$ , hence, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial h_{B}(y,t+s)}{\\partial s}+\\langle\\nabla_{y}h_{B}(y,t+s),b_{t+s}(y)\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}h_{B}(y,t+s)=0\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "without any loss of generality we can set $t=0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial h_{B}(y,s)}{\\partial s}+\\big\\langle\\nabla_{y}h_{B}(y,s),b_{s}(y)\\big\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}h_{B}(y,s)=0\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as desired to prove the optimality condition in (8b). ", "page_idx": 16}, {"type": "text", "text": "To prove (8a), denote $p(y,s)=p(x_{s}=y\\,|\\,x_{0}=x)$ and differentiate $p(x_{s}=y\\,\\vert\\,x_{0}=x,x_{T}\\in B)=$ hBB(x,0)p(y, s) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial s}p(x_{s}=y\\mid x_{0}=x,x_{T}\\in B)}\\\\ &{=\\frac{1}{h_{S}(x,0)}\\bigg[p(y,s)\\frac{\\partial h_{S}(y,s)}{\\partial s}+h_{B}(y,s)\\frac{\\partial p(y,s)}{\\partial s}\\bigg]}\\\\ &{=\\frac{1}{h_{S}(x,0)}\\bigg[-\\langle\\nabla_{y}h_{S}(y,s),p(y,s)b_{s}(y)\\rangle-p(y,s)\\sum_{i j}(G_{i})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}h_{B}(y,s)}\\\\ &{\\qquad\\qquad-h_{B}(y,s)\\langle\\nabla_{y},p(y,s)b_{s}(y)\\rangle+h_{B}(y,s)\\sum_{i j}(G_{i})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}p(y,s)\\bigg]}\\\\ &{=\\ -\\Big\\langle\\nabla_{y}\\cdot\\frac{h_{B}(y,s)}{h_{S}(x,0)}p(y,s)b_{s}(y)\\Big\\rangle-p(y,s)\\Big\\langle\\nabla_{y},2D\\nabla_{y}\\frac{h_{B}(y,s)}{h_{B}(x,0)}\\Big\\rangle}\\\\ &{\\qquad\\qquad+\\Big\\langle\\nabla_{y}p(y,s),2D\\nabla_{y}\\frac{h_{B}(y,s)}{h_{B}(x,0)}\\Big\\rangle+\\sum_{i j}(G_{i})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}\\Big(\\frac{h_{B}(y,s)}{h_{B}(x,0)}p(y,s)\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $h_{B}(x,0)$ can be pulled outside the differential operator because it is a function of $x$ . The PDE for the new kernel $\\bar{p(y,s\\,|\\,B)}=p(x_{s}=y\\,|\\,x_{0}=x,\\mathbf{\\bar{\\boldsymbol{x}}}_{T}\\in\\mathcal{B})$ (conditioned on the end-point) becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial s}p(y,s\\,|\\,B)=-\\big\\langle\\nabla_{y},p(y,s\\,|\\,B)(b_{s}(y)+2D\\nabla_{y}\\log h_{B}(y,s))\\big\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial y_{i}\\partial y_{j}}p(y,s\\,|\\,B)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which matches the desired PDE in (8a) thereby proving the first two parts of Prop. 2. ", "page_idx": 16}, {"type": "text", "text": "Finally, to show (8c), we index time using $t$ in Eq. (22) and change variables $h_{B}(x,t)=e^{s(x,t)}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{\\partial e^{s(x,t)}}{\\partial t}+\\left\\langle\\nabla_{x}e^{s(x,t)},b_{t}(x)\\right\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}e^{s(x,t)}=0\\,.}\\\\ &{}&{\\displaystyle e^{s(x,t)}\\frac{\\partial s(x,t)}{\\partial t}+e^{s(x,t)}\\bigl\\langle\\nabla_{x}s(x,t),b_{t}(x)\\bigr\\rangle+\\Bigl\\langle\\nabla,G_{t}\\nabla e^{s(x,t)}\\Bigr\\rangle=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we simplify $\\begin{array}{r c l}{\\left\\langle\\nabla,G_{t}\\nabla e^{s(x,t)}\\right\\rangle}&{=}&{\\left\\langle\\nabla,G_{t}e^{s(x,t)}\\nabla s(x,t)\\right\\rangle\\ =\\ \\left\\langle\\nabla e^{s(x,t)},G_{t}\\nabla s(x,t)\\right\\rangle\\ +}\\end{array}$ $e^{s(x,t)}\\langle\\nabla,G_{t}\\nabla s(x,t)\\rangle=e^{s(x,t)}\\langle\\nabla s(x,t),G_{t}\\nabla s(x,t)\\rangle+e^{s(x,t)}\\langle\\nabla,G_{t}\\nabla s(x,t)\\rangle\\mathrm{~to~finally}$ write ", "page_idx": 16}, {"type": "equation", "text": "$$\ns(x,t)\\left(\\frac{\\partial s(x,t)}{\\partial t}+\\langle\\nabla_{x}s(x,t),b_{t}(x)\\rangle+\\langle\\nabla s(x,t),G_{t}\\nabla s(x,t)\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}s(x,t)\\right)=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which demonstrates (8c) since the inner term must be zero. ", "page_idx": 16}, {"type": "text", "text": "A.2 Proofs from Sec. 3.1 (Lagrangian Action Minimization for Doob\u2019s $h$ -Transform) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We begin by proving Cor. 1, whose proof actually contains the initial steps needed to prove our main theorem Thm. 1. In both proofs, we omit conditioning notation $q_{t}\\gets q_{t|0,T}$ for simplicity and assume $q_{t}(x)s_{t}(x)\\to0$ vanishes at the boundary $x\\rightarrow\\pm\\infty$ , which is used when integrating by parts in $x$ . Corollary 1. The Lagrangian objective in Thm. 1 which solves Doob\u2019s $h$ -transform is equivalent to ${\\mathrm{?}}=\\operatorname*{min}_{q}\\operatorname*{max}_{s}\\,s_{B}(B,T)-s_{B}(A,0)-\\int_{0}^{T}\\!\\!d t\\int\\!d x\\,q_{t|0,T}\\bigg(\\frac{\\partial s_{B}}{\\partial t}+\\big\\langle\\nabla s_{B},G_{t}\\nabla s_{B}\\big\\rangle+\\big\\langle\\nabla s_{B},b_{t}\\big\\rangle+\\big\\langle\\nabla,G_{t}\\nabla s_{B}\\big\\rangle\\bigg)$ $i f q_{t\\vert0,T}$ satisfies $(9\\mathrm{c})$ . Note $v_{t|0,T}(x)=\\nabla_{x}s_{B}(x,t),$ , with $s_{B}^{*}(x,t)=\\log h_{B}(x,t)$ at optimality. 4 ", "page_idx": 17}, {"type": "text", "text": "Proof. Consider the following action functional ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{{\\displaystyle{\\mathcal{S}}=}}&{{\\displaystyle\\operatorname*{min}_{q,v}\\int d t\\,\\int d x\\ q_{t}(x)\\big\\langle v_{t}(x),G_{t}v_{t}(x)\\big\\rangle\\,,}}&{{}}\\\\ {{\\mathrm{s.t.}}}&{{\\displaystyle\\frac{\\partial q_{t}(x)}{\\partial t}=-\\big\\langle\\nabla_{x},q_{t}(x)\\big(b_{t}(x)+2G_{t}v_{t}(x)\\big)\\big\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}q_{t}(x)\\,,}}&{{}}\\\\ {{\\mathrm{}}}&{{\\displaystyle q_{0}(x)=\\delta(x-A),\\ \\ q_{1}(x)=\\delta(x-B)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The Lagrangian of this optimization problem is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\int_{0}^{T}d t\\;\\int d x\\left[q_{t}\\bigl\\langle v_{t},G_{t}v_{t}\\bigr\\rangle+s_{t}\\left(\\frac{\\partial q_{t}}{\\partial t}+\\bigl\\langle\\nabla,q_{t}(b_{t}+2G_{t}v_{t})\\bigr\\rangle-\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}q_{t}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $s_{t}$ is the dual variable and we omit the optimization arguments, with $\\begin{array}{r}{S=\\operatorname*{min}_{\\boldsymbol{q},\\boldsymbol{v}}\\operatorname*{max}_{\\boldsymbol{s}}\\mathcal{L}}\\end{array}$ Swapping the order of optimizations under strong duality, we take the variation with respect to $v_{t}$ in an arbitrary direction ${\\mathfrak{h}}_{t}$ . Using $G_{t}=G_{t}^{T}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\delta\\mathcal{L}}{\\delta v_{t}}[\\mathfrak{h}_{t}]=\\ q_{t}\\Big\\langle(G_{t}+G_{t}^{T})v_{t},\\mathfrak{h}_{t}\\Big\\rangle-q_{t}\\Big\\langle2G_{t}^{T}\\nabla s_{t},\\mathfrak{h}_{t}\\Big\\rangle=0}\\\\ &{\\quad\\quad\\quad\\implies v_{t}=\\nabla s_{t}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting into the above, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\int_{0}^{T}d t\\;\\int d x\\;\\left[s_{t}\\frac{\\partial q_{t}}{\\partial t}-q_{t}\\Big\\langle\\nabla s_{t},G_{t}\\nabla s_{t}\\Big\\rangle+s_{t}\\Big\\langle\\nabla,q_{t}b_{t}\\Big\\rangle-s_{t}\\Big\\langle\\nabla,G_{t}\\nabla q_{t}\\Big\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Integrating by parts in $t$ and in $x$ , assuming that $q_{t}(x)s_{t}(x)\\to0$ as $x\\rightarrow\\pm\\infty$ , yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\dot{\\Sigma}=\\int d x\\;q_{T}s_{T}-\\int d x\\;q_{0}s_{0}+\\int_{0}^{T}d t\\;\\int d x\\;\\biggl[-q_{t}\\frac{\\partial s_{t}}{\\partial t}-q_{t}\\Bigl\\langle\\nabla s_{t},G_{t}\\nabla s_{t}\\Bigr\\rangle-q_{t}\\Bigl\\langle\\nabla s_{t},b_{t}\\Bigr\\rangle+\\Bigl\\langle\\nabla s_{t},G_{t}\\nabla q_{t}\\Bigr\\rangle\\biggr]_{\\Gamma_{\\theta}}}}\\\\ {{\\displaystyle}}\\\\ {{\\,=\\int d x\\;q_{T}s_{T}-\\int d x\\;q_{0}s_{0}+\\int_{0}^{T}d t\\;\\int d x\\;\\biggl[-q_{t}\\frac{\\partial s_{t}}{\\partial t}-q_{t}\\Bigl\\langle\\nabla s_{t},G_{t}\\nabla s_{t}\\Bigr\\rangle-q_{t}\\Bigl\\langle\\nabla s_{t},b_{t}\\Bigr\\rangle-q_{t}\\Bigl\\langle\\nabla,G_{t}\\nabla s_{t},\\dots\\Bigr\\rangle\\biggr]_{\\Gamma_{\\theta}}}}\\\\ {{\\displaystyle}}\\\\ {{\\,=\\int d x\\;q_{T}s_{T}-\\int d x\\;q_{0}s_{0}-\\int_{0}^{T}d t\\;\\int d x\\;q_{t}\\Bigl[\\frac{\\partial s_{t}}{\\partial t}+\\Bigl\\langle\\nabla s_{t},G_{t}\\nabla s_{t}\\Bigr\\rangle+\\Bigl\\langle\\nabla s_{t},b_{t}\\Bigr\\rangle+\\Bigl\\langle\\nabla,G_{t}\\nabla s_{t}\\Bigr\\rangle\\Bigr]_{\\Gamma_{\\theta}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the second line, we integrate by parts in $x$ again. Enforcing $q_{T}(x)\\;=\\;\\delta(x\\,-\\,B)$ and $q_{0}(x)=\\delta(x-A)$ and recalling $S=\\operatorname*{min}_{q}\\operatorname*{max}_{s.}\\mathcal{L}$ $\\mathcal{L}$ after eliminating $v_{t}$ , we recover the optimization in the statement of the corollary. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Theorem. 1. The following Lagrangian action functional has a unique solution which matches the Doob $h$ -transform in Prop. 2, ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\cal S}=\\ \\operatorname*{min}_{q.,v.}\\int_{0}^{T}d t\\ \\int d x\\;q_{t|0,T}(x)\\langle v_{t|0,T}(x),G_{t}\\;v_{t|0,T}(x)\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\iota.t\\quad\\frac{\\partial q_{t|0,T}(x)}{\\partial t}=-\\bigl\\langle\\nabla_{x},q_{t|0,T}(x)\\bigl(b_{t}(x)+2G_{t}\\;v_{t|0,T}(x)\\bigr)\\bigr\\rangle+\\sum_{i j}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}q_{t|0,T}(x),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{0}(x)=\\delta(x-A),\\qquad q_{T}(x)=\\delta(x-B)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Namely, the optimal $q_{t|0,T}^{*}(x)$ obeys (8a) and the optimal $\\boldsymbol{v}_{t|0,T}^{*}(\\boldsymbol{x})=\\nabla_{\\boldsymbol{x}}\\log h_{B}(\\boldsymbol{x},t)=\\nabla_{\\boldsymbol{x}}s(\\boldsymbol{x},t)$ follows (8b) or (8c). ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof proceeds from (25) above, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S=\\underset{q}{\\operatorname*{min}}\\underset{s}{\\operatorname*{max}}\\,\\mathcal{L}=\\underset{q}{\\operatorname*{min}}\\,\\underset{s}{\\operatorname*{max}}\\int_{0}^{T}d t\\;\\int d x\\,\\left[s_{t}\\frac{\\partial q}{\\partial t}-q_{t}\\big\\langle\\nabla s_{t},G_{t}\\nabla s_{t}\\big\\rangle+s_{t}\\big\\langle\\nabla,q_{t}b_{t}\\big\\rangle-s_{t}\\big\\langle\\nabla,G_{t}\\nabla q_{t}\\big\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We first show that the optimality condition with respect to $s_{t}$ yields the Fokker-Planck equation for $q_{t}$ in Prop. 2 (8a), before deriving the PDE in (8b) as the optimality condition with respect to $q_{t}$ . ", "page_idx": 18}, {"type": "text", "text": "Optimality Condition for (27) recovers Prop. 2 (8a): The variation with respect to $s_{t}$ of (28) is simple, apart from the intermediate term. For a perturbation direction ${\\mathfrak{h}}_{t}$ , we seek ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int d x\\;{\\frac{\\delta(\\bullet)}{\\delta s_{t}}}\\;\\mathfrak{h}_{t}={\\frac{d}{d\\varepsilon}}\\left[-\\int d x\\;q_{t}\\langle\\nabla(s_{t}+\\varepsilon\\mathfrak{h}_{t}),G_{t}\\nabla(s_{t}+\\varepsilon\\mathfrak{h}_{t})\\rangle\\right]\\Big|_{\\varepsilon=0},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\cdot$ indicates the functional on the right hand side. Proceeding to differentiate with respect to $\\varepsilon$ , we use linearity to pull $\\frac{d}{d\\varepsilon}$ inside the integral and apply it first to obtain $\\begin{array}{r}{\\frac{d}{d\\varepsilon}\\big(s_{t}+\\varepsilon\\mathfrak{h}_{t}\\big)=\\mathfrak{h}_{t}}\\end{array}$ . Using the product rule, recognizing the symmetry of terms, and evaluating at $\\varepsilon=0$ , we are left with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int d x\\;{\\frac{\\delta(\\bullet)}{\\delta\\,s_{t}}}\\;\\mathfrak{h}_{t}=\\left[-2\\int d x\\;q_{t}\\bigl\\langle\\nabla\\mathfrak{h}_{t},G_{t}\\nabla s_{t}\\bigr\\rangle\\right]\\overset{(i)}{=}\\left[\\int d x\\;\\mathfrak{h}_{t}\\Bigl(2\\bigl\\langle\\nabla,q_{t}G_{t}\\nabla s_{t}\\bigr\\rangle\\Bigr)\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in $(i)$ we integrate by parts $x$ . ", "page_idx": 18}, {"type": "text", "text": "We are now ready to set the variation of (28) with respect to $s_{t}$ (in an arbitrary direction ${\\mathfrak{h}}_{t}$ ) equal to zero. Using (29), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\delta\\mathcal{L}}{\\delta s_{t}}[\\mathfrak{h}_{t}]=0=\\frac{\\partial q_{t}}{\\partial t}+2\\bigl\\langle\\nabla,q_{t}G_{t}\\nabla s_{t}\\bigr\\rangle+\\bigl\\langle\\nabla,q_{t}b_{t}\\bigr\\rangle-\\bigl\\langle\\nabla,G_{t}\\nabla q_{t}\\bigr\\rangle}}\\\\ &{}&{\\implies\\ 0=\\frac{\\partial q_{t}}{\\partial t}+\\Bigl\\langle\\nabla,q_{t}\\Bigl(b_{t}+2G_{t}\\nabla s_{t}\\Bigr)\\Bigr\\rangle-\\bigl\\langle\\nabla,G_{t}\\nabla q_{t}\\bigr\\rangle\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which matches the desired optimality condition for the conditioned process in Prop. 2 (8a). ", "page_idx": 18}, {"type": "text", "text": "Optimality Condition for (27) recovers Prop. 2 (8b): Starting again from (28), we take the variation with respect to $q_{t}$ . First, we repeat identical steps (integrate by parts in both $x$ and $t$ ) to reach (26), ", "page_idx": 18}, {"type": "equation", "text": "$$\n:=\\int d x\\;q_{T}s_{T}-\\int d x\\;q_{0}s_{0}-\\int_{0}^{T}d t\\;\\int d x\\;q_{t}\\biggl[\\frac{\\partial s_{t}}{\\partial t}+\\bigl<\\nabla s_{t},G_{t}\\nabla s_{t}\\bigr>+\\bigl<\\nabla s_{t},b_{t}\\bigr>+\\bigl<\\nabla,G_{t}\\nabla s_{t}\\bigr>\\biggr]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where it is now clear that taking the variation with respect to $q_{t}$ and setting equal to zero yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\delta\\mathcal{L}}{\\delta q_{t}}[\\mathfrak{h}_{t}]=0=\\frac{\\partial s_{t}}{\\partial t}+\\left\\langle\\nabla s_{t},G_{t}\\nabla s_{t}\\right\\rangle+\\left\\langle\\nabla s_{t},b_{t}\\right\\rangle+\\left\\langle\\nabla,G_{t}\\nabla s_{t}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the desired PDE for $s(x,t)=\\log h_{B}(x,t)$ in (8c). To obtain (8b), we note an identity used to simplify the last term ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i i}(G_{t})_{i j}\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}\\log h_{t}=\\left\\langle\\nabla,G_{t}\\nabla\\log h_{t}\\right\\rangle=\\left\\langle\\nabla,\\frac{1}{h_{t}}G_{t}\\nabla h_{t}\\right\\rangle=-\\frac{1}{h_{t}^{2}}\\Big\\langle\\nabla h_{t},G_{t}\\nabla h_{t}\\Big\\rangle+\\frac{1}{h_{t}}\\Big\\langle\\nabla,G_{t}\\nabla h_{t}\\Big\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, substituting $s(x,t)=\\log h_{B}(x,t)$ into Eq. (31) and abbreviating $\\log h_{B}(\\bullet,t)=\\log h_{t}(\\bullet)$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{h_{t}}\\frac{\\partial h_{t}}{\\partial t}+\\frac{1}{h_{t}^{2}}\\langle\\nabla h_{t},G_{t}\\nabla h_{t}\\rangle+\\frac{1}{h_{t}}\\langle\\nabla h_{t},b_{t}\\rangle-\\frac{1}{h_{t}^{2}}\\langle\\nabla h_{t},G_{t}\\nabla h_{t}\\rangle+\\frac{1}{h_{t}}\\langle\\nabla,G_{t}\\nabla h_{t}\\rangle=0\\,,}\\\\ {\\implies\\quad\\frac{\\partial h_{t}(x)}{\\partial t}+\\langle\\nabla h_{t}(x),b_{t}(x)\\rangle+\\langle\\nabla,G_{t}\\nabla h_{t}\\rangle=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which matches (8b) as desired. ", "page_idx": 18}, {"type": "text", "text": "The last equation defines the backward Kolmogorov equation for the diffusion process with the drift $\\boldsymbol{b}_{t}(\\boldsymbol{x})$ and covariance matrix $G_{t}$ , i.e. the function $h_{t}(x)$ defines the conditional density $h_{t}(x)\\,=$ $p({\\stackrel{.}{x}}{\\stackrel{.}{T}}\\in B^{\\prime}\\,|\\,x_{t}=x)$ for some set $B^{\\prime}$ , which agrees with the forward process with the same drift and covariance. The boundary condition $q_{T}(x)=\\bar{\\delta}(x-B)$ together with the backward equation define the unique solution to this PDE. Since the PDEs and the boundary conditions are the same as in Doob\u2019s $h$ -transform, we have $h_{t}(x)=p(x_{T}=B\\,|\\,x_{t}=x)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Corollary 2. The following SB problem ", "page_idx": 19}, {"type": "equation", "text": "$$\nS:=\\operatorname*{min}_{\\mathbb{Q}\\ {s.t.}\\ \\mathbb{Q}=\\delta,\\mathbb{Q}=\\delta}D_{K L}[\\mathbb{Q}_{0:T}^{v}:\\mathbb{P}_{0:T}^{r e f}]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "yields the path measure $\\mathbb{P}_{0:T}^{*}$ associated with the SDE in (6) as its unique minimizing argument. The temporal marginals of $\\mathbb{P}_{0:T}^{*}$ are equal to those which optimize the Lagrangian objective in Thm. 1. ", "page_idx": 19}, {"type": "text", "text": "Proof. We use the Girsanov theorem (S\u00e4rkk\u00e4 and Solin, 2019, Sec. 7.3) to calculate the KL divergence between the following two Brownian diffusions with fixed initial condition $x_{0}=A$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{P}_{0:T}^{\\mathrm{ref}}:}&{\\qquad}&{d x_{t}=b_{t}(x_{t})\\cdot d t+\\Xi_{t}\\;d W_{t}\\,,}\\\\ &{\\mathbb{Q}_{0:T}^{v}:}&{\\qquad}&{d x_{t}=\\big(b_{t}(x_{t})+2G_{t}\\;v_{t|0,T}(x_{t|0,T})\\big)\\cdot d t+\\Xi_{t}\\;d W_{t}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, noting the difference of drifts is $b_{t}(x_{t})+2G_{t}\\ v_{t|0,T}(x_{t})-b_{t}(x_{t})=2G_{t}\\ v_{t|0,T}(x_{t}),$ the likelihood ratio is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{d\\mathbb{Q}_{0:T}^{v}}{d\\mathbb{P}_{0:T}^{\\mathrm{ref}}}=\\frac{q_{t|0,T}\\left(x_{0},\\dots x_{T}\\right)}{\\rho\\left(x_{0},\\dots x_{T}\\right)}=\\exp\\Big\\{-\\frac{1}{2}\\int_{0}^{T}\\bigl\\langle2G_{t}\\;v_{t|0,T}(x_{t}),(G_{t})^{-1}\\;2G_{t}\\;v_{t|0,T}(x_{t})\\bigr\\rangle d t}\\\\ &{}&{\\displaystyle-\\int2\\bigl(G_{t}\\;v_{t|0,T}(x_{t})\\bigr)^{T}G_{t}^{-1}d W_{t}\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We finally calculate the $\\mathrm{KL}$ divergence, noting that, after taking the log, the expectation of the integral $\\int(\\bullet)d W_{t}$ in the final term vanishes, ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{K L}[\\mathbb{Q}_{0;T}^{v}:\\mathbb{P}_{0;T}^{\\mathrm{ref}}]=2\\int_{0}^{T}d t\\;\\int d x_{t}\\;q_{t|0,T}(x_{t})\\;\\langle v_{t|0,T}(x_{t}),G_{t}\\;v_{t|0,T}(x_{t})\\rangle,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which matches (9a) up to a constant factor of 2 does not change the optimum. We finally compare to the constraints in Thm. 1. First, it is clear that the diffusion in (34) satisfies the Fokker-Planck equation in (9b) (S\u00e4rkk\u00e4 and Solin, 2019, Sec. 5.2). We respect (9c) by optimizing over endpoint-constrained path measures, which yields ", "page_idx": 19}, {"type": "equation", "text": "$$\nS=\\operatorname*{min}_{\\mathbb{Q}\\mathrm{~s.t.~}\\mathbb{Q}=\\delta,\\mathbb{Q}=\\delta}D_{K L}[\\mathbb{Q}_{0:T}^{v}:\\mathbb{P}_{0:T}^{\\mathrm{ref}}]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as desired. ", "page_idx": 19}, {"type": "text", "text": "B Gaussian Path Parameterizations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition 3. For the family of endpoint-conditioned marginals $q_{t\\mid0,T}(x)=\\mathcal{N}(x\\mid\\mu_{t\\mid0,T},\\Sigma_{t\\mid0,T}),$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nu_{t|0,T}^{(q,\\theta)}(x):=\\frac{\\partial\\mu_{t|0,T}}{\\partial t}+\\left[\\frac{1}{2}\\frac{\\partial\\Sigma_{t|0,T}}{\\partial t}\\Sigma_{t|0,T}^{-1}-G_{t}\\,\\Sigma_{t|0,T}^{-1}\\right]\\left(x-\\mu_{t|0,T}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "satisfies the Fokker-Planck equation (12) for $q_{t|0,T}$ and diffusion coefficients $\\begin{array}{r}{G_{t}=\\frac{1}{2}\\Xi_{t}\\Xi_{t}^{T}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. Consider the following identities for the Gaussian family of marginals $q_{t}(x)=\\mathcal{N}(x|\\mu_{t},\\Sigma_{t})$ , where we omit conditioning $q_{t}\\gets q_{t|0,T}$ for simplicity of notation, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\log q_{t}(x)=-\\displaystyle\\frac{1}{2}(x-\\mu_{t})^{T}\\Sigma_{t}^{-1}(x-\\mu_{t})-\\displaystyle\\frac{d}{2}\\log(2\\pi)-\\displaystyle\\frac{1}{2}\\log\\operatorname*{det}\\Sigma_{t}\\,,}}\\\\ {{\\nabla_{x}\\log q_{t}(x)=-\\Sigma_{t}^{-1}(x-\\mu_{t})\\,,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(35\\mu_{t})\\Sigma_{t}^{-1}(x-\\mu_{t})\\,,}}\\\\ {{\\displaystyle\\frac{\\partial}{\\partial t}\\log q_{t}(x)=(x-\\mu_{t})^{T}\\Sigma_{t}^{-1}\\frac{\\partial\\mu_{t}}{\\partial t}+\\displaystyle\\frac{1}{2}(x-\\mu_{t})^{T}\\Sigma_{t}^{-1}\\frac{\\partial\\Sigma_{t}}{\\partial t}\\Sigma_{t}^{-1}(x-\\mu_{t})-\\displaystyle\\frac{1}{2}\\mathrm{tr}\\biggl(\\Sigma_{t}^{-1}\\frac{\\partial\\Sigma_{t}}{\\partial t}\\biggr)\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We begin by solving for a vector field $u_{t}^{\\mathrm{0}}(x)$ that satisfies the continuity equation (where $u_{t}^{\\mathrm{o}}$ denotes the drift of an ODE) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\partial q_{t}}{\\partial t}=-\\big\\langle\\nabla_{x},q_{t}u_{t}^{\\circ}\\big\\rangle=-q_{t}\\big\\langle\\nabla_{x},u_{t}^{\\circ}\\big\\rangle+\\big\\langle\\nabla_{x}q_{t},\\nabla_{x}u_{t}^{\\circ}\\big\\rangle}\\\\ &{}&{\\implies\\frac{\\partial}{\\partial t}\\log q_{t}=-\\big\\langle\\nabla_{x},u_{t}^{\\circ}\\big\\rangle-\\big\\langle\\nabla_{x}\\log q_{t},u_{t}^{\\circ}\\big\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The vector field satisfying this equation is ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{t}^{0}(x)=\\frac{\\partial\\mu_{t}}{\\partial t}+\\frac{1}{2}\\frac{\\partial\\Sigma_{t}}{\\partial t}\\Sigma_{t}^{-1}(x-\\mu_{t})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which we can confirm using the identities in (38). Indeed, for the terms on the RHS of Eq. (39), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\big\\langle\\nabla_{x},u_{t}^{\\mathrm{o}}\\big\\rangle=}&{-\\displaystyle\\frac{1}{2}\\mathrm{tr}\\bigg(\\Sigma_{t}^{-1}\\frac{\\partial\\Sigma_{t}}{\\partial t}\\bigg)\\,,}\\\\ {-\\big\\langle\\nabla_{x}\\log{q_{t}},u_{t}^{\\mathrm{o}}\\big\\rangle=}&{\\bigg\\langle\\Sigma_{t}^{-1}(x-\\mu_{t}),\\frac{\\partial\\mu_{t}}{\\partial t}\\bigg\\rangle+\\displaystyle\\frac{1}{2}(x-\\mu_{t})^{T}\\Sigma_{t}^{-1}\\frac{\\partial\\Sigma_{t}}{\\partial t}\\Sigma_{t}^{-1}(x-\\mu_{t})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Putting these terms and the time derivative from (38c) into Eq. (39) we conclude the proof. ", "page_idx": 20}, {"type": "text", "text": "However, we are eventually interested in finding the formula for the drift $u_{t}$ that satisfies the FokkerPlanck equation in (12). That is, to describe the same evolution of density \u2202q\u2202t(tx ), the relationship between $u_{t}$ and $u_{t}^{\\mathrm{0}}$ is as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial q_{t}(x)}{\\partial t}=-\\big\\langle\\nabla_{x},q_{t}u_{t}^{\\mathrm{o}}\\big\\rangle=}&{-\\left\\langle\\nabla_{x},q_{t}\\;u_{t}\\right\\rangle+\\big\\langle\\nabla_{x},G_{t}\\nabla_{x}q_{t}\\big\\rangle}\\\\ {=}&{-\\left\\langle\\nabla_{x},q_{t}\\;u_{t}\\right\\rangle+\\big\\langle\\nabla_{x},G_{t}q_{t}\\nabla_{x}\\log q_{t}\\big\\rangle}\\\\ {=}&{-\\left\\langle\\nabla_{x},q_{t}\\underbrace{\\left(u_{t}-G_{t}\\nabla_{x}\\log q_{t}\\right)}_{u^{\\mathrm{o}}}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we use the identities in (38) to obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{u_{t}=\\,u_{t}^{\\mathrm{0}}+G_{t}\\nabla_{x}\\log q_{t}=\\,{\\cfrac{\\partial\\mu_{t}}{\\partial t}}+{\\cfrac{1}{2}}{\\cfrac{\\partial\\Sigma_{t}}{\\partial t}}\\Sigma_{t}^{-1}(x-\\mu_{t})-G_{t}\\Sigma_{t}^{-1}(x-\\mu_{t})}\\\\ &{\\qquad\\implies\\quad u_{t}=\\,{\\cfrac{\\partial\\mu_{t}}{\\partial t}}+\\left[{\\cfrac{1}{2}}{\\cfrac{\\partial\\Sigma_{t}}{\\partial t}}\\Sigma_{t}^{-1}-G_{t}\\Sigma_{t}^{-1}\\right](x-\\mu_{t})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proposition 4. Given a set of processes $q_{t\\mid0,T}^{k}(x)$ and mixtures weights $w^{k}$ , the vector field satisfying the Fokker-Planck equation in (12) for the mixture $\\begin{array}{r}{q_{t|0,T}(x)=\\sum_{k}w^{k}q_{t|0,T}^{k}(x)}\\end{array}$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{t|0,T}^{(q,\\theta)}(x)=\\sum_{k=1}^{K}\\frac{w^{k}q_{t|0,T}^{k}(x)}{\\sum_{j=1}^{K}w^{j}q_{t|0,T}^{j}(x)}u_{t|0,T}^{(q,k)}(x)\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "bwohtehr ef $u_{t|0,T}^{(q,k)}(x)$ d syantaismfiiecss  tihn es pFaotkikael rc-oPolradnicnka teeqs uoantiloy no ri ns e(c1o2n) df-oor $q_{t\\mid0,T}^{k}(x)$ .m iTchs iisn .ds for irst-order  rder dyna $x=\\left({\\bar{x}},{\\bar{v}}\\right)$ ", "page_idx": 20}, {"type": "text", "text": "Proof. See Peluchetti (2023) Theorem 1 and its proof in their App. A. ", "page_idx": 20}, {"type": "text", "text": "C Extended Related Work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Machine Learning for Molecular Simulation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The main dilemma of molecular dynamics comes from the accuracy and efficiency trade-off\u2014accurate simulation requires solving the Schr\u00f6dinger equation which is computationally intractable for large systems, while efficient simulation relies on empirical force fields which is inaccurate. Recently, there has been a surge of work in applying machine learning approaches to accelerate molecular simulation. One successful paradigm is machine learning force field (MLFF) which leverages the transferability and efficiency of machine learning methods to fit force/energy prediction models on quantum mechanical data and transfer across different atomic systems Smith et al. (2017); Wang et al. (2018). More recently, increasing attention has been focused on building atomic foundation models to encompass all types of molecular structures Batatia et al. (2023); Shoghi et al. (2023); Zhang et al. (2022). ", "page_idx": 20}, {"type": "text", "text": "Sampling is a classical problem in molecular dynamics to draw samples from the Boltzmann distribution of molecular systems. Classical methods mainly rely on Markov chain Monte Carlo (MCMC) or MD which requires long mixing time for multimodal distributions with high energy barriers Rotskoff (2024). Generative models in machine learning demonstrate promises in alleviating this problem by learning to draw independent samples from the Boltzmann distribution of molecular systems (known as Boltzmann generator) No\u00e9 et al. (2019). Numerous methods have been developed to utilize generative models as a proposal distribution for escaping local minima in running MCMC methods Gabri\u00e9 et al. (2022). However, one critical issue is that generative models rely on training from samples. Although recent advances have been developed to learn from unnormalized density (i.e., energy) function, the training inefficiency limits their applicability to solve high-dimensional molecular dynamics problems. To circumvent the curse of dimensionality for the sampling problem, another branch of work study to learn coarse-grained representation with neural networks Sidky et al. (2020). For broader literature of applying machine learning to enhanced sampling, we refer the reader to Mehdi et al. (2024). ", "page_idx": 21}, {"type": "text", "text": "D Further Experimental Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Evaluation Metrics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To assess the quality of our approach in terms of performance and physicalness of paths, we compare them under different metrics to well-established TPS techniques. One important describing factor of a trajectory is the molecule\u2019s highest energy during the transition. These high-energy states are often referred as transition states and less likely to occur but they determine importance factors during chemical reaction such as reaction rate. As such, we will look at the maximum energy along the transition path and use it to compare the ensemble of trajectories more efficiently. The main goal is to show that lower energy of the transition states can be sampled by the methods. ", "page_idx": 21}, {"type": "text", "text": "However, the maximum energy does not account for the fact that the transition path needs to be sequential, and each step needs to be coherent based on the previous position and momentum. For this, we also compare the likelihood of the paths (i.e., unnormalized density) by computing the probably of being in the start state $\\rho(x_{0})$ and multiplying it with the step probability such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nL(x_{0},x_{1},\\ldots,x_{N-1})=\\rho(x_{0})\\cdot\\prod_{i=0}^{N-2}\\pi(x_{i+1}\\,|\\,x_{i})\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the step probability $\\pi$ , we solve the Langevin leap-frog implementation as implemented in OpenMM to solve ${\\cal N}(\\dot{x}_{i+1}\\mid x_{i}+d t\\cdot b_{t}(x),d\\bar{t}\\sigma_{i}^{2})$ . As for the starting probability, we compute the unnormalized density of the Boltzmann distribution for our start state $z$ and assume that the velocity $v$ can be sampled independently (Castellan, 1983, Sec. 4.6) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\rho(z,v)\\propto\\exp\\!\\bigg(\\!-\\!\\frac{U(z)}{k_{B}T}\\bigg)\\cdot\\!{\\mathcal{N}}\\big(v\\:|\\:0,k_{B}T\\cdot M^{-1}\\big)\\:,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with the Boltzmann constant $k_{B}$ and the diagonal matrix $M$ containing the mass of each atom. ", "page_idx": 21}, {"type": "text", "text": "As for the performance, the number of energy evaluations will be the main determining factor of the runtime for larger molecular systems, especially for proteins. We hence compare the use of the number of energy computations as a proxy for hardware-independent relative measurements. In our tests, this number aligned with the relative runtime of these approaches. ", "page_idx": 21}, {"type": "text", "text": "D.2 Toy Potentials ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The toy systems move according to the following integration scheme (first-order Euler) ", "page_idx": 21}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}-d t\\cdot\\nabla_{x}U(x_{t})+\\sqrt{d t}\\cdot\\mathbf{diag}(\\xi)\\cdot\\boldsymbol\\varepsilon,\\quad\\boldsymbol\\varepsilon\\sim\\mathcal{N}(0,1)\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "following the definition of our stochastic system in Sec. 2.2 with a time-independent Wiener process, where $\\xi$ is a constant time-independent standard deviation for all dimensions. ", "page_idx": 21}, {"type": "text", "text": "M\u00fcller-Brown. The underlying M\u00fcller-Brown potential that has been used for our experiments can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U(x,y)=-\\ 200\\cdot\\exp\\bigl(-(x-1)^{2}-10y^{2}\\bigr)}\\\\ &{\\phantom{=}-100\\cdot\\exp\\bigl(-x^{2}-10\\cdot(y-0.5)^{2}\\bigr)}\\\\ &{\\phantom{=}-170\\cdot\\exp\\bigl(-6.5\\cdot(0.5+x)^{2}+11\\cdot(x+0.5)\\cdot(y-1.5)-6.5\\cdot(y-1.5)^{2}\\bigr)}\\\\ &{\\phantom{=}+15\\cdot\\exp\\bigl(0.7\\cdot(1+x)^{2}+0.6\\cdot(x+1)\\cdot(y-1)+0.7\\cdot(y-1)^{2}\\bigr)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We used a first-order Euler integration scheme to simulate transition paths with 275 steps and a $d t$ of $10^{-4}s$ . $\\xi$ was chosen to be 5 and 1,000 transition paths were simulated. We have used an MLP with four layers and a hidden dimension of 128 each, with swish activations. It has been trained for 2,500 steps with a batch size of 512 and a single Gaussian. ", "page_idx": 22}, {"type": "text", "text": "In Fig. 5a, we compare the likelihood of the sampled paths. We can see that one-way shooting takes time until the path is decorrelated from the initial trajectory, which is shorter and thus has a higher likelihood. All MCMC methods exhibit this behavior, which is typically alleviated by using a warmup period in which all paths are discarded. After that, all methods exhibit similar likelihood, with our method having a slightly lower likelihood. Looking at the transition state (i.e., maximum energy on the trajectory) in Fig. 5b reveals that all methods have a similar quality of paths. ", "page_idx": 22}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/a98db6eaa055ab7c665852098f11512ab91ecc353585a2873ef0a6a204e0b652.jpg", "img_caption": ["Figure 5: In a, we compare the log likelihood of sampled trajectories, where a higher likelihood is generally more favorable. The plot in b shows the maximum energy of each individual trajectory. A high maximum energy means that the molecule needs to be in an excited state during the transition, making it less likely to occur under lower temperatures. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "We can further analyze the quality of our method by investigating the difference between the \u201cground truth\u201d marginal $\\rho_{t|0,T}(x)$ and the learned marginal $q_{t\\mid0,T}(x)$ . For this, we compute the Wasserstein W1 distance (Flamary et al., 2021) between the marginal observed by fixed-length two-way shooting (which we assume to be close to the ground truth) and our variational approach. We observe a mean W1 distance of $0.130\\pm0.026$ and visualize it along the time coordinate $t$ (in steps) in Fig. 6. ", "page_idx": 22}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/b266c4bc523cc91e7311c094eccc7ca3abd68ea47ee8623f91100b519c50ea3a.jpg", "img_caption": ["Figure 6: In this figure, we compare the Wasserstein W1 distance between the marginals. The densities are almost identical at the beginning and end state. Note that at the third local minimum of the M\u00fcller-Brown potential along the trajectory (i.e., reached at about 200 steps), the marginals align more closely as well. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Dual-Channel Double-Well. To demonstrate the advantage of mixtures, we have used the twodimensional potential ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U(x,y)=+\\,2\\cdot\\exp\\bigl(-(12x^{2}+12y^{2})\\bigr)}\\\\ &{\\qquad\\qquad-\\,1\\cdot\\exp\\bigl(-(12\\cdot(x+0.5)^{2}+12y^{2})\\bigr)}\\\\ &{\\qquad\\qquad-\\,1\\cdot\\exp\\bigl(-(12\\cdot(x-0.5)^{2}+12y^{2})\\bigr)+x^{6}+y^{6}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In this case, we have used $d t=5*10^{-4}s$ with a transition time of $T=1s$ and $\\xi=0.1$ . As for the MLP, we have used the same structure as in the M\u00fcller-Brown example but trained it for 20,000 iterations. The corresponding weights to Prop. 4 are $\\begin{array}{r}{w=\\left[\\frac{1}{2},\\frac{1}{2}\\right]}\\end{array}$ and are fixed for this experiment and hence $w\\not\\in\\theta$ . ", "page_idx": 23}, {"type": "text", "text": "D.3 Neural Network Ablation Study ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Fig. 7 we compare how different parameterizations of $\\mu_{t|0,T}^{(\\theta)}$ , and \u03a3t(|\u03b80),T impact the quality of trajectories on the M\u00fcller-Brown potential. For this, we compare linear and cubic splines (with 20 knots) with neural networks. As a metric to estimate the quality, we compare the Wasserstein W1 distance between the learned marginal $q_{t\\mid0,T}(x)$ and the marginal observed by fixed-length two-way shooting (i.e., baseline). We notice that using linear splines results in the highest W1 distance, while cubic splines improve the quality. Using neural networks, however, yields the best approximation. ", "page_idx": 23}, {"type": "text", "text": "We have fixed the computational budget for all systems, which means that we have trained splines for more epochs than the neural network (since they are slower to train). For high-dimensional systems, the runtime is mostly determined by the number of potential evaluations and not the complexity of the architecture. We thus conclude that the additional expressivity provided by neural networks is necessary for more complicated (molecular) systems and does not introduce much computational overhead. ", "page_idx": 23}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/3c2c9f78e9643406e3fcb9726dd7df14e76be3dd6b3e967522798c8c6f6421d5.jpg", "img_caption": ["Figure 7: We compare the Wasserstein W1 distance between the marginals with different parameterization techniques of $\\mu_{t|0,T}^{(\\theta)}$ , and $\\boldsymbol{\\Sigma}_{t\\mid0,T}^{(\\theta)}$ "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.4 Molecular Systems ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To simulate molecular dynamics, we rely on the AMBER14 force field (amber14/protein.ff14SB Maier et al. (2015)) without a solvent, as implemented in OpenMM (Eastman et al., 2017). As OpenMM does not support auto-differentiation, we do not use OpenMM for the simulations themselves, but utilize DMFF (Wang et al., 2023) which is a differentiable framework implemented in JAX (Bradbury et al., 2018) for molecular simulation. This is needed because during training we compute $\\nabla_{\\boldsymbol{\\theta}}U\\Big(\\mathbf{\\boldsymbol{x}}_{t\\mid0,T}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{t\\mid0,T}^{(\\boldsymbol{\\theta})},\\boldsymbol{\\Sigma}_{t\\mid0,T}^{(\\boldsymbol{\\theta})})\\Big)$ , where the concrete $x_{t|0,T}$ is sampled based on the parameters of the neural network. ", "page_idx": 23}, {"type": "text", "text": "For the concrete simulations, we ran them with the timestep $d t=1f s$ , with $T=1p s$ , $\\gamma=1p s$ , and Tem $\\Psi=300K$ . To compute the MCMC two-way shooting baselines, we use the same settings and consider trajectories as failed, if they exceed 2,000 steps without reaching the target. ", "page_idx": 23}, {"type": "text", "text": "Neural Network Parameterization. We parameterize our model with neural networks, a 5-layer MLP with ReLU activation function and 256/512 hidden units for alanine dipeptide and Chignolin, respectively. The neural networks are trained using an Adam optimizer with learning rate $10^{=4}$ . ", "page_idx": 23}, {"type": "text", "text": "We represent the molecular system in two ways: (1) in Cartesian coordinates, which are the 3D coordinates of each atoms, and with (2) internal coordinate which instead uses bond length, angle and dihedral angle along the molecule, where we use the same parameterization as in (No\u00e9 et al., 2019). ", "page_idx": 23}, {"type": "text", "text": "Our state definition includes a variance parameter for the initial and target marginal distributions at $t=0$ and $t=T$ , we choose the variance to be $10^{-8}$ which almost does not change the energy of the perturbed system. ", "page_idx": 23}, {"type": "text", "text": "Visualization of Transition for Alanine Dipeptide. In Fig. 8, we show a transition sampled without any noise from the model with internal coordinates and 2 Gaussian mixtures. ", "page_idx": 23}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/b91ec1d3dbd6b80a80d4769b851d82a0246916a133a79cd582292de5d7f5bf21.jpg", "img_caption": ["Figure 8: Transition path for the alanine dipeptide. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Comparison of Sampled Paths During Training. In our training procedure, the marginal starts with a linear interpolation between $A$ and $B$ , which produces very unlikely paths with potentially high energy states. In Fig. 9, we compare how the quality of paths changes depending on the number of training iterations (i.e., the number of potential evaluations). We show the curve for a single Gaussian mixture with Cartesian coordinates. Similar trends can be observed in other settings. ", "page_idx": 24}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/764f799afcc8173f0f8a7f9051dfe1ea6d11e7d72b2bb2adde601be66107631d.jpg", "img_caption": ["Figure 9: In this figure, we compare the quality of paths based on the current training step (i.e., potential evaluations). We observe that with increasing training time, paths with higher likelihood are sampled. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Loss Curves. In this section, we would like to investigate the training losses of different configurations. For this, we plot the exponential moving average of the loss $(\\alpha=0.001)$ ) to better highlight the trends of the noisy variational loss. Fig. 10 compares the results of different training settings. We can observe that mixtures can decrease the overall loss, but all model variations converge to a similar loss value. ", "page_idx": 24}, {"type": "image", "img_path": "ShJWT0n7kX/tmp/83ae6bdb1825f79feddd52d1c8e7b15e8c14bbe0d7332939dbe36703da405415.jpg", "img_caption": ["Figure 10: Visualization of the loss for different training setups. These setups are identical to what has been reported in Table 2. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.5 Computational Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "All our experiments involving training were conducted on a single NVIDIA A100 80GB. The baselines themselves were computed on a M3 Pro 12-core CPU. ", "page_idx": 24}, {"type": "text", "text": "E Societal Impact ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our research concerns the efficient sampling of transition paths which are crucial for a variety of tasks in biology, chemistry, materials science and engineering. Our research could potentially benefti research areas from combustion, catalysis, protein design to battery design. Nevertheless, we do not foresee special potential negative impacts to be discussed here. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The major claims made in the abstract and introduction are backed up by the theoretical and empirical results in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: In Sec. 6, we discuss potential limitations and future work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: A detailed set of assumptions is presented in Sec. 3 and further proofs for theoretical results are included in App. A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We disclose all the information needed to reproduce the main experimental results and make our code available at https://github.com/plainerman/variational-doob. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The data we use are synthetic datasets that can be generated by publicly available codes and real-world molecular systems taken from previous literature cited in the paper. Our code is publicly available at https://github.com/plainerman/variational-doob. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We report the experimental details in Sec. 5 and further specify the parameter and settings in App. D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We appropriately report the mean and standard deviation in the experiments where suitable (compare Table 1 and Table 2). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We include computational resources used to run the model in App. D.5. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we believe our research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the potential societal impact of our method in App. E. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not believe that any safe guards needs to be in place, as our approach does not pose any direct risks. As for the data, only publicly available and commonly used resources have been used and, hence, does not pose a risk for misuse. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We use public datasets and libraries in this research and have cited the sources. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: No new assets are introduced in this research. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects are involved in this research. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No human subjects are involved in this research. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]