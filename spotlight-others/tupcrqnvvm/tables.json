[{"figure_path": "tUpcRQNvVM/tables/tables_9_1.jpg", "caption": "Table 1: Ablations on Imagenet100. The importance of feedback, learnt features, enforcing submodularity, and GPC (in each column, bold is best, underline is 2nd best). \"w/o feedback\" indicates a DSPN model trained without feedback, while \"w/o target feedback\" is when the oracle is only queried. For Set Transformer, streaming maximization fails to produce a summary of the required size (therefore it is omitted). \"PC rather than GPC\" uses non-graded pairwise comparisons, showing the benefits of grading, consistent with [61]. All results are for a size-100 set.", "description": "This table presents ablation studies on the Imagenet100 dataset to assess the impact of different components of the proposed DSPN framework.  It compares the performance of the full DSPN model against variations that remove components such as feedback, type-II sampling, or the learnt pillar.  It also contrasts using graded pairwise comparisons (GPC) against binary pairwise comparisons (PC) and compares the DSPN against other baseline models (Deep Sets and Set Transformers).  The results highlight the importance of each component for optimal performance.", "section": "5.3 Ablations"}, {"figure_path": "tUpcRQNvVM/tables/tables_30_1.jpg", "caption": "Table 3: Loss coefficients used for each dataset.", "description": "This table lists the values of the hyperparameters \u03bb\u2081, \u03bb\u2082, \u03bb\u2083, and \u03bb\u2084 used in the augmentation regularizer (Equation 2) and redundancy regularizer (Equation 3) for each of the four datasets (Imagenette, Imagewoof, CIFAR100, and Imagenet100).  These hyperparameters control the strength of the regularization terms in the overall loss function used to train the Deep Submodular Peripteral Networks (DSPNs).", "section": "Additional Experimental Details"}, {"figure_path": "tUpcRQNvVM/tables/tables_31_1.jpg", "caption": "Table 4: Sensitivity of \u03b2 based on normalized FL evaluation.", "description": "This table shows the normalized facility location (FL) evaluation results for different values of the hyperparameter \u03b2 and various summary sizes (k).  The normalized FL evaluation measures how well the learned DSPN model approximates the target FL function.  Higher values indicate better approximation.", "section": "5.3 Ablations"}, {"figure_path": "tUpcRQNvVM/tables/tables_31_2.jpg", "caption": "Table 5: Sensitivity of \u03c4 based on normalized FL evaluation.", "description": "This table presents the normalized Facility Location (FL) evaluation results for different summary sizes (k) and values of the hyperparameter \u03c4. The results show how the performance of the model varies with different values of \u03c4 for different summary sizes. It demonstrates the sensitivity of the model's performance to changes in the hyperparameter \u03c4.", "section": "Experiments"}, {"figure_path": "tUpcRQNvVM/tables/tables_33_1.jpg", "caption": "Table 6: Effect of setting \u03bb3 = \u03bb4 = 0 on normalized FL evaluations on CIFAR100", "description": "This table shows the normalized facility location (FL) evaluation results on the CIFAR100 dataset for different summary sizes (k).  It compares the performance of the DSPN model with and without redundancy regularizers (\u03bb3 and \u03bb4) against a random baseline.  The results demonstrate that the redundancy regularizers improve performance but are not essential, indicating robustness of the DSPN approach.", "section": "5.3 Ablations"}]