[{"figure_path": "KY07A73F3Y/tables/tables_5_1.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations.\n(a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents a comparison of the average success rates achieved by different representation learning methods across three distinct embodied AI benchmarks: Meta-World, Franka-Kitchen, and OVMM.  Meta-World and Franka-Kitchen are few-shot imitation learning benchmarks focused on manipulation tasks, while ImageNav and OVMM are reinforcement learning benchmarks focused on navigation tasks.  The table shows the mean success rates and standard deviations for each method, allowing for a comprehensive comparison of their performance across different task types. The results show that the Stable Control Representations (SCR) and its variants generally outperform other methods across all benchmarks.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_6_1.jpg", "caption": "Table 2: We analyze the impact of varying the denoising timestep, layers selection, and input text prompt for the performance of SCR on the Franka-Kitchen benchmark. We report the mean and standard error over 3 random seeds.", "description": "This table presents an ablation study on the design choices for Stable Control Representations (SCR) using the Franka-Kitchen benchmark. It investigates the effects of three design parameters:\n\n(a) Denoising timestep:  Examines the impact of different levels of noise applied to the input images before the representation is extracted.\n(b) Layers selection: Evaluates different combinations of layers from the U-Net (used for generating representations) that are concatenated to form the final representation. \n(c) Input text prompt: Tests whether providing text descriptions affects the performance. Three cases are tested: no prompt, relevant prompts, and irrelevant prompts.", "section": "5 Deconstructing Stable Control Representations"}, {"figure_path": "KY07A73F3Y/tables/tables_7_1.jpg", "caption": "Table 3: Layer-selection ablations across different benchmarks. (a) Ablations for CLIP on Franka-Kitchen. (b) Ablations for SCR on Meta-World.", "description": "This table presents ablation studies on the impact of layer selection on the performance of CLIP and SCR models on two different benchmark tasks: Franka-Kitchen and Meta-World. For CLIP, different combinations of layers from the model are tested on Franka-Kitchen to assess their relative effectiveness.  For SCR, different combinations of layers are tested on Meta-World, along with variations in the noise level applied to the input.  The results showcase the effect of different layer combinations and noise levels on the overall success rate of the models.", "section": "5 Deconstructing Stable Control Representations"}, {"figure_path": "KY07A73F3Y/tables/tables_7_2.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations.\n(a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents the average success rates and standard errors for different representation learning methods across three robotic control tasks: Meta-World, ImageNav, and OVMM.  For each task and method, the average success rate and standard error are reported. This table allows for a comparison of the performance of different representation learning methods across various tasks, highlighting the strengths and weaknesses of each approach.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_8_1.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations.\n(a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents a comparison of the average success rates across three different robotic control tasks (Meta-World, Franka-Kitchen, and OVMM) using various visual representation learning methods.  The success rate is the percentage of times the agent successfully completes the task.  The table shows that Stable Control Representations (SCR) and its fine-tuned variant (SCR-FT) achieve high success rates across all three tasks, often outperforming state-of-the-art baselines (like R3M, CLIP, and VC-1).  The standard error is included to show the variability in performance.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_17_1.jpg", "caption": "Table 7: Performance on Franka-Kitchen after fine-tuning CLIP.", "description": "This table shows the average success rate and standard error for the Franka-Kitchen task after fine-tuning the CLIP model.  It compares the performance of the original CLIP model to the CLIP model after fine-tuning. The results indicate a slight decrease in performance after fine-tuning.", "section": "4.1 Few-shot Imitation Learning"}, {"figure_path": "KY07A73F3Y/tables/tables_18_1.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations. (a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents the average success rates and standard errors for different representation learning methods across three robotic control tasks: Meta-World, Franka-Kitchen, and OVMM.  The results show how each method performed on these different tasks, allowing for a comparison of their effectiveness.  The table is divided into three subsections, each corresponding to a specific task and showing the success rate and standard deviation for different models on that task.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_18_2.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations. (a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents the average success rate and standard error for different visual representation learning methods across three distinct embodied AI tasks: Meta-World, ImageNav, and OVMM.  Meta-World and Franka-Kitchen are few-shot imitation learning benchmarks for manipulation tasks. ImageNav is an indoor visual navigation task, and OVMM is an open-vocabulary mobile manipulation benchmark. The table allows for a comparison of the performance of Stable Control Representations (SCR) and its variants against other state-of-the-art methods, highlighting SCR's competitive performance and generalization capabilities.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_19_1.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations. (a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents a comparison of the average success rates achieved by different representation learning methods across three distinct embodied AI benchmark tasks: Meta-World, Franka-Kitchen, and OVMM.  Each method's performance is shown separately for each task. The table also shows the standard error associated with each average success rate, indicating the uncertainty in the results.  The results are divided into three subtables, one for each group of tasks.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_19_2.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations. (a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents a comparison of the average success rates achieved by different representation learning methods across three embodied AI benchmark tasks: Meta-World, ImageNav, and OVMM.  The table is broken down into three subtables: (a) shows results for Meta-World and Franka Kitchen, (b) for ImageNav and (c) for OVMM. For each task and method, the average success rate and its standard error are reported, allowing for a quantitative comparison of the different approaches' performance across various robotic control tasks. Note that different metrics are used to calculate the success rate of each task, hence the values should be interpreted in their own contexts.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_21_1.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations.\n(a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents the average success rate and standard error for different vision-language representation models across three embodied AI benchmarks: Meta-World, Franka-Kitchen, and OVMM.  The results show the performance of Stable Control Representations (SCR) and its variants compared to several baseline models, including CLIP, R3M, VC-1, and Voltron.  It highlights the competitive performance of SCR across diverse tasks, including manipulation and navigation, demonstrating its versatility in learning control policies.", "section": "4 Empirical Evaluation"}, {"figure_path": "KY07A73F3Y/tables/tables_21_2.jpg", "caption": "Table 1: Average Success Rate and standard error evaluated across different representations.\n(a) Meta-World & Franka-Kitchen. (b) ImageNav (c) OVMM", "description": "This table presents the average success rates and standard errors for different vision-language representations across three distinct embodied AI benchmarks: Meta-World and Franka-Kitchen (few-shot imitation learning), ImageNav (reinforcement learning-based indoor navigation), and OVMM (reinforcement learning-based open-vocabulary mobile manipulation).  The results showcase the performance of Stable Control Representations (SCR) and its variants compared to various baselines, demonstrating the efficacy of diffusion-model-based representations for control tasks.", "section": "4 Empirical Evaluation"}]