[{"type": "text", "text": "ResAD: A Simple Framework for Class Generalizable Anomaly Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xincheng $\\mathbf{Yao}^{1}$ , Zixin Chen1, Chao $\\mathbf{Gao^{3}}$ , Guangtao Zhai1, Chongyang Zhang1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University 2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3China Pacific Insurance (Group) Co., Ltd. {i-Dover, CZX15724137864, zhaiguangtao, sunny_zhang}@sjtu.edu.cn1 gaochao-027@cpic.com.cn3 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper explores the problem of class-generalizable anomaly detection, where the objective is to train one unified AD model that can generalize to detect anomalies in diverse classes from different domains without any retraining or fine-tuning on the target data. Because normal feature representations vary significantly across classes, this will cause the widely studied one-for-one AD models to be poorly classgeneralizable (i.e., performance drops dramatically when used for new classes). In this work, we propose a simple but effective framework (called ResAD) that can be directly applied to detect anomalies in new classes. Our main insight is to learn the residual feature distribution rather than the initial feature distribution. In this way, we can significantly reduce feature variations. Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. Therefore, the learned model can be directly adapted to new classes. ResAD consists of three components: (1) a Feature Converter that converts initial features into residual features; (2) a simple and shallow Feature Constraintor that constrains normal residual features into a spatial hypersphere for further reducing feature variations and maintaining consistency in feature scales among different classes; (3) a Feature Distribution Estimator that estimates the normal residual feature distribution, anomalies can be recognized as out-of-distribution. Despite the simplicity, ResAD can achieve remarkable anomaly detection results when directly used in new classes. The code is available at https://github.com/xcyao00/ResAD. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anomaly detection (AD) has achieved rapid advances in many application domains, such as industrial inspection, video surveillance, and medical lesion detection [9, 26]. However, applying AD algorithms in real-world scenarios still confronts many challenges. A critical challenge is that there are usually diverse classes2 and new classes are continually emerging. Most previous one-for-one and also one-for-many (i.e., learning one AD model for multiple classes) AD methods [13, 17, 43, 12, 30, 47, 46, 44] are still insufficient to satisfy the requirements of real-world applications. Because such methods still require retraining or fine-tuning when encountering new classes, but application users generally don\u2019t have such ability. Another more fatal point is that some scenarios may not allow retraining on target classes due to data privacy issues [41]. Therefore, the class-generalizable ability is a critical issue in the AD community, but it still hasn\u2019t been well studied in most AD literatures. ", "page_idx": 0}, {"type": "image", "img_path": "zNiJZUAlxg/tmp/c2600e5d97bd38fe1437b74d060dfd4977170df02727741234a0d45f412c3378.jpg", "img_caption": ["Figure 1: (a): Intuitive illustration of class-generalizable anomaly detection (b): Conceptual illustration of residual features. The residual feature space has fewer variations compared to the initial feature space. The decision boundary of the residual feature distribution can more effectively distinguish anomalies in new classes, rather than treating features of new classes as anomalies. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to tackle an academy-valuable and application-required task: few-shot classgeneralizable anomaly detection, i.e., one unified model is trained with samples from multiple known classes, and the goal is that the trained model can generalize to detect anomalies in new3 classes without any retraining or fine-tuning on the target data, only few-shot new class normal samples are required. Nonetheless, solving such a task is quite challenging. The current one-for-one/many AD models have almost no ability to directly generalize to new classes. The main challenge is: the normal patterns from different classes are significantly different. This can lead to many normal misdetections of new classes, i.e., normal patches from new classes may be mistaken as abnormal as they are quite different from the learned normal patterns. Thus, how to design a class-generalizable AD model under the feature variation circumstance? Our design philosophy is: \u201cseeking invariation from variation\u201d. We think that residual features (i.e., formed by subtracting normal reference features) can be regarded as class-invariant4 representations compared to the significantly variant initial features. As shown in Fig.1(b), the main merit of normal residual features is: even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. Regardless of classes, larger residuals are expected for abnormal features than normal features (please see Sec.3.1). ", "page_idx": 1}, {"type": "text", "text": "To this end, we propose a simple but effective class-generalizable AD framework, called ResAD (i.e., Residual Feature Learning based Class-Generalizable Anomaly Detection). ResAD is based on one key insight: residual feature learning, and consists of two key designs: feature hypersphere constraining and feature distribution estimating. First, we propose to use residual features for reducing class feature variations. We employ a pre-trained feature extractor to generate normal reference features from few-shot normal reference samples. Each input feature will match the nearest normal reference feature and subtract it to form the residual feature. In this way, the most variable class-related components are very likely to be mutually eliminated, resulting in residual features distributed in a relatively fixed origin-centered region (please see Sec.3.1). Second, to further reduce the variations in the residual feature space, we take the idea from the one-class-classification (OCC) learning [37, 22] to constrain the feature space. Specifically, we employ a simple and shallow network and propose an abnormal invariant OCC loss to transform normal residual features into a constrained spatial hypersphere. Third, with the hypersphere-constrained feature space, we can easily utilize a feature distribution estimator [14] to learn and estimate the normal residual feature distribution, anomalies can be recognized as out-of-distribution. For new classes, as the residual features have fewer variations or are covered by the learned distribution, the whole framework is more class-generalizable. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. To accomplish class-generalizable anomaly detection, we propose a simple but effective framework: ResAD, which can be applied to detect and localize anomalies in new classes. 2. We are innovatively based on residual feature learning to address the issue of previous one-forone/many AD methods not being able to generalize to new classes. ", "page_idx": 1}, {"type": "text", "text": "3. Comprehensive experiments on six real-world AD datasets are performed to evaluate the AD model\u2019s class-generalizable ability. With only 4-shot normal samples as reference, ResAD can achieve remarkable AD results, significantly outperforming the state-of-the-art competing methods. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One-for-One/Many AD Methods. Most AD methods follow the one-for-one/many paradigm. (1) Reconstruction-based methods are the most popular AD methods. These methods hold the insight that models trained by normal samples would fail in abnormal image regions. Many previous works utilize auto-encoders [8, 27, 42], masked auto-encoders [46], variational auto-encoders [21] and generative adversarial networks [36, 1] to encode and reconstruct normal data. UniAD [47] is a transformer-based reconstruction model and mainly based on neighbor masked attention to address the \u201cidentical shortcut\u201d issue to achieve one-for-many AD. (2) Distillation-based methods [6] can also be considered as belonging to the reconstruction type. These methods train student networks to reconstruct the representation of teacher networks on normal samples, and the assumption is that the student would fail in abnormal features. Recent works mainly focus on feature pyramid [35, 39], reverse distillation [13, 38], and asymmetric distillation [33]. (3) Embedding-based methods mainly rely on good feature representation and assume that abnormal features are usually far from the normal clusters. Most superior methods [11, 4, 12, 29, 30] utilize ImageNet pre-trained networks for feature extraction. However, industrial images generally have an obvious distribution shift from ImageNet. To better account for the distribution shift, subsequent adaptations should be done. The normalizing flow-based methods [31, 17, 32, 48, 44] are proposed to transform the pre-trained feature distribution into latent Gaussian distribution, and thus can better learn the normal data distribution. HGAD [44] proposes a novel hierarchical Gaussian mixture normalizing flow modeling method to address the \u201chomogeneous mapping\u201d issue for accomplishing one-for-many AD. ", "page_idx": 2}, {"type": "text", "text": "Few-Shot AD Methods. The few-shot AD methods have more similarities with ours. Distance-based approaches such as SPADE [11], PaDiM [12], and PatchCore [30] can be adapted to address few-shot AD by only making use of few-shot normal samples to calculate distance-based anomaly scores without training networks. RegAD [18] proposes to train a feature registration network to align input images and follows PaDiM [12] to model Multivariate Gaussian distribution with few-shot normal samples. The idea in FastRecon [15] is to reconstruct an anomalous sample to its normal version by few-shot support samples. A novel regression with distribution regularization is proposed to obtain the optimal transformation from support to query features. Recently, the CLIP-based AD methods, including WinCLIP [19] and VAND [10] show better few-shot AD performance. They both employ a text prompt ensemble strategy to obtain the language-guided anomaly map. ", "page_idx": 2}, {"type": "text", "text": "We think class-generalizable AD and few-shot AD are still not the same, they still have some differences. Class-generalizable AD requires the model to be class-generalizable, and we only extract features of normal samples in the new class as reference. Few-shot AD mainly focuses on how to effectively utilize few-shot normal samples to construct AD models, some dedicated modules may be introduced to handle the few-shot normal samples. These methods usually still need to re-model in new classes based on few-shot normal samples, e.g., RegAD needs to re-model Multivariate Gaussian distribution for new classes. The CLIP-based methods can be seen as class-generalizable, as these methods can obtain anomaly maps by aligning vision features with text features. However, they heavily rely on the visual-language comprehension abilities of CLIP and handcrafted text prompts about defects, making them difficult to generalize to anomalies in diverse classes. Compared to the few-shot AD methods, our method can learn a class-generalizable AD model, which can be directly applied to new classes only requiring extracting features of few-shot normal samples as reference. ", "page_idx": 2}, {"type": "text", "text": "More recently, InCTRL [50] proposes to use few-shot normal images as sample prompts and learn to capture in-context residuals between the query image and sample prompts. The idea of in-context residuals in InCTRL is very similar to ours. But our method has obvious differences with InCTRL in the definition and utilization of residuals (please see the detailed differences in Appendix A.1). ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Statement. In the class-generalizable AD task, we focus on the performance of new classes. Formally, let ${\\mathcal{T}}_{t r a i n}={\\mathcal{T}}^{n}\\cup{\\mathcal{T}}^{a}$ be a training dataset with normal images and some anomalies (i.e., anomalies that exist in training set should also be effectively utilized), where $\\mathcal{T}^{n}=\\{I_{i}^{n}\\}_{i=1}^{N}$ and $\\mathcal{T}^{a}=\\{I_{j}^{a}\\}_{j=1}^{M}$ indicate the collection of normal samples and abnormal samples. As for testing, the model is evaluated on a collection of other AD datasets $(\\mathcal{T}=\\{\\mathcal{Z}_{1}^{t e s t},\\mathcal{Z}_{2}^{t e s t},\\ldots,\\mathcal{Z}_{T}^{t e s t}\\})$ except the training dataset. The classes in the test set are drawn from unknown classes $\\mathcal{C}_{u}$ that are different from the known classes $\\mathcal{C}_{k}$ in the training set. Then the goal is to learn one unified model $\\mathcal{M}:\\mathcal{T}\\rightarrow\\mathbb{R}$ that is trained with known classes $\\mathcal{C}_{k}$ and can directly adapt to unknown classes $\\mathcal{C}_{u}$ without any retraining or fine-tuning on the target data (only few-shot (e.g., 4) normal samples as reference). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "zNiJZUAlxg/tmp/e2a919389c9a1168f724243a263de7a40c4862794b2c3181560bc6cfc588c294.jpg", "img_caption": ["Figure 2: Framework overview. Note that the training samples belong to different classes. First, few-shot normal reference samples are fed into a pre-trained Feature Extractor to obtain normal reference features. Each initial feature will match the nearest normal reference feature and subtract it to form the residual feature. Then, a Feature Constraintor is utilized to transform the normal residual features into a constrained spatial hypersphere. Finally, we employ a normalizing flow model as the Feature Distribution Estimator to learn and estimate the residual feature distribution. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Overview. The proposed ResAD framework is illustrated in Fig.2. The ResAD framework consists of three parts: a Feature Extractor, a Feature Constraintor, and a Feature Distribution Estimator. These modules will be described below in sequence. ", "page_idx": 3}, {"type": "text", "text": "3.1 Residual Feature Generating ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Residual feature learning is our core insight for solving class-generalizable anomaly detection. In this subsection, we describe how to generate residual features. For any input image $\\check{I}_{i}\\in\\mathbb{R}^{H\\times W\\times3}$ we follow the common practice of previous AD methods to employ a pre-trained feature extraction network $\\phi$ to extract features from different levels. Formally, we define $L$ as the total number of levels for use. The feature map from level $l\\in\\{1,2,\\dots,L\\}$ is denoted as $\\phi^{l}(I_{i})\\,\\in\\,\\mathbb{R}^{H_{l}\\times W_{l}\\times C_{l}}$ where $H_{l},W_{l}$ and $C_{l}$ are the height, width, and channel dimension of the feature map. For an entry $x_{h,w}^{l}\\,=\\,\\phi^{l}(I_{i})_{h,w}\\,\\in\\,\\mathbb{R}^{C_{l}}$ at level $l$ and location $(h,w)$ , we will match it with the nearest normal reference feature from the corresponding reference feature pool, and then convert it into the residual feature. The details are described in the following: ", "page_idx": 3}, {"type": "text", "text": "Reference Feature Pools. The reference feature pools are utilized to store some normal features as reference. For new classes, we will provide few-shot normal samples (i.e., randomly selected and then fixed, please see our discussion on sample selection in Appendix A.2) as reference. The pre-trained network $\\phi$ will extract hierarchical features for these normal reference images, then the extracted features are sent into the feature pools as reference features. For $l$ th level, the lth reference feature pool is composed of $\\mathcal{P}_{l}=\\{x_{h,w}^{l,i}|h\\in\\{1,\\ldots,H_{l}\\},w\\in\\{1,\\ldots,W_{l}\\},l\\in\\{1,\\ldots,L\\},i\\in\\{1,\\ldots,N_{f s}\\}\\}$ where denotes the $i$ th normal sample, the $N_{f s}$ is the number of normal reference samples. ", "page_idx": 3}, {"type": "text", "text": "Residual Features. For each initial feature $x_{h,w}^{l}$ , we can search the nearest nominal reference feature $\\boldsymbol{x}_{n}^{*}=\\operatorname*{argmin}_{\\boldsymbol{x}\\in\\mathcal{P}_{l}}||\\boldsymbol{x}-\\boldsymbol{x}_{h,w}^{l}||_{2}$ from the $l$ th reference feature pool $\\mathcal{P}_{l}$ . Then, we define the residual representation of $x_{h,w}^{l}$ to its closest normal reference feature as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{h,w}^{l,r}=x_{h,w}^{l}-x_{n}^{*}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Why can residual features be less sensitive to new classes compared to initial features? Because they are obtained by matching and then subtracting. From the principles of representation learning, we know that features of each class generated by well-trained neural networks usually have some class-related attributes to the class for distinguishing from other classes [3]. The \u201cclass-related\u201d means these attributes are typical to the class and distinctive from other classes, representing the most discriminative characteristics of the class. Thus, features from different classes are usually located in different feature domains [40]. However, as class-related attributes can also exist in normal reference features (they are usually in the same feature domain as the input query feature), the matching process can be seen as matching the most similar class-related attributes to each query feature. Therefore, by subtracting, the class-related components in the initial features are very likely to be mutually eliminated, leaving the highlighted discrepancy between normals and anomalies (i.e., larger residuals are more likely to be anomalies than normal features). Thus, it can be imagined that the normal residual features generally will be distributed in an origin-centered region, even in new classes, the feature distribution region would not remarkably shift (please see the t-SNE visualization in Fig.3). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Feature Hypersphere Constraining ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Even if the feature variations in the residual feature space will be significantly reduced relative to the initial feature space. Features of different classes may still have significant differences in scale, namely, the numerical value scales in the features of different classes may be remarkably different. This can lead to difficulty in obtaining a unified normal-abnormal decision boundary of different classes, i.e., the scales of decision boundaries in different classes may be significantly different, a good decision boundary in one class may be poor in another class. In order to further reduce feature variations and also maintain the consistency in feature scales among different classes, we take the idea from one-class-classification (OCC) learning [34, 22] and propose a Feature Constraintor to constrain the initial normal residual features to a spatial hypersphere. The Feature Constraintor $C_{\\theta_{1}}$ projects the initial residual feature $x_{h,w}^{l,r}$ to the constrained feature $x_{h,w}^{\\prime,l,r}$ as $x_{h,w}^{\\prime,l,r}=C_{\\theta_{1}}(x_{h,w}^{l,r})$ ", "page_idx": 4}, {"type": "text", "text": "Because we only want to further reduce the variations in the initial residual distribution by constraining and don\u2019t want to change the distribution overly, we adopt a simple Conv+BN+ReLU layer as the network of our Feature Constraintor. A complex network may lead to overfitting known features, reducing the generalization ability for new classes (please see ablation studies in Tab.2(b)). ", "page_idx": 4}, {"type": "text", "text": "Abnormal Invariant OCC Loss. We propose an abnormal invariant OCC loss to optimize our Feature Constraintor. The loss is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{o c c}=\\frac{1}{L}\\sum_{l=1}^{L}\\left(\\frac{1}{H_{l}W_{l}}\\sum_{h=1}^{H_{l}}\\sum_{w=1}^{W_{l}}(1-y_{h,w}^{l})||\\sqrt{||x_{h,w}^{\\prime,l,r}||_{2}+1}-1||_{1}+y_{h,w}^{l}||x_{h,w}^{\\prime,l,r}-x_{h,w}^{l,r}||_{2}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $y_{h,w}^{l}=1$ denotes the $(h,w)$ position on the feature map is anomalous and $y_{h,w}^{l}=0$ denotes a normal position (we can downsample the ground-truth mask to a low-resolution mask, which can indicate normal and abnormal positions). The first part in the loss function is a pseudo-Huber loss [22], which is used for constraining the normal residual features to a hypersphere. However, if we only constrain features to the hypersphere, the network may more easily overfit and simply map all features to the hypersphere. If we give the network another objective for anomalous features, this will urge the network to distinguish between normal and abnormal, rather than forming a shortcut solution. $||x_{h,w}^{\\prime,l,r}-x_{h,w}^{l,r}||_{2}$ r.  i\u201cnItrnovadruicaen ta\u201d n maebannosr tmhael  aibnnvaorrimanatl  treersimd ubayl  fseiamtpulrye s prreedmiacitni nrge ltahtiev ienliyt iualn cfheaatnugreeds relative to themselves and will not be mapped to the hypersphere. In this way, our proposed abnormal invariant OCC loss can not only make the distribution of normal residual features more compact but also keep abnormal residual features as invariant as possible. In addition, by constraining normal features into a hypersphere, the normal feature scales of different classes can also be more consistent. Therefore, after the Feature Constraintor, the normal and abnormal residual features are more distinguishable (see Fig.3), namely, we can obtain a better unified decision boundary. ", "page_idx": 4}, {"type": "text", "text": "3.3 Feature Distribution Estimating ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We employ the normalizing flow (NF) model [14] as our Feature Distribution Estimator to estimate the residual feature distribution. Note that our framework is not limited to normalizing flow, and other generative models can also be used as the distribution estimator. Formally, we denote $\\varphi_{\\partial}:$ X \u2208RCl \u2192Z \u2208RCl as our NF model. The input residual feature x\u2032h,,l,wr will be transformed into a latent feature $z_{h,w}^{l}=\\varphi_{\\theta_{2}}(x_{h,w}^{\\prime,l,r})$ by the NF model. The estimated residual distribution $p_{\\theta_{2}}(x)$ can be calculated according to the change of variables formula as follows [14, 20]: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{log}p_{\\theta_{2}}(x)=\\mathrm{log}p_{Z}(z)+\\mathrm{log}|\\mathrm{det}J|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the $J=\\nabla_{x}z$ is the Jacobian matrix of the bijective transformation $\\varphi_{\\theta_{2}}$ . The model parameters $\\theta_{2}$ can be optimized by maximizing the log-likelihoods, and the latent variables $Z$ for normal features are usually assumed to obey ${\\mathcal{N}}(0,I)$ . The maximum likelihood loss function for learning normal residual feature distribution is derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m l}=\\frac{1}{L}\\sum_{l=1}^{L}\\left(\\frac{1}{H_{l}W_{l}}\\sum_{h=1}^{H_{l}}\\sum_{w=1}^{W_{l}}\\frac{C_{l}}{2}\\mathrm{log}(2\\pi)+\\frac{1}{2}(z_{h,w}^{l})^{T}z_{h,w}^{l}-\\mathrm{log}|\\mathrm{det}J_{h,w}^{l}|\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the class-generalizable AD task, in addition to learning from normal samples, it\u2019s also valuable for us to effectively utilize abnormal samples that exist in known classes. Considering that we focus on detecting unknown anomalies in new classes, we cannot overfit the anomalies in known classes. Thus, following BGAD [45], we employ the explicit boundary guided semi-push-pull loss to learn a more discriminative and also generalizable feature distribution estimator. The loss is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{b g-s p p}=\\sum_{i=1}^{N_{n}}\\left|\\operatorname*{min}(\\log p_{i}-b_{n},0)\\right|+\\sum_{j=1}^{N_{a}}\\left|\\operatorname*{max}(\\log p_{j}-b_{n}+\\tau,0)\\right|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $b_{n}$ is an explicit normal boundary, $\\tau$ is a margin, $N_{n}$ and $N_{a}$ denote the number of normal and abnormal features in a training batch. We set $b_{n}$ according to the way in BGAD, and $\\tau$ is set to 0.1. Then, the whole loss function for training is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{o c c}+\\mathcal{L}_{m l}+\\mathcal{L}_{b g-s p p}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix E, we further discuss the sensitivity of balancing among the three loss terms. ", "page_idx": 5}, {"type": "text", "text": "3.4 Inference and Anomaly Scoring ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For new classes, our method only requires few-shot normal samples to extract features as reference, without any fine-tuning. We feed each test feature $\\boldsymbol{x}_{i}^{l}$ into the Feature Constraintor $C_{\\theta_{1}}$ and the Feature Distribution Estimator $\\varphi_{\\theta_{2}}$ to get the latent feature $z_{i}^{l}$ . The anomaly score is calculated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns(x_{i}^{l})=1-\\exp\\!\\bigg(-\\frac{C_{l}}{2}\\mathrm{log}(2\\pi)-\\frac{1}{2}(z_{i}^{l})^{T}z_{i}^{l}+\\log\\!|\\mathrm{det}J_{i}^{l}|\\bigg).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, we upsample all $s(x_{i}^{l})$ in the $l$ th level to the input image resolution $(H\\times W)$ using bilinear interpolation and combine all levels (i.e., sum) to obtain the final anomaly map. The maximum score of the anomaly map is taken as the anomaly detection score of the image. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Metrics. We conduct comprehensive experiments on four real-world industrial AD datasets, including MVTecAD [5], VisA [51], BTAD [25], and MVTec3D [7]. The detailed introduction to these datasets is provided in Appendix D. For MVTec3D, we only use RGB images in the dataset. As for our method\u2019s generalizability to other domains, we further evaluate our method on a medical image dataset, BraTS [24] (for brain tumor segmentation) and a video AD dataset, ShanghaiTech [23]. As our method is image-based, we extract video frames in ShanghaiTech as images for use. ", "page_idx": 5}, {"type": "text", "text": "Following previous works [5, 6], the anomaly detection performance is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC). ", "page_idx": 5}, {"type": "text", "text": "To examine the model\u2019s class-generalizable ability, we evaluate the cross-dataset performance. We combine the training and test sets of the MVTecAD dataset to train AD methods, and they are subsequently evaluated on the test set of other five datasets without any retraining, e.g., we train AD models on MVTecAD and test on VisA. For MVTecAD, we train AD models on VisA. We report the performance with the number of few-shot normal samples set to $K=2,4$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Implementation Details. All the training and test images are resized and cropped to $224\\times224$ resolution. Following the common practice in AD literatures, we utilize the commonly used WideResNet50 [49] as the feature extractor, and the outputs from the [1, 2, 3] layers of WideResNet50 are used as the pre-trained features. The parameters of the feature extractor are frozen during training. The layer numbers of the NF model are set as 8. We use the Adam [28] optimizer with weight decay $5e^{-4}$ to train the model. The total training epochs are set as 100, and the batch size is 32. The learning rate is $1e^{-5}$ initially and dropped by 0.1 after [70, 90] epochs. During training, we randomly select reference samples for each input image to increase residual feature diversity. The network details are in Appendix B, we also evaluate the computation costs of our model and other competing models. We run all the experiments with a single NVIDIA RTX 4090 GPU and random seed 42. ", "page_idx": 6}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/1c8008f8d5ec45a4f9a957d2f50daea2ee3d5da6c796bec90a10c8c90024cb0c.jpg", "table_caption": ["Table 1: Anomaly detection and localization results with AUROC metric on six real-world AD datasets under various few-shot AD settings. \u00b7/\u00b7 means image-level and pixel-level AUROCs. RDAD and UniAD don\u2019t utilize the few-shot normal samples to fine-tune, so the results under 2-shot and 4-shot are the same. For each input image, InCTRL only outputs an image-level anomaly score. Thus, the pixel-level AUROCs of InCTRL are missing. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Competing Methods. We select the representative one-for-one AD method (RDAD [13]) and the one-for-many AD method (UniAD [47]) as baselines. Our method is mainly compared with fewshot AD methods. Following WinCLIP [19], we adapt three conventional full-shot AD methods, including SPADE [11], PaDiM [12], and PatchCore [30], to the few-shot setting by making use of few-shot normal samples to calculate distance-based anomaly scores. We also compare with the few-shot AD method RegAD [18]. Most of these methods are based on WideResNet50 to extract features. However, these methods still need to re-model in new classes based on few-shot normal samples (see Sec.2), while our ResAD can be directly applied to new classes only requiring extracting features of few-shot normal samples as reference. Then, we also compare with the recent CLIP-based few-shot AD methods, including WinCLIP $[19]^{5}$ and InCTRL [50]. To guarantee the rationality of result comparison, we ensure all methods use the same few-shot normal samples, and all results are evaluated based on $224\\!\\times\\!224$ resolution. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tab.1 represents the comparison results of our ResAD and other SOTA competing methods in imagelevel AUROC and pixel-level AUROC, respectively, on six real-world AD datasets. Note that all the results are dataset-level average results across their respective data subsets. Compared to the results on known classes (results in the original papers), the performance of conventional AD methods will drop dramatically when used for new classes, whether it is the one-for-one6 (RDAD) or the one-for-many (UniAD) AD method. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "By comparison, we can see that our ResAD can significantly outperform all non-CLIP-based AD methods on both the 2-shot and 4-shot settings. With more few-shot normal images, the performance of all methods generally becomes better. On average, our ResAD outperforms the best competing model, RegAD, with up to $7.2\\%/2.9\\%$ and $7.6\\%/2.9\\%$ improvements under the 2-shot and 4-shot settings, respectively. In addition, please note that when evaluating RegAD, we utilize the few-shot normal samples to re-model the Multivariate Gaussian distribution for each new class (see Sec.2), while our ResAD is directly applied to each new class without any re-modeling or fine-tuning. Even with re-modeling, our method still has advantages over the conventional few-shot AD methods in cross-class detection. ", "page_idx": 7}, {"type": "text", "text": "We further implement a ResAD\u2020 model by utilizing the powerful ImageBind [16] as the feature extractor. The outputs from the [8, 16, 24, 32] layers of ImageBind-Huge are used as the pre-trained features. ImageBind is a recently proposed large-scale pre-trained multimodal model, which shows emergent zero-shot and few-shot recognition capabilities across many vision tasks. As shown in Tab.1, by employing a model with stronger representation capability, our method can achieve better cross-dataset performance, which significantly outperforms the SOTA CLIP-based AD methods, WinCLIP and InCTRL. This demonstrates that our framework can effectively combine the latest vision models to manifest stronger class-generalizable ability. What\u2019s more, under the 4-shot setting, our ResAD by only using WideResNet50 can achieve comparable or even better results than WinCLIP and InCTRL (with more powerful CLIP-based $\\mathrm{ViT-B}/16+1$ ), further demonstrating our superiority. Moreover, these two CLIP-based methods also heavily rely on CLIP-based image encoders. When we employ WideResNet50 in these two methods, our method has more advantages than these two methods (please see Appendix Tab.7). ", "page_idx": 7}, {"type": "text", "text": "When applied to other domains (medical images and video scenarios), our method also has better cross-domain generalization ability, despite it being trained on industrial data (the MVTecAD dataset). ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In ablation studies, we conduct experiments under the \u201cVisA to MVTecAD\u201d case and use the commonly used WideResNet50 [49] as the feature extractor. ", "page_idx": 7}, {"type": "text", "text": "Residual Feature Learning. As shown in Tab.2(a), without residual feature learning, the crossdataset performance drops dramatically from $90.5\\%/95.7\\%$ to $72.8\\%/82.9\\%$ . This verifies our confirmation that residual feature learning is of vital significance for class-generalizable anomaly detection. Analogously, any method that can reduce the variations of new class distribution relative to known class distributions is also promising to achieve class-generalizable anomaly detection. ", "page_idx": 7}, {"type": "text", "text": "Feature Constraintor. The ablation study on the effectiveness of the Feature Constraintor is also in Tab.2(a). \u201cw/o Feature Constraintor\u201d means the $\\mathcal{L}_{o c c}$ in E.q.(6) is not used. The effectiveness indicates that by further reducing the variations in the feature distribution and making the distribution of new classes more consistent with the learned distribution, we can achieve better cross-class AD results. In Fig.3, we also present a visualization figure to intuitively show the effect of the Feature Constraintor. ", "page_idx": 7}, {"type": "text", "text": "Abnormal Invariant OCC Loss. The effectiveness of abnormal invariant OCC loss is validated in Tab.2(a). \u201cw/o Abnormal Invariant OCC Loss\u201d means the $\\mathcal{L}_{o c c}$ only has the first part of E.q.(2). With the abnormal invariant OCC loss, image-level and pixel-level AUROCs can be improved by $5.6\\%$ and $1.8\\%$ , respectively. Moreover, we also find that without this loss, the results would rapidly decrease after certain epochs of training (i.e., overftiting). This shows that keeping abnormal residual features as invariant as possible is beneficial to avoid the Feature Constraintor overfitting and thus achieve better results. ", "page_idx": 7}, {"type": "text", "text": "Feature Constraintor Configuration. We further ablate the network architectures of the Feature Constraintor, the results are shown in Tab.2(b). The results indicate that the simple Conv+BN+ReLU ", "page_idx": 7}, {"type": "text", "text": "Table 2: Ablation studies on MVTecAD. (a) \u201cOurs\u201d implementation follows the same configuration as in Tab.1. \u201cw/o...\u201d indicates that we remove a certain component relative to \u201cOurs\u201d. I-AUROC and P-AUROC mean image-level AUROC and pixel-level AUROC, respectively. (b) \u201cConvBnRelu\u201d implements a simple Conv+BN+ReLU network. \u201cBasicBlock\u201d adopts the BasicBlock in ResNet. \u201cBottleNeck\u201d adopts the BottleNeck in ResNet. \u201cMultiScaleFusion\u201d is a FPN-like architecture to fuse multi-scale features. In \u201cMultiScaleFusion $^+$ BasicBlock/BottleNeck\u201d, we add BasicBlock/BottleNeck after the multi-scale fusion. ", "page_idx": 8}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/6c3d0fcdded480e77d41c370564e4455b4ca291b016da50613151b4ea75bb711.jpg", "table_caption": ["(a) Framework ablation studies. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/ccca99e2ab72741419ac90a8b6012ab9b2583251f87fa00ca9db2b2e3ee0d68c.jpg", "table_caption": ["(b) Comparison of different feature constraintors. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "zNiJZUAlxg/tmp/23e0ccd58b2eafcf7440b0d0b16d188720971c006b8d65cd2b490b7a5d5cfd59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Feature t-SNE visualization. (a) In the initial feature space, the features from different classes are significantly different. (b) In the residual feature space, even the residual feature distribution of unknown classes would not remarkably shift from the known distribution. Note that in (a) and (b), we only show normal residual features and use different colors to represent different classes. (c) The initial residual features. (d) The residual features after the Feature Constraintor. ", "page_idx": 8}, {"type": "text", "text": "network can yield the best performance. We observe a significant performance drop with a more complex feature constraintor (e.g., Bottleneck, MultiScaleFusion). One possible reason is that a complex network may lead to overftiting, reducing the generalization ability for various anomalies in new classes. ", "page_idx": 8}, {"type": "text", "text": "Cross-Class Within One Dataset. We show the results of training with $n$ classes from MVTecAD and testing on the remaining $15\\!-\\!n$ classes. By varying $n$ , we can demonstrate the sensitivity of the model to different numbers of training classes. Note that different $n$ means the number of test classes is different (this will cause the test results of different $n$ cannot be compared with each other). Thus, we use fixed 5 classes as the test classes, including hazelnut, pill, tile, carpet, and zipper. For $n=5$ , the training classes include bottle, cable, capsule, grid, and leather. For $n=10$ , the training classes include bottle, cable, capsule, grid, leather, metal nut, screw, toothbrush, transistor, and wood. The results under the 4-shot setting are in Tab.3. The results demonstrate that cross-dataset generalization is more challenging than cross-class generalization in a single dataset. With more training classes, the results will be better, but the model is not very sensitive. ", "page_idx": 8}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/ac95d1965f42a2030356255bf166b08e2a40d4e25e13b29ab295b58a68134c78.jpg", "table_caption": ["Table 3: Cross-class results with different numbers of training classes n. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Generalization to Other Anomaly Detection Frameworks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Furthermore, we think that our residual feature learning insight is not limited to the model proposed in this paper, but can be considered as an effective and general method for solving class-generalizable anomaly detection. The main reasons are: 1) The process of converting initial features to residual features can be easily applied to other AD models. 2) Residual features are less sensitive to new classes (see Sec.3.1). In this subsection, we further extend our method to the popular reconstruction-based AD framework. Specifically, we employ UniAD [47] as baseline and incorporate our method into it. As UniAD is feature-based AD method, combining our residual feature learning with it is straightforward. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Anomaly detection and localization results when incorporating our method into UniAD. \u201cRFL\u201d represents residual feature learning. ", "page_idx": 9}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/24ad84c7d001c30a672afdc78903c760f7cb6014adc132af05874c5ac0dadc2d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We can convert the initial features into residual features and then perform subsequent feature reconstruction. The experimental results are shown in Tab.4. It can be found that the performance of UniAD is quite poor when used for new classes, while converting to residual feature learning can significantly improve the model\u2019s class-generalizable capacity. The remarkable improvements (e.g., $25.6\\%/13.8\\%$ on ", "page_idx": 9}, {"type": "text", "text": "MVTecAD) validate the effectiveness and generalizability of residual features for designing generalizable AD models. ", "page_idx": 9}, {"type": "text", "text": "4.5 Visualization and Qualitative Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Visualization Results. Fig.3(a) and (b) show the tSNE visualization of initial features and residual features. It can be found that in the initial feature space, the feature distribution of new classes is significantly different from the distribution of known classes, resulting in poor adaptability of AD models to new classes. However, the variations between different classes can be significantly reduced by converting into residual space. In this way, the model\u2019s generalizability to new classes can be effectively improved. Fig.3(c) and (d) show the t-SNE visualization of initial residual features and residual features after the Feature Constraintor. Results show that the Feature Constraintor can make the normal residual features more compact and more separated from the abnormal features. ", "page_idx": 9}, {"type": "image", "img_path": "zNiJZUAlxg/tmp/b819baf613545734110d0b854f5e4ea2083e571bfa7b6c27cc35371c4d485ed3.jpg", "img_caption": ["Figure 4: Qualitative results. The anomaly score maps are generated under the \u201cVisA to MVTecAD\u201d case. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Qualitative Results. Fig.4 shows qualitative results under the \u201cVisA to MVTecAD\u201d case with WideResNet50 as the feature extractor. It can be seen that most SOTA methods fail to generate good anomaly localization maps for new classes, mainly existing many false positives in normal regions. However, our method can effectively avoid false positives in normal regions and locate anomalies more accurately. More qualitative results are in Appendix Fig.5. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a simple but effective framework: ResAD, for achieving class-generalizable anomaly detection. ResAD consists of several simple neural network modules that are easy to train and apply in real-world scenarios. Despite the simplicity, ResAD achieves remarkable anomaly detection results in new classes. We conclude our findings for future research: residual features are really effective for designing generalizable AD models, and our feature constraining insight also has good reference values for future work. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The limitations of our method are discussed in Appendix C. ", "page_idx": 9}, {"type": "text", "text": "Social Impacts. As a unified model for class-generalizable anomaly detection, the proposed method does not suffer from particular ethical concerns or negative social impacts. All datasets used are public. All qualitative visualizations are based on industrial product images, which does not infringe personal privacy. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Fund of China (62371295), the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and the Science and Technology Commission of Shanghai Municipality (22DZ2229005). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Samet Akcay, Amir Atapour-Abarghouei, and Toby P. Breckon. Ganomaly: Semi-supervised anomaly detection via adversarial training. In ACCV, page 622\u2013637, 2018. [2] Lynton Ardizzone, Carsten L\u00fcth, Jakob Kruse, Carsten Rother, and Ullrich K\u00f6the. Guided image generation with conditional invertible neural networks. arXiv preprint arXiv: 1907.02392, 2019.   \n[3] Kourosh Teimouri Baghaei, Amirreza Payandeh, Pooya Fayyazsanavi, Shahram Rahimi, Zhiqian Chen, and Somayeh Bakhtiari Ramezani. Deep representation learning: Fundamentals, perspectives, applications, and open challenges. arXiv preprint arXiv:2211.14732, 2022. [4] Liron Bergman, Niv Cohen, and Yedid Hoshen. Deep nearest neighbor anomaly detection. arXiv preprint arXiv: 2002.10445, 2020.   \n[5] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad - a comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, 2019.   \n[6] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In CVPR, 2020.   \n[7] Paul Bergmann, Xin Jin, David Sattlegger, and Carsten Steger. The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization. arXiv preprint arXiv:2112.09045, 2021.   \n[8] Paul Bergmann, Sindy Lowe, Michael Fauser, David Sattlegger, and Carsten Steger. Improving unsupervised defect segmentation by applying structural similarity to autoencoders. In International Conference on Computational Vision Technologies and Applications, 2019. [9] Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang, Guansong Pang, and Weiming Shen. A survey on visual anomaly detection: Chanllenge, approach, and prospect. arXiv preprint arXiv:2401.16402, 2024.   \n[10] Xuhai Chen, Yue han, and Jiangning Zhang. A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vnad workshop challenge tracks 1&2: 1st place on zeroshot ad and 4th place on few-shot ad. arXiv preprint arXiv:2305.17382, 2023.   \n[11] Niv Cohen and Yedid Hoshen. Sub-image anomaly detection with deep pyramid correspondences. arXiv preprint arXiv: 2005.02357v3, 2020.   \n[12] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution modeling framework for anomaly detection and localization. In 1st International Workshop on Industrial Machine Learning, 2021.   \n[13] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In CVPR, 2022.   \n[14] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In International Conference on Learning Representations, 2017.   \n[15] Zheng Fang, Xiaoyang Wang, Haocheng Li, Jiejie Liu, Qiugui Hu, and Jimin Xiao. Fastrecon: Few-shot industrial anomaly detection via fast feature reconstruction. In ICCV, 2023.   \n[16] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023.   \n[17] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In IEEE Winter Conference on Application of Computer Vision, 2022.   \n[18] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yanfeng Wang. Registration based few-shot anomaly detection. In ECCV, 2022.   \n[19] Jongheon Jeong, Yang Zou, Taewan Kim Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In CVPR, 2023.   \n[20] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Conference and Workshop on Neural Information Processing Systems, 2019.   \n[21] Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J. Radke, and Octavia Camps. Towards visually explaining variational autoencoders. In CVPR, 2020.   \n[22] Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus-Robert Muller. Explainable deep one-class classification. In International Conference on Learning Representations, 2021.   \n[23] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked rnn framework. In ICCV, 2017.   \n[24] Bjoern H. Menze, Andr\u00e1s Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin S. Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, Levente Lanczi, Elizabeth R. Gerstner, Marc-Andr\u00e9 Weber, Tal Arbel, Brian B. Avants, Nicholas Ayache, Patricia Buendia, D. Louis Collins, Nicolas Cordier, Jason J. Corso, Antonio Criminisi, Tilak Das, Herve Delingette, \u00c7agatay Demiralp, Christopher R. Durst, Michel Dojat, Senan Doyle, Joana Festa, Florence Forbes, Ezequiel Geremia, Ben Glocker, Polina Golland, Xiaotao Guo, Andac Hamamci, Khan M. Iftekharuddin, Raj Jena, Nigel M. John, Ender Konukoglu, Danial Lashkari, Jos\u00e9 Antonio Mariz, Raphael Meier, S\u00e9rgio Pereira, Doina Precup, Stephen J. Price, Tammy Riklin Raviv, Syed M. S. Reza, Michael T. Ryan, Duygu Sarikaya, Lawrence H. Schwartz, Hoo-Chang Shin, Jamie Shotton, Carlos A. Silva, Nuno J. Sousa, Nagesh K. Subbanna, G\u00e1bor Sz\u00e9kely, Thomas J. Taylor, Owen M. Thomas, Nicholas J. Tustison, G\u00f6zde B. \u00dcnal, Flor Vasseur, Max Wintermark, Dong Hye Ye, Liang Zhao, Binsheng Zhao, Darko Zikic, Marcel Prastawa, Mauricio Reyes, and Koen Van Leemput. The multimodal brain tumor image segmentation benchmark (brats). IEEE Transactions on Medical Imaging, 2015.   \n[25] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. Vt-adl: A vision transformer network for image anomaly detection and localization. arXiv preprint arXiv:2104.10036, 2021.   \n[26] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 2021.   \n[27] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly detection. In CVPR, 2020.   \n[28] Diederik P.Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In ICLR, 2015.   \n[29] Oliver Rippel, Patrick Mertens, and Dorit Merhof. Modeling the distribution of normal data in pre-trained deep features for anomaly detection. arXiv preprint arXiv: 2005.14140, 2020.   \n[30] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In CVPR, 2022.   \n[31] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semisupervised defect detection with normalizing flows. In IEEE Winter Conference on Application of Computer Vision, 2021.   \n[32] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Fully convolutional cross-scale-flows for image-based defect detection. In IEEE Winter Conference on Application of Computer Vision, 2022.   \n[33] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Asymmetric studentteacher networks for industrial anomaly detection. In WACV, 2023.   \n[34] Lukas Ruff, Robert A. Vandermeulen, Nico Gornitz, Lucas Deecke, and Shoaib A. Siddiqui. Deep one-class classification. In International Conference on Machine Learning, 2021.   \n[35] Mohammadreza Salehi, Niousha Sadjadi, Soroos Hossein Rohban, and Hamid R.Rabiee. Multiresolution knowledge distillation for anomaly detection. In CVPR, 2021.   \n[36] Thomas Schlegl, Philipp Seeb\u00a8ock, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on Information Processing in Medical Imaging, 2017.   \n[37] David M.J. Tax and Robert P.W. Duin. Support vector data description. In Machine Learning, pages 45\u201366, 2004.   \n[38] Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran, Ta Duc Huy, Soan T.M Duong, Chanh D. Tr. Nguyen, and Steven Q.H. Truong. Revisting reverse distillation for anomaly detection. In CVPR, 2023.   \n[39] Guodong Wang, Shumin Han, Errui Ding, and Di Huang. Student-teacher feature pyramid matching for unsupervised anomaly detection. In British Machine Vision Conference, 2021.   \n[40] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization. arXiv preprint arXiv:2103.03097, 2021.   \n[41] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S Yu. Machine unlearning: A survey. ACM Computing Surveys, 2023.   \n[42] Jie Yang, Yong Shi, and Zhiquan Qi. Dfr: Deep feature reconstruction for unsupervised anomaly segmentation. arXiv preprint arXiv: 2012.07122, 2020.   \n[43] Xincheng Yao, Ruoqi Li, Zefeng Qian, Yan Luo, and Chongyang Zhang. Focus the discrepancy: Intra- and inter-correlation learning for image anomaly detection. In ICCV, 2023.   \n[44] Xincheng Yao, Ruoqi Li, Zefeng Qian, Lu Wang, and Chongyang Zhang. Hierarchical gaussian mixture normalizing flow modeling for unified anomaly detection. In ECCV, 2024.   \n[45] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and Chongyang Zhang. Explicit boundary guided semi-push-pull contrastive learning for supervised anomaly detection. In CVPR, 2023.   \n[46] Xincheng Yao, Chongyang Zhang, Ruoqi Li, Jun Sun, and Zhenyu Liu. One-for-all: Proposal masked cross-class anomaly detection. In AAAI, 2023.   \n[47] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. A unified model for multi-class anomaly detection. arXiv preprint arXiv:2206.03687, 2022.   \n[48] Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. arXiv preprint arXiv:2111.07677, 2021.   \n[49] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.   \n[50] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024.   \n[51] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-thedifference self-supervised pre-training for anomaly detection and segmentation. In ECCV, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A More Discussions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Detailed Comparison with InCTRL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We should have proposed the idea of residual learning independently of InCTRL and almost at the same time (i.e., we completed the initial version of our method during CVPR 2024). But our method has obvious differences with InCTRL in the definition and utilization of residuals. This (i.e., two independent works almost simultaneously proposed the residual learning idea) also demonstrates residual learning is an effective way to achieve class-generalizable anomaly detection. The main differences between our method and InCTRL [50] are as follows: ", "page_idx": 13}, {"type": "text", "text": "(1) The definition of residuals in InCTRL is based on feature distances. The residual map is defined by ((E.q.(1) in the InCTRL paper): $M_{x}^{l}(i,j)=1-\\left\\langle T_{x}^{l}(i,j),h(T_{x}^{l}(i,j)|\\mathcal{P}^{\\prime})\\right\\rangle$ , where $h(\\dot{T}_{x}^{l}(i,j)|\\mathcal{P}^{\\prime})$ returns the embedding of the patch token that is most similar to $T_{x}^{l}(i,j)$ among all image patches in $\\mathcal{P}^{\\prime}$ , and $\\langle\\cdot\\rangle$ is the cosine similarity function. Thus, InCTRL is based on residual distance maps, while our method is based on residual features. ", "page_idx": 13}, {"type": "text", "text": "By comparison, we think that residual distances in InCTRL can limit the range of residual representation (as the cosine similarity is in [-1,1]). This is not beneficial for distinguishing between normal and abnormal regions, as a position on the residual map is only represented by a residual distance value. Within a limited representation range $(1\\!-\\![-1,1]\\to[0,2])$ , normal and abnormal residual distance values are more likely to be not strictly separable. Thus, for a position on the residual map, it\u2019s hard for us to make decision based on a scalar value. So, InCTRL makes image-level classification based on a whole residual map (see the following (2)). In contrast, our residual features don\u2019t limit the range of residual representation and can retain the feature properties. In high-dimensional feature space, we can also establish better decision boundaries between normal and abnormal (a basic idea in machine learning: solving low-dimensional inseparability by converting to high-dimension). ", "page_idx": 13}, {"type": "text", "text": "(2) InCTRL devises a holistic anomaly scoring function $\\phi$ to learn the residual distance map $M_{x}=$ $\\textstyle{\\frac{1}{n}}\\sum_{l=1}^{n}M_{x}^{l}$ and convert it to an anomaly score: $s(x)\\;=\\;\\phi(M_{x}^{+};\\Theta_{\\phi})\\:+\\:\\alpha s_{p}(x)$ (E.q.(8) in the InCTRL paper), where $M_{x}^{+}=M_{x}\\oplus s_{i}(x)\\oplus s_{a}(x)$ (E.q.(7)). $s_{i}(x)$ is an anomaly score based on an image-level residual map $F_{x}$ (see E.q.(4) in the InCTRL paper) and $s_{a}(x)$ is a text prompt-based anomaly score. Thus, InCTRL is to train a binary classification network based on residual distance maps. For each input image, InCTRL finally only outputs an image-level anomaly score. Our method is to learn the distribution of residual features, an anomaly score can be estimated for each feature, thus can be used to locate anomalies. ", "page_idx": 13}, {"type": "text", "text": "(3) Due to the designs in InCTRL that we mentioned above, one main advantage of our method is that it can achieve image-level anomaly detection and also pixel-level anomaly localization, while InCTRL only achieves image-level anomaly detection functionality. ", "page_idx": 13}, {"type": "text", "text": "As for performance, the average results on six AD datasets of our method are better than InCTRL\u2019s (please see Tab.1 in the main text). ", "page_idx": 13}, {"type": "text", "text": "A.2 Discussion on Few-Shot Normal Sample Selection Strategy ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In our paper, the few-shot normal reference samples are randomly selected and fixed. This will raise concerns about whether random selection is reasonable and whether it may lead to insufficient representativeness of the reference feature pools. From the perspective of method comparison, we think that random selection is feasible, as long as we ensure that all methods use the same reference samples, the result comparison is reasonable. However, when the difference between normal images is too large, it may cause the reference feature pools are not representative. Nonetheless, please further note that, in our method, we only extract the features of the few-shot normal samples and store all the features in the reference feature pools. The reference feature pools don\u2019t impair or lose any representation features. For the few-shot normal samples, the reference feature pools are representative enough to them. Therefore, whether the representativeness is sufficient is determined by the few-shot normal samples themselves rather than our method. For some classes, the few-shot normal samples are representative, while for some hard classes, they may mot be representative enough. ", "page_idx": 13}, {"type": "text", "text": "For practical applications, this issue should be particularly focused and reasonably addressed. We expect that the reference samples can fully represent their class, so it\u2019s best to have sufficient differences between the reference samples. Thus, the sample selection strategy cannot be random. Of course, the simplest resolution is to increase the number of reference samples. This is feasible, as in practical applications, the number of reference samples is usually not as strict as the 2-shot and 4-shot in our paper. ", "page_idx": 14}, {"type": "text", "text": "A feasible method is to first cluster all available normal samples into different clusters based on a clustering algorithm (e.g., KMeans). Then, based on the number of reference samples, we evenly distribute it to each cluster. When selecting from a cluster, we can prioritize selecting samples closer to the center. During clustering, we think that the FID and LPIPS metrics are good ways to calculate the difference between two samples. In addition, when there are a large number of reference samples, we can also use the method in PatchCore [30] to select coreset features as reference features, which will be more efficient and also representative. ", "page_idx": 14}, {"type": "text", "text": "A.3 Feature Constraintor and Feature Distribution Estimator ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The goal of our Feature Constraintor is to constrain initial residual features to a spatial hypersphere for further reducing feature variations. After the Feature Constraintor, feature variations can effectively be reduced, but this does not mean that the feature distribution is fixed within the hypersphere. The ideal situation is that even in new classes, normal feature distribution is fixed within a hypersphere, while all anomalous features are outside the hypersphere. Then, only the Feature Constraintor part is enough to achieve good AD results. However, in practical optimization, it\u2019s hard to achieve the ideal situation. After the Feature Constraintor, normal and abnormal features may still not be fully separable based on the distances from the features to the center. Therefore, the Feature Distribution Estimator (namely the normalizing flow model used in our method) is used to learn the feature distribution, which can assist us in better distinguishing normal and abnormal features. ", "page_idx": 14}, {"type": "text", "text": "B Model Architecture and Complexity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Normalizing Flow Model Architecture. The normalizing flow model is mainly based on Real-NVP [14] architecture, which is composed of the so-called coupling layers. All coupling layers have the same architecture, where a learnable subnet is utilized to predict the affine parameters [14]. The convolutional subnet in Real-NVP is replaced with a two-layer MLP network. Each coupling layer is followed by a random and fixed soft permutation of channels [2] and a fixed scaling by a constant, similar to ActNorm layers introduced by [20]. Furthermore, we adopt the soft clamping of multiplication coefficients used by [14]. Following [17], we add positional embeddings to each coupling layer, which are concatenated with the first half of the input features. The dimension of all positional embeddings is set to 256. ", "page_idx": 14}, {"type": "text", "text": "Complexity Comparison. With the image size fixed as $224\\times224$ , we compare the number of parameters and per-image inference time with all competitors. We conclude that the advantage of ResAD does not come from a larger model capacity. ", "page_idx": 14}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/3cf67ac6fe79e8c73b4cb058f85597bc5b43da6c226aa9b3fc58764ea6d38b55.jpg", "table_caption": ["Table 5: Complexity comparison between our ResAD and other competing methods "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we propose a simple but effective AD framework, ResAD, to accomplish classgeneralizable anomaly detection. Even if our method manifests good AD performance on six real-world industrial AD datasets, there are still some limitations of our work. One limitation of our work is that we only conducted experiments on data of image modality, it\u2019s very valuable to further extend our method to other application domains and data modalities, such as video data and time series, to more comprehensively validate our method\u2019s generalizability. Our future work will focus on further generalizing our method to other data modalities, not only to achieve class-generalizable but also domain-generalizable anomaly detection. Another valuable future work is to incorporate our method into recent SOTA AD methods for achieving better class-generalizable AD performance. In Sec.4.4, we incorporate our method into UniAD and gain remarkable improvements. How to upgrade the other types of anomaly detection methods to class-generalizable AD methods and how to find a general approach for class-generalizable (or even domain-generalizable) anomaly detection will be the future works. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "MVTecAD. The MVTecAD [5] dataset is widely used as a standard benchmark for evaluating unsupervised anomaly detection methods. This dataset contains 5354 high-resolution images (3629 images for training and 1725 images for testing) of 15 different product categories. 5 classes consist of textures and the other 10 classes contain objects. A total of 73 different defect types are presented and almost 1900 defective regions are manually annotated in this dataset. ", "page_idx": 15}, {"type": "text", "text": "BTAD. The BeanTech Anomaly Detection dataset [25] is an another popular benchmark, which contains 2830 real-world images of 3 industrial products. Product 1, 2, and 3 of this dataset contain 400, 1000, and 399 training images respectively. ", "page_idx": 15}, {"type": "text", "text": "MVTec3D. The MVTec3D [7] dataset is for 3D anomaly detection, which contains 4147 highresolution 3D point cloud scans paired with 2D RGB images from 10 real-world categories. In this dataset, most anomalies can also be detected only through RGB images. Since we focus on image anomaly detection, we only use RGB images of the MVTec3D dataset. ", "page_idx": 15}, {"type": "text", "text": "VisA. The Visual Anomaly dataset [51] is a larger anomaly detection dataset compared to MVTecAD [5]. This dataset contains 10821 images with 9621 normal and 1200 anomalous samples. In addition to images that only contain single instance, the VisA dataset also have images that contain multiple instances. Moreover, some product categories of the VisA dataset, such as Cashew, Chewing gum, Fryum and Pipe fryum, have objects that are roughly aligned. These characteristics make the VisA dataset more challenging than the MVTecAD dataset, whose images only have single instance and are better aligned. ", "page_idx": 15}, {"type": "text", "text": "E Sensitivity of Balancing The Loss Terms ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "During training, we found that summing up the three loss terms (see E.q.(6)) and then backpropagating gradients to optimize the whole model would lead to unstable training. Then, we used the \u201ctorch.detach()\u201d method in the Pytorch library to detach the features after the Feature Constraintor and then sent the detached features into the normalizing flow (NF) model. This simple way can make the model training more stable. Thus, the weight of $\\mathcal{L}_{o c c}$ can be set as 1 (i.e., we can not need to balance $\\mathcal{L}_{o c c}$ with $\\mathcal{L}_{m l}$ and $\\mathcal{L}_{b g-s p p}$ , as the Feature Constraintor and the NF model parts are separated in the gradient graph). When training the NF model, the $\\mathcal{L}_{m l}$ is the basic loss. Thus, we keep the weight of $\\mathcal{L}_{m l}$ as 1 and set a variable $\\lambda$ as the weight of $\\mathcal{L}_{b g-s p p}$ . By varying different $\\lambda$ values, the results (under the 4-shot setting, from VisA to MVTecAD) about the sensitivity are in Tab.6. ", "page_idx": 15}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/401910713c7bf40c44b67ffaa6940b796ea3d8e8a5c0752c76df861d1ff2d6a3.jpg", "table_caption": ["Table 6: Anomaly detection and localization results when varying different $\\lambda$ values for balancing the three loss terms. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Both small and large $\\lambda$ values can lead to performance degradation. $\\mathcal{L}_{b p-s p p}$ is to assist model in learning abnormal residual features. Small $\\lambda$ may cause the impact of abnormal features on the whole loss $\\mathcal{L}$ (E.q.(6)) to be relatively small. Large $\\lambda$ may lead to overftiting to known anomalies, which is not conducive to generalization. ", "page_idx": 15}, {"type": "text", "text": "F Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Extra Few-shot AD Results. In the main text, the WinCLIP and InCTRL utilize the CLIP-based ViT-B $^{\\prime16+}$ as the feature extractor. We further employ the WideReset50, which is commonly used in anomaly detection, as the feature extractor in these methods. We note that the two methods do not necessarily need CLIP-based vision encoders. We can remove the vision-language alignment part in the two methods and the remaining modules can also achieve anomaly detection. For example, we can send image patches provided by WinCLIP\u2019s window mechanism into WideResNet50, and also obtain the window embedding maps of different scales as shown in Figure 4 of the WinCLIP paper. However, because the features of WideResNet50 are not aligned with the text features, we remove the language-guided anomaly score map and only generate the vision-based anomaly score map based on the few-shot normal samples (the $\\scriptstyle\\mathrm{WinCLIP}+$ in the WinCLIP paper). The results under the 4-shot setting are in Tab.7. The results show that our method has more significant superiorities on networks with weaker representation capability. Thus, compared to WinCLIP and InCTRL, our method is less reliant on the representation capability of the backbone network and is more widespreadly applicable for various backbones. ", "page_idx": 16}, {"type": "text", "text": "Table 7: Anomaly detection and localization results with WideResNet50 as the feature extractor. ", "page_idx": 16}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/025ed719c3ac2c260deead35248ca10364a85ef3a153dd7f88c4aa67dc9d8a29.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Additional Results on Other Data Groups. In the main text, the results are evaluated on a single group of few-shot reference samples. However, the selection of few-shot reference samples may affect the performance of the model. To fully represent our model\u2019s robustness, we further randomly select two groups of few-shot reference samples. The results under the 4-shot setting are in Tab.8. ", "page_idx": 16}, {"type": "table", "img_path": "zNiJZUAlxg/tmp/3f9a9258bb3d01e32031f6287dda4c9c13c6fcad63b1d7ccfac147e75ffc4f3e.jpg", "table_caption": ["Table 8: Anomaly detection and localization results on other two groups of few-shot reference samples. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Additional Qualitative Results. We present in Fig.5 additional anomaly localization results of categories from the MVTecAD dataset. The anomaly score maps are generated under the \u201cVisA to MVTecAD\u201d case, where AD models are trained on the VisA dataset. ", "page_idx": 16}, {"type": "image", "img_path": "zNiJZUAlxg/tmp/63b3770ce2af6b3cd6dd75c53b8e1780cdd781ffd1b1b1298f25dfd28e0ee5a6.jpg", "img_caption": ["Figure 5: Additional qualitative results on MVTecAD. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have summarized our contributions well in the abstract and introduction, and the method and experiments sections also reflected these contributions. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Please see Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please see the implementation details in Sec.4 and network details in Appendix B. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: As mentioned in Abstract, the open-source code will be available at https: //github.com/xcyao/ResAD. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please see Sec.4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Due to the large amount of experiments (please see Tab.1), we don\u2019t have enough time and resources to use different random seeds for multiple experiments. However, we ensured that all experiments were conducted under the same condition of the random seed set to 42. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please see the computation costs in Tab.5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics, and believe that our research conforms the Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the Conclusion section, we discussed the potential social impacts of our method. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We think that our paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The datasets used in the paper are all open-sourced, we have also cited corresponding papers. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper proposes a new anomaly detection model, does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]