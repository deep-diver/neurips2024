[{"figure_path": "pR37AmwbOt/figures/figures_1_1.jpg", "caption": "Figure 1: Images generated by the baseline model SD v2.1 and models trained by our method. The top row contains harmful images, and the bottom row contains clean images. Harmful images generated by our methods before and after malicious fine-tuning both show quality degradation because the models are safely aligned before malicious fine-tuning and can also resist malicious fine-tuning. The generation quality of clean images is maintained in the safety alignment before malicious fine-tuning and slightly decreases after malicious fine-tuning in color and texture details. Orange boxes are added by the authors for publication.", "description": "This figure compares the image generation results of the baseline model (Stable Diffusion v2.1) and the models trained using the proposed method.  The top row showcases harmful images generated by both models before and after malicious fine-tuning.  The bottom row displays clean images generated by both models.  The results demonstrate that the method effectively prevents the generation of high-quality harmful images, both before and after malicious fine-tuning attempts. While clean image quality is slightly reduced after malicious fine-tuning, it largely remains unaffected.", "section": "1 Introduction"}, {"figure_path": "pR37AmwbOt/figures/figures_3_1.jpg", "caption": "Figure 2: Left. Diagram illustrating the method of leveraging catastrophic forgetting. The method leverages catastrophic forgetting by widening the distribution between clean and harmful data. Right. The method uses contrastive learning to leverage catastrophic forgetting against malicious fine-tuning.", "description": "This figure illustrates the core idea of the proposed method. The left panel shows how the method leverages catastrophic forgetting by widening the distribution between clean and harmful data in the latent space.  The right panel shows how contrastive learning is used to achieve this separation, training the model to distinguish between clean and harmful data distributions.  The goal is to make generating harmful images a more difficult task for the model, effectively preventing malicious fine-tuning from degrading the model's safety.", "section": "3 Method"}, {"figure_path": "pR37AmwbOt/figures/figures_8_1.jpg", "caption": "Figure 1: Images generated by the baseline model SD v2.1 and models trained by our method. The top row contains harmful images, and the bottom row contains clean images. Harmful images generated by our methods before and after malicious fine-tuning both show quality degradation because the models are safely aligned before malicious fine-tuning and can also resist malicious fine-tuning. The generation quality of clean images is maintained in the safety alignment before malicious fine-tuning and slightly decreases after malicious fine-tuning in color and texture details. Orange boxes are added by the authors for publication.", "description": "This figure compares the image generation results of the baseline Stable Diffusion v2.1 model and models trained using the proposed method.  The top row shows harmful images generated by both models, illustrating the degradation in quality caused by the malicious fine-tuning that the proposed method mitigates. The bottom row shows clean images generated by both models before and after malicious fine-tuning, demonstrating the method's success in preserving the model's ability to generate clean images even after malicious fine-tuning.", "section": "1 Introduction"}, {"figure_path": "pR37AmwbOt/figures/figures_15_1.jpg", "caption": "Figure 1: Images generated by the baseline model SD v2.1 and models trained by our method. The top row contains harmful images, and the bottom row contains clean images. Harmful images generated by our methods before and after malicious fine-tuning both show quality degradation because the models are safely aligned before malicious fine-tuning and can also resist malicious fine-tuning. The generation quality of clean images is maintained in the safety alignment before malicious fine-tuning and slightly decreases after malicious fine-tuning in color and texture details. Orange boxes are added by the authors for publication.", "description": "This figure compares image generation results from the Stable Diffusion v2.1 baseline model and the model trained using the proposed method.  The top row displays harmful images generated by both models; the bottom row shows clean images generated by both. The results illustrate the effectiveness of the proposed method in maintaining clean image generation quality while preventing the generation of harmful images even after malicious fine-tuning. The method's safety alignment prevents the model from generating harmful images initially, and it resists the malicious fine-tuning attempts which only produce low-quality images.", "section": "1 Introduction"}, {"figure_path": "pR37AmwbOt/figures/figures_17_1.jpg", "caption": "Figure 1: Images generated by the baseline model SD v2.1 and models trained by our method. The top row contains harmful images, and the bottom row contains clean images. Harmful images generated by our methods before and after malicious fine-tuning both show quality degradation because the models are safely aligned before malicious fine-tuning and can also resist malicious fine-tuning. The generation quality of clean images is maintained in the safety alignment before malicious fine-tuning and slightly decreases after malicious fine-tuning in color and texture details. Orange boxes are added by the authors for publication.", "description": "This figure shows the results of the proposed method compared to the baseline model (Stable Diffusion v2.1).  The top row displays images generated by the baseline model, illustrating the generation of harmful content before and after malicious fine-tuning.  The bottom row shows images generated by the proposed method, demonstrating the method's ability to maintain the generation quality of clean images while preventing the generation of harmful images, even after malicious fine-tuning. The difference in image quality between the baseline and proposed method highlights the effectiveness of the proposed approach.", "section": "1 Introduction"}, {"figure_path": "pR37AmwbOt/figures/figures_17_2.jpg", "caption": "Figure 1: Images generated by the baseline model SD v2.1 and models trained by our method. The top row contains harmful images, and the bottom row contains clean images. Harmful images generated by our methods before and after malicious fine-tuning both show quality degradation because the models are safely aligned before malicious fine-tuning and can also resist malicious fine-tuning. The generation quality of clean images is maintained in the safety alignment before malicious fine-tuning and slightly decreases after malicious fine-tuning in color and texture details. Orange boxes are added by the authors for publication.", "description": "This figure compares image generation results from the baseline Stable Diffusion v2.1 model and the models trained using the proposed method.  The top row shows examples of harmful images generated by both models, illustrating that the proposed method successfully degrades the quality of harmful images even after malicious fine-tuning, indicating improved safety. The bottom row shows clean images generated by both models, showcasing that the proposed method maintains the generation quality of clean images, even with a slight decrease in color and texture detail after malicious fine-tuning. The orange boxes were added for publication purposes and are not part of the original figure.", "section": "1 Introduction"}]