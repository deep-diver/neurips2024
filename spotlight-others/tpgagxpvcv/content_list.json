[{"type": "text", "text": "Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Paul Krzakala LTCI & CMAP , T\u00e9l\u00e9com paris, IP Paris ", "page_idx": 0}, {"type": "text", "text": "Junjie Yang LTCI, T\u00e9l\u00e9com paris, IP Paris ", "page_idx": 0}, {"type": "text", "text": "R\u00e9mi Flamary CMAP, Ecole polytechnique, IP Paris ", "page_idx": 0}, {"type": "text", "text": "Florence d\u2019Alch\u00e9-Buc LTCI, T\u00e9l\u00e9com paris, IP Paris ", "page_idx": 0}, {"type": "text", "text": "Charlotte Laclau LTCI, T\u00e9l\u00e9com paris, IP Paris ", "page_idx": 0}, {"type": "text", "text": "Matthieu Labeau LTCI, T\u00e9l\u00e9com paris, IP Paris ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose Any2Graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e. a deep learning model that predicts an entire graph for any kind of input. The framework is built on a novel Optimal Transport loss, the Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary properties (permutation invariance, differentiability) and is designed to handle any-sized graphs. Numerical experiments showcase the versatility of the approach that outperforms existing competitors on a novel challenging synthetic dataset and a variety of real-world tasks such as map construction from satellite image (Sat2Graph) or molecule prediction from fingerprint (Fingerprint2Graph). 1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work focuses on the problem of Supervised Graph Prediction (SGP), at the crossroads of Graph-based Learning and Structured Prediction. In contrast to node and graph classification or link prediction widely covered in recent literature by graph neural networks, the target variable in SGP is a graph and no particular assumption is made about the input variable. Emblematic applications of SGP include knowledge graph extraction [28] or dependency parsing [17] in natural language processing, conditional graph scene generation in computer vision [53], [12], or molecule identification in chemistry [10, 55, 49], to name but a few. Moreover, close to SGP is the unsupervised task of graph generation notably motivated by de novo drug design [8, 15, 41]. ", "page_idx": 0}, {"type": "text", "text": "SGP raises some specific issues related to the complexity of the output space and the absence of widely accepted loss functions. First, the non-Euclidean nature of the output to be predicted makes both inference and learning challenging while the size of the output space is extremely large. Second, the arbitrary size of the output variable to predict requires a model with a flexible expressive power in the output space. Third, graphs are characterized by the absence of natural or ground-truth ordering of their nodes, making comparison and prediction difficult. This particular issue calls for a node permutation invariant distance to predict graphs. Scrutinizing the literature through the lens of these issues, we note that existing methodologies circumvent the difficulty of handling output graphs in various ways. A first body of work avoids end-to-end learning by relying on some relaxations. For instance, energy-based models (see for instance [38]) convert the problem into the learning of an energy function of input and output while surrogate regression methods [10] implicitly embed output graphs into a given Hilbert space where the learning task boils down to vector-valued regression. Note that these two families of approaches generally involve a rather expensive decoding step at inference time. In what follows, we focus on methods that directly output graphs or close relaxations, enabling end-to-end learning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "One strategy to overcome the need for a permutation invariant loss is to exploit the nature of the input data to determine a node ordering, with the consequence that application to new types of data requires similar engineering. For instance, in de novo drug generation SMILES representations [8] are generally used to determine atom ordering. In semantic parsing, the target graph is a tree that can be serialized [3] while in text-to-knowledge-graph, the task is re-framed into a sequence-to-sequence problem, often addressed with large autoregressive models. Finally, for road map extraction from satellite images, one can leverage the spatial positions of the nodes to define a unique ordering [5]. ", "page_idx": 1}, {"type": "text", "text": "Another line of research proposes to address this problem more directly by seeking to solve a graph-matching problem, i.e., finding the one-to-one correspondence between nodes of the graphs. Among approaches in this category, we note methods dedicated to molecule generation [25] where the invariant loss is based on a characterization of graphs, ad-hoc to the molecule application. While being fully differentiable their loss does not generalize to other applications. In the similar topic of graph generation, Simonovsky and Komodakis [37] propose a more generic definition of the similarity between graphs by considering both feature and structural matching. However, they solve the problem using a two-step approach by using first a smooth matching approximation followed by a rounding step using the Hungarian algorithm to obtain a proper one-to-one matching, which comes with a high computational cost and introduces a non-differentiable step. For graph scene generation, Relationformer [36] is based on a bipartite object matching approach solved using a Hungarian matcher [11]. The main shortcoming of this approach is that it fails to consider structural information in the matching process. The same problem is encountered by Melnyk et al. [28]. We discuss Relationformer in more detail later in the article. ", "page_idx": 1}, {"type": "text", "text": "Finally, another way to approach end-to-end learning is to leverage the notion of graph barycenter to define the predicted graph. Relying on the Implicit Loss Embedding (ILE) property of surrogate regression, Brogat-Motte et al. [9] have exemplified this idea by exploiting an Optimal Transport loss, the Fused Gromov-Wasserstein (FGW) distance [45] for which barycenters can be computed efficiently [32, 44]. They proposed two variants, a non-parametric kernel-based one and a neural network-based one, referred to as FGW-Bary and FGW-BaryNN, respectively. However, to calculate the barycenter, the size must be known upstream, leaving the challenge of arbitrary size unresolved. In addition, prediction accuracy is highly dependent on the expressiveness of the barycenter, i.e. the nature and number of graph templates, resulting in high training and inference costs. ", "page_idx": 1}, {"type": "text", "text": "In contrast to existing works, our goal is to address the problem of supervised graph prediction in an end-to-end fashion, for different types of input modalities and for output graphs whose size and node ordering can be arbitrary. ", "page_idx": 1}, {"type": "text", "text": "Main contributions This paper presents Any2Graph, a versatile framework for end-to-end SGP. Any2Graph leverages a novel, fully differentiable, OT-based loss that satisfies all the previously mentioned properties, i.e., size agnostic and invariant to node permutation. In addition, the encoder part of Any2Graph allows us to leverage inputs of various types, such as images or sets of tokens. We complete our framework with a novel challenging synthetic dataset which we demonstrate to be suited for benchmarking SGP models. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. After a reminder and a discussion about the relation between graph matching and optimal transport (Section 2), we introduce in Section 3, a size-agnostic graph representation and an associated differentiable and node permutation invariant loss. This loss, denoted as Partially Masked Fused Gromov Wasserstein (PMFGW) is a novel and necessary adaptation of the FGW distance [45]. This loss is then integrated into Any2Graph, an end-to-end learning framework depicted in Figure 1 and presented in Section 4. We express the whole framework objective as an ERM problem and highlight the adaptations necessary for extending existing deep learning architectures [36] to more general input modalities. ", "page_idx": 1}, {"type": "text", "text": "Section 5, presents a thorough empirical study of Any2Graph on various datasets. We evaluate our approach on four real-world problems with different input modalities as well as Coloring, a novel synthetic dataset. As none of the existing approaches could cover the range of input modalities, nor scale to very large-sized datasets, we adapted them for the purpose of fair comparison. The numerical results showcase the state-of-the-art performances of the proposed method in terms of prediction accuracy and ability to retrieve the right size of target graphs as well as computational efficiency. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background on graph matching and optimal transport ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Graph representation and notations An attributed graph $g$ with $m$ nodes can be represented by a tuple $(\\mathbf{F},\\mathbf{A})$ where $\\mathbf{F}=[\\mathbf{f}_{1},\\ldots,\\mathbf{f}_{m}]^{\\top}\\in\\mathbb{R}^{m\\times d}$ encodes node features with $\\mathbf{f}_{i}\\in\\mathbb{R}^{d}$ labeling each node indexed by $i$ , $\\mathbf{A}\\in\\mathbb{R}^{m\\times m}$ is a symmetric pairwise distance matrix that describes the graph relationships between the nodes such as the adjacency matrix or the shortest path matrix. Further, we denote $\\mathcal{G}_{m}$ the set of attributed graphs of $m$ nodes and $\\begin{array}{r}{\\mathcal{G}=\\bigcup_{m=1}^{M}\\mathcal{G}_{m}}\\end{array}$ , the set of attributed graphs of size up to $M$ , where the size refers to the number of nodes in a graph and the largest size $M$ is an important hyperparameter. In the following, $\\mathbf{1}_{m}\\in\\mathbb{R}^{m}$ is the all one vector and we denote $\\sigma_{m}=\\{\\mathbf{\\hat{P}}\\in\\{0,\\bar{1}\\}^{\\mathbf{\\hat{\\scriptstyle{m}}}\\times\\mathbf{\\hat{\\boldsymbol{m}}}}\\mid\\mathbf{P1}_{m}=\\mathbf{1}_{m},\\mathbf{P}^{T}\\mathbf{1}_{m}=\\mathbf{\\check{1}}_{m}\\}$ the set of permutation matrices. ", "page_idx": 2}, {"type": "text", "text": "Graph Isomorphism Two graphs $g_{1}=(\\mathbf{F}_{1},\\mathbf{A}_{1}),g_{2}=(\\mathbf{F}_{2},\\mathbf{A}_{2})\\in\\mathcal{G}_{m}$ are said to be isomorphic whenever there exists $\\textbf{P}\\in\\;\\sigma_{m}$ such that $({\\bf F}_{1},{\\bf A}_{1})\\,=\\,({\\bf P}{\\bf F}_{2},{\\bf P}{\\bf A}_{2}{\\bf P}^{T})$ , in which case we denote $g_{1}\\sim g_{2}$ . In this work, we consider all graphs to be unordered, meaning that all operations should be invariant by Graph Isomorphism (GI). ", "page_idx": 2}, {"type": "text", "text": "Comparing graphs of the same size Designing a discrepancy to compare graphs is challenging, for instance, even for two graphs of the same size $\\hat{g}\\,=\\,(\\bar{\\hat{\\mathbf{F}}},\\hat{\\mathbf{A}})$ , $g\\,=\\,({\\bf F},{\\bf A})$ , one cannot simply compute a point-wise comparison as it would not satisfy GI invariance. A solution is to solve a Graph Matching (GM) problem, i.e., to find the optimal matching between the graphs and compute the pairwise errors between matched nodes and edges. This problem can be written as the following ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{G}\\mathbf{M}(\\hat{g},g)=\\operatorname*{min}_{\\mathbf{P}\\in\\sigma_{m}}\\sum_{i,j=1}^{m}\\mathbf{P}_{i,j}\\ell_{F}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})+\\sum_{i,j,k,l=1}^{m}\\mathbf{P}_{i,j}\\mathbf{P}_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In particular, with the proper choice of ground metrics $\\ell_{f}$ and $\\ell_{A}$ , this is equivalent to the popular Graph Edit Distance (GED) [33]. The minimization problem however is a Quadratic Assignment Problem (QAP) which is known to be one of the most difficult problems in the NP-Hard class [27]. To mitigate this computational complexity, Aflalo et al. [2] suggested to replace the space of permutation matrices with a convex relaxation. The Birkhoff polytope (doubly stochastic matrices) $\\dot{\\boldsymbol{\\pi}}_{m}=\\{\\mathbf{T}\\in[0,1]^{m\\times m}\\ |\\ \\mathbf{T}\\mathbb{1}_{m}=\\mathbb{1}_{m},\\mathbf{T}^{T}\\mathbb{1}_{m}=\\mathbb{1}_{m}\\}$ is the tightest of those relaxations as it is exactly the convex hull of $\\sigma_{m}$ which makes it a suitable choice [21]. Interestingly, the resulting metric is known in OT [46] field as a special case of the (Fused) Gromov-Wasserstein (FGW) distance proposed by [29]. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{FGW}(\\hat{g},g)=\\operatorname*{min}_{\\mathbf{T}\\in\\pi_{m}}\\sum_{i,j=1}^{m}\\mathbf{T}_{i,j}\\ell_{F}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})+\\sum_{i,j,k,l=1}^{m}\\mathbf{T}_{i,j}\\mathbf{T}_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, these two points of view differ in their interpretation of the FGW metric. From the GM perspective, FGW is cast as an approximation of the original problem, and the optimal transport plan is typically projected back to the space of permutation via Hungarian Matching [47]. From the OT perspective, FGW is used as a metric between distributions with interesting topological properties [44]. This raises the question of the tightness of the relaxation between GM and FGW. In the linear case, i.e., when $\\ell_{A}=0$ , the relaxation is tight and this phenomenon is known in the OT literature as the equivalence between Monge and Kantorovitch formulation [31]. The quadratic case, however, is much more complex, and sufficient conditions under which the tightness holds have been studied in both fields [1, 35]. ", "page_idx": 2}, {"type": "text", "text": "As seen above, both OT and GM perspectives offer ways to characterize the same objects. In the remainder of this paper, we adopt the OT terminology, e.g., we use the term transport plan in place of doubly stochastic matrix. We provide a quantitative analysis of the effect of the relaxation in F.2. ", "page_idx": 2}, {"type": "text", "text": "Numerical solver Computing the FGW distance requires solving the optimization problem presented in Equation (2) whose objective rewrites $\\langle\\mathbf{T},\\mathbf{U}\\rangle+\\langle\\mathbf{T},\\mathbf{L}\\otimes\\mathbf{T}\\rangle$ where $\\mathbf{U}$ is a fixed matrix, L a fixed tensor and $\\otimes$ the tensor matrix product. A standard way of solving this problem [47] is to use a conditional gradient (CG) algorithm which iteratively solves a linearization of the problem. Each step of the algorithm requires solving a linear OT/Matching problem of cost $\\langle\\mathbf{T},\\mathbf{C}^{(k)}\\rangle$ where the linear cost $\\mathbf{C}^{(k)}=\\mathbf{U}+\\mathbf{L}\\otimes\\mathbf{T}^{(k)}$ is updated at each iteration. The linear problem can be solved with a Hungarian solver with cost $\\mathcal{O}(\\dot{M}^{3})$ while the overall complexity of computing the tensor product $\\mathbf{L}\\otimes\\mathbf{T}^{(k)}$ is theoretically $\\mathcal{O}(M^{4})$ . Fortunately, this bottleneck can be avoided thanks to a $\\mathcal{O}(M^{3})$ factorization proposed originally by Peyr\u00e9 et al. [32]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Comparing graphs of arbitrary size The metrics defined above cannot directly be used to compare graphs of different sizes. To overcome this problem, Vayer et al. [45] proposed a more general formulation that fully leverages OT to model weights on graph nodes and can be used to compare graphs of different sizes as long as they have the same total mass. However, this approach raises specific issues. In scenarios where masses are uniform, nodes in larger graphs receive lower mass which might not be suitable for practical applications. Conversely, employing non-uniform masses complicates interpretation, as decoding a discrete object from a weighted one becomes less straightforward. Those issues can be mitigated by leveraging Unbalanced Optimal Transport (UOT) [40], which relaxes marginal constraints, allowing for different total masses in the graphs. Unfortunately, UOT introduces several additional regularization parameters that are difficult to tune, especially in scenarios like SGP, where model predictions exhibit wide variability during training. ", "page_idx": 3}, {"type": "text", "text": "Another close line of work is Partial Matching (PM) [13], which consists in matching a small graph $g$ to a subgraph of the larger graph $\\hat{g}$ . In practice, this can be done by adding dummy nodes to $g$ through some padding operator $\\mathcal{P}$ after which one can directly compute $\\mathbf{PM}(\\bar{\\hat{g}},g)=\\bar{\\mathbf{G}}\\mathbf{M}(\\hat{g},\\mathcal{P}(g))$ [21]. However, PM is not suited to train a model as the learned model would only be able to predict a graph that includes the target graph. Partial Matching and its relationship with our proposed loss is discussed in more detail in Appendix B.3. ", "page_idx": 3}, {"type": "text", "text": "3 Optimal Transport loss for Supervised Graph Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A size-agnostic representation for graphs Our first step toward building an end-to-end SGP framework is to introduce a space $\\hat{\\mathcal{V}}$ to represent any graph of size up to $M$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{Y}}=\\{y=(\\mathbf{h},\\mathbf{F},\\mathbf{A})\\mid\\mathbf{h}\\in[0,1]^{M},\\mathbf{F}\\in\\mathbb{R}^{M\\times d},\\mathbf{A}\\in[0,1]^{M\\times M}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We refer to the elements of $\\hat{y}$ as continuous graphs, in opposition with discrete graphs of $\\mathcal{G}$ . Here $h_{i}$ (resp. $A_{i,j}$ ) should be interpreted as the probability of the existence of node $i$ (resp. edge $[i,j]$ ). Any graph of $\\mathcal{G}$ can be embedded into $\\hat{y}$ with a padding operator $\\mathcal{P}$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}(g)=\\left(\\left(\\!\\!\\begin{array}{l}{\\mathbf{1}_{m}}\\\\ {\\mathbf{0}_{M-m}}\\end{array}\\!\\!\\right),\\left(\\!\\!\\begin{array}{c c}{\\mathbf{F}_{m}}\\\\ {\\mathbf{0}_{M-m}}\\end{array}\\!\\!\\right),\\left(\\!\\!\\begin{array}{c c}{\\mathbf{A}_{m}}&{\\mathbf{0}_{m,M-m}}\\\\ {\\mathbf{0}_{M-m,m}^{T}}&{\\mathbf{0}_{M-m,M-m}}\\end{array}\\!\\!\\right)\\!\\right),\\;\\mathrm{for}\\;g=(\\mathbf{F}_{m},\\mathbf{A}_{m})\\in\\mathcal{G}_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We denote $\\mathcal{V}=\\mathcal{P}(\\mathcal{G})\\subset\\hat{\\mathcal{V}}$ the space of padded graphs. For any padded graph in $\\boldsymbol{\\wp}$ , the padding operator can be inverted to recover a discrete graph ${\\dot{\\mathcal{P}}}^{-1}:{\\mathcal{V}}\\mapsto{\\dot{\\mathcal{G}}}$ . Besides, any continuous graph $\\bar{y}\\in\\hat{\\mathcal{V}}$ can be projected back to padded graphs $\\boldsymbol{\\wp}$ by a threshold operator $\\mathcal{T}:\\hat{\\mathcal{y}}\\mapsto\\mathcal{y}$ . Note that $\\hat{\\mathcal{V}}$ is convex and of fixed dimension which makes it ideal for parametrization with a neural network. Hence, the core idea of our work is to use a neural network to make a prediction $\\hat{y}\\,\\in\\,\\hat{y}$ and to compare it to a target $g\\in{\\mathcal{G}}$ through some loss $\\ell(\\hat{y},\\mathcal{P}(g))$ . This calls for the design of an asymmetric loss $\\ell:\\hat{\\mathcal{Y}}\\times\\mathcal{Y}\\mapsto\\mathbb{R}_{+}$ . ", "page_idx": 3}, {"type": "text", "text": "An Asymmetric loss for SGP The Partially Masked Fused Gromov Wasserstein (PMFGW) is a loss between a padded target graph $\\mathcal{P}(g)=\\mathbf{\\dot{\\Phi}}(\\mathbf{h},\\mathbf{F},\\mathbf{A})\\in\\mathcal{P}$ with real size $m=\\|\\mathbf{h}\\|_{1}\\leq M$ and a continuous prediction $\\hat{y}=(\\hat{\\mathbf{h}},\\hat{\\mathbf{F}},\\hat{\\mathbf{A}})\\in\\hat{\\mathcal{Y}}$ . We define PMFGW $(\\hat{y},\\mathcal{P}(g))$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{T}\\in\\pi_{M}}\\ \\frac{\\alpha_{\\mathrm{h}}}{M}\\sum_{i,j}T_{i,j}\\ell_{h}(\\hat{h}_{i},h_{j})+\\frac{\\alpha_{\\mathrm{f}}}{m}\\sum_{i,j}T_{i,j}\\ell_{f}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})h_{j}+\\frac{\\alpha_{\\mathrm{A}}}{m^{2}}\\sum_{i,j,k,l}T_{i,j}T_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l})h_{j}h_{l}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let us decompose this loss function to understand the extend to which it simultaneously takes into account each property of the graph. The first term ensures that the padding of a node is well predicted. In particular, this requires the model to predict correctly the number of nodes in the target graph. The second term ensures that the features of all non-padding nodes $\\left[h_{i}=1\\right]$ ) are well predicted. Similarly, the last term ensures that the pairwise relationships between non-padded nodes $(h_{i}\\,=\\,h_{j}\\,=\\,1)$ are well predicted. The normalizations in front of the sums ensure that each term is a weighted average of its internal losses as $\\sum T_{i,j}\\,=\\,M$ , $\\sum T_{i,j}h_{j}=m$ and $\\sum T_{i,j}T_{k,l}h_{j}h_{l}=m^{2}$ . Finally $\\alpha=[\\alpha_{\\mathrm{h}},\\alpha_{\\mathrm{f}},\\alpha_{\\mathrm{A}}]\\in\\Delta_{3}$ is a tripl et of hyperpar ameters on the simp lex balancing the relative scale of the different terms. For $\\ell_{A}$ and $\\ell_{h}$ we use the cross-entropy between the predicted value after a sigmoid and the actual binary value in the target. This is equivalent to a logistic regression loss after the OT plan has matched the nodes. For $\\ell_{f}$ we use the squared $\\ell_{2}$ or the cross-entropy loss when the node features are continuous or discrete, respectively. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "A key feature of this loss is its flexibility. Not only other ground losses can be considered but it is also straightforward to introduce richer spectral representations of the graph [4]. For instance, in Section 5, we explore the benefits of leveraging a diffused version of the nodes features. ", "page_idx": 4}, {"type": "text", "text": "Finally, PMFGW translates all the good properties of FGW to the new size-agnostic representation.   \nProposition 1 (Complexity). The objective of the inner optimization can be evaluated in $\\mathcal{O}(M^{3})$ .   \nProposition 2 (GI Invariance). If $\\hat{y}\\sim\\hat{y}^{\\prime}$ and $g\\sim g^{\\prime}$ then PMFGW $^{\\prime}(\\hat{y},\\mathcal{P}(g))=P M F G W(\\hat{y}^{\\prime},\\mathcal{P}(g^{\\prime}))$ .   \nProposition 3 (Positivity). PMFGW $(\\hat{y},\\mathcal{P}(g))\\ge0$ with equality if and only if $\\hat{y}\\sim\\mathcal{P}(g)$ . ", "page_idx": 4}, {"type": "text", "text": "See Appendix A for a toy example illustrating the behavior of the loss and Appendix B.1 and B.2 for formal statements and proofs of Proposition 1, 2 and 3. ", "page_idx": 4}, {"type": "text", "text": "Relation to existing metrics PMFGW is an asymmetric extension of FGW [44] suited for comparing a continuous predicted graph with a padded target. The extension is achieved by adding (1) a novel term to quantify the prediction of node padding, and (2) the partial masking of the components of the second and third terms to reflect padding. It should be noted that in contrast to what is usually done in OT, the node masking vectors $\\mathbf{h}$ and $\\mathbf{\\bar{h}}$ ) are not used as a marginal distribution but directly integrated into the loss. In that sense, the additional node masking term is very similar to the one of $\\mathrm{OTL}_{p}$ [39] that proposed to use uniform marginal weight and move the part that measures the similarity between the distribution weights in an additional linear term. However, $\\mathrm{OTL}_{p}$ is restricted to linear OT problems and does not use the marginal distributions as a masking for other terms as in PMFGW. ", "page_idx": 4}, {"type": "text", "text": "PMFGW also relates to Partial GM/GW Chapel et al. [13] as both metrics compare graphs by padding the smallest one with zero-cost dummy nodes. The critical difference lies in the new vector $\\bar{\\hat{\\mathbf{h}}}$ which predicts which sub-graphs are activated, i.e., should be matched to the target. The exact relationship between Partial Fused Gromov Wasserstein (PFGW) and PMFGW is summarized below ", "page_idx": 4}, {"type": "text", "text": "Proposition 4. If $l_{h}$ is set to a constant value, PMFGW is equal to PFGW (up to a constant). ", "page_idx": 4}, {"type": "text", "text": "Remark 1. In that case, the vector $\\hat{\\mathbf{h}}$ disappears from the loss and cannot be trained. In particular, this would prevent the model from learning to predict the size of the target graph. ", "page_idx": 4}, {"type": "text", "text": "The formal definition of PFGW and the proof of Proposition 4 are provided in Appendix B.3. ", "page_idx": 4}, {"type": "text", "text": "4 Any2Graph: a framework for end-to-end SGP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Any2Graph problem formulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The goal of Supervised Graph Prediction (SGP) is to learn a function $f:\\mathcal{X}\\to\\mathcal{G}$ using the training samples $\\{(x_{i},\\bar{g}_{i})\\}_{i=1}^{n}\\in(\\bar{\\mathcal{X}}\\times\\mathcal{G})^{n}$ . In Any2Graph, we relax the output space and learn a function ${\\hat{f}}:{\\mathcal{X}}\\to{\\hat{\\mathcal{Y}}}$ that predicts a continuous graph ${\\hat{y}}:=f(x)$ as defined in the previous section. Assuming $\\hat{f}$ is a parametric model (in this work, a deep neural network) completely determined by a parameter $\\theta$ , the Any2Graph objective writes as the following empirical risk minimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\quad\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{PMFGW}(\\hat{f}_{\\theta}(x_{i}),\\mathcal{P}(g_{i})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "At inference time, we recover a discrete prediction by a straightforward decoding $f(x)=\\mathcal{P}^{-1}\\circ\\mathcal{T}(\\hat{y})$ , where $\\tau$ is the thresholding operator with threshold $^1\\!/\\!2$ on the edges and nodes and $\\mathcal{P}^{-1}$ is the inverse ", "page_idx": 4}, {"type": "image", "img_path": "tPgagXpvcV/tmp/e98480cc80b505db929492f9c10a29ebdecb16f6fdab637a633f78933054ee36.jpg", "img_caption": ["Figure 1: Illustration of the architecture for a target graph of size 3 and $M=4$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "padding defined in the previous section. In other words, the full decoding pipeline $\\mathcal{P}^{-1}\\circ\\mathcal{T}$ removes the nodes $i$ (resp. edges $(i,j))$ whose predicted probability is smaller than $1/2$ i.e. $\\hat{h}_{i}<\\!\\,\\!\\eta_{2}$ (resp. $\\hat{A}_{i,j}<\\!\\!\\sqrt{2})\\!\\!\\}$ ). Unlike surrogate regression methods, this decoding step is very efficient. ", "page_idx": 5}, {"type": "text", "text": "4.2 Neural network architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The model $\\hat{f}_{\\theta}:\\mathcal{X}\\to\\hat{\\mathcal{Y}}$ (left part of Figure 1) is composed of three modules , namely the encoder that extracts features from the input, the transformer that convert these features into $M$ nodes embeddings, that are expected to capture both feature and structure information, and the graph decoder that predicts the properties of our output graph, i.e., $(\\hat{\\bf h},\\hat{\\bf F},\\hat{\\bf A})$ . As we will discuss later, the proposed architecture draws heavily on that of Relationformer [36] since the latter has been shown to yield to state-of-the-art results on the Image2Graph task. ", "page_idx": 5}, {"type": "text", "text": "Encoder The encoder extracts $k$ feature vectors in $\\mathbb{R}^{d_{e}}$ from the input. Note that $k$ is not fixed a priori and can depend on the input (for instance sequence length in case of text input). This is critical for encoding structures as complex as graphs and the subsequent transformer is particularly apt at treating this kind of representation. By properly designing the encoder, we can accommodate different types of input data. In Appendix C, we describe how to handle images, text, graphs, and vectors and provide general guidelines to address other input modalities. ", "page_idx": 5}, {"type": "text", "text": "Transformer This module takes as input a set of feature vectors and outputs a fixed number of $M$ node embeddings. This resembles the approach taken in machine translation, and we used an architecture based on a stack of transformer encoder-decoders, akin to Shit et al. [36]. ", "page_idx": 5}, {"type": "text", "text": "Graph decoder This module decodes a graph from the set of node embeddings $\\mathbf{Z}=[\\mathbf{z}_{1},\\hdots,\\mathbf{z}_{M}]^{T}$ using the following equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{h}_{i}=\\sigma(\\mathrm{MLP}_{m}(\\mathbf{z}_{i})),\\quad\\hat{F}_{i}=\\mathrm{MLP}_{f}(\\mathbf{z}_{i}),\\quad\\hat{A}_{i,j}=\\sigma(\\mathrm{MLP}_{s}^{2}(\\mathrm{MLP}_{s}^{1}(\\mathbf{z}_{i})+\\mathrm{MLP}_{s}^{1}(\\mathbf{z}_{j})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\sigma$ is the sigmoid function and $\\mathrm{MLP}_{m}$ , $\\mathrm{MLP}_{f}$ , $\\mathrm{MLP}_{s}^{k}$ are multi-layer perceptrons heads corresponding to each component of the graph (mask, features, structure). The adjacency matrix is expected to be symmetric which motivate us to parameterize it as suggested by Zaheer et al. [56]. ", "page_idx": 5}, {"type": "text", "text": "Positioning with Relationformer As discussed above, the architecture is similar to the one proposed in Relationformer [36], with two modifications: (1) we use a symmetric operation with a sum to compute the adjacency matrix while Relationformer uses a concatenation that is not symmetric; (2) we investigate more general encoders to enable graph prediction from data other than images. However, as stated in the previous section, the main originality of our framework lies in the design of the PMFGW loss. Interestingly Relationformer uses a loss that presents similarities with FGW but where the matching is done on the node features only, before computing a quadratic-linear loss similar to PMFGW. In other words, they solve a bi-level optimization problem, where the plan is computed on only part of the information, leading to potentially suboptimal results on heterophilic graphs as demonstrated in the next section. ", "page_idx": 5}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section is organized as follows. First, we describe the experimental setting (5.1) including baselines. Next, we showcase the state-of-the-art performances of Any2Graph for a wide range of metrics and datasets (5.2). Finally, we provide an empirical analysis of the key hyperparameters of Any2Graph (5.3). The code for Any2Graph and all the experiments will be released on GitHub. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets We consider 5 datasets thus covering a wide spectrum of different input modalities, graph types, and sizes. The first one, Coloring, is a new synthetic dataset that we proposed, inspired by the four-color theorem. The input is a noisy image partitioned into regions of colors and the goal is to predict the graph representing the regions as nodes (4 color classes) and their connectivity in the image. An example is provided in Figure 5 and more details are in Appendix D. Then, we consider four real-world benchmarks. Toulouse [5] is Sat2Graph datasets where the goal is to extract the road network from binarized satellite images of a city. USCities is also a Sat2Graph dataset but features larger and more convoluted graphs. Note that we leave aside the more complex RGB version of USCities as it was shown to require complex multi-level attention architecture [36], which is beyond the scope of this paper. Finally, following Ucak et al. [42], we address the Fingerprint2Graph task where the goal is to reconstruct a molecule from its fingerprint representation (list of tokens). We consider two widely different datasets for this tasks: QM9 [50], a scarce dataset of tiny molecules (up to 9 nodes) and GBD13Blum and Reymond [7], a large dataset 2featuring molecules with up to 13 heavy atoms. Additional details concerning the datasets (e.g. dataset size, number of edges, number of nodes) are provided in Appendix E.1. ", "page_idx": 6}, {"type": "text", "text": "Compared methods We compare Any2Graph, to our direct end-to-end competitor Relationformer [36] that has shown to be the state-of-the-art method for Image2Graph. For a fair comparison, we use the same architecture (presented in Figure 1) for both approaches so that the only difference is the loss. We conjecture that Any2Graph and Relationformer might benefit from feature diffusion, that is replacing the node feature vector $\\mathbf{F}$ by the concatenation $[\\mathbf{\\bar{F}},\\mathbf{AF}]$ before training. We denote by $\\mathbf{\\dot{+}}\\mathbf{F}\\mathbf{D}^{\\circ}$ the addition of feature diffusion before training. Moreover, we also compare with a surrogate regression approach (FGW-Bary) based on FGW barycenters [9]. We test both the end-to-end parametric variant, FGWBary-NN, where weight functions $\\alpha_{k}$ , as well as $K=10$ templates, are learned by a neural network and the non-parametric variant, FGWBary-ILE, where the templates are training samples and $\\alpha_{k}$ are learned by sketched kernel ridge regression [54, 18] with gaussian kernel. Both have been implemented using the codes provided by Brogat-Motte et al. [9], modified to incorporate sketching. Hyperparameters regarding architectures and optimization are provided in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "Performance metrics The heterogeneity of our datasets, calls for task-agnostic metrics focusing on different fine-grained levels of the graph. At the graph level, we report the PMFGW loss between continuous prediction $\\hat{y}$ and padded target $\\mathcal{P}(g)$ and the graph edit distance Gao et al. [19] between predicted graph $\\mathcal{P}^{-1}\\bar{\\mathcal{T}}\\hat{y}$ and target $g$ . We also report the Graph Isomorphism Accuracy (GI ACC), a metric that computes the proportion of perfectly predicted graphs. At the edge level, we treat the adjacency matrix prediction as a binary prediction problem and report the Precision and Recall. Finally, at the node level, we report NODE, the fraction of well-predicted node features, and SIZE a metric reporting the accuracy of the predicted number of nodes. See Appendix E.3 for more details. ", "page_idx": 6}, {"type": "text", "text": "5.2 Comparison with existing SGP methods on diverse modalities ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Prediction Performances Table 1 shows the performances of the different methods on the five datasets. First, we observe that Any2Graph achieves state-of-the-art performances for all datasets and graph level metrics. On the Sat2Graph tasks (Toulouse and USCities) we note that Relationformer performs very close to Any2Graph. In fact, the features (2D positions) are enough to uniquely identify the nodes, making Relationformer\u2019s Hungarian matching sufficient. Moreover, both methods highly benefit from feature diffusion on Fingerprint2Graph tasks which we discuss further in Appendix F. Note that both barycenter methods struggle on Toulouse, possibly due to a lack of expressivity. On ", "page_idx": 6}, {"type": "text", "text": "Table 1: Graph level, edge level, and node level metrics reported on test for the different models and datasets. \u2217denotes methods that use the actual size of the graph at inference time, hence the performance reported is a non-realistic upper bound. We where not able to train FGWBary on all dataset due to the prohibitive cost of barycenter computations. N.A. stands for not applicable. ", "page_idx": 7}, {"type": "table", "img_path": "tPgagXpvcV/tmp/09aa09b6592d3f699b44c8b77ed5c1a50bbd2343d533e1b09c92908775118404.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "QM9 on the other hand, FGW-Bary-ILE performs close to Any2Graph but is still outperformed, even more so if we add feature diffusion. It should be stressed that FGW-Bary-ILE is placed here under the most favorable conditions possible, in particular with the use of a SOTA kernel and being given the ground truth number of nodes. Finally, on GDB13, the most challenging dataset, Any2Graph strongly outperforms all competitors. To summarize this part, we observe that Any2Graph and its variant Any2Graph $\\mathbf{+FD}$ are consistently better by a large margin on three out of five benchmarks and tied for first place with RelationFormer on the remaining ones. ", "page_idx": 7}, {"type": "text", "text": "Computational Performances Table 2 shows the cost of training and inference of all methods, expressed as the number of graphs processed per second. All values are computed on NVIDIA V100/Intel Xeon E5-2660. The heavy cost of barycenters computation in FGWBary makes it several orders of magnitude slower than Any2Graph. We note that Relationformer is faster at learning time because it avoids the need to solve a QAP. Overall, our proposed approach offers the best of both worlds, achieving SOTA prediction performance at all levels of the graph, at a very low computational inference cost. ", "page_idx": 7}, {"type": "table", "img_path": "tPgagXpvcV/tmp/f6bbdb4091e98f3df7efc643ca272b67ff7f4eed02005d405fa99dc60b30ade5.jpg", "table_caption": ["Table 2: Computational speed for the different methods in terms of graphs per second on $\\it Q M9$ . Training performance is not provided for FGWBary-ILE as the closed-form expression is computed at once on CPU. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Qualitative Performances We display a sample of the graph predictions performed by the models in Figure 5 (left) along with a larger collection in appendix G. We observe that not only Any2Graph can adapt to different input modalities, but also to the variety of target graphs at hand. For instance, Coloring graphs tend to be denser, while Sat2Graph maps can contain multiple connected components and Fingerprint2Graph molecules exhibit unique substructures such as loops. ", "page_idx": 7}, {"type": "text", "text": "5.3 Empirical study of Any2Graph properties ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Sensitivity to dataset size In figure 5, we provide the test edit distance for different numbers of training samples in the explored datasets. Interestingly, we observe that a performance plateau is reached for all datasets. We also observe that Coloring/Toulouse are simpler than USCities/QM9 (in terms of best edit distance) and illustrate the flexibility of Coloring by bridging this complexity gap with the more challenging variation described in D. ", "page_idx": 7}, {"type": "image", "img_path": "tPgagXpvcV/tmp/116e0f8f62852ef0da8776dba3c6331b6a2c584694acc59ce1177fbb34c9cbe6.jpg", "img_caption": ["Figure 5: (Left): a sample of predictions made by Any2Graph and Relationformer for a given input and ground truth target. Many more are provided in G. (Right): we truncate the train datasets to provide an overview of Any2Graph training curves (test performances against train set size). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Scalability to larger graphs As stated in property 1, each iteration of the PMFGW solver scales with $\\mathcal{O}(M^{3})$ . Denoting $k(M)$ the average number of iterations required for convergence, this means that the actual cost of computing the loss scales with $\\mathcal{O}\\left(k(M)M^{3}\\right)$ . We provide an empirical estimation of $k(M)$ in figure 2 which we obtain by computing $\\mathrm{PMFGW}(\\bar{\\mathcal{P}}(g_{1}),\\mathcal{P}(g_{2}))$ for pairs of graphs $g_{1},g_{2}$ sampled from the Coloring dataset. We observe that $k(M)$ seems linear but can be made sub-linear using feature diffusion (FD). Still, the cubic cost prevents Any2Graph from scaling beyond a few tens of nodes. ", "page_idx": 8}, {"type": "image", "img_path": "tPgagXpvcV/tmp/56cb5de966eea559b2c703639f753ddb7a8dc7192b5ad2fb36be63209be0f028.jpg", "img_caption": ["Figure 2: Average number of solver iterations required for computing PMFGW loss. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Choice of maximal graph size $M$ The default value of $M$ is the size of the largest graph in the train set. We explore whether or not overparametrizing the model with higher values bring substantial benefits. To this end, we train our model on Coloring for $M$ between 10 (default value) and 25 and report the (test) edit distance. To quantify the effective number of nodes used by the model, we also record the number of active nodes, i.e., that are masked less than $99\\%$ of the time (see Figure 3). Interestingly, we observe that performances are robust w.r.t. the choice of $M$ which can be explained by the number of useful nodes reaching a plateau. This suggests the model automatically learns the number of nodes it needs to achieve the required expressiveness. ", "page_idx": 8}, {"type": "image", "img_path": "tPgagXpvcV/tmp/a3819d3b9d7fe18a8579e2110ca3f7d271488d6921711b22e2f2c9cfed9f54fd.jpg", "img_caption": ["Figure 3: Effect of $M$ on test edit distance and number of active nodes for Coloring. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Sensitivity to the weights $_{\\alpha}$ We investigate the sensitivity of the proposed loss to the triplet of weight hyperparameters $_{\\alpha}$ . To this end, we train our model on Coloring for different values $_{\\alpha}$ on the simplex and report the (test) edit distance on Figure 4. We observe that the performance is optimal for uniform $_{\\alpha}$ and robust to other choices as long as there is not too much weight on the structure loss term (corner $\\alpha_{\\mathrm{{A}}}~=~1)$ ). Indeed, the quadratic term of the loss being the hardest, putting too much emphasis on it might slow down the training. This explains the efficiency of feature diffusion, as it moves parts of the structure prediction to the linear term. Further evidence backing this intuitive explanation is provided in F.1. ", "page_idx": 8}, {"type": "image", "img_path": "tPgagXpvcV/tmp/bae4dc84ad789193373557ba1109510f51f1a3e8ad960e2da06638382c49795c.jpg", "img_caption": ["Figure 4: Effect of $_{\\alpha}$ on the test edit distance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present Any2Graph, a novel deep learning approach to Supervised Graph Prediction (SGP) leveraging an original asymmetric Partially-Masked Fused Gromov-Wasserstein loss. To the best of our knowledge, Any2Graph stands as the first end-to-end and versatile framework to consistently achieve state-of-the-art performances across a wide range of graph prediction tasks and input modalities. Notably, we obtain excellent results in both accuracy and computational efficiency. Finally, we illustrate the adaptability of the proposed loss (using diffused features), and the robustness of the method to sensitive parameters such as maximum graph size and weights of the different terms in PMFGW. ", "page_idx": 9}, {"type": "text", "text": "The main limitation of Any2Graph is its scalability to graphs of larger size. Considering the tasks at hand, in this paper, we limit ourselves to relatively small graphs of up to 20 nodes. ", "page_idx": 9}, {"type": "text", "text": "For the future, we envision two working directions to address this issue. First, given the promising results with feature diffusion, we plan to introduce more tools from spectral graph theory [14, 20, 4] in Any2Graph, e.g., using diffusion on the adjacency matrix to capture higher-order interactions that may occur in large graphs. More generally, this extension could be useful even for smaller graphs. Secondly, we expect that the solver computing the optimal transport plan can be accelerated using approximation. In particular, the entropic regularization [32] might unlock the possibility of fully parallelizing the optimization on a GPU while low-rank OT solvers Scetbon et al. [34] could allow Any2Graph to scale to large output graphs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thanks Alexis Thual and Quang Huy Tran for providing their insights and code about the Fused Unbalanced Gromov Wasserstein metric. This work received funding from the European Union\u2019s Horizon Europe research and innovation programme under grant agreement 101120237 (ELIAS). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the granting authority can be held responsible for them. This research was also supported in part by the French National Research Agency (ANR) through the PEPR IA FOUNDRY project (ANR-23-PEIA0003) and the MATTER project (ANR-23-ERCC-0006-01). The first and second authors respectively received PhD scholarships from Institut Polytechnique de Paris and Hi!Paris. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Aflalo, Y., Bronstein, A., and Kimmel, R. (2014). Graph matching: relax or not? arXiv preprint arXiv:1401.7623.   \n[2] Aflalo, Y., Bronstein, A., and Kimmel, R. (2015). On convex relaxation of graph isomorphism. Proceedings of the National Academy of Sciences, 112(10):2942\u20132947.   \n[3] Babu, A., Shrivastava, A., Aghajanyan, A., Aly, A., Fan, A., and Ghazvininejad, M. (2021). Non-autoregressive semantic parsing for compositional task-oriented dialog. arXiv preprint arXiv:2104.04923.   \n[4] Barbe, A., Sebban, M., Gon\u00e7alves, P., Borgnat, P., and Gribonval, R. (2020). Graph diffusion wasserstein distances. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 577\u2013592. Springer.   \n[5] Belli, D. and Kipf, T. (2019). Image-conditioned graph generation for road network extraction. arXiv preprint arXiv:1910.14388.   \n[6] Birkhoff, G. (1946). Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A, 5:147\u2013151.   \n[7] Blum, L. C. and Reymond, J.-L. (2009). 970 million druglike small molecules for virtual screening in the chemical universe database gdb-13. Journal of the American Chemical Society, 131(25):8732\u20138733.   \n[8] Bresson, X. and Laurent, T. (2019). A two-step graph convolutional decoder for molecule generation. arXiv preprint arXiv:1906.03412.   \n[9] Brogat-Motte, L., Flamary, R., Brouard, C., Rousu, J., and d\u2019Alch\u00e9 Buc, F. (2022). Learning to predict graphs with fused gromov-wasserstein barycenters. In International Conference on Machine Learning, pages 2321\u20132335. PMLR.   \n[10] Brouard, C., Shen, H., D\u00fchrkop, K., d\u2019Alch\u00e9-Buc, F., B\u00f6cker, S., and Rousu, J. (2016). Fast metabolite identification with input output kernel regression. Bioinformatics, 32(12):i28\u2013i36.   \n[11] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer.   \n[12] Chang, X., Ren, P., Xu, P., Li, Z., Chen, X., and Hauptmann, A. (2021). A comprehensive survey of scene graphs: Generation and application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):1\u201326.   \n[13] Chapel, L., Alaya, M. Z., and Gasso, G. (2020). Partial optimal tranport with applications on positive-unlabeled learning. Advances in Neural Information Processing Systems, 33:2903\u20132913.   \n[14] Chung, F. R. (1997). Spectral graph theory, volume 92. American Mathematical Soc.   \n[15] De Cao, N. and Kipf, T. (2018). Molgan: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973.   \n[16] De Plaen, H., De Plaen, P.-F., Suykens, J. A., Proesmans, M., Tuytelaars, T., and Van Gool, L. (2023). Unbalanced optimal transport: A unified framework for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3198\u20133207.   \n[17] Dozat, T. and Manning, C. D. (2017). Deep biaffine attention for neural dependency parsing. In International Conference on Learning Representations, ICLR. OpenReview.net.   \n[18] El Ahmad, T., Laforgue, P., and d\u2019Alch\u00e9-Buc, F. (2023). Fast Kernel Methods for Generic Lipschitz Losses via $\\backslash{\\mathfrak{p}}\\backslash$ -Sparsified Sketches. Transactions on Machine Learning Research.   \n[19] Gao, X., Xiao, B., Tao, D., and Li, X. (2010). A survey of graph edit distance. Pattern Analysis and applications, 13:113\u2013129.   \n[20] Gasteiger, J., Wei\u00dfenberger, S., and G\u00fcnnemann, S. (2019). Diffusion improves graph learning. Advances in neural information processing systems, 32.   \n[21] Gold, S. and Rangarajan, A. (1996). A graduated assignment algorithm for graph matching. IEEE Transactions on pattern analysis and machine intelligence, 18(4):377\u2013388.   \n[22] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778.   \n[23] Karp, R. M. (2010). Reducibility among combinatorial problems. Springer.   \n[24] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.   \n[25] Kwon, Y. et al. (2019). Efficient learning of non-autoregressive graph variational autoencoders for molecular graph generation. J Cheminform 11.   \n[26] Landrum, G. et al. (2013). Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8(31.10):5281.   \n[27] Loiola, E. M., De Abreu, N. M. M., Boaventura-Netto, P. O., Hahn, P., and Querido, T. (2007). A survey for the quadratic assignment problem. European journal of operational research, 176(2):657\u2013690.   \n[28] Melnyk, I., Dognin, P., and Das, P. (2022). Knowledge graph generation from text. In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1610\u20131622.   \n[29] M\u00e9moli, F. (2011). Gromov-wasserstein distances and the metric approach to object matching. Foundations of Computational Mathematics, 11(4):417\u2013487.   \n[30] Okabe, A., Boots, B., Sugihara, K., and Chiu, S. N. (2009). Spatial tessellations: concepts and applications of voronoi diagrams.   \n[31] Peyr\u00e9, G., Cuturi, M., et al. (2019). Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607.   \n[32] Peyr\u00e9, G., Cuturi, M., and Solomon, J. (2016). Gromov-wasserstein averaging of kernel and distance matrices. In International conference on machine learning, pages 2664\u20132672. PMLR.   \n[33] Sanfeliu, A. and Fu, K.-S. (1983). A distance measure between attributed relational graphs for pattern recognition. IEEE Transactions on Systems, Man, and Cybernetics, pages 353\u2013362.   \n[34] Scetbon, M., Peyr\u00e9, G., and Cuturi, M. (2022). Linear-time gromov wasserstein distances using low rank couplings and costs. In International Conference on Machine Learning, pages 19347\u201319365. PMLR.   \n[35] S\u00e9journ\u00e9, T., Vialard, F.-X., and Peyr\u00e9, G. (2021). The unbalanced gromov wasserstein distance: Conic formulation and relaxation. Advances in Neural Information Processing Systems, 34:8766\u2013 8779.   \n[36] Shit, S., Koner, R., Wittmann, B., Paetzold, J., Ezhov, I., Li, H., Pan, J., Sharifzadeh, S., Kaissis, G., Tresp, V., et al. (2022). Relationformer: A unified framework for image-to-graph generation. In European Conference on Computer Vision, pages 422\u2013439. Springer.   \n[37] Simonovsky, M. and Komodakis, N. (2018). Graphvae: Towards generation of small graphs using variational autoencoders. In Artificial Neural Networks and Machine Learning\u2013ICANN, pages 412\u2013422. Springer.   \n[38] Suhail, M., Mittal, A., Siddiquie, B., Broaddus, C., Eledath, J., Medioni, G., and Sigal, L. (2021). Energy-based learning for scene graph generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13936\u201313945.   \n[39] Thorpe, M., Park, S., Kolouri, S., Rohde, G. K., and Slepc\u02c7ev, D. (2017). A transportation l\u02c6 p l p distance for signal analysis. Journal of mathematical imaging and vision, 59:187\u2013210.   \n[40] Thual, A., Tran, H., Zemskova, T., Courty, N., Flamary, R., Dehaene, S., and Thirion, B. (2022). Aligning individual brains with fused unbalanced gromov-wasserstein. In Neural Information Processing Systems (NeurIPS).   \n[41] Tong, X., Liu, X., Tan, X., Li, X., Jiang, J., Xiong, Z., Xu, T., Jiang, H., Qiao, N., and Zheng, M. (2021). Generative models for de novo drug design. Journal of Medicinal Chemistry, 64(19):14011\u201314027.   \n[42] Ucak, U. V., Ashyrmamatov, I., and Lee, J. (2023). Reconstruction of lossless molecular representations from fingerprints. Journal of Cheminformatics, 15(1):1\u201311.   \n[43] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.   \n[44] Vayer, T., Chapel, L., Flamary, R., Tavenard, R., and Courty, N. (2019). Optimal transport for structured data with application on graphs. In International Conference on Machine Learning (ICML).   \n[45] Vayer, T., Chapel, L., Flamary, R., Tavenard, R., and Courty, N. (2020). Fused gromovwasserstein distance for structured objects. Algorithms, 13 (9):212.   \n[46] Villani, C. (2009). Optimal transport : old and new. Springer, Berlin.   \n[47] Vogelstein, J. T., Conroy, J. M., Lyzinski, V., Podrazik, L. J., Kratzer, S. G., Harley, E. T., Fishkind, D. E., Vogelstein, R. J., and Priebe, C. E. (2011). Fast approximate quadratic programming for large (brain) graph matching. arXiv preprint arXiv:1112.5507.   \n[48] Wang, R., Guo, Z., Pan, W., Ma, J., Zhang, Y., Yang, N., Liu, Q., Wei, L., Zhang, H., Liu, C., Jiang, Z., Yang, X., and Yan, J. (2024). Pygmtools: A python graph matching toolkit. Journal of Machine Learning Research, 25(33):1\u20137.   \n[49] Wishart, D. S. (2011). Advances in metabolite identification. Bioanalysis, 3(15):1769\u20131782.   \n[50] Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. (2018). Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513\u2013530.   \n[51] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. (2020). On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524\u201310533. PMLR.   \n[52] Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018). How powerful are graph neural networks? arXiv preprint arXiv:1810.00826.   \n[53] Yang, J., Ang, Y. Z., Guo, Z., Zhou, K., Zhang, W., and Liu, Z. (2022). Panoptic scene graph generation. In European Conference on Computer Vision, pages 178\u2013196. Springer.   \n[54] Yang, Y., Pilanci, M., and Wainwright, M. J. (2017). Randomized sketches for kernels: Fast and optimal nonparametric regression. The Annals of Statistics, 45(3):991\u20131023.   \n[55] Young, A., Wang, B., and R\u00f6st, H. (2021). Massformer: Tandem mass spectrum prediction with graph transformers. arXiv preprint arXiv:2111.04824.   \n[56] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017). Deep sets. Advances in neural information processing systems, 30. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Illustration of PMFGW On A Toy Example ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "On the one hand, we consider a target graph of size 2, $\\mathbf{g}=\\left(\\mathbf{F}_{2},\\mathbf{A}_{2}\\right)$ where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{F}_{2}=\\left(\\mathbf{f}_{1}\\right);\\mathbf{A}_{2}=\\left({0\\atop1}\\quad0\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some node features ${\\bf f}_{1}$ and $\\mathbf{f}_{2}$ . For $M=3$ the padded target is ${\\mathcal{P}}(\\mathbf{g})=(\\mathbf{h},\\mathbf{F},\\mathbf{A})$ where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{h}=\\left({\\overset{1}{1}}\\right);\\mathbf{F}=\\left({\\overset{\\mathbf{f}_{1}}{\\mathbf{f}_{2}}}\\right);\\mathbf{A}=\\left({\\overset{0}{1}}\\quad{\\overset{1}{0}}\\quad-\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "On the other hand, we consider a predicted graph $\\hat{\\mathbf{y}}_{a,h}=(\\hat{\\mathbf{h}},\\hat{\\mathbf{F}},\\hat{\\mathbf{A}})$ that has the form ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\hat{\\mathbf{h}}}={\\left(\\begin{array}{l}{1}\\\\ {h}\\\\ {1-h}\\end{array}\\right)}\\,;{\\hat{\\mathbf{F}}}={\\left(\\begin{array}{l}{\\mathbf{f}_{1}}\\\\ {\\mathbf{f}_{2}}\\\\ {\\mathbf{f}_{2}}\\end{array}\\right)}\\,;{\\hat{\\mathbf{A}}}={\\left(\\begin{array}{l l l}{0}&{a}&{1-a}\\\\ {a}&{0}&{0}\\\\ {1-a}&{0}&{0}\\end{array}\\right)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some $a,h\\in[0,1]$ . The loss between the prediction and the (padded) target is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{train}}(a,h)=\\mathrm{PMFGW}(\\hat{\\mathbf{y}}_{a,h},\\mathcal{P}_{3}(\\mathbf{g}))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We are interested in the landscape of this loss. First of all, it appears that $\\hat{\\mathbf{y}}_{1,1}$ and $\\hat{\\mathbf{y}}_{0,0}$ and $\\mathcal{P}(\\mathbf{g})$ are isomorphic, thus we get two global minima $\\mathcal{L}_{\\mathrm{train}}(1,1)=\\mathcal{L}_{\\mathrm{train}}(0,0)=0$ . Going into greater detail, it can be shown that for $\\ell_{h}(a,b)=\\ell_{A}(a,b)=(a-b)^{2}$ we have the following expression ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{train}}(a,h)=\\operatorname*{min}\\left((1-a)^{2}+\\frac23(1-h)^{2};a^{2}+\\frac23h^{2}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the optimal transport plan is the permutation $(1,2,3)$ when $\\begin{array}{r}{(1-a)^{2}+\\frac{2}{3}(1-h)^{2}<a^{2}+\\frac{2}{3}h^{2}}\\end{array}$ and $(1,3,2)$ otherwise. In this toy example, the optimal transport plan is always a permutation. ", "page_idx": 13}, {"type": "text", "text": "At inference time, we could similarly be interested in the edit distance between the (discrete) prediction and the target ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{eval}}(a,h)=\\mathrm{ED}(\\mathcal{P}_{3}^{-1}\\mathcal{T}(\\hat{\\mathbf{y}}_{a,h}),\\mathbf{g}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Once again, the expression can be computed explicitly ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{eval}}(a,h)=\\mathbb{1}[a<0.5\\mathrm{~and~}h>0.5]+\\mathbb{1}[a>0.5\\mathrm{~and~}h<0.5]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We provide in Figure 6 an illustration of the edit distance and the proposed loss that is clearly a continuous and smoothed version of the edit distance which allows for learning the NN parameters. ", "page_idx": 13}, {"type": "text", "text": "B Formal Statements And Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we write ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{MFGW}(\\hat{y},\\mathcal{P}(g))=\\operatorname*{min}_{\\Sigma\\in\\pi_{M}}\\;\\sum_{i,j}T_{i,j}\\ell_{h}(\\hat{h}_{i},h_{j})+\\sum_{i,j}T_{i,j}\\ell_{f}(\\hat{\\mathbf{t}}_{i},\\mathbf{f}_{j})h_{j}+\\sum_{i,j,k,l}T_{i,j}T_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l})h_{j}h_{l},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "meaning that we absorb the normalization factors in the ground losses to lighten the notation. Alternatively, we also consider the matrix formulation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{PMFGW}(\\boldsymbol{\\hat{y}},\\mathcal{P}(\\boldsymbol{g}))=\\operatorname*{min}_{\\mathbf{T}\\in\\pi_{M}}\\langle\\mathbf{T},\\mathbf{C}\\rangle+\\langle\\mathbf{T},\\mathbf{L}\\otimes\\mathbf{T}\\rangle,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "image", "img_path": "tPgagXpvcV/tmp/3f4832bd8249e6d465dff50c51cfebb6efe1cffde7fbe9e80d2434463e2726b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: Heatmap of $\\mathcal{L}_{\\mathrm{train}}$ (left) and $\\mathcal{L}_{\\mathrm{eval}}$ (right). The red line represents the transition between the regime where the optimal transport plan is the permutation $(1,2,3)$ and that where it is $(1,3,2)$ . In both cases, the optimal transport plan is a permutation. ", "page_idx": 14}, {"type": "text", "text": "where $C_{i,j}\\,=\\,\\ell_{h}(\\hat{h}_{i},h_{j})+\\ell_{f}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})h_{j}$ , $L_{i,j,k,l}\\,=\\,\\ell_{A}(\\hat{A}_{i,k},A_{j,l})h_{j}h_{l}$ and $\\otimes$ is the tensor/matrix product. ", "page_idx": 14}, {"type": "text", "text": "B.1 PMFGW fast computation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following results generalize the Proposition 1 of Peyr\u00e9 et al. [32] so that it can be applied to the computation of PMFGW. ", "page_idx": 14}, {"type": "text", "text": "Proposition 5. Assuming that the ground loss than can decomposed as $\\ell(a,b)=f_{1}(a)+f_{2}(b)-$ $h_{1}(\\bar{a})h_{2}(b)$ , for any transport plan $\\mathbf{T}\\in\\mathbb{R}^{n\\times m}$ and matrices A $\\mathbf{\\Delta},\\mathbf{W}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{A}^{\\prime},\\mathbf{W}^{\\prime}\\in\\mathbb{R}^{m\\times m}$ , then the tensor product of the form ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathbf{L}\\otimes\\mathbf{T})_{i,i^{\\prime}}=\\sum_{j,j^{\\prime}}T_{j,j^{\\prime}}\\ell(A_{i,j},A_{i^{\\prime},j^{\\prime}}^{\\prime})W_{i,j}W_{i^{\\prime},j^{\\prime}}^{\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "can be computed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{L}\\otimes\\mathbf{T}=\\mathbf{U}_{1}\\mathbf{T}\\mathbf{W}^{\\prime T}+\\mathbf{W}\\mathbf{T}\\mathbf{U}_{2}^{T}-\\mathbf{V}_{1}\\mathbf{T}\\mathbf{V}_{2}^{T},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{U}_{1}=f_{1}(\\mathbf{A})\\cdot\\mathbf{W}$ , $\\mathbf{U}_{2}=f_{2}(\\mathbf{A}^{\\prime})\\cdot\\mathbf{W}^{\\prime}$ , $\\mathbf{V}_{1}=h_{1}(\\mathbf{A})\\cdot\\mathbf{W}$ , $\\mathbf{V}_{2}=h_{2}(\\mathbf{A}^{\\prime})\\cdot\\mathbf{W}^{\\prime}$ and $[\\cdot]$ is the point-wise multiplication. ", "page_idx": 14}, {"type": "text", "text": "Proof. Thanks to the decomposition assumption the tensor product can be decomposed into 3 terms: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle)_{i,i^{\\prime}}=\\sum_{j,j^{\\prime}}T_{j,j^{\\prime}}f_{1}(A_{i,j})W_{i,j}W_{i^{\\prime},j^{\\prime}}^{\\prime}+\\sum_{j,j^{\\prime}}T_{j,j^{\\prime}}f_{2}(A_{i^{\\prime},j^{\\prime}}^{\\prime})W_{i,j}W_{i^{\\prime},j^{\\prime}}^{\\prime}-\\sum_{j,j^{\\prime}}T_{j,j^{\\prime}}h_{1}(A_{i,j})h_{2}(A_{i^{\\prime},j^{\\prime}}^{\\prime})W_{i,j^{\\prime}}~~,}}\\\\ {{\\displaystyle f_{1}(A_{i,j})W_{i,j}\\sum_{j^{\\prime}}T_{j,j^{\\prime}}W_{i^{\\prime},j^{\\prime}}^{\\prime}+\\sum_{j^{\\prime}}f_{2}(A_{i^{\\prime},j^{\\prime}}^{\\prime})W_{i^{\\prime},j^{\\prime}}^{\\prime}\\sum_{j}T_{j,j^{\\prime}}W_{i,j}-\\sum_{j}h_{1}(A_{i,j})W_{i,j}\\sum_{j^{\\prime}}T_{j,j^{\\prime}}h_{2}(A_{i^{\\prime},j^{\\prime}}^{\\prime})~~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Introducing $\\mathbf{U}_{1},\\mathbf{U}_{2},\\mathbf{V}_{1}$ and $\\mathbf{U}_{2}$ as defined above, we write: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{L}\\otimes\\mathbf{T})_{i,i^{\\prime}}=\\displaystyle\\sum_{j}(U_{1})_{i,j}\\sum_{j^{\\prime}}T_{j,j^{\\prime}}W_{i^{\\prime},j^{\\prime}}^{\\prime}+\\displaystyle\\sum_{j^{\\prime}}(U_{2})_{i^{\\prime},j^{\\prime}}\\sum_{j}T_{j,j^{\\prime}}W_{i,j}-\\displaystyle\\sum_{j}(V_{1})_{i,j}\\sum_{j^{\\prime}}T_{j,j^{\\prime}}(V_{2})_{i^{\\prime},j^{\\prime}},}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{j}(U_{1})_{i,j}(T W^{\\prime T})_{j,i^{\\prime}}+\\displaystyle\\sum_{j^{\\prime}}(U_{2})_{i^{\\prime},j^{\\prime}}(W T)_{i,j^{\\prime}}-\\displaystyle\\sum_{j}(V_{1})_{i,j}(T_{j,j^{\\prime}}V_{2}^{T})_{j,i^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which concludes that $\\mathbf{L}\\otimes\\mathbf{T}=\\mathbf{U}_{1}\\mathbf{T}\\mathbf{W}^{\\prime T}+\\mathbf{W}\\mathbf{T}\\mathbf{U}_{2}^{T}-\\mathbf{V}_{1}\\mathbf{T}\\mathbf{V}_{2}^{T}.$ ", "page_idx": 14}, {"type": "text", "text": "Remark 2 (Computational cost). $\\mathbf{U}_{1},\\mathbf{U}_{2},\\mathbf{V}_{1},\\mathbf{V}_{2}$ can be pre-computed for a cost of $O(n^{2}+m^{2})$ , after which $\\mathbf{L}\\otimes\\mathbf{T}$ can be computed (for any $\\mathbf{T}$ ) at a cost of $O(m n^{\\mathrm{2}}+n m^{\\mathrm{2}})$ . ", "page_idx": 14}, {"type": "text", "text": "Remark 3 (Kullback-Leibler divergence decomposition). The Kullback-Leibler divergence KL(p, q) = q log qp + (1 \u2212q) log ((11\u2212\u2212qp)), which we use as ground loss in our experiments satisfies the required decomposition given $\\ddot{f}_{1}(p)\\,=\\,-\\log(p).$ , $f_{2}(q)\\,=\\,q\\log(q)+(1-q)\\log(1-q)$ , $\\begin{array}{r}{h_{1}(p)=\\log(\\frac{1-p}{p})}\\end{array}$ and, $h_{2}(q)=1-q$ ", "page_idx": 15}, {"type": "text", "text": "Remark 4. The tensor product that appears in PMFGW is a special case of this theorem that corresponds to $n=m=M$ , $W_{i,j}=1$ and $W_{i^{\\prime},j^{\\prime}}^{\\prime}=h_{i^{\\prime}}h_{j^{\\prime}}$ . Thus, proposition 1 is a direct corollary. ", "page_idx": 15}, {"type": "text", "text": "B.2 PMFGW divergence properties ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First, we provide below a more detailed version of Proposition 2 ", "page_idx": 15}, {"type": "text", "text": "Proposition 6 (GI Invariance). For any $m\\leq M,\\,\\hat{y},\\hat{y}^{\\prime}\\in\\hat{\\mathcal{Y}}$ and $g,g^{\\prime}\\in\\mathcal{G}_{m}$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{y}\\sim\\hat{y}^{\\prime},g\\sim g^{\\prime}\\implies P M F G W(\\hat{y},\\mathcal{P}(g))=P M F G W(\\hat{y}^{\\prime},\\mathcal{P}(g^{\\prime})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We denote $\\hat{y}\\,=\\,(\\hat{\\bf h},\\hat{\\bf F},\\hat{\\bf A})$ and ${\\mathcal P}(g)\\;=\\;(\\mathbf{h},\\mathbf{F},\\mathbf{A})$ . Since $\\hat{y}$ and $\\hat{y}^{\\prime}$ are isomorphic, there exist a permutation $\\mathbf{P}\\in\\sigma_{M}$ such that $\\hat{y}^{\\prime}=(\\mathbf{P}\\hat{\\mathbf{h}},\\mathbf{P}\\hat{\\mathbf{F}},\\mathbf{P}\\hat{\\mathbf{A}}\\mathbf{P}^{T})$ . Moreover, the fact that $g$ and $g^{\\prime}$ are isomorphic implies that $\\mathcal{P}(g)$ and $\\mathcal{P}(g^{\\prime})$ are isomorphic as well, thus there exist a permutation $\\mathbf{Q}\\in\\sigma_{M}$ such that $\\mathcal{P}(g^{\\prime})=(\\mathbf{Q}\\mathbf{h},\\mathbf{Q}\\mathbf{F},\\mathbf{Q}\\mathbf{A}\\mathbf{Q}^{T})$ . Plugging into the PMFGW objective we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathbf{PMFGW}(\\hat{y}^{\\prime},\\mathcal{P}(g^{\\prime}))=\\operatorname*{min}_{\\mathbf{T}\\in\\pi_{M}}}&{\\displaystyle\\sum_{i,j}T_{i,j}\\ell_{h}((\\mathbf{P\\hat{h}})_{i},(\\mathbf{Qh})_{j})+\\sum_{i,j}T_{i,j}\\ell_{f}((\\mathbf{P\\hat{F}})_{i},(\\mathbf{QF})_{j})(\\mathbf{Qh})_{j}}\\\\ &{+\\displaystyle\\sum_{i,j,k,l}T_{i,j}T_{k,l}\\ell_{A}((\\mathbf{P\\hat{AP}}^{T})_{i,k},(\\mathbf{QAQ}^{T})_{j,l})(\\mathbf{Qh})_{j}(\\mathbf{Qh})_{l}}\\\\ &{\\displaystyle=\\operatorname*{min}_{\\mathbf{T}\\in\\pi_{M}}}&{\\displaystyle\\sum_{i,j}(\\mathbf{P}^{T}\\mathbf{TQ})_{i,j}\\ell_{h}(\\mathbf{\\hat{h}}_{i},\\mathbf{h}_{j})+\\displaystyle\\sum_{i,j}(\\mathbf{P}^{T}\\mathbf{TQ})_{i,j}\\ell_{f}(\\hat{\\mathbf{t}}_{i},\\mathbf{f}_{j})h_{j}}\\\\ &{+\\displaystyle\\sum_{i,j,k,l}(\\mathbf{P}^{T}\\mathbf{TQ})_{i,j}(\\mathbf{P}^{T}\\mathbf{TQ})_{k,l}\\ell_{A}(\\boldsymbol{\\hat{A}}_{i,k},A_{j,l})h_{j}h_{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denoting $\\tilde{\\mathbf{T}}=\\mathbf{P}^{T}\\mathbf{T}\\mathbf{Q}$ we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathrm{PMFGW}(\\hat{y}^{\\prime},\\mathcal{P}(g^{\\prime}))=\\operatorname*{min}_{\\hat{\\mathbf{T}}\\in\\pi_{M}}}&{\\displaystyle\\sum_{i,j}\\tilde{T}_{i,j}\\ell_{h}(\\hat{h}_{i},h_{j})+\\sum_{i,j}\\tilde{T}_{i,j}\\ell_{f}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})h_{j}}\\\\ &{+\\displaystyle\\sum_{i,j,k,l}\\tilde{T}_{i,j}\\tilde{T}_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l})h_{j}h_{l}}\\\\ &{=\\mathrm{PMFGW}(\\hat{y},\\mathcal{P}(g)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now provide a more detailed version of Proposition 3 ", "page_idx": 15}, {"type": "text", "text": "Definition 1. We say that $\\ell:\\hat{\\mathcal{X}}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ is positive when for any $x,y\\in\\hat{\\mathcal{X}}\\times\\mathcal{X},\\,\\ell(x,y)\\geq0$ with equality if and only if $x=y$ . ", "page_idx": 15}, {"type": "text", "text": "Proposition 7 (Positivity). Let us assume that $\\ell_{h}:[0,1]\\times\\{0,1\\}\\mapsto\\mathbb{R},\\ell_{f}:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ and $\\ell_{A}:[0,1]\\times\\{0,1\\}\\mapsto\\mathbb{R}$ are positive. Then we have that for any $\\hat{y}\\in\\hat{\\mathcal{Y}},g\\in\\mathcal{G}$ : ", "page_idx": 15}, {"type": "text", "text": "$\\bullet\\,\\,i)\\,P M F G W(\\hat{y},\\mathcal{P}(g))\\geq0$ \u2022 ii) There is equality if and only if $\\hat{y}\\sim\\mathcal{P}(g)$ \u2022 iii) In that case $\\mathcal{P}^{-1}\\mathcal{T}(\\hat{y})\\sim g$ ", "page_idx": 15}, {"type": "text", "text": "Proof. The direct implication of ii) is the only statement that is not trivial. First, let us show that if $\\mathrm{PMFGW}(\\hat{y},\\mathcal{P}(g))=\\overline{{0}}$ , the optimal transport $\\mathbf{T}^{*}$ is a permutation. Recall that any transport plan is ", "page_idx": 15}, {"type": "text", "text": "a convex combination of permutations [6] i.e. there exist $\\lambda_{1},...,\\lambda_{K}\\in]0,1]$ and $\\mathbf{P}_{1},...,\\mathbf{P}_{K}\\in\\sigma_{M}$ such that $\\textstyle\\sum_{k=1}^{K}\\lambda_{k}=1$ and $\\begin{array}{r}{\\mathbf{T}^{*}=\\sum_{k=1}^{K}\\lambda_{k}\\mathbf{P}_{k}}\\end{array}$ . Thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle0=\\langle{\\bf T}^{*},{\\bf C}\\rangle+\\langle{\\bf T}^{*},{\\bf L}\\otimes{\\bf T}^{*}\\rangle}\\ ~}\\\\ {{\\displaystyle~~=\\sum_{k=1}^{K}\\lambda_{k}\\langle{\\bf P}_{k},{\\bf C}\\rangle+\\sum_{k=1}^{K}\\lambda_{k}^{2}\\langle{\\bf P}_{k},{\\bf L}\\otimes{\\bf P}_{k}\\rangle+\\sum_{k\\ne l}^{K}\\lambda_{k}\\lambda_{l}\\langle{\\bf P}_{k},{\\bf L}\\otimes{\\bf P}_{l}\\rangle}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is a sum of positive terms, thus all terms are null and in particular, for any $k$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0=\\langle\\mathbf{P}_{k},\\mathbf{C}\\rangle+\\langle\\mathbf{P}_{k},\\mathbf{L}\\otimes\\mathbf{P}_{k}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus all the $\\mathbf{P}_{k}$ are optimal transport plans. In the following, we chose one of them and denote it $\\mathbf{P}$ Moving back to the developed formulation of PMFGW we get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{\\Gamma}=\\mathrm{PMFGW}(\\hat{y},\\mathcal{P}(g))=\\sum_{i,j}P_{i,j}\\ell_{h}(\\hat{h}_{i},h_{j})+\\sum_{i,j}P_{i,j}\\ell_{f}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})h_{j}+\\sum_{i,j,k,l}P_{i,j}P_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l})h_{j}h_{l}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Once again this is a sum of positive terms thus for all $i,j,k$ , and $l$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n0=P_{i,j}\\ell_{h}(\\hat{h}_{i},h_{j})=P_{i,j}\\ell_{f}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})h_{j}=P_{i,j}P_{k,l}\\ell_{A}(\\hat{A}_{i,k},A_{j,l})h_{j}h_{l}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n0=\\ell_{h}((\\mathbf{P}^{T}\\hat{\\mathbf{h}})_{j},h_{j})=\\ell_{f}((\\mathbf{P}^{T}\\hat{\\mathbf{F}})_{j},F_{j})h_{j}=\\ell_{A}((\\mathbf{P}^{T}\\hat{\\mathbf{A}}\\mathbf{P})_{j,l},A_{j,l})h_{j}h_{l}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And from the positivity of $\\ell_{h},\\ell_{f}$ and $\\ell_{A}$ we get that: $\\mathbf{P}^{T}\\hat{\\mathbf{h}}\\;=\\;\\mathbf{h},\\;\\mathbf{P}^{T}\\hat{\\mathbf{F}}[$ [: $m]\\ =\\ \\mathbf{F}[:\\ m]$ and $\\mathbf{P}^{T}\\hat{\\mathbf{A}}\\mathbf{P}[:m,:m]=\\mathbf{A}[:m,:m]$ . Since the nodes $i>m$ are not activated, by abuse of notation we simply write $\\mathbf{P}^{T}\\hat{\\mathbf{F}}=\\mathbf{F}$ and $\\mathbf{P}^{T}\\hat{\\mathbf{A}}\\mathbf{P}=\\mathbf{A}$ . This concludes that $\\hat{y}\\sim\\mathcal{P}(g)$ . ", "page_idx": 16}, {"type": "text", "text": "B.3 PMFGW and Partial Fused Gromov Wasserstein ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Following Chapel et al. [13], we define an OT relaxation of the Partial Matching problem. ", "page_idx": 16}, {"type": "text", "text": "For a large graph $\\hat{g}=(\\hat{\\mathbf{F}},\\hat{\\mathbf{A}})\\in\\mathcal{G}_{M}$ and a smaller graph $g=(\\mathbf{F},\\mathbf{A})\\in\\mathcal{G}_{m}$ , the set of transport plan transporting a subgraph of $\\hat{g}$ to $g$ can be defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi_{M,m}=\\{\\mathbf{T}\\in[0,1]^{M\\times m}\\;|\\;\\mathbf{T}\\mathbb{1}_{m}\\leq\\mathbb{1}_{M},\\mathbf{T}^{T}\\mathbb{1}_{M}=\\mathbb{1}_{m},\\mathbb{1}_{M}^{T}\\mathbf{T}\\mathbb{1}_{m}=m\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the associated partial Fused Gromov Wasserstein distance is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{partialFGW}(\\hat{g},g)=\\operatorname*{min}_{\\mathbf{T}\\in\\pi_{M,m}}\\sum_{i=1}^{M}\\sum_{j=1}^{m}\\mathbf{T}_{i,j}\\ell_{F}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})+\\sum_{i,k=1}^{M}\\sum_{j,l=1}^{m}\\mathbf{T}_{i,j}\\mathbf{T}_{k,l}\\ell_{A}(A_{i,k},A_{j,l}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the following, we show that partialFGW $({\\hat{g}},g)$ is equivalent to the padded Fused Gromov Wasserstein distance defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{paddedFGW}(\\hat{g},g)=\\operatorname*{min}_{\\mathbf T\\in\\pi_{M}}\\sum_{i=1}^{M}\\sum_{j=1}^{m}\\mathbf T_{i,j}\\ell_{F}(\\hat{\\mathbf{f}}_{i},\\mathbf{f}_{j})+\\sum_{i,k=1}^{M}\\sum_{j,l=1}^{m}\\mathbf T_{i,j}\\mathbf T_{k,l}\\ell_{A}(A_{i,k},A_{j,l}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 5. Any transport plan $\\mathbf{T}\\in\\pi_{M}$ has the form $\\mathbf{T}=\\left(\\mathbf{T}_{p}\\quad\\mathbf{T}_{2}\\right)$ where $\\mathbf{T}_{p}$ is a partial transport plan i.e. $\\mathbf{T}_{p}\\in\\pi_{M,m}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Let us check that $\\mathbf{T}_{p}$ is in $\\pi_{M}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{1}_{M}=\\mathbf{T}\\mathbb{1}_{M}=\\mathbf{T}_{p}\\mathbb{1}_{m}+\\mathbf{T}_{2}\\mathbb{1}_{M-m}\\geq\\mathbf{T}_{p}\\mathbb{1}_{m}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 6. For any partial transport plan $\\mathbf{T}_{p}\\;\\in\\;\\pi_{M,m}$ there exist $T_{2}\\ \\in\\ \\mathbb{R}^{M\\times(M-m)}$ such that $\\mathbf{T}=(\\mathbf{T}_{p}\\quad\\mathbf{T}_{2})\\in\\dot{\\pi}_{M}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let us define $p=\\mathbb{1}_{M}-\\mathbf{T}_{p}\\mathbb{1}_{m}$ . This is the mass of the larger graph that is not matched by $\\mathbf{T}_{p}$ . Note that since $\\mathbf{T}_{p}\\in\\pi_{M,m}$ we have that $p\\geq0$ . Thus we can set $\\begin{array}{r}{\\mathbf{T}_{2}=\\frac{1}{M-m}p\\mathbb{1}_{M-m}^{T}}\\end{array}$ i.e. we spread the remaining mass uniformly across the padding nodes. Let us check that $\\mathbf{T}=(\\mathbf{T}_{p}\\quad\\mathbf{T}_{2})\\in\\pi_{M}$ is indeed a valid transport plan. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;{\\bf T}\\mathbb{1}_{M}={\\bf T}_{p}\\mathbb{1}_{m}+{\\bf T}_{2}\\mathbb{1}_{M-m}={\\bf T}_{p}\\mathbb{1}_{m}+p=\\mathbb{1}_{m}}\\\\ &{\\bullet\\;{\\bf T}^{T}\\mathbb{1}_{M}\\qquad\\qquad=\\qquad\\left(\\mathbf{T}_{p}^{T}\\mathbb{1}_{M}\\right)\\qquad\\qquad=\\qquad\\left(\\frac{\\mathbb{1}_{m}}{\\mathbb{M}-m}(p^{T}\\mathbb{1}_{M})\\mathbb{1}_{M-m}\\right)}\\\\ &{\\quad\\bigg(\\frac{1}{M-m}\\bigl(\\mathbb{1}_{M}^{T}\\mathbb{1}_{M}-\\mathbb{1}_{m}^{T}\\mathbf{T}_{p}^{T}\\mathbb{1}_{M}\\bigr)\\mathbb{1}_{M-m}\\bigg)=\\left(\\mathbb{1}_{M-m}\\right)=\\mathbb{1}_{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 8. paddedFGW and partialFGW are equal and any optimal plan $\\mathbf{T}^{*}$ of paddedFGW has the form $\\mathbf{T}^{*}=(T_{p}^{*},T_{2})$ where $T_{p}^{\\ast}$ is optimal for partialFGW. ", "page_idx": 17}, {"type": "text", "text": "Proof. Follows directly from the two previous lemmas. ", "page_idx": 17}, {"type": "text", "text": "Remark 7. Since paddedFGW is equivalent to the proposed PMFGW loss if and only if $\\ell_{h}$ is set to a constant, proposition 4 is a direct corollary of Proposition 8. ", "page_idx": 17}, {"type": "text", "text": "Remark 8. The algorithm proposed to compute PMFGW can be applied to paddedFGW and thus to partialFGW. Hence, we have indirectly introduced an alternative to the algorithm of Chapel et al. [13]. Further comparisons are left for future work. ", "page_idx": 17}, {"type": "text", "text": "C Encoding Any Input To Graph ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Philosophy of the Any2Graph encoder Any2Graph is compatible with different types of inputs, given that one selects the appropriate encoder. The role of the encoder is to extract a set of feature vectors from the inputs $x$ i.e. each input is mapped to a list of $k$ feature vectors of dimension $d_{e}$ where $k$ is not necessarily fixed. This is critically different from extracting a unique feature vector $\\left[k=1\\right]$ ). If $k$ is set to 1, the rest of the architecture must reconstruct an entire graph from a single vector, and the architecture is akin to that of an auto-encoder. In Any2Graph, we avoid this undesirable bottleneck by opting for a richer $\\left(k>1\\right)$ ) and more flexible $k$ is not fixed) representation of the input. The $k$ feature vectors are then fed to a transformer which is well suited to process sets of different sizes. Since the transformer module is permutation-invariant any meaningful ordering is lost in the process. To alleviate this issue, we add positional encoding to the feature vectors whenever the ordering carries information. Finally, note that the encoder might highly benefit from pre-training whenever applicable; but this goes beyond the scope of this paper. ", "page_idx": 17}, {"type": "text", "text": "We now provide a general description of the encoders that can be used for each input modality. ", "page_idx": 17}, {"type": "image", "img_path": "tPgagXpvcV/tmp/6462c137349ac7fc08999d2af1a3a0c60cab03dacf56692456f30a61900aab91.jpg", "img_caption": ["Figure 7: Illustration of encoders extracting $k$ features vectors for different input modalities. For text/fingerprint, $k$ is the number of input tokens. For graphs, $k$ is the size of the input graph. For images, $k$ depends on the resolution of the image and the CNN kernel size. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Images For Image2Graph task we use Convolutional Neural Networks (CNN) as suggested in [36]. From an input image of shape $h\\times w\\times c$ the CNN outputs a tensor of shape $H\\times W\\times C$ which is seen as $H\\times W$ feature vectors of dimension $C$ . The raw output of the CNN is reshaped and passed through a linear layer to produce the final output of shape $H\\times W\\times d_{e}$ . Since the ordering of the $H\\times W$ features carries spatial information we add positional encoding accordingly. ", "page_idx": 18}, {"type": "text", "text": "Fingerprint/text For tasks where the input is a list of tokens (e.g. Text2Graph or Fingerprint2Graph) we use the classical NLP pipeline: each token is transformed into a vector by an embedding layer and the list of vectors is then processed by a transformer encoder module. In text2graph the tokens ordering carries semantic meaning and positionnal encoding should be added. On the contrary, in Fingerprint2Graph, the fingerprint ordering carries no information and the permutation invariance of the transformer module is a welcomed property. ", "page_idx": 18}, {"type": "text", "text": "Graph For a graph2graph task (not featured in this paper) we would suggest using a Graph Neural Network (GNN) [52]. A GNN naturally extracts $k$ feature vectors from an input graph, where $k$ is the number of nodes in the input graph. No positional encoding is required. ", "page_idx": 18}, {"type": "text", "text": "Vector We explore a Vect2Graph task in Appendix D. The naive encoder we use is composed of $k$ parallel MLPs devoted to the extraction of the $k$ feature vectors. This approach is arguably simplistic and more suited encoders should be considered depending on the type of data. ", "page_idx": 18}, {"type": "text", "text": "D COLORING: a new synthetic dataset for benchmarking Supervised Graph Prediction ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We introduce Coloring, a new synthetic dataset well suited for benchmarking SGP methods. The main advantages of Coloring are: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The output graph is uniquely defined from the input image.   \n\u2022 The complexity of the task can be finely controlled by picking the distribution of the graph sizes, the number of node labels (colors) and the resolution of the input image.   \n\u2022 One can generate as many pairs (inputs, output) as needed to explore different regimes, from abundant to scarce data. ", "page_idx": 18}, {"type": "text", "text": "To generate a new instance of Coloring, we apply the following steps: ", "page_idx": 18}, {"type": "text", "text": "\u2022 0) Sample the number of nodes (graph size) $m$ . In this paper, we sample uniformly on some interval $[M_{\\operatorname*{min}},M_{\\operatorname*{max}}]$ . ", "page_idx": 18}, {"type": "image", "img_path": "tPgagXpvcV/tmp/98f34a0eae7d67c4e736d55497716536eeab975bafb568bf7ff3462c4cf09819.jpg", "img_caption": ["Figure 8: Illustration of the five steps to follow to generate a new instance of Coloring. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "\u2022 1) Sample $m$ centroids on $[0,1]\\times[0,1]$ . In this paper, we sample the centroids as uniform i.i.d. variables.   \n$\\bullet\\ 2)$ Partition $[0,1]\\times[0,1]$ (the image) in a Voronoi diagram fashion [30]. In this paper, we use the $L_{1}$ distance. and an image of resolution $H\\times H$ .   \n\u2022 3) Create the associated graph i.e. each node is a region of the image and two nodes are linked by an edge whenever the two associated regions are adjacent.   \n\u2022 4) Color the graph with $K>4$ colors. In this paper, we use $K=4$ . A coloring is said to be valid whenever no adjacent nodes have the same color. Note that graph coloring is known to be NP-complete [23].   \n\u2022 5) Color the original image accordingly. ", "page_idx": 19}, {"type": "text", "text": "As highlighted above, Coloring is a flexible dataset. Beyond the default dataset simply referred as Coloring we also explore 2 variations in our experiments. ColoringBig is a more challenging dataset that features larger graphs. ColoringVect is a variation of Coloring where the input image is flattened and treated as a vector allowing us to explore a synthetic Vect2Graph task. The properties of these datasets, along with the performances of Any2Graph are reported in table 3. We hope that Coloring will be used to benchmark future SGP methods. ", "page_idx": 19}, {"type": "table", "img_path": "tPgagXpvcV/tmp/fe1475362b18a7529a48815d56acf4dd5157688bf48bf2b0a85a74841740aef2.jpg", "table_caption": ["Table 3: Summary of the properties of the 3 variations of Coloring considered in this paper. We also report the test edit distance achieved by the different models. For FGWBary we report the best performing variant that is ILE for ColoringVect and NN for Coloring. None scales to ColoringBig. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Additional Details On The Experimental setting ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this paper, we consider five datasets for which we provide a variety of statistics in Table 4. Coloring is a new synthetic dataset which we describe in detail in Appendix D. Toulouse (resp. USCities) is a Sat2Graph dataset [5] where the inputs are images of size $64\\times64$ (resp. $128\\times128)$ ). QM9 [50] and GDB13 [7] are datasets of small molecules which we use to address the Fingerprint2Graph task. Here, we compute a fingerprint representation of the molecule and attempt to reconstruct the original molecule from this loss representation. Following Ucak et al. [42] we use the Morgan Radius-2 fingerprint [26] which represents a molecule by a bit vector of size 2048, where each bit represents the presence/absence of a given substructure. Finally, we feed our model with the list of non-zeros bits, i.e. the list of substructures (tokens) present in the molecule. The list of substructures has a min/average/max length of $2/21/27$ for $\\it Q M9$ and $7/29/36$ for GDB13. ", "page_idx": 20}, {"type": "table", "img_path": "tPgagXpvcV/tmp/f4ecfbdd6d4e17cf5d353880c3c7d525394e0b72d87968e9a2c8fe3c0d6cd128.jpg", "table_caption": ["Table 4: Table summarizing the properties of the datasets considered. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.2 Training And Architecture Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Encoder We follow the guidelines established in C for the choice of the encoder. In particular, all encoders for Coloring, Toulouse and USCities are CNNs. The encoder for Coloring is a variation of Resnet18 [22], where we remove the first max-pooling layer and the last two blocks to accommodate for the low resolution of our input image. We proceed similarly for Toulouse except that we only remove the last block. For USCities we keep the full Resnet18. For the Fingerprint2Graph datasets, we use a transformer encoder. In practice, this transformer encoder and that of the encoder-decoder module are merged to avoid redundancy. All encoders end with a linear layer projecting the feature vectors to the hidden dimension $d_{e}$ . ", "page_idx": 20}, {"type": "text", "text": "Transformer We use the Pre-LN variant Xiong et al. [51] of the transformer encoder-decoder model as described in [43]. To reduce the number of hyperparameters, encoder and decoder modules both consist of stacks of $N_{\\tau}$ layers, with $N_{h}$ heads and the hidden dimensions of all MLP is set to $4\\times d_{e}$ . ", "page_idx": 20}, {"type": "text", "text": "Decoder All MLPs in the decoder module have one hidden layer. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Optimizer We train all neural networks with the Adam optimizer Kingma and Ba [24], learning rate $\\eta$ , 8000 warm-up steps and all other hyperparameters set to default values. We also use gradient clipping with a max norm set to 0.1. ", "page_idx": 20}, {"type": "text", "text": "All hyperparameters are given in Table 5. ", "page_idx": 20}, {"type": "table", "img_path": "tPgagXpvcV/tmp/21dbef84ca65b4fd13d33a2f1fbeee9a9be2d1fd280056d5a071c0d585ba6169.jpg", "table_caption": ["Table 5: Hyperparameters used to train our models. We also report the total training time on a NVIDIA V100. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.3 Metrics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the following, we provide a detailed description of the metrics reported in table 1. ", "page_idx": 21}, {"type": "text", "text": "Graph Level First we report the PMFGW loss between continuous prediction $\\hat{y}$ and padded target $\\mathcal{P}(g)$ . For this computation, we set $_{\\alpha}$ to the values displayed in table 5. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{PMFGW}(\\hat{y},\\mathcal{P}(g))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we report the graph edit distance Gao et al. [19] between predicted graph $\\mathcal{P}^{-1}\\mathcal{T}\\hat{y}$ and target $g$ which we compute using Pygmtools Wang et al. [48]. All edit costs (nodes and edges) are set to 1. Note that for Toulouse and USCities, node labels are 2D positions and we consider two nodes features to be equal (edit cost of 0) whenever the L2 distance is smaller than $5\\%$ than the image width. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{EDIT}(\\mathcal{P}^{-1}\\mathcal{T}\\hat{y},g)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, we report the Graph Isomorphism Accuracy GI ACC that is ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathrm{GI~Acc}}({\\hat{y}},g)=1[{\\mathrm{EDIT}}({\\mathcal{P}}^{-1}{\\mathcal{T}}{\\hat{y}},g)=0]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Node level Recall that for a prediction $\\hat{y}=(\\hat{\\mathbf{h}},\\hat{\\mathbf{F}},\\hat{\\mathbf{A}})$ the size of the predicted graph is $\\hat{m}=||\\hat{h}>$ $0.5||_{1}$ . Denoting $m$ the size of the target graph we report the size accuracy: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{SIZE~ACC}({\\hat{y}},g)=\\mathbb{1}[{\\hat{m}}=m].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The remaining node and edge-level metrics need the graphs to have the same number of nodes. To this end, we select the $m$ nodes with the highest probability $\\hat{h}_{i}$ , resulting in a graph $\\hat{g}=(\\tilde{\\mathbf{F}},\\tilde{\\mathbf{A}})$ with ground truth size. This is equivalent to assuming that the size of the graph is well predicted. Then we use Pygmtools to compute a one-to-one matching $\\sigma$ between the nodes of $\\hat{g}$ and $g$ that can be used to align graphs (we use the matching that minimizes the edit distance). In the following, we assume that $g$ and $\\hat{g}$ have been aligned. We can now define the node accuracy NODE ACC as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{NODE~ACC}(\\hat{g},g)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}[\\Tilde{F}_{i}=F_{i}],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is the average number of node features that are well predicted. ", "page_idx": 21}, {"type": "text", "text": "Edge level Since the target adjacency matrices are typically sparse, the edge prediction accuracy is a poorly informative metric. To mitigate this issue we report both Edge Precision and Edge Recall : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{EDGE~PREC.}(\\hat{g},g)=\\frac{\\sum_{i,j=1}^{m}\\mathbb{1}[\\tilde{A}_{i,j}=1,A_{i,j}=1]}{\\sum_{i,j=1}^{m}\\mathbb{1}[\\tilde{A}_{i,j}=1]}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{EDGE~PREC.}(\\hat{g},g)=\\frac{\\sum_{i,j=1}^{m}\\mathbb{1}[\\tilde{A}_{i,j}=1,A_{i,j}=1]}{\\sum_{i,j=1}^{m}\\mathbb{1}[A_{i,j}=1]}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "All those metrics are then averaged other the test set. ", "page_idx": 21}, {"type": "text", "text": "E.4 Compute resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We estimate the total computational cost of this work to be approximately 1000 hours of GPU (mostly Nvidia V100). We estimate that up to $70\\%$ of this computation time was used in preliminary work and experiments that did not make it to the paper. ", "page_idx": 21}, {"type": "text", "text": "F Additional Experiments and figures ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Learning dynamic ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The PMFGW loss is composed of three terms, two of them are linear and account for the prediction of the nodes and their features, one is quadratic and accounts for the prediction of edges. The last term is arguably the harder to minimize for the model, as a consequence, we observe that the training performs best when the two first terms are minimized first which then guides the minimization of the structure term. In other words, the model must first learn to predict the nodes before addressing their relationship. Fortunately, this behavior naturally arises in Any2Graph as long as $\\alpha_{\\mathrm{{A}}}$ , the hyperparameter controlling the importance of the quadratic term, is not too large. This is illustrated in figure 9. ", "page_idx": 22}, {"type": "image", "img_path": "tPgagXpvcV/tmp/bf895977c6f392c477c78cfb5fa423d6d34d89318afd20401dc18cfeb37c349c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "", "img_caption": ["Figure 9: First epochs of training for Coloring. The test values of the 3 components of the loss are reported. On the left (resp. right) $_{\\alpha}$ is set to $[1,1,1]$ (resp. [1,1,10]). In the first scenario, the first two terms of the loss are learned very fast and the structure is optimized next. In the second scenario, setting $\\alpha_{\\mathrm{{A}}}=10$ prevents this desirable learning dynamic. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "For the datasets where many nodes in the graphs share the same features (QM9 and GDB13) the good prediction of the nodes and their features is not enough to guide the prediction of the edges and this desirable dynamic does not occur. This motivates us to perform Feature Diffusion (FD) before training. The diffused node features carry a portion of the structural information. This makes the node feature term slightly harder to minimize but in turn, the subsequent prediction of the structure is much easier and we recover the previous dynamic. This is illustrated in figure 10. ", "page_idx": 22}, {"type": "image", "img_path": "tPgagXpvcV/tmp/3bd461e07475b18d47d4c570a1e251a63595b41bd98db7a17496be12ccf60267.jpg", "img_caption": ["Figure 10: First epochs of training for GDB13. The test values of the 3 components of the loss are reported. On the left, we perform FD before training, on the right, we leave node features unchanged. We observe that the feature loss decreases slightly slower with FD (the features are more complex) but the minimization of the structure term is largely accelerated. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.2 Effect of the OT relaxation on the performances ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As stated in 2, we adopted the OT point of view when designing Any2Graph. In practice, this means that we do not project the OT plan back to the set of permutations with a Hungarian matcher before plugging it in the loss as in Simonovsky and Komodakis [37]. Testing the effect of adding this extra step we observed a $5\\%$ to $10\\%$ increase of the edit distance across datasets (table 6) along with a more unstable training curve (figure 11). This confirms that a continuous transport plan provides a slightly more stable gradient than a discrete permutation, which aligns with the findings of De Plaen et al. [16] on the similar topic of object detection. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "tPgagXpvcV/tmp/f9e4739ab98f53c77d2a015158908d200f14bdae4d65e31731cf25efb5a09de9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 6: Effect of adding Hungarian Matching on the performances evaluated with the test edit distance. We observe, that Hungarian Matching slightly decreases the performances on all datasets but $\\it Q M9$ . ", "page_idx": 23}, {"type": "image", "img_path": "tPgagXpvcV/tmp/e4d4154f483a837557844f616c5040ef112fb376d071f58fb3cab7354c29c1f6.jpg", "img_caption": ["Figure 11: First epochs of training for GDB13 with and without projection of the optimal transport plan to the set of permutations with Hungarian matching. Hungarian matching slightly decreases the performances and induces more oscillations of the loss, which could be explained by a less stable gradient. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F.3 Repeatability ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We checked the robustness of Any2Graph to the seed used for training. For each dataset, we ran 5 times the whole training pipeline with 5 random seeds. The results are displayed in table 7. Notably, we observe that all state-of-the-art performances observed in table 1 hold even if we had reported the worst seed (we reported the first seed). ", "page_idx": 23}, {"type": "table", "img_path": "tPgagXpvcV/tmp/9fd6c5bb4179d048ff4477bc16a3415d5a01871bd8f232990dcb1cca3dc6336f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 7: For each dataset, we report the maximum, average, minimum and the standard deviation of the Any2Graph test edit distance over 5 random seeds. ", "page_idx": 23}, {"type": "text", "text": "G Additional Qualitative Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Qualitative results on COLORING ", "page_idx": 24}, {"type": "image", "img_path": "tPgagXpvcV/tmp/233a5e7ed051d5f2d7d0b6ab0020da7fe9bc892e97ff16767311cdc8e673e5e0.jpg", "img_caption": ["Figure 12: Graph prediction on the Coloring dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "tPgagXpvcV/tmp/ad5201b61dc2df5e30e10016b67984bcaa796f0c8572c93e457953f96e1836ea.jpg", "table_caption": ["G.2 Qualitative results on QM9 "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "tPgagXpvcV/tmp/49bfc50e373f1f4ee41589429b5e02f63399ff633ee484811c25b65d79f82a70.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "tPgagXpvcV/tmp/6f4fe690e33a0cdd4b98fd940f2bf85eea2cc57112bc074bbd003315b41b99db.jpg", "img_caption": ["Figure 15: Graph prediction on the Toulouse dataset. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "tPgagXpvcV/tmp/de818b74b405f247711968a00d59caa6cd3e7b7cf3931f6dc352a61ae2f0e2c0.jpg", "img_caption": ["Figure 16: Graph prediction on the USCities dataset. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "G.6 Out of distribution performances ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We tested if, once trained on Toulouse dataset, the predictive model is able to cope with out-ofdistribution data. Figure 17 shows that this is the case on these toy images, that are not related to satellite images or road maps. We leave for future work the investigation of this property. ", "page_idx": 29}, {"type": "image", "img_path": "tPgagXpvcV/tmp/7e02a3e19778ca2d7ac56c2ad4780beec3bd08d1c46d98b9be41ef08375330cf.jpg", "img_caption": ["Figure 17: Any2Graph trained on Toulouse performing on out-of-distribution inputs. Input images are displayed on top row and prediction in the bottom row. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The claims about the performances of Any2Graph are demonstrated numerically in section 5. Those about the properties of PMFGW are stated in 3 and proofs are provided in B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The current limitations of the work are stated explicitly in the dedicated last section. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 31}, {"type": "text", "text": "Justification: To the best of our knowledge, all proofs provided are correct. Note that the propositions stated in the core of the paper are informal. All formal propositions and proofs are provided in B. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We make a special effort to provide all hyperparameters and experimental settings in section 5 and appendix E. We also provide the code of the paper and will release a GitHub repository upon publication of the paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the code of the paper in supplementary materials and plan to release a GitHub repository upon publication of the paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All those details are provided in appendix E. Note that for the data splits we used the existing splits when available (QM9, Toulouse and USCities) and created our own random split for the other datasets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: It would be computationally too expensive to report the error bar for all experiments. However, we make an extra computational effort to run 5 times (with random seeds) what we consider to be the main experiments to check that our conclusions are statistically significant (appendix F.3). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We include a discussion regarding the computational cost of the different models in the main paper. Besides, we report the exact computing resources used to train Any2Graph and an estimate of the total computing cost of the research project in appendix E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: To the best of our knowledge, this work is not harmful in any of the ways detailed in the NeurIPS Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper is a fundamental research paper about supervised learning. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: No need for responsible release of code or data. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All datasets are properly referenced and cited. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All new assets (model, loss, synthetic dataset) are described in the core paper and supplementary, and the associated code is provided. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We did not use any crowdsourcing nor human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We did not use any crowdsourcing nor human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]