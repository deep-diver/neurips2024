[{"type": "text", "text": "Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Romain Ilbert\u22171, 2 Malik Tiomoko1 Cosme Louart3 Ambroise Odonnat1, 4 Vasilii Feofanov 1 Themis Palpanas2 Ievgen Redko1 ", "page_idx": 0}, {"type": "text", "text": "1Huawei Noah\u2019s Ark Lab, Paris, France 2LIPADE, Paris Descartes University, Paris, France 3 School of Data Science, The Chinese University of Hong Kong, Shenzhen, China 4 Inria, Univ. Rennes 2, CNRS, IRISA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions. We formulate a multi-task optimization problem as a regularization technique to enable single-task models to leverage multi-task learning information. We derive a closed-form solution for multi-task optimization in the context of linear models. Our analysis provides valuable insights by linking the multi-task learning performance to various model statistics such as raw data covariances, signal-generating hyperplanes, noise levels, as well as the size and number of datasets. We finally propose a consistent estimation of training and testing errors, thereby offering a robust foundation for hyperparameter optimization in multi-task regression scenarios. Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The principle of multi-task learning, which encompasses concepts such as \"learning to learn\" and \"knowledge transfer,\" offers an efficient paradigm that mirrors human intelligence. Unlike traditional machine learning, which tackles problems on a task-specific basis with tailored algorithms, multi-task learning leverages shared information across various tasks to enhance overall performance. This approach not only improves accuracy but also addresses challenges related to insufficient data and data cleaning. By analyzing diverse and multimodal data and structuring representations multi-task learning can significantly boost general intelligence, similar to how humans integrate skills. This concept is well-established [33] and has been successfully applied to a wide range of domains, such as computer vision [41], natural language processing [35, 37] and biology [17, 28, 42]. ", "page_idx": 0}, {"type": "text", "text": "Our Method. We consider the multi-task regression framework [5] with $T$ tasks with the input space $\\mathcal{X}^{(t)}\\subset\\mathbb{R}^{d}$ and the output space $\\mathcal{V}^{(t)}\\subset\\mathbb{R}^{q}$ for $t\\in\\{1,\\ldots,T\\}$ . For each task $t$ , we assume that we are given training examples organized into the feature matrix $\\mathbf{X}^{(t)}=[\\mathbf{x}_{1}^{(t)},\\ldots,\\mathbf{x}_{n_{t}}^{(t)}]\\in\\mathbb{R}^{d\\times n_{t}}$ and the corresponding response matrix $\\mathbf{Y}^{(t)}=[\\mathbf{y}_{1}^{(t)},\\cdot\\cdot\\cdot,\\mathbf{y}_{n_{t}}^{(t)}]\\in\\mathbb{R}^{q\\times n_{t}}$ , whe1re $\\mathbf{x}_{i}^{(t)}\\in\\mathcal{X}^{(t)}$ represents the -th feature vector of the -th task and $\\mathbf{y}_{i}^{(t)}\\in\\mathcal{Y}^{(t)}$ is the associated response. In order to have a more tractable and insightful setup, we follow Romera-Paredes et al. [36] and consider the linear multi-task regression model. In particular, we study a straightforward linear signal-plus-noise model that evaluates the response $\\mathbf{y}_{i}^{(t)}$ for the $i$ -th sample of the $t$ -th task as follows: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(t)}=\\frac{\\mathbf{X}^{(t)}^{\\top}\\mathbf{W}_{t}}{\\sqrt{T d}}+\\pmb{\\varepsilon}^{(t)},\\quad\\forall t\\in\\{1,\\ldots,T\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\boldsymbol{\\varepsilon}^{(t)}\\in\\mathbb{R}^{n_{t}\\times q}$ is a matrix of noise vectors with each \u03b5i(t)\u223cN(0, \u03a3N), \u03a3N \u2208Rq\u00d7q denoting the covariance matrix. The matrix $\\mathbf{W}_{t}\\in\\mathbb{R}^{d\\times q}$ denotes the signal-generating hyperplane for task $t$ . We denote the concatenation of all task-specific hyperplanes by $\\breve{\\mathbf{W}}=[\\mathbf{W}_{1}^{\\top},\\therefore.\\therefore,\\dot{\\mathbf{W}}_{T}^{\\top}]^{\\top}\\in\\mathbb{R}^{T d\\times q}$ We assume that $\\mathbf{W}_{t}$ can be decomposed as a sum of a common matrix $\\mathbf{W}_{0}\\in\\mathbb{R}^{d\\times q}$ , which captures the shared information across all the tasks, and a task-specific matrix $\\mathbf{V}_{t}\\in\\mathbb{R}^{d\\times q}$ , which captures deviations specific to the task $t$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{W}_{t}=\\mathbf{W}_{0}+\\mathbf{V}_{t}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Given the multitask regression framework and the linear signal-plus-noise model, we now want to retrieve the common and specific hyperplanes, $\\mathbf{W}_{0}$ and $\\mathbf{V}_{t}$ , respectively. To achieve this, we study the following minimization problem governed by a parameter $\\lambda$ that controls the balance between the common and specific components of $\\mathbf{W}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{W}_{0}^{*},\\{\\mathbf{V}_{t}^{*}\\}_{t=1}^{T},\\lambda^{*}=\\mathrm{argmin}\\quad\\frac{1}{2\\lambda}\\|\\mathbf{W}_{0}\\|_{F}^{2}+\\frac{1}{2}\\sum_{t=1}^{T}\\frac{\\|\\mathbf{V}_{t}\\|_{F}^{2}}{\\gamma_{t}}+\\frac{1}{2}\\sum_{t=1}^{T}\\left\\|\\mathbf{Y}^{(t)}-\\frac{\\mathbf{X}^{(t)}^{\\top}\\mathbf{W}_{t}}{\\sqrt{T d}}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\gamma=[\\gamma_{1},\\dots,\\gamma_{T}]$ is a vector of task-specific regularization hyperparameters. Each $\\gamma_{t}$ controls how much the model overfits (small $\\gamma_{t}$ ) or underfits (large $\\gamma_{t}$ ) on task $t$ . ", "page_idx": 1}, {"type": "text", "text": "Contributions and Main Results. We seek to provide a rigorous theoretical study of (3) in high dimension, providing practical insights that make the theoretical application implementable in practice. Our contributions can be summarized in four major points: ", "page_idx": 1}, {"type": "text", "text": "1. We provide exact train and test risks for the solutions of (3) using random matrix theory. We then decompose the test risk into a signal and noise term responsible for the effectiveness of multi-task learning and the negative transfer, respectively.   \n2. We show how the signal and noise terms compete with each other depending on $\\lambda$ for which we obtain an optimal value optimizing the test risk.   \n3. In a particular case, we derive a closed-form data-dependent solution for the optimal $\\lambda^{\\star}$ , involving raw data covariances, signal-generating hyperplane, noise levels, and the size and number of data sets.   \n4. We offer empirical estimates of these model variables to develop a ready-to-use method for hyperparameter optimization in multitask regression problems within our framework. ", "page_idx": 1}, {"type": "text", "text": "Applications. As an application, we view multivariate time series forecasting (MTSF) as a multitask problem and show how the proposed regularization method can be used to efficiently employ univariate models in the multivariate case. Firstly, we demonstrate that our method improves PatchTST [32] and DLinear [53] compared to the case when these approaches are applied independently to each variable. Secondly, we show that the univariate models enhanced by the proposed regularization achieve performance similar to the current state-of-the-art multivariate forecasting models such as SAMformer [19] and iTransformer [24]. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Usual Bounds in Multi-Task Learning. Since its introduction [1, 5, 26], multi-task learning (MTL) has demonstrated that transfer learning concepts can effectively leverage shared information to solve related tasks simultaneously. Recent research has shifted focus from specific algorithms or minimization problems to deriving the lowest achievable risk given an MTL data model. The beneftis of transfer learning are highlighted through risk bounds, using concepts such as contrasts [23], which study high-dimensional linear regression with auxiliary samples characterized by the sparsity of their contrast vector or transfer distances [29], which provide lower bounds for target generalization error based on the number of labeled source and target data and their similarities. Additionally, task-correlation matrices [30] have been used to capture the relationships inherent in the data model. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "However, these studies remain largely theoretical, lacking practical algorithms to achieve the proposed bounds. They provide broad estimates and orders of magnitude that, while useful for gauging the feasibility of certain objectives, do not offer precise performance evaluations. Moreover, these estimates do not facilitate the optimal tuning of hyperparameters within the MTL framework, a critical aspect of our study. We demonstrate the importance of precise performance evaluations and effective hyperparameter tuning to enhance the practical application of MTL. ", "page_idx": 2}, {"type": "text", "text": "Random Matrix Theory in Multi-Task Learning. In the high-dimensional regime, Random Matrix Theory allows obtaining precise theoretical estimates on functionals (e.g., train or test risk) of data matrices with the number of samples comparable to data dimension [2, 13, 31, 46]. In the multi-task classification case, large-dimensional analysis has been performed for Least Squares SVM [48] and supervised PCA [47] showing counterproductivity of tackling tasks independently. We inspire our optimization strategy from [48] and conduct theoretical analysis for the multi-task regression, which introduces unique theoretical challenges and with a different intuition since we need to consider another data generation model and a regression learning objective. Our data modeling shares similarity with [52] which theoretically studied conditions of a negative transfer under covariate and model shifts. Our work focuses on the selection of hyperparameters within a general optimization framework, considering both a hyperparameter $\\lambda$ to relate all tasks and specific parameters $\\gamma_{t}$ to balance overftiting and underftiting within each task, accommodating non-Gaussian distributions and employing recent developments such as deterministic equivalents and concentration inequalities. ", "page_idx": 2}, {"type": "text", "text": "High dimensional analysis for regression. Regression is an important problem that has been extensively explored in the context of single-task learning for high-dimensional data, employing tools such as Random Matrix Theory [12], physical statistics [9, 15], and the Convex Gaussian MinMax Theorem (CGMT) [43], among others. Typically, authors employ a linear signal plus noise model to calculate the asymptotic test risk as a function of the signal-generating parameter and the covariance of the noise. Our research builds upon these studies, extending them to a multi-task learning framework. In doing so, we derive several insights unique to multi-task learning. However, none of these previous studies offer a practical method for estimating the asymptotic test risk, which is dependent on the ground truth signal-generating parameter. Therefore, our work not only extends previous studies into the multi-task case within a generic data distribution (under the assumption of a concentrated random vector), but also provides a practical method for estimating these quantities. ", "page_idx": 2}, {"type": "text", "text": "3 Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Throughout this study, matrices are represented by bold uppercase letters (e.g., matrix A), vectors by bold lowercase letters (e.g., vector $\\mathbf{v}$ ), and scalars by regular, non-bold typeface (e.g., scalar $a$ ). The notation $\\mathbf{A}\\otimes\\mathbf{B}$ for matrices or vectors $\\mathbf{A},\\mathbf{B}$ is the Kronecker product. $\\mathcal{D}_{\\mathbf{x}}$ or $\\operatorname{Diag}(\\mathbf{x})$ stands for a diagonal matrix containing on its diagonal the elements of the vector $\\mathbf{x}$ . The superscripts $t$ and $i$ are used to denote the task and the sample number, respectively, e.g., $\\mathbf{x}_{i}^{(t)}$ writes the $i$ -th sample of the $t$ -th task. The canonical vector of $\\mathbb{R}^{T}$ is denoted by ${\\bf e}_{t}^{[T]}$ with $[e_{t}^{[T]}]_{i}=\\delta_{t i}$ . Given a matrix $M\\in\\mathbb{R}^{p\\times n}$ , the Frobenius norm of $\\mathbf{M}$ is denoted $\\|\\mathbf{M}\\|_{F}\\equiv\\sqrt{\\mathrm{tr}(\\mathbf{M}^{\\top}\\mathbf{M})}$ . For our theoretical analysis, we introduce the following notation of training data: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathbf{Y}}=[{\\mathbf{Y}}^{(1)},\\ldots,{\\mathbf{Y}}^{(T)}]\\in\\mathbb{R}^{q\\times n},\\qquad{\\mathbf{Z}}=\\sum_{t=1}^{T}\\left({\\mathbf{e}}_{t}^{[T]}{\\mathbf{e}}_{t}^{[T]}{\\mathbf{\\Lambda}}^{\\top}\\right)\\otimes{\\mathbf{X}}^{(t)}\\in\\mathbb{R}^{T d\\times n}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\textstyle n=\\sum_{t=1}^{T}n_{t}$ is the total number of samples in all the tasks. ", "page_idx": 2}, {"type": "text", "text": "3.1 Regression Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define $\\mathbf{V}=[\\mathbf{V}_{1}^{\\top},\\dots,\\mathbf{V}_{T}^{\\top}]^{\\top}\\in\\mathbb{R}^{d T\\times q}$ and propose to solve the linear multi-task problem by finding $\\begin{array}{r}{\\hat{\\mathbf{W}}=[\\hat{\\mathbf{W}}_{1}^{\\top},\\hdots,\\hat{\\mathbf{W}}_{T}^{\\top}]^{\\top}\\in\\mathbb{R}^{d T\\times q}}\\end{array}$ which solves the following optimization problem under the assumption of relatedness between the tasks, i.e., $\\mathbf{W}_{t}=\\mathbf{W}_{0}+\\mathbf{V}_{t}$ for all tasks $t$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\mathbf{W}_{0},\\mathbf{V})\\in\\mathbb{R}^{d}\\times\\mathfrak{q}\\times\\mathbb{R}^{d T\\times\\mathfrak{q}}}\\mathcal{I}(\\mathbf{W}_{0},\\mathbf{V})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\mathbf{W}_{0},\\mathbf{V})\\equiv\\frac{1}{2\\lambda}\\lVert\\mathbf{W}_{0}\\rVert_{F}^{2}+\\frac{1}{2}\\sum_{t=1}^{T}\\frac{\\lVert\\mathbf{V}_{t}\\rVert_{F}^{2}}{\\gamma_{t}}+\\frac{1}{2}\\sum_{t=1}^{T}\\left\\lVert\\mathbf{Y}^{(t)}-\\frac{\\mathbf{X}^{(t)}^{\\top}\\mathbf{W}_{t}}{\\sqrt{T d}}\\right\\rVert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The objective function consists of three components: a regularization term for $\\mathbf{W}_{0}$ to mitigate overfitting, a task-specific regularization term that controls deviations $\\mathbf{V}_{t}$ from the shared weight matrix $\\mathbf{W}_{0}$ , and a loss term quantifying the error between the predicted outputs and the actual responses for each task. ", "page_idx": 3}, {"type": "text", "text": "The optimization problem is convex for any positive values of $\\lambda,\\gamma_{1},\\dots,\\gamma_{T}$ , and it possesses a unique solution. The details of the calculus are given in Appendix B. The formula for $\\hat{\\mathbf{W}}_{t}$ is given as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{W}}_{t}=\\left(\\mathbf{e}_{t}^{[T]}\\,^{\\top}\\otimes\\mathbf{I}_{d}\\right)\\frac{{\\mathbf{A}}\\mathbf{Z}\\mathbf{Q}\\mathbf{Y}}{\\sqrt{T d}},\\qquad\\hat{\\mathbf{W}}_{0}=\\left(\\mathbb{1}_{T}^{\\top}\\otimes\\lambda\\mathbf{I}_{d}\\right)\\frac{{\\mathbf{Z}}\\mathbf{Q}\\mathbf{Y}}{\\sqrt{T d}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\upbeta}=\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}+\\mathbf{I}_{n}\\right)^{-1}\\in\\mathbb{R}^{n\\times n},\\,\\mathrm{and}\\;\\mathbf{A}=\\left(\\mathcal{D}_{\\gamma}+\\lambda\\mathbb{1}_{T}\\mathbb{1}_{T}^{\\top}\\right)\\otimes\\mathbf{I}_{d}\\in\\mathbb{R}^{T d\\times T d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to use Random Matrix Theory (RMT) tools, we make two assumptions on the data distribution and the asymptotic regime. Following [50], we adopt a concentration hypothesis on the feature vectors $\\mathbf{x}_{i}^{(t)}$ , which was shown to be highly effective for analyzing machine learning problems [11, 14]. The justification and implications of these assumptions can be found in appendix A.2 and A.3. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Concentrated Random Vector). We assume that there exists two constants $C,c>0$ (independent of dimension $d$ ) such that, for any task $t$ , for any 1-Lipschitz function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}.$ , any feature vector $\\mathbf{\\dot{x}}^{(t)}\\in\\mathcal{X}^{(t)}$ verifies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall t>0:\\mathbb{P}(|f(\\mathbf{x}^{(t)})-\\mathbb{E}[f(\\mathbf{x}^{(t)})]|\\geq t)\\leq C e^{-(t/c)^{2}},}\\\\ &{\\qquad\\qquad\\mathbb{E}[\\mathbf{x}^{(t)}]=0\\quad a n d\\quad\\mathrm{Cov}[\\mathbf{x}^{(t)}]=\\Sigma^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, we distinguish the following scenarios: $\\mathbf{x}_{i}^{(t)}\\in\\mathbb{R}^{d}$ are concentrated when they are (i) independent Gaussian random vectors with covariance \u221aof bounded norm, (ii) independent random vectors uniformly distributed on the $\\mathbb{R}^{d}$ sphere of radius $\\sqrt{d}$ , and most importantly (iii) any Lipschitz transformation $\\dot{\\phi}\\big(\\mathbf{x}_{i}^{(t)}\\big)$ of the above two cases, with bounded Lipschitz norm. Scenario (iii) is especially pertinent for modeling data in realistic settings. Recent research [39] has demonstrated that images produced by generative adversarial networks (GANs) are inherently qualified as concentrated random vectors. ", "page_idx": 3}, {"type": "text", "text": "Next, we present a classical RMT assumption that establishes a commensurable relationship between the number of samples and dimension. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (High-dimensional asymptotics). As $d\\rightarrow\\infty$ , $n_{t}\\,=\\,\\mathcal{O}(d)$ and $T=\\mathcal{O}(1)$ . More specifically, we assume that $n/d\\,{\\xrightarrow{a.s.}}\\,c_{0}<\\infty$ with $\\textstyle n=\\sum_{t=1}^{T}n_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Although different from classical asymptotic where the number of samples is implicitly assumed to be exponentially larger than the dimension, the high-dimensional asymptotic finds many applications including telecommunications [10], finance [34] and machine learning [11, 14, 48]. ", "page_idx": 3}, {"type": "text", "text": "4 Main Theoretical Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Estimation of the Performances ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the training dataset $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ with the corresponding response variable $\\mathbf{Y}\\in\\mathbb{R}^{n\\times q}$ and for any test dataset $\\mathbf{x}^{(t)}\\in\\mathbb{R}^{d}$ and the corresponding response variable $\\mathbf{y}^{(t)}\\in\\mathbb{R}^{q}$ , we aim to derive the theoretical training and test risks defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t r a i n}^{\\infty}=\\frac{1}{T n}\\mathbb{E}\\left[\\|\\mathbf{Y}-g(\\mathbf{X})\\|_{2}^{2}\\right],\\quad\\mathcal{R}_{t e s t}^{\\infty}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[\\|\\mathbf{y}^{(t)}-g(\\mathbf{x}^{(t)})\\|_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The output score $g(\\mathbf{x}^{(t)})\\in\\mathbb{R}^{q}$ for data $\\mathbf{x}^{(t)}\\in\\mathbb{R}^{d}$ of task $t$ is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(\\mathbf{x}^{(t)})=\\frac{1}{\\sqrt{T d}}\\left(\\mathbf{e}_{t}^{[T]}\\otimes\\mathbf{x}^{(t)}\\right)^{\\top}\\hat{\\mathbf{W}}_{t}=\\frac{1}{T d}\\left(\\mathbf{e}_{t}^{[T]}\\otimes\\mathbf{x}^{(t)}\\right)^{\\top}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Y}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In particular, the output for the training score is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(\\mathbf{X})=\\frac{1}{T d}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Y},\\qquad\\mathbf{Q}=\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}+\\mathbf{I}_{T d}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To understand the statistical properties of $\\mathcal{R}_{t r a i n}^{\\infty}$ and $\\mathcal{R}_{t e s t}^{\\infty}$ for the linear generative model, we study the statistical behavior of the resolvent matrix $\\mathbf{Q}$ defined in (6). The notion of a deterministic equivalent from random matrix theory [16] is particularly useful here as it allows us to design a deterministic matrix equivalent to $\\mathbf{Q}$ in the sense of linear forms. Specifically, a deterministic equivalent $\\bar{\\bf M}$ of a random matrix $\\mathbf{M}$ is a deterministic matrix that satisfies $u(\\mathbf{M}\\-\\bar{\\mathbf{M}})\\to0$ almost surely for any bounded linear form $u:\\mathbb{R}^{d\\times d}\\rightarrow\\mathbb{R}$ . We denote this as $\\mathbf{M}\\leftrightarrow\\bar{\\mathbf{M}}$ . This concept is crucial to our analysis because we need to estimate quantities such as $\\textstyle{\\frac{1}{d}}\\mathrm{tr}\\left(\\mathbf{A}\\mathbf{Q}\\right)$ or $\\mathbf{a}^{\\top}\\mathbf{Qb}$ , which are bounded linear forms of $\\mathbf{Q}$ with $\\mathbf{A},\\mathbf{a}$ and $\\mathbf{b}$ all having bounded norms. For convenience, we further express some contributions of the performances with the so-called \u201ccoresolvent\u201d defined as $\\begin{array}{r}{\\tilde{\\mathbf{Q}}\\equiv(\\frac{\\mathbf{A}^{\\frac{1}{2}}\\mathbf{\\bar{Z}Z}^{T}\\mathbf{A}^{\\frac{1}{2}}}{T d}+\\mathbf{I}_{T d})^{-1}.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Using Lemma 1 provided in the Appendix C.1, whose proofs are included in Appendices C.2, C.3 and C.4, we establish the deterministic equivalents that allow us to introduce our Theorems 5 and 1, characterizing the asymptotic behavior of both training and testing risks. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Asymptotic train and test risk). Assuming that the training data vectors $\\mathbf{x}_{i}^{(t)}$ and the test data vectors $\\mathbf{x}^{(t)}$ are concentrated random vectors, and given the growth rate assumption (Assumption 2), it follows that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t e s t}^{\\infty}=\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{A})\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}\\right)}{T d}+\\frac{\\mathrm{tr}(\\Sigma_{n}\\bar{\\mathbf{Q}}_{2})}{T d}+\\mathrm{tr}\\left(\\Sigma_{n}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In addition, the asymptotic risk on the training data is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t r a i n}^{\\infty}\\leftrightarrow\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\bar{\\mathbf{Q}}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)}{T n}-\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\bar{\\mathbf{Q}}_{2}(\\mathbf{I}_{T d})\\mathbf{A}^{-1/2}\\mathbf{W}\\right)}{T n}+\\frac{\\mathrm{tr}\\left(\\Sigma_{n}\\bar{\\mathbf{Q}}_{2}\\right)}{T n},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{\\tilde{\\mathbf{Q}}},\\,\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{I}_{T d})$ and $\\bar{\\bf Q}_{2}$ are respectively deterministic equivalents for $\\tilde{\\mathbf{Q}}_{}$ , ${\\tilde{\\mathbf{Q}}}^{2}$ and $\\mathbf{Q}^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "We defer the full proof of this theorem to Appendix D and provide an outline of the test risk proof below. Note that derivations of the asymptotic risk for the training data are more complex and involve computing deterministic equivalents (Lemma 1 of Appendix C.1), which is a powerful tool commonly used in Random Matrix Theory. In the derived theorem, we achieve a stronger convergence result compared to the almost surely convergence. Specifically, we prove a concentration result for the training and test risk with an exponential convergence rate for sufficiently large values of $n$ and $d$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{C}_{t\\times t}^{\\infty}=\\frac{1}{T}\\mathbb{E}[\\|\\nabla-g(\\mathbf{x})\\|_{2}^{2}]}\\\\ &{\\qquad=\\frac{1}{T}\\mathbb{E}\\left[\\|\\frac{\\mathbf{x}^{\\top}\\mathbf{M}}{\\sqrt{T}d}+\\varepsilon-\\frac{\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{W}}{T d\\sqrt{T d}}-\\frac{\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Z}}{T d}\\|_{2}^{2}\\right]}\\\\ &{\\qquad=\\frac{1}{T}\\mathbb{E}\\left[\\frac{\\mathbf{t}(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{X})}{T d}-\\frac{2\\mathbf{t}\\cdot\\mathbf{W}\\left(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{W}\\right)}{(T d)^{2}}+\\mathrm{tr}\\left(\\varepsilon^{\\top}\\varepsilon\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}\\mathbf{Z}\\mathbf{Z}\\right)\\mathbf{Z}}{(T d)^{3}}\\right.}\\\\ &{\\qquad\\left.\\frac{\\mathrm{tr}\\left(\\varepsilon^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}\\mathbf{A}\\mathbf{Z}\\mathbf{Z}\\mathbf{Q}\\varepsilon\\right)}{(T d)^{2}}\\right]}\\\\ &{\\qquad=\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{W}\\right)}{T d}-\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{A}^{\\top}\\left(\\mathbf{I}\\mathbf{\\bar{r}}d-\\mathbf{\\bar{Q}}\\right)\\mathbf{A}^{-\\frac{1}{T}}\\mathbf{W}\\right)}{T d}+\\mathrm{tr}\\left(\\varepsilon^{\\top}\\varepsilon\\right)+\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{W}\\right)}{T d}}\\\\ &{\\qquad\\qquad-\\frac{\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{A}^{\\top}\\bar{\\mathbf{Q}}\\mathbf{\\bar{t}}^{-\\frac{1}{T}}\\mathbf{W}\\right)}{T d}+\\frac{\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{T}}\\bar{\\mathbf{Q}}_{2}\\left(\\mathbf{A}\\mathbf{A}\\right)\\mathbf{A}^{-\\frac{1\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2 Error Contribution Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To gain theoretical insights, we analyze (ATR) consisting of the signal and the noise components. ", "page_idx": 5}, {"type": "text", "text": "Signal Term. The signal term can be further approximated, up to some constants as $\\mathrm{tr}(\\mathbf{W}^{\\top}(\\mathbf{A}\\Sigma+$ I)\u22122W) with \u03a3 = tT=1ndt \u03a3 (t). The matrix (A\u03a3 + I)\u22122 plays a crucial role in amplifying the signal term $\\mathrm{tr}(\\mathbf{W}^{\\top}\\mathbf{W})$ , which in turn allows the test risk to decrease. The off-diagonal elements of $\\mathbf{A}\\Sigma+\\mathbf{I})^{-2}$ amplify the cross terms $(\\mathrm{tr}({\\mathbf W}_{v}^{\\top}{\\mathbf W}_{t})$ for $t\\neq v$ ), enhancing the multi-task aspect, while the diagonal elements amplify the independent terms $(\\lVert\\mathbf{W}_{t}\\rVert_{2}^{2})$ . This structure is significant in determining the effectiveness of multi-task learning. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, both terms decrease with an increasing number of samples $n_{t}$ , smaller values of $\\gamma_{t}$ , and a larger value of $\\lambda$ . The cross term, which is crucial for multi-task learning, depends on the matrix $\\bar{\\Sigma_{t}^{-1}}\\Sigma_{v}$ . This matrix represents the shift in covariates between tasks. When the features are aligned (i.e., $\\pmb{\\Sigma}_{t}^{-1}\\pmb{\\Sigma}_{v}=\\mathbf{I}_{d})$ ), the cross term is maximized, enhancing multi-task learning. However, a larger Fisher distance between the covariances of the tasks results in less favorable correlations for multi-task learning. ", "page_idx": 5}, {"type": "text", "text": "Noise term. Similar to the signal term, the noise term can be approximated, up to some constants, as $\\mathrm{tr}(\\pmb{\\Sigma}_{N}(\\pmb{\\Lambda}^{-1}+\\pmb{\\Sigma})^{-1})$ . However, there is a major difference between the way both terms are expressed in the test risk. The noise term does not include any cross terms because the noise across different tasks is independent. In this context, only the diagonal elements of the matrix are significant. This diagonal term increases with the sample size and the value of $\\lambda$ . It is responsible for what is known as negative transfer. As the diagonal term increases, it negatively affects the transfer of learning from one task to another. This is a critical aspect to consider in multi-task learning scenarios. ", "page_idx": 5}, {"type": "text", "text": "4.3 Simplified Model for Clear Insights ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we specialize the theoretical analysis to the simple case of two tasks $\\mathcal{T}=2)$ ). We assume that the tasks share the same identity covariance and that $\\gamma_{1}\\,=\\,\\gamma_{2}\\,\\,\\equiv\\,\\gamma$ . Under these conditions, the test risk can be approximated, up to some constants, as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t e s t}^{\\infty}=\\mathbf{D}_{I L}\\left(\\|\\mathbf{W}_{1}\\|_{2}^{2}+\\|\\mathbf{W}_{2}\\|_{2}^{2}\\right)+\\mathbf{C}_{M T L}\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{2}+\\mathbf{N}_{N T}\\mathrm{tr}\\boldsymbol\\Sigma_{n}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the diagonal term (independent learning) $\\mathbf{D}_{I L}$ , the cross term (multi-task learning) $\\mathbf{C}_{M T L}$ , and the noise term (negative transfer) ${\\bf N}_{N T}$ have closed-form expressions depending on $\\gamma$ and $\\lambda$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{I L}=\\frac{\\left(c_{0}(\\lambda+\\gamma)+1\\right)^{2}+c_{0}^{2}\\lambda^{2}}{\\left(c_{0}(\\lambda+\\gamma)+1\\right)^{2}-c_{0}^{2}\\lambda^{2}},\\quad\\mathbf{C}_{M T L}=\\frac{-2c_{0}\\lambda\\left(c_{0}(\\lambda+\\gamma)+1\\right)}{\\left(c_{0}(\\lambda+\\gamma)+1\\right)^{2}-c_{0}^{2}\\lambda^{2}}}\\\\ &{\\mathbf{N}_{N T}=\\frac{\\left(c_{0}(\\lambda+\\gamma)^{2}+(\\lambda+\\gamma)-c_{0}\\lambda^{2}\\right)^{2}+\\lambda^{2}}{\\left(\\left(c_{0}(\\lambda+\\gamma)+1\\right)^{2}-c_{0}^{2}\\lambda^{2}\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "FFW6rPz48Z/tmp/302c99c979d23abe0fed627d1ffd689febceebef592c5532f24ec58cabc0de58.jpg", "img_caption": ["Figure 1: Test loss contributions $\\mathbf{D}_{I L}$ , $\\mathbf{C}_{M T L}$ , $\\mathbf{N}_{N T}$ across three sample size regimes. Test risk exhibits decreasing, increasing, or convex shapes based on the regime. $\\lambda^{*}$ from theory are marked. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We recall that $c_{0}$ has been defined in the Assumption 2. As previously mentioned, the test risk is primarily composed of two terms: the signal term and the noise term, which are in competition with each other. The more similar the tasks are, the stronger the signal term becomes. In the following plot, we illustrate how this competition can influence the risk. Depending on the value of the parameter $\\lambda$ and the sample sizes, the risk can either decrease monotonically, increase monotonically, or exhibit a convex behavior. This competition can lead to an optimal value for $\\lambda$ , which interestingly has a simple closed-form expression that can be obtained by deriving the $\\mathcal{R}_{t e s t}^{\\infty}$ w.r.t. $\\lambda$ as follows (see details in Appendix E.4): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda^{\\star}=\\frac{n}{d}S N R-\\frac{\\gamma}{2},\\mathrm{~with~}S N R=\\frac{\\|\\mathbf{W}_{1}\\|_{2}^{2}+\\|\\mathbf{W}_{2}\\|_{2}^{2}}{\\mathrm{tr}\\Sigma_{N}}+\\frac{\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{2}}{\\mathrm{tr}\\Sigma_{N}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.4 Comparison between Empirical and Theoretical Predictions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare the theoretical predictions with the empirical results. Our \u221aexperiment is based on a two-task setting $T=2$ ) defined as $\\mathbf{W}_{1}\\sim\\mathcal{N}(0,I_{p})$ with $\\mathbf{W}_{2}=\\alpha\\mathbf{W}_{1}+\\sqrt{1-\\alpha^{2}}\\mathbf{W}_{1}^{\\perp}$ . $\\mathbf{W}_{1}^{\\perp}$ represents any vector orthogonal to $\\mathbf{W}_{1}$ and $\\alpha\\in[0,1]$ . This setting allows us to adjust the similarity between tasks through $\\alpha$ . ", "page_idx": 6}, {"type": "text", "text": "Figure 2 shows a comparison of the theoretical and empirical classification errors for different values of $\\lambda$ , highlighting the error-minimizing value of $\\lambda$ . Despite the relatively small values of $n$ and $p$ , there is a very precise match between the asymptotic theory and the practical experiment. This is particularly evident in the accurate estimation of the optimal value for $\\lambda$ . ", "page_idx": 6}, {"type": "image", "img_path": "FFW6rPz48Z/tmp/388d40d15794d0b2af5bc41677035c1af5d6ba3f82a898a715e2da1423505c09.jpg", "img_caption": ["Figure 2: Empirical and theoretical train and test MSE as functions of the parameter $\\lambda$ for different values of $\\alpha$ . The smooth curves represent the theoretical predictions, while the corresponding curves with the same color show the empirical results, highlighting that the empirical observations indeed match the theoretical predictions. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "FFW6rPz48Z/tmp/62ff32055433d7a71a21593bf428b65c1a5b107865c1bde2ea4531ae97076cfc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Theoretical vs Empirical MSE as function of regularization parameter $\\lambda$ . Close fti between the theoretical and the empirical predictions which underscores the robustness of the theory in light of varying assumptions as well as the accuracy of the suggested estimates. We consider the first two channels as the the two tasks and $d=144.\\;95$ samples are used for the training and 42 samples are used for the test. ", "page_idx": 7}, {"type": "text", "text": "5 Hyperparameter Optimization in Practice ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Empirical Estimation of Task-wise Signal, Cross Signal, and Noise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "All the quantities defined in Theorem 1 are known except for the bilinear expressions $\\begin{array}{r}{\\frac{1}{T d}\\mathrm{tr}(\\mathbf{W}^{\\top}\\mathbf{M}\\mathbf{W})}\\end{array}$ and $\\begin{array}{r}{{\\frac{1}{T d}}\\mathrm{tr}\\left(\\sum_{N}\\mathbf{M}\\right)}\\end{array}$ . These quantities can be consistently estimated under Assumptions 2 as follows (see details in Section $\\mathrm{F}$ of the Appendix) : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{1}{T d}\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{M}\\mathbf{W}\\right)-\\zeta(\\mathbf{M})\\xrightarrow{\\mathrm{a.s.}}0,\\qquad\\frac{1}{T d}\\mathrm{tr}\\left(\\pmb{\\Sigma}_{N}\\mathbf{M}\\right)-\\hat{\\sigma}\\mathrm{trM}\\xrightarrow{\\mathrm{a.s.}}0}\\\\ &{}&{\\displaystyle\\geq\\zeta(\\mathbf{M})=\\frac{1}{T d}\\mathrm{tr}(\\hat{\\mathbf{W}}^{\\top}\\kappa^{-1}(\\mathbf{M})\\mathbf{M}\\hat{\\mathbf{W}})-\\frac{\\hat{\\sigma}}{T d}\\mathrm{tr}\\hat{\\mathbf{Q}}_{2}(\\mathbf{A}^{\\frac{1}{2}}\\kappa^{-1}(\\mathbf{M})\\mathbf{A}^{\\frac{1}{2}}).\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We define the estimate of the noise as $\\hat{\\sigma}\\,=\\,\\operatorname*{lim}_{\\lambda\\to0}\\,\\mathcal{R}_{t r a i n}^{\\infty}$ and the function $\\kappa^{-1}$ is the functional inverse of the mapping $\\kappa:\\mathbb{R}^{T d\\times T d}\\rightarrow\\mathbb{R}^{q\\times q}$ defined as follows ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\kappa(\\mathbf{M})=\\mathbf{M}-2\\mathbf{A}^{-{\\frac{1}{2}}}{\\tilde{\\bar{\\mathbf{Q}}}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}+\\mathbf{A}^{-{\\frac{1}{2}}}{\\tilde{\\bar{\\mathbf{Q}}}}_{2}(\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}})\\mathbf{A}^{-{\\frac{1}{2}}}-{\\frac{n}{(T d)^{2}}}\\mathbf{A}^{-{\\frac{1}{2}}}\\Sigma\\mathbf{A}^{-{\\frac{1}{2}}}{\\tilde{\\bar{\\mathbf{Q}}}}_{2}(\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5.2 Application to Multi-task Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our study, we apply the theoretical framework presented in our paper to a real-world regression problem, specifically, the Appliance Energy dataset which aims to predict the total usage of a house. This dataset is a multivariate regression dataset containing 138 time series each of dimension 24. We select two of these features as 2 tasks to cast the problem as a multi-task learning regression problem. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 presents a comparison between the theoretical predictions and empirical outcomes of our proposed linear framework. Despite the assumptions made in the main body of the paper, the theoretical predictions closely align with the experimental results. This demonstrates that our estimates are effective in practice and provide a reliable approximation of the optimal regularization. ", "page_idx": 7}, {"type": "text", "text": "In essence, our theoretical framework, when applied to real-world multi-task learning regression problems, yields practical and accurate estimates, thereby validating its effectiveness and applicability. ", "page_idx": 7}, {"type": "text", "text": "5.3 Relevance of the theoretical insights beyond the case of linear models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While non-linear models are widely used, establishing their theoretical foundations is challenging. Therefore, we focused on linear models, which, despite their simplicity, provide valuable insights into more complex models. ", "page_idx": 7}, {"type": "text", "text": "Our results show that test risk curves for non-linear models follow patterns predicted by our theory. This is expected because non-linear models in time series forecasting typically use a linear output layer for prediction. Thus, we can apply our theory to the inputs of this final linear layer. This approach is valid due to data concentration and the Lipschitz nature of neural networks, ensuring outputs of the non-linear part don\u2019t deviate significantly from the inputs. ", "page_idx": 8}, {"type": "text", "text": "Moreover, multivariate time series models often treat channels separately using univariate methods, missing cross-channel information. Our results in Section 5.4 show that our method surpasses univariate baselines by optimally regularizing with $\\lambda$ and $\\gamma$ , supporting our theory\u2019s applicability to non-linear models as the final linear layer effectively leverages concentrated inputs. ", "page_idx": 8}, {"type": "text", "text": "Finally, our regularization approach differs from traditional cross-task regularizations that use one task per dataset. We consider each prediction as a task and introduce $\\gamma_{t}$ parameters alongside $\\lambda$ . These parameters enforce multivariate regularization and control underfitting or overfitting per task. This method is tractable since it\u2019s applied at the model\u2019s final layer. ", "page_idx": 8}, {"type": "text", "text": "The similarity between curves for non-linear and linear models indicates our findings are robust; nonlinear models also exhibit optimal regularization parameters, enhancing performance in multivariate forecasting. ", "page_idx": 8}, {"type": "text", "text": "5.4 Application to Multivariate Time Series Forecasting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our theoretical framework is applied in the context of Multivariate Time Series Forecasting, with related work detailed in Appendix G.1. We previously applied this framework in a linear setting, and now aim to evaluate its empirical validity in the non-linear setting of neural networks. The results presented in this section represent the best test MSE, assuming the ability to find the optimal lambda value, which can be considered as an oracle scenario. A study of these limitations can be found in Appendix H. ", "page_idx": 8}, {"type": "text", "text": "Motivation. Our approach is applied to the MTSF setting for several reasons. Firstly, many models currently used are essentially univariate, where predictions for individual series are simply concatenated without exploiting the multivariate information inherent in traditional benchmarks. Given that these benchmarks are designed for multivariate forecasting, leveraging multivariate information should yield better results. Secondly, our theoretical framework can benefti this domain, as most predictive models use a linear layer on top of the model to project historical data of length $d$ for predicting the output of length $q$ . This characteristic aligns well with our method, making it a promising fit for enhancing forecasting accuracy. ", "page_idx": 8}, {"type": "text", "text": "Our approach. We propose a novel method for MTSF by modifying the loss function to incorporate both individual feature transformations $f_{t}$ and a shared transformation $f_{0}$ . Each univariate-specific transformation $f_{t}$ is designed to capture the unique dynamics of its respective feature, while $f_{0}$ serves as a common transformation applied across all features to capture underlying patterns shared among them. We consider a neural network $f$ with inputs $\\mathbf{X}=[\\mathbf{X}^{(1)},\\mathbf{X}^{(2)},\\ldots,\\mathbf{X}^{(T)}]$ , where $T$ is the number of channels and $\\mathbf{X}^{\\left(t\\right)}\\in\\mathbb{R}^{n\\times d}$ . For a univariate model without MTL regularization, we predict $\\mathbf{Y}=[\\mathbf{Y}^{(1)},\\mathbf{Y}^{(2)},\\ldots,\\mathbf{Y}^{(T)}]=[f_{1}(\\mathbf{X}^{(1)}),\\ldots,f_{T}(\\mathbf{X}^{(T)})]$ and $\\mathbf{Y}^{(t)}\\in\\mathbb{R}^{n\\times q}$ . We compare these models with their corresponding versions that include MTL regularization, formulated as: $f_{t}^{M T L}(\\mathbf{X}^{(t)})=f_{t}(\\mathbf{X}^{(t)})+f_{0}(\\mathbf{Y})$ with $f_{t}:\\mathbb{R}^{n\\times d}\\rightarrow\\mathbb{R}^{n\\times q}$ and $f_{0}:\\mathbb{R}^{n\\times q T}\\rightarrow\\mathbb{R}^{n\\times q}$ . We define our regularized loss as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{X},\\mathbf{Y})=\\sum_{t=1}^{T}\\|\\mathbf{Y}^{(t)}-f_{t}^{M T L}(\\mathbf{X}^{(t)})\\|_{F}^{2}+\\lambda\\|f_{0}(\\mathbf{X})\\|_{F}^{2}+\\sum_{t=1}^{T}\\gamma_{t}\\|f_{t}(\\mathbf{X}^{(t)}\\|_{F}^{2},\\quad\\forall t\\in\\{1,\\ldots,T\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{Y}^{(t)}$ are the true predictions, $f_{t}$ represents the univariate model for each channel $t$ , and $\\lambda$ is our regularization parameter, for which we have established a closed form in the case of linear $f_{t}.~f_{0}$ serves a role equivalent to $W_{0}$ , which was defined in our theoretical study and allows for the regularization of the common part. This component can be added at the top of a univariate model. The parameters $\\gamma_{t}$ enable the regularization of the specialized parts $f_{t}$ . ", "page_idx": 8}, {"type": "text", "text": "In our setup, $f_{t}$ is computed in a similar way as in the model without regularization and $f_{0}$ is computed by first flattening the concatenation of the predictions of $\\mathbf{X}^{(t)}$ , then applying a linear projection leveraging common multivariate information before reshaping. The loss function is specifically designed to balance fitting the multivariate series using $f_{0}$ and the specific channels using $f_{t}$ . This approach enhances the model\u2019s generalization across various forecasting horizons and datasets. More details on our regularized loss function can be found in Appendix G.2 ", "page_idx": 8}, {"type": "table", "img_path": "FFW6rPz48Z/tmp/277c4af800cb7fbc3e26b35e2f75d6e59cb3e2490c36e204040f108a4823d000.jpg", "table_caption": ["Table 1: MTL regularization results. Algorithms marked with \u2020 are state-of-the-art multivariate models and serve as baseline comparisons. All others are univariate. We compared the models with MTL regularization to their corresponding versions without regularization. Each MSE value is derived from 3 different random seeds. MSE values marked with \\* indicate that the model with MTL regularization performed significantly better than its version without regularization, according to a Student\u2019s t-test with a p-value of 0.05. MSE values are in bold when they are the best in their row, indicating the top-performing models. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results. We present experimental results on different forecasting horizons, using common benchmark MTSF datasets, the characteristics of which are outlined in the Appendix G.3. Our models include PatchTST [32], known to be on par with state-of-the-art in MTSF while being a univariate model, a univariate DLinear version called DLinearU compared to its multivariate counterpart DLinearM [53], and a univariate Transformer [19] with temporal-wise attention compared to the multivariate state-of-the-art models SAMformer [19] and iTransformer [24] . Table 1 provides a detailed comparison of the test mean squared errors (MSE) for different MTSF models, emphasizing the impact of MTL regularization. Models with MTL regularization are compared to their versions without regularization, as well as SAMformer and iTransformer. All the experiments can be found in Appendix G.4. ", "page_idx": 9}, {"type": "text", "text": "Adding MTL regularization improves the performance of PatchTST, DLinearU, and Transformer in most cases. When compared to state-of-the-art multivariate models, the MTL-regularized models are often competitive. SAMformer is outperformed by at least one MTL-regularized method per horizon and dataset, except for ETTh1 with horizons of 336 and 720. iTransformer is consistently outperformed by at least one MTL-regularized methods regardless of the dataset and horizon. ", "page_idx": 9}, {"type": "text", "text": "The best performing methods are PatchTST and DLinearU with MTL regularization. These models not only outperform their non-regularized counterparts, often significantly as shown by Student\u2019s t-tests with a p-value of 0.05, but also surpass state-of-the-art multivariate models like SAMformer and Transformer. This superior performance is indicated by the bold values in the table. ", "page_idx": 9}, {"type": "text", "text": "Finally, MTL regularization enhances the performance of univariate models, making them often competitive with state-of-the-art multivariate methods like SAMformer and iTransformer. This approach seems to better captures shared dynamics among tasks, leading to more accurate forecasts. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this article, we have explored linear multi-task learning by deriving a closed-form solution for an optimization problem that capitalizes on the wealth of information available across multiple tasks. Leveraging Random Matrix Theory, we have been able to obtain the asymptotic training and testing risks, and have proposed several insights into high-dimensional multi-task learning regression. Our theoretical analysis, though based on a simplified model, has been effectively applied to multi-task regression and multivariate forecasting, using both synthetic and real-world datasets. We believe that our work lays a solid foundation for future research, paving the way for using random matrix theory with more complex models, such as deep neural networks, within the multi-task learning framework. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Work partially funded by EU project AI4Europe (101070000). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ando, R. K., Zhang, T., and Bartlett, P. (2005). A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of machine learning research, 6(11).   \n[2] Bai, Z. and Silverstein, J. W. (2010). Spectral Analysis of Large Dimensional Random Matrices. Springer Series in Statistics. Springer New York, NY.   \n[3] Box, G. E. P. and Jenkins, G. (1990). Time Series Analysis, Forecasting and Control. Holden-Day, Inc., USA.   \n[4] Box, G. E. P., Jenkins, G. M., and MacGregor, J. F. (1974). Some Recent Advances in Forecasting and Control. Journal of the Royal Statistical Society Series C, 23(2):158\u2013179.   \n[5] Caruana, R. (1997). Multitask learning. Machine learning, 28:41\u201375.   \n[6] \u02c7Cepulionis, P. and Luko\u0161evi\u02c7ci\u00afut\u02d9e, K. (2016). Electrocardiogram time series forecasting and optimization using ant colony optimization algorithm. Mathematical Models in Engineering, 2(1):69\u201377.   \n[7] Chen, R. and Tao, M. (2021). Data-driven prediction of general hamiltonian dynamics via learning exactly-symplectic maps. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1717\u20131727. PMLR.   \n[8] Chen, S.-A., Li, C.-L., Arik, S. O., Yoder, N. C., and Pfister, T. (2023). TSMixer: An all-MLP architecture for time series forecasting. Transactions on Machine Learning Research.   \n[9] Clart\u00e9, L., Loureiro, B., Krzakala, F., and Zdeborov\u00e1, L. (2023). Theoretical characterization of uncertainty in high-dimensional linear classification. Machine Learning: Science and Technology, 4(2):025029.   \n[10] Couillet, R. and Debbah, M. (2011). Random matrix methods for wireless communications. Cambridge University Press.   \n[11] Couillet, R. and Liao, Z. (2022). Random matrix methods for machine learning. Cambridge University Press.   \n[12] Dobriban, E. and Wager, S. (2018). High-dimensional asymptotics of prediction: Ridge regression and classification. The Annals of Statistics, 46(1):247\u2013279.   \n[13] Erdo\u02dds, L. and Yau, H.-T. (2017). A Dynamical Approach to Random Matrix Theory, volume 28 of Courant Lecture Notes. American Mathematical Society.   \n[14] Feofanov, V., Tiomoko, M., and Virmaux, A. (2023). Random matrix analysis to balance between supervised and unsupervised learning under the low density separation assumption. In Proceedings of the 40th International Conference on Machine Learning, pages 10008\u201310033.   \n[15] Gerbelot, C., Abbara, A., and Krzakala, F. (2022). Asymptotic errors for teacher-student convex generalized linear models (or: How to prove kabashima\u2019s replica formula). IEEE Transactions on Information Theory, 69(3):1824\u20131852.   \n[16] Hachem, W., Loubaton, P., and Najim, J. (2007). Deterministic equivalents for certain functionals of large random matrices.   \n[17] Hu, Y., Li, M., Lu, Q., Weng, H., Wang, J., Zekavat, S. M., Yu, Z., Li, B., Gu, J., Muchnik, S., et al. (2019). A statistical framework for cross-tissue transcriptome-wide association analysis. Nature genetics, 51(3):568\u2013576.   \n[18] Ilbert, R., Hoang, T. V., and Zhang, Z. (2024a). Data augmentation for multivariate time series classification: An experimental study. In 2024 IEEE 40th International Conference on Data Engineering Workshops (ICDEW), pages 128\u2013139.   \n[19] Ilbert, R., Odonnat, A., Feofanov, V., Virmaux, A., Paolo, G., Palpanas, T., and Redko, I. (2024b). Unlocking the potential of transformers in time series forecasting with sharpness-aware minimization and channel-wise attention.   \n[20] Ilbert, R., Tiomoko, M., Louart, C., Feofanov, V., Palpanas, T., and Redko, I. (2024c). Enhancing multivariate time series forecasting via multi-task learning and random matrix theory. In NeurIPS Workshop on Time Series in the Age of Large Models.   \n[21] Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. (2021). Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations.   \n[22] Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA.   \n[23] Li, S., Cai, T. T., and Li, H. (2022). Transfer learning for high-dimensional linear regression: Prediction, estimation and minimax optimality. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):149\u2013173.   \n[24] Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. (2024). itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations.   \n[25] Louart, C. and Couillet, R. (2021). Spectral properties of sample covariance matrices arising from random matrices with independent non identically distributed columns. arXiv preprint arXiv:2109.02644.   \n[26] Lounici, K., Pontil, M., Tsybakov, A. B., and Van De Geer, S. (2009). Taking advantage of sparsity in multi-task learning. arXiv preprint arXiv:0903.1468.   \n[27] Max Planck Institute (n.d.). Weather dataset.   \n[28] Mei, S., Fei, W., and Zhou, S. (2011). Gene ontology based transfer learning for protein subcellular localization. BMC bioinformatics, 12:1\u201312.   \n[29] Mousavi Kalan, M., Fabian, Z., Avestimehr, S., and Soltanolkotabi, M. (2020). Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks. Advances in Neural Information Processing Systems, 33:1959\u20131969.   \n[30] Nguyen, M.-T. and Couillet, R. (2023). Asymptotic bayes risk of semi-supervised multitask learning on gaussian mixture. In International Conference on Artificial Intelligence and Statistics, pages 5063\u20135078. PMLR.   \n[31] Nica, A. and Speicher, R. (2006). Lectures on the Combinatorics of Free Probability. London Mathematical Society Lecture Note Series. Cambridge University Press.   \n[32] Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. (2023). A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations.   \n[33] Niu, S., Liu, Y., Wang, J., and Song, H. (2020). A decade survey of transfer learning (2010\u2013 2020). IEEE Transactions on Artificial Intelligence, 1(2):151\u2013166.   \n[34] Potters, M., Bouchaud, J.-P., and Laloux, L. (2005). Financial applications of random matrix theory: Old laces and new pieces. Acta Physica Polonica B, 36(9):2767.   \n[35] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1\u201367. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[36] Romera-Paredes, B., Aung, H., Bianchi-Berthouze, N., and Pontil, M. (2013). Multilinear multitask learning. In International Conference on Machine Learning, pages 1444\u20131452. PMLR. ", "page_idx": 12}, {"type": "text", "text": "[37] Ruder, S., Peters, M. E., Swayamdipta, S., and Wolf, T. (2019). Evolution of transfer learning in natural language processing. arXiv preprint arXiv:1910.07370.   \n[38] Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T. (2020). Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181\u20131191.   \n[39] Seddik, M. E. A., Louart, C., Tamaazousti, M., and Couillet, R. (2020). Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures. In International Conference on Machine Learning.   \n[40] Sen, R., Yu, H.-F., and Dhillon, I. (2019). Think globally, act locally: a deep neural network approach to high-dimensional time series forecasting. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, Red Hook, NY, USA. Curran Associates Inc.   \n[41] Shao, L. (2015). Transfer learning for visual categorization: a survey. IEEE Transactions on Neural Networks and Learning Systems, 26(5):1019\u20131034.   \n[42] Shin, H.-C., Roth, H. R., Gao, M., Lu, L., Xu, Z., Nogues, I., Yao, J., Mollura, D., and Summers, R. M. (2016). Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning. IEEE transactions on medical imaging, 35(5):1285\u20131298.   \n[43] Sifaou, H., Kammoun, A., and Alouini, M.-S. (2021). A precise performance analysis of support vector regression. In International Conference on Machine Learning, pages 9671\u20139680. PMLR.   \n[44] Sonkavde, G., Dharrao, D. S., Bongale, A. M., Deokate, S. T., Doreswamy, D., and Bhat, S. K. (2023). Forecasting stock market prices using machine learning and deep learning models: A systematic review, performance analysis and discussion of implications. International Journal of Financial Studies, 11(3).   \n[45] Sorjamaa, A., Hao, J., Reyhani, N., Ji, Y., and Lendasse, A. (2007). Methodology for long-term prediction of time series. Neurocomputing, 70(16):2861\u20132869. Neural Network Applications in Electrical Engineering Selected papers from the 3rd International Work-Conference on Artificial Neural Networks (IWANN 2005).   \n[46] Tao, T. (2012). Topics in Random Matrix Theory, volume 132 of Graduate Studies in Mathematics. American Mathematical Society.   \n[47] Tiomoko, M., Couillet, R., and Pascal, F. (2023). PCA-based multi-task learning: a random matrix approach. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 34280\u201334300. PMLR.   \n[48] Tiomoko, M., Couillet, R., and Tiomoko, H. (2020). Large dimensional analysis and improvement of multi task learning.   \n[49] UCI (n.d.). Electricity dataset.   \n[50] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press.   \n[51] Wu, H., Xu, J., Wang, J., and Long, M. (2021). Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting. In Advances in Neural Information Processing Systems.   \n[52] Yang, F., Zhang, H. R., Wu, S., R\u00e9, C., and Su, W. J. (2023). Precise high-dimensional asymptotics for quantifying heterogeneous transfers.   \n[53] Zeng, A., Chen, M., Zhang, L., and Xu, Q. (2023). Are transformers effective for time series forecasting? In Proceedings of the AAAI Conference on Artificial Intelligence.   \n[54] Zhang, H., Zheng, Y., and Qi, D. (2019). Multi-task learning for time series forecasting using neural networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1305\u20131313. ACM.   \n[55] Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. (2021). Informer: Beyond efficient transformer for long sequence time-series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference, volume 35, pages 11106\u201311115. AAAI Press.   \n[56] Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. (2022). FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022). ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Roadmap. This appendix provides the technical details omitted in the main paper. It starts with an overview of the setup considered in the paper in Section A. Section B offers a detail computation for $\\hat{\\mathbf{W}}_{t}$ and $\\hat{\\mathbf{W}}_{0}$ . Section C contains a proof of Lemma 1. Section D explains the theoretical steps for deriving the training and test risks, as well as the deterministic equivalents. Section E discusses the technical tools used to derive the main intuitions presented by the theory. Section F focuses on the derivation of the estimations of the main quantities involved in the training and test risks. Section G complements the experimental study by presenting additional experiments. Finally, Section H deals with the limitations of our approach in a non-linear setting. ", "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Multi-task Learning setup 16 ", "page_idx": 14}, {"type": "text", "text": "A.1 On the zero-mean assumption 16   \nA.2 On the Assumption 1 . 16   \nA.3 On the Assumption 2 16 ", "page_idx": 14}, {"type": "text", "text": "B Minimization Problem 16 ", "page_idx": 14}, {"type": "text", "text": "B.1 Computation of $\\hat{\\mathbf{W}}_{t}$ and $\\hat{\\mathbf{W}}_{0}$ 16 ", "page_idx": 14}, {"type": "text", "text": "C Lemma 1 and proof with Random Matrix Theory 18 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Lemma 1 18   \nC.2 Deterministic equivalent of the resolvent $\\tilde{\\mathbf{Q}}$ 18   \nC.3 Deterministic equivalent of bilinear forms of the resolvent 19   \nC.4 Estimation of the deterministic equivalent of ${\\bf{Q}}^{2}$ . 21 ", "page_idx": 14}, {"type": "text", "text": "D Risk Estimation (Proof of Theorem 1) 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Test Risk 22   \nD.2 Train Risk 23 ", "page_idx": 14}, {"type": "text", "text": "E Interpretation and insights of the theoretical analysis 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Analysis of the test risk 23   \nE.2 Interpretation of the signal term 24   \nE.3 Interpretation and insights of the noise terms . . 24   \nE.4 Optimal Lambda 24 ", "page_idx": 14}, {"type": "text", "text": "F Theoretical Estimations 25 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Estimation of the training and test risk 25   \nF.2 Estimation of the noise covariance 25 ", "page_idx": 14}, {"type": "text", "text": "G Multivariate Time Series Forecasting 25 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 Related Work 25   \nG.2 Architecture and Training Parameters 26   \nG.3 Datasets 26 ", "page_idx": 14}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Multi-task Learning setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 On the zero-mean assumption ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We would like to note that we are assuming that both the noise $\\varepsilon$ and the feature xi(t)have zero mean. This is a common assumption in many statistical models and it simplifies the analysis. However, this assumption is not restrictive. In practice, if the data or the response variable are not centered, we can always preprocess the data by subtracting the mean. This preprocessing step brings us back to the zero-mean setting that we consider in our theoretical analysis. ", "page_idx": 15}, {"type": "text", "text": "A.2 On the Assumption 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Data are concentrated random vectors, meaning high-dimensional data maintain stable under complex (Lipschitz) transformations. The strong performance of neural networks on tasks like image recognition and NLP suggests that these models produce stable predictions. As Lipschitz transformations, they maintain controlled distances between inputs, ensuring stability. Recent studies have demonstrated and experimentally confirmed that both real-world data and synthetically generated data using GANs exhibit concentration properties, supporting this assumption. This makes our assumption more realistic than traditional Gaussian assumptions, as it does not rely on specific hypotheses about the shape of the data distribution, but rather on the stability of statistical properties after transformation. Consequently, analyzing a framework of concentrated random vectors is more theoretically challenging than using Gaussian assumptions and represents a key novelty of our theory. ", "page_idx": 15}, {"type": "text", "text": "A.3 On the Assumption 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The dimension $d$ is of the same order of magnitude as the sample size $n$ . This joint growth captures data complexity better than assuming a fixed feature size with increasing samples, which can oversimplify models. Our theory works for fixed $d$ and $n$ , unbiased by specific parameter choices. The accuracy of empirical predictions depends on both $d$ and $n$ , with variance scaling as $\\textstyle O\\left({\\frac{1}{\\sqrt{d n}}}\\right)$ . Larger $d$ and $n$ reduce variance, making empirical results more reliable and closer to theoretical values. Conversely, smaller $d$ and $n$ increase variance, affecting single predictions. However, this scaling is still better than $n$ growing indefinitely with fixed $d$ , where variance scales as $\\textstyle O\\left({\\frac{1}{\\sqrt{n}}}\\right)$ , leading to increased bias. Implication of the assumptions. In Section 5.2, Figure 3, the Appliance Energy dataset illustrates our assumptions\u2019 realism. Despite the moderate sample size of 42 samples with 142 dimensions and non-synthetic data, the theoretical curve ftis the empirical predictions well. Additionally, with synthetic data ( $d=100$ , $n=100)$ ), the theory matches the empirical curve across various hyperparameters (Figure 2), showcasing the predictive power of Random Matrix Theory. ", "page_idx": 15}, {"type": "text", "text": "B Minimization Problem ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Computation of $\\hat{\\mathbf{W}}_{t}$ and $\\hat{\\mathbf{W}}_{0}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proposed multi task regression finds $\\hat{\\mathbf{W}}\\;=\\;[\\hat{\\mathbf{W}}_{1}^{\\top},\\hdots,\\hat{\\mathbf{W}}_{k}^{\\top}]^{\\top}\\;\\in\\;\\mathbb{R}^{d T\\times q}$ which solves the following optimization problem using the additional assumption of relatedness between the tasks $(\\mathbf{W}_{t}=\\mathbf{W}_{0}+\\mathbf{V}_{t}$ for all tasks $t$ ): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\mathbf{W}_{0},\\mathbf{V})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times T\\times\\mathbb{R}^{T}}\\mathcal{I}(\\mathbf{W}_{0},\\mathbf{V})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}(\\mathbf{W}_{0},\\mathbf{V})\\equiv\\displaystyle\\frac{1}{2\\lambda}\\mathrm{tr}\\left(\\mathbf{W}_{0}^{\\top}\\mathbf{W}_{0}\\right)+\\displaystyle\\frac{1}{2}\\sum_{t=1}^{T}\\frac{\\mathrm{tr}\\left(\\mathbf{V}_{t}^{\\top}\\mathbf{V}_{t}\\right)}{\\gamma_{t}}+\\frac{1}{2}\\sum_{t=1}^{T}\\mathrm{tr}\\left(\\pmb{\\xi}_{t}^{\\top}\\pmb{\\xi}_{t}\\right)}\\\\ &{\\quad\\quad\\quad\\xi_{t}=\\mathbf{Y}^{(t)}-\\displaystyle\\frac{\\mathbf{X}^{(t)^{\\top}}\\mathbf{W}_{t}}{\\sqrt{T d}},\\quad\\forall t\\in\\{1,\\ldots,T\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The Lagrangian introducing the lagrangian parameters for each task $t$ , $\\pmb{\\alpha}_{t}\\in\\mathbb{R}^{n_{t}\\times q}$ reads as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{L}(\\mathbf{W}_{0},\\mathbf{V}_{t},\\xi_{t},\\alpha_{t})=\\displaystyle\\frac{1}{2\\lambda}\\mathrm{tr}\\left(\\mathbf{W}_{0}^{\\top}\\mathbf{W}_{0}\\right)+\\displaystyle\\frac{1}{2}\\sum_{t=1}^{T}\\frac{\\mathrm{tr}\\left(\\mathbf{V}_{t}^{\\top}\\mathbf{V}_{t}\\right)}{\\gamma_{t}}+\\displaystyle\\frac{1}{2}\\sum_{t=1}\\mathrm{tr}\\left(\\xi_{t}^{\\top}\\xi_{t}\\right)}&\\\\ &{\\quad}&{+\\displaystyle\\sum_{t=1}^{T}\\mathrm{tr}\\left(\\alpha_{t}^{\\top}\\left(\\mathbf{Y}^{(t)}-\\frac{\\mathbf{X}^{(t)}^{\\top}\\left(\\mathbf{W}_{0}+\\mathbf{V}_{t}\\right)}{\\sqrt{T d}}-\\xi_{t}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Differentiating with respect to the unknown variables $\\hat{\\mathbf{W}}_{0},\\hat{\\mathbf{V}}_{t},\\xi_{t},\\alpha_{t}$ and ${\\bf b}_{t}$ , we get the following system of equation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{1}{\\lambda}\\hat{\\mathbf{W}}_{0}-\\sum_{t=1}^{T}\\frac{\\mathbf{X}^{(t)}\\alpha_{t}}{\\sqrt{T d}}=0}\\\\ {\\displaystyle\\frac{1}{\\gamma_{t}}\\hat{\\mathbf{V}}_{t}-\\frac{\\mathbf{X}^{(t)}\\alpha_{t}}{\\sqrt{T d}}=0}\\\\ {\\displaystyle\\hat{\\xi}_{t}-\\alpha_{t}=0}\\\\ {\\displaystyle\\mathbf{Y}^{(t)}-\\frac{\\mathbf{X}^{(t)^{\\top}}\\hat{\\mathbf{W}}_{0}}{\\sqrt{T d}}-\\frac{\\mathbf{X}^{(t)^{\\top}}\\hat{\\mathbf{V}}_{t}}{\\sqrt{T d}}-\\xi_{t}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging the expression of $\\hat{\\mathbf{W}}_{0},\\hat{\\mathbf{V}}_{t}$ and $\\xi_{t}$ into the expression of $\\mathbf{Y}^{(t)}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(t)}=\\lambda\\sum_{t=1}^{T}\\frac{\\mathbf{X}^{(t)}^{\\top}\\mathbf{X}^{(t)}}{T d}\\alpha_{t}+\\gamma_{t}\\frac{\\mathbf{X}^{(t)}^{\\top}\\mathbf{X}^{(t)}}{T d}\\alpha_{t}+\\alpha_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which can be rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(t)}=\\left(\\lambda+\\gamma_{t}\\right)\\frac{\\mathbf{X}^{{(t)}^{\\top}}\\mathbf{X}^{(t)}}{T d}\\alpha_{t}+\\lambda\\sum_{v\\neq t}\\frac{\\mathbf{X}^{{(t)}^{\\top}}\\mathbf{X}^{(v)}}{T d}\\alpha_{v}+\\alpha_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With $\\begin{array}{r l r l r l r l r l}{{\\mathbf Y}}&{{}=}&{[\\underline{{\\mathbf Y}}_{*}^{(1)^{\\top}},\\dots,\\underline{{\\mathbf Y}}^{(T)^{\\top}}]^{\\top}}&{\\in}&{{}\\mathbb{R}^{n\\times q},}&{\\alpha}&{{}=}&{[\\alpha_{1}^{\\top},\\dots,\\alpha_{k}^{\\top}]^{\\top}}&{\\in}&{{}\\mathbb{R}^{n\\times q},}\\end{array}$ $\\begin{array}{r}{\\mathbf{Z}=\\sum_{t=1}^{T}\\mathbf{e}_{t}^{[T]}\\mathbf{e}_{t}^{[T]}\\mathbf{\\Lambda}^{\\top}\\otimes\\mathbf{X}^{(t)}\\in\\mathbb{R}^{T d\\times n}}\\end{array}$ , this system of equations can be written under the following compact matrix form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Q}^{-1}\\alpha=\\mathbf{Y}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Q}=\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}+\\mathbf{I}_{n}\\right)^{-1}\\in\\mathbb{R}^{n\\times n},\\,\\mathrm{and}\\;\\mathbf{A}=\\left(\\mathcal{D}_{\\gamma}+\\lambda\\mathbb{1}_{T}\\mathbb{1}_{T}^{\\top}\\right)\\otimes\\mathbf{I}_{d}\\in\\mathbb{R}^{T d\\times T d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Solving for $_{\\alpha}$ then gives: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha=\\mathbf{QY}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, using $\\hat{\\mathbf{W}}_{t}=\\hat{\\mathbf{W}}_{0}+\\hat{\\mathbf{V}}_{t}$ , the expression of $\\mathbf{W}_{t}$ becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{W}}_{t}=\\left(\\mathbf{e}_{t}^{[T]}\\right)^{\\top}\\otimes\\mathbf{I}_{d}\\right)\\frac{\\mathbf{A}\\mathbf{Z}\\alpha}{\\sqrt{T d}},}\\\\ {\\hat{\\mathbf{W}}_{0}=\\left(\\mathbb{1}_{T}^{\\top}\\otimes\\lambda\\mathbf{I}_{d}\\right)\\frac{\\mathbf{Z}\\alpha}{\\sqrt{T d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C Lemma 1 and proof with Random Matrix Theory ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Deterministic equivalents for $\\tilde{\\mathbf{Q}}$ , $\\tilde{\\bf Q}\\mathbf{M}\\tilde{\\bf Q}$ and $\\mathbf{Q}^{2}$ for any $\\textbf{M}\\in\\ \\mathbb{R}^{n\\times n};$ ). Under the concentrated random vector assumption for each feature vector $\\mathbf{x}_{i}^{(t)}$ and under the growth rate assumption (Assumption 2), for any deterministic $\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$ , we have the following convergence: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{Q}}\\leftrightarrow\\bar{\\tilde{\\mathbf{Q}}},\\qquad\\tilde{\\mathbf{Q}}\\mathbf{M}\\tilde{\\mathbf{Q}}\\leftrightarrow\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{M}),\\qquad\\qquad\\mathbf{Q}^{2}\\leftrightarrow\\bar{\\mathbf{Q}}_{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\bar{\\tilde{\\bf Q}}_{2}$ , $\\bar{\\tilde{\\mathbf{Q}}}$ and $\\bar{\\bf Q}_{2}$ are defined as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\bar{\\dot{\\mathbf{Q}}}=\\left(\\sum_{t=1}^{T}\\frac{c_{0}\\mathbf{C}^{(t)}}{1+\\delta_{t}}+\\mathbf{I}_{T d}\\right)^{-1},\\quad\\delta_{t}=\\frac{1}{T d}\\mathrm{tr}\\left(\\mathbf{\\Sigma}^{(t)}\\bar{\\dot{\\mathbf{Q}}}\\right),\\quad\\mathbf{C}^{(t)}=\\mathbf{A}^{\\frac{1}{2}}\\left(\\mathbf{e}_{t}^{[T]}\\otimes\\mathbf{\\Sigma}^{(t)}\\right)\\mathbf{A}^{\\frac{1}{2}}}\\\\ {\\displaystyle\\bar{\\dot{\\mathbf{Q}}}_{2}(\\mathbf{M})=\\bar{\\dot{Q}}\\mathbf{M}\\bar{\\dot{\\mathbf{Q}}}+\\frac{1}{T d}\\sum_{t=1}^{T}\\frac{d_{t}}{1+\\delta_{t}}\\bar{\\dot{\\mathbf{Q}}}\\mathbf{C}^{(t)}\\bar{\\dot{\\mathbf{Q}}},\\quad\\quad\\mathbf{d}=\\left(\\mathbf{I}_{T}-\\frac{1}{T d}\\boldsymbol{\\Psi}\\right)^{-1}\\boldsymbol{\\Psi}(\\mathbf{M})\\in\\mathbb{R}^{T}}\\\\ {\\displaystyle\\bar{\\mathbf{Q}}_{2}=\\mathbf{I}_{n}-D i a g_{t\\in[T]}(v_{t}\\mathbf{I}_{n}),\\quad v_{t}=\\frac{1}{T d}\\frac{\\mathrm{tr}(\\mathbf{C}^{(t)}\\bar{\\dot{\\mathbf{Q}}})}{(1+\\delta_{t})^{2}}+\\frac{1}{T d}\\frac{\\mathrm{tr}\\left(\\mathbf{C}^{(t)}\\bar{\\dot{\\mathbf{Q}}}_{2}(\\mathbf{I}_{n})\\right)}{(1+\\delta_{t})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{I}(M)=\\left(\\frac{n_{t}}{T d}\\frac{\\operatorname{tr}\\left(\\mathbf{C}^{(t)}\\bar{\\mathbf{Q}}\\mathbf{M}\\bar{\\tilde{\\mathbf{Q}}}\\right)}{1+\\delta_{t}}\\right)_{t\\in[T]}\\in\\mathbb{R}^{T},\\qquad\\Psi=\\left(\\frac{n_{t}}{T d}\\frac{\\operatorname{tr}\\left(\\mathbf{C}^{(t)}\\bar{\\mathbf{Q}}\\mathbf{C}^{(t^{\\prime})}\\bar{\\tilde{\\mathbf{Q}}}\\right)}{(1+\\delta_{t})(1+\\delta_{t^{\\prime}})}\\right)_{t,t^{\\prime}\\in[T]}\\in\\mathbb{R}^{T\\times T}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 Deterministic equivalent of the resolvent $\\tilde{\\mathbf{Q}}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The evaluation of the expectation of linear forms on $\\tilde{\\mathbf{Q}}$ and $\\tilde{\\mathbf{Q}}^{2}$ can be found in the literature. To find a result that meets exactly our setting, we will cite [25] that is a bit more general since it treats cases where $\\mathbb{E}[x_{i}^{(t)}]\\neq0$ for $t\\in[T]$ and $i\\in[n_{t}]$ . Unlike the main paper, and to be more general, the study presented below is \u201cquasi asymptotic\u201d meaning that the results are true for finite value of $d,n$ . Let us first rewrite the general required hypotheses, adapting them to our setting. For that purpose, we consider in the rest of this paper a certain asymptotic $I\\;{\\bar{\\subset}}\\;\\{(d,n),d\\in\\mathbb{N},n\\;{\\bar{\\in}}\\;\\mathbb{N}\\}=\\mathbb{N}^{2}$ satisfying: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\{d,\\exists\\,n\\in\\mathbb{N}:\\,(d,n)\\in I\\}=\\mathbb{N}\\qquad\\quad{\\mathrm{and~}}\\qquad\\{n,\\exists\\,d\\in\\mathbb{N}:\\,(d,n)\\in I\\}=\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "such that $n$ and $d$ can tend to $\\infty$ but with some constraint that is given in the first item of Assumption 3 below. Given two sequences $(a_{d,n})_{d,n\\in I}$ , $(b_{d,n})_{d,n\\in I}\\,>\\,0$ , the notation $a_{d,n}\\,\\leq\\,O(b_{d,n})$ (or $a\\leq$ $O(b)$ ) means that there exists a constant $C>0$ such that for all $(d,n)\\in I$ , $a_{d,n}\\leq C b_{d,n}$ . ", "page_idx": 17}, {"type": "text", "text": "Assumption 3. There exists some constants $C,c>0$ independent such that: ", "page_idx": 17}, {"type": "text", "text": "\u2022 $n\\leq O(d)$   \n\u2022 ${\\bf Z}=({\\bf z}_{1},\\dots,{\\bf z}_{n})\\in\\mathbb{R}^{T d\\times n}$ has independent columns   \n\u2022 for any $(d,n)\\in I_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ , and any $\\boldsymbol{f}:\\mathbb{R}^{T d\\times n}\\rightarrow\\mathbb{R}\\mathrm{~}\\mathrm{~}$ -Lipschitz for the euclidean norm:   \n\u2022 $\\begin{array}{r}{\\mathbb{P}\\left(|f(\\mathbf{Z})-\\mathbb{E}[f(\\mathbf{Z})]|\\ge t\\right)\\le C e^{-c t^{2}}.}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left\\{n,\\exists d\\in\\mathbb{N},(d,n)\\in I\\right\\}\\!\\!:\\left\\|\\mathbb{E}[\\mathbf{z}_{i}]\\right\\|\\le O(1).\\quad\\quad}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Theorem 2 ([25], Theorem 0.9.). Given $T\\in\\mathbb N$ , $\\mathbf{Z}\\in\\mathbb{R}^{T d\\times n}$ and two deterministic $A\\in\\mathbb{R}^{T d\\times T d}$ , we note $\\begin{array}{r}{\\tilde{\\mathbf{Q}}\\;\\equiv\\;(\\frac{1}{T d}{\\bf A}^{\\frac{1}{2}}{\\bf Z}{\\bf Z}^{\\top}{\\bf A}_{\\cdot}^{\\frac{1}{2}}+{\\cal I}_{T d})^{-1}}\\end{array}$ . If $\\mathbf{Z}$ satisfies Assumption $3$ and $\\textbf{M}\\in\\~\\mathbb{R}^{T d\\times T d}$ is $a$ deterministic matrix satisfying $\\|\\mathbf{M}\\|_{F}\\leq1$ , one has the concentration: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\mathrm{tr}\\big(M\\tilde{\\mathbf{Q}}\\big)-\\mathrm{tr}\\big(M\\bar{\\tilde{\\mathbf{Q}}}_{\\delta(\\mathbf{S})}(\\mathbf{S})\\big)\\right|\\geq t\\right)\\leq C e^{-c t^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{S}=(\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{n})=(\\mathbb{E}[\\mathbf{z}_{1}\\mathbf{z}_{1}^{\\top}],\\ldots,\\mathbb{E}[\\mathbf{z}_{n}\\mathbf{z}_{n}^{\\top}])$ , for $\\pmb{\\delta}\\in\\mathbb{R}^{n}$ , $\\bar{\\tilde{\\bf Q}}_{\\delta}$ is defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\tilde{\\mathbf{Q}}}_{\\delta}(\\mathbf{S})=\\left(\\frac{1}{T d}\\sum_{i\\in[n]}\\frac{\\mathbf{A}^{\\frac{1}{2}}\\mathbf{S}_{i}\\mathbf{A}^{\\frac{1}{2}}}{1+\\delta_{i}}+I_{T d}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\delta(\\mathbf{S})$ is the unique solution to the system of equations: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall i\\in[n]:\\quad\\delta(\\mathbf{S})_{i}=\\frac{1}{n}\\mathrm{tr}\\left(\\mathbf{A}^{\\frac{1}{2}}\\mathbf{S}_{i}\\mathbf{A}^{\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{\\delta(\\mathbf{S})}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We end this subsection with some results that will be useful for next subsection on the estimation of bilinear forms on $\\tilde{\\mathbf{Q}}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 2 ([25], Lemmas 4.2, 4.6). Under the setting of Theorem 2, given a deterministic vector $\\mathbf{u}\\in\\mathbb{R}^{T d}$ such that $\\|\\mathbf{u}\\|\\leq O(1)$ and two deterministic matrices $\\mathbf{U},\\mathbf{V}$ such that $\\|\\mathbf{U}\\|,\\|\\mathbf{V}\\|\\leq O(1)$ and a power $r>0$ , $r\\le{\\cal O}(1)$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,\\mathbb{E}\\left[\\left|\\mathbf{u}^{\\top}\\mathbf{U}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{V}\\mathbf{z}_{i}\\right|^{r}\\right]\\leq O(1)}\\\\ &{\\bullet\\,\\,\\mathbb{E}\\left[\\left|\\frac{1}{T d}\\mathbf{z}_{i}^{\\top}\\mathbf{U}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{V}\\mathbf{z}_{i}-\\mathbb{E}\\left[\\frac{1}{T d}\\mathrm{tr}\\left(\\Sigma_{i}\\mathbf{U}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{B}\\right)\\right]\\right|^{r}\\right]\\leq O\\left(\\frac{1}{d^{\\frac{r}{2}}}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.3 Deterministic equivalent of bilinear forms of the resolvent ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To simplify the expression of the following theorem, we take $\\mathbf{A}=\\mathbf{I}_{T d}$ . One can replace $\\mathbf{Z}$ with A2 Z to retrieve the result necessary for the main paper. ", "page_idx": 18}, {"type": "text", "text": "Theorem 3. Under the setting of Theorem 2, with $\\mathbf{A}=\\mathbf{I}_{T d},$ one can estimate for any deterministic matrices $\\mathbf{U},\\mathbf{V}\\in\\mathbb{R}^{T d}$ such that $\\|\\mathbf{U}\\|,\\|\\mathbf{V}\\|\\leq O(1)$ and any deterministic vector u $\\mathbf{\\dot{\\Omega}},\\mathbf{v}\\in\\mathbb{R}^{T d}$ such that $\\|\\mathbf{u}\\|,\\|\\mathbf{v}\\|\\leq1,$ , if one notes $\\begin{array}{r}{\\mathbf{B}=\\frac{1}{T d}\\mathbf{V}}\\end{array}$ or $\\mathbf{B}=\\mathbf{u}\\mathbf{v}^{\\top}$ , one can estimate: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[\\operatorname{tr}(\\mathbf{B}\\tilde{\\mathbf{Q}}\\mathbf{U}\\tilde{\\mathbf{Q}})\\right]-\\Psi(\\mathbf{U},\\mathbf{B})-\\frac{1}{T d}\\Psi(\\mathbf{U})^{\\top}\\left(\\mathbf{I}_{n}-\\frac{1}{T d}\\Psi\\right)^{-1}\\Psi(\\mathbf{B})\\right|\\leq O\\left(\\frac{1}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we noted: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi\\equiv\\frac{1}{T d}\\left(\\frac{\\mathrm{tr}\\left(\\mathbf{S}_{i}\\tilde{\\tilde{\\mathbf{Q}}}\\mathbf{S}_{j}\\tilde{\\tilde{\\mathbf{Q}}}\\right)}{(1+\\delta_{i})(1+\\delta_{j})}\\right)_{i,j\\in[n]}\\in\\mathbb{R}^{n,n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\e}\\cdot\\forall\\mathbf{U}\\in\\mathbb{R}^{n\\times n}:\\Psi(\\mathbf{U})\\equiv\\frac{1}{T d}\\left(\\frac{\\mathrm{tr}\\left(\\mathbf{U}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\right)}{1+\\delta_{i}}\\right)_{i\\in[n]}\\in\\mathbb{R}^{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\Xi}\\bullet\\forall\\mathbf{U},\\mathbf{V}\\in\\mathbb{R}^{n\\times n}:\\Psi(\\mathbf{U},\\mathbf{V})\\equiv\\frac{1}{T d}\\mathrm{tr}\\left(\\mathbf{U}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{V}\\bar{\\tilde{\\mathbf{Q}}}\\right)\\in\\mathbb{R}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If there exist $T<n$ dinstinct matrices $\\mathbf{C}_{1},\\dots,\\mathbf{C}_{T}$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{{\\bf S}_{1},\\ldots,{\\bf S}_{n}\\right\\}=\\left\\{{\\bf C}_{1},\\ldots,{\\bf C}_{T}\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and if we denote $\\forall t\\in[T]\\;n_{t}=\\#\\{i\\in[n]\\;|\\;\\mathbf{S}_{i}=\\mathbf{C}_{t}\\}$ and: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{P}\\equiv\\left(I_{T}-\\left(\\frac{n_{t}n_{v}}{(T d)^{2}}\\frac{\\mathrm{tr}\\,\\left(\\mathbf S_{t}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf S_{v}\\bar{\\tilde{\\mathbf{Q}}}\\right)}{(1+\\delta_{t})(1+\\delta_{v})}\\right)_{t,v\\in[T]}\\right)^{-1}\\in{\\mathbb{R}}^{T,T}}\\\\ {\\forall\\mathbf{U}\\in{\\mathbb{R}}^{T d\\times T d}:\\quad\\bar{\\mathbf{Q}}_{2}(\\mathbf{U})\\equiv\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\bar{\\mathbf{Q}}+\\frac{1}{(T d)^{2}}\\sum_{t,v=1}^{T}\\frac{\\mathrm{tr}(\\mathbf S_{t}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\bar{\\tilde{\\mathbf{Q}}})P_{t,v}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf S_{v}\\bar{\\tilde{\\mathbf{Q}}}}{(1+\\delta_{t})(1+\\delta_{v})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the result of Theorem 3 rewrites: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\tilde{\\mathbf{Q}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\right]-\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{U})\\right\\|\\leq O\\left(\\frac{1}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Given $i\\in[n]$ , let us note $\\mathbf{Z}_{-i}=\\left(\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{i-1},0,\\mathbf{z}_{i+1},\\ldots,\\mathbf{z}_{n}\\right)$ and $\\begin{array}{r}{\\tilde{\\mathbf{Q}}_{-i}=(\\frac{1}{T d}\\mathbf{Z}_{-i}\\mathbf{Z}_{-i}^{\\top}+}\\end{array}$ ${\\bf{I}}_{T d})^{-1}$ , then we have the identity: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{Q}}-\\tilde{\\mathbf{Q}}_{-i}=\\frac{1}{T d}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\qquad\\qquad\\mathrm{and}\\qquad\\qquad\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}=\\frac{\\tilde{\\mathbf{Q}}_{-i}\\mathbf{z}_{i}}{1+\\frac{1}{T d}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{z}_{i}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{T d}$ , such that $\\|\\mathbf{u}\\|,\\|\\mathbf{v}\\|\\leq1$ , let us express: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T d}\\mathbf{u}^{\\top}\\left(\\tilde{\\mathbf{Q}}-\\bar{\\tilde{\\mathbf{Q}}}\\right)\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\left(\\frac{\\mathbf{S}_{i}}{1+\\delta_{i}}-\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\right)\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First, given $i\\in[n]$ , let us estimate thanks to (10): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]=\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]-\\frac{1}{T d}\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "H\u00f6lder inequality combined with Lemma 2 allows us to bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{T d}\\left|\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]\\right|\\leq\\frac{1}{T d}\\mathbb{E}\\left[\\left|\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\right|^{2}\\right]^{\\frac{1}{2}}\\mathbb{E}\\left[\\left|\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right|^{2}\\right]^{\\frac{1}{2}}\\leq O\\left(\\frac{1}{d}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "one can thus deduce: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]=\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]+O\\left(\\frac{1}{d}\\right)=\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{v}\\right]+O\\left(\\frac{1}{d}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Second, one can also estimate thanks to Lemma 10: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb E\\left[\\mathbf u^{\\top}\\tilde{\\mathbf Q}\\mathbf z_{i}\\mathbf z_{i}^{\\top}\\bar{\\tilde{\\mathbf Q}}\\mathbf U\\tilde{\\mathbf Q}\\mathbf v\\right]=\\mathbb E\\left[\\frac{\\mathbf u^{\\top}\\tilde{\\mathbf Q}_{-i}\\mathbf z_{i}\\mathbf z_{i}^{\\top}\\bar{\\tilde{\\mathbf Q}}\\mathbf U\\tilde{\\mathbf Q}\\mathbf v}{1+\\frac{1}{T d}\\mathbf z_{i}^{\\top}\\mathbf Q_{-i}\\mathbf z_{i}}\\right]}&{=\\mathbb E\\left[\\frac{\\mathbf u^{\\top}\\tilde{\\mathbf Q}_{-i}\\mathbf z_{i}\\mathbf z_{i}^{\\top}\\bar{\\tilde{\\mathbf Q}}\\mathbf U\\tilde{\\mathbf Q}\\mathbf v}{1+\\delta_{i}}\\right]+O\\left(\\frac{1}{\\sqrt d}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "again thanks to H\u00f6lder inequality combined with Lemma 2 that allow us to bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|\\frac{\\delta_{i}-\\frac{1}{T d}{\\bf Z}_{i}^{\\top}{\\bf Q}_{-i}{\\bf z}_{i}}{(1+\\delta_{i})\\,\\left(1+\\frac{1}{T d}{\\bf z}_{i}^{\\top}{\\bf Q}_{-i}{\\bf z}_{i}\\right)}\\right|\\,|{\\bf u}^{\\top}\\tilde{\\bf Q}_{-i}{\\bf z}_{i}{\\bf z}_{i}^{\\top}\\tilde{\\bf Q}\\bf U\\tilde{Q}{\\bf v}|\\right]}\\\\ &{\\qquad\\le\\mathbb{E}\\left[\\left|\\delta_{i}-\\frac{1}{T d}{\\bf z}_{i}^{\\top}{\\bf Q}_{-i}{\\bf z}_{i}\\right|^{2}\\right]^{\\frac{1}{2}}\\mathbb{E}\\left[\\left|{\\bf u}^{\\top}\\tilde{\\bf Q}_{-i}{\\bf z}_{i}{\\bf z}_{i}^{\\top}\\tilde{\\bf Q}{\\bf U}\\tilde{\\bf Q}{\\bf v}\\right|^{2}\\right]^{\\frac{1}{2}}\\ \\le\\ O\\left(\\frac{1}{\\sqrt{d}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The independence between $\\mathbf{z}_{i}$ and $\\tilde{\\mathbf{Q}}_{-i}$ (and $\\bar{\\tilde{\\mathbf{Q}}}$ ) then allow us to deduce (again with formula (10)): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\bar{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right\\rangle=\\mathbb{E}\\left[\\frac{\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\tilde{\\bar{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{v}}{1+\\delta_{i}}\\right]+\\frac{1}{T d}\\mathbb{E}\\left[\\frac{\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\bar{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{v}}{1+\\delta_{i}}\\right]+O\\left(\\delta_{i}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us inject (13) and (14) in (11) to obtain (again with an application of H\u00f6lder inequality and Lemma 2 that we do not detail this time): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}\\left(\\frac{\\mathbf{S}_{i}}{1+\\delta_{i}}-\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\right)\\tilde{\\mathbf{Q}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{v}\\right]=\\frac{1}{T d}\\mathbb{E}\\left[\\frac{\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\bar{\\mathbf{Q}}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{v}}{\\left(1+\\frac{1}{n}\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{z}_{i}\\right)^{2}}\\right]+O\\left(\\frac{1}{\\sqrt{d}}\\right),}\\\\ {=\\frac{1}{T d}\\frac{\\mathbb{E}\\left[\\mathbf{u}^{\\top}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\tilde{\\mathbf{Q}}_{-i}\\mathbf{v}\\right]}{\\left(1+\\delta_{i}\\right)^{2}}\\mathrm{tr}\\left(\\mathbf{S}_{i}\\tilde{\\mathbf{Q}}\\mathbf{U}\\bar{\\mathbf{Q}}\\right)+O\\left(\\frac{1}{\\sqrt{d}}\\right),~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Putting all the estimations together, one finally obtains: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\tilde{\\mathbf{Q}}\\mathbf{U}\\tilde{\\mathbf{Q}}\\right]-\\mathbb{E}\\left[\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\bar{\\tilde{\\mathbf{Q}}}\\right]-\\frac{1}{(T d)^{2}}\\sum_{i=1}^{n}\\frac{\\mathrm{tr}\\left(\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{U}\\bar{\\tilde{\\mathbf{Q}}}\\right)}{\\left(1+\\delta_{i}\\right)^{2}}\\mathbb{E}\\left[\\tilde{\\mathbf{Q}}_{-i}\\mathbf{S}_{i}\\tilde{\\mathbf{Q}}_{-i}\\right]\\right\\|\\leq O\\left(\\frac{1}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "One then see that if we introduce for any $\\mathbf{V}\\in\\mathbb{R}^{n\\times n}$ the block matrices: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ \\theta=\\frac{1}{T d}(\\frac{\\mathbb{E}\\left[\\mathrm{tr}({\\bf S}_{j}\\tilde{\\bf{Q}}{\\bf S}_{i}\\tilde{\\bf{Q}}^{Y})\\right]}{(1+\\delta_{i})(1+\\delta_{j})})_{i,j\\in[n]}\\in\\mathbb{R}^{n\\times n}}\\\\ {\\bullet\\ \\theta({\\bf V})=\\frac{1}{T d}(\\frac{\\mathbb{E}\\left[\\mathrm{tr}({\\bf V}\\tilde{\\bf{Q}}{\\bf S}_{i}\\tilde{\\bf{Q}}^{Y})\\right]}{1+\\delta_{i}})_{i\\in[n]}\\in\\mathbb{R}^{n},}\\\\ {\\bullet\\ \\theta({\\bf U},{\\bf V})=\\frac{1}{T d}\\mathbb{E}\\left[\\mathrm{tr}({\\bf V}\\tilde{\\bf{Q}}{\\bf U}\\tilde{\\bf Q}^{Y})\\right]\\in\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then, if $\\|\\mathbf{V}\\|\\le O(1)$ , multiplying (15) with $\\mathbf{V}$ and taking the trace leads to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta(\\mathbf{U},\\mathbf{V})=\\Psi(\\mathbf{U},\\mathbf{V})+{\\frac{1}{T d}}\\Psi(\\mathbf{U})^{\\top}\\theta(\\mathbf{V})+O\\left({\\frac{1}{\\sqrt{d}}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, taking $\\begin{array}{r}{\\mathbf{U}=\\frac{\\mathbf{S}_{1}}{1+\\delta_{1}},\\,.\\,.\\,.\\,,\\frac{\\mathbf{S}_{n}}{1+\\delta_{n}}}\\end{array}$ ,1S+n\u03b4  , one gets the vectorial equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta(\\mathbf{V})=\\Psi(\\mathbf{V})+{\\frac{1}{T d}}\\Psi\\theta(\\mathbf{V})+O\\left({\\frac{1}{\\sqrt{d}}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $\\begin{array}{r}{\\left(I_{T d}-\\frac{1}{T d}\\Psi\\right)}\\end{array}$ is invertible, one gets $\\begin{array}{r}{\\theta(\\mathbf{V})=(I_{T d}-\\frac{1}{T d}\\Psi)^{-1}\\Psi(\\mathbf{V})+O\\left(\\frac{1}{\\sqrt{d}}\\right)}\\end{array}$ , and combining with (16), one finally obtains: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta(\\mathbf{U},\\mathbf{V})=\\Psi(\\mathbf{U},\\mathbf{V})+{\\frac{1}{T d}}\\Psi(\\mathbf{U})^{\\top}(I_{T d}-{\\frac{1}{T d}}\\Psi)^{-1}\\Psi(\\mathbf{V})+O\\left({\\frac{1}{\\sqrt{d}}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.4 Estimation of the deterministic equivalent of ${\\bf{Q}}^{2}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 4. Under the setting of Theorem 3, one can estimate: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\mathbf{Q}^{2}\\right]-\\mathbf{I}_{n}+\\mathcal{D}_{v}\\right\\|\\leq O\\left(\\frac{1}{\\sqrt{d}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with, $\\forall i\\in[n]$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nv_{i}\\equiv\\frac{1}{T d}\\frac{\\mathrm{tr}\\left({\\bf S}_{i}\\bar{\\tilde{\\bf Q}}\\right)}{(1+\\delta_{i})^{2}}+\\frac{1}{T d}\\frac{\\mathrm{tr}\\left({\\bf S}_{i}\\bar{\\tilde{\\bf Q}}_{2}({\\bf I}_{n})\\right)}{(1+\\delta_{i})^{2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The justifications are generally the same as in the proof of Theorem 3, we will thus allow ourselves to be quicker in this proof. ", "page_idx": 20}, {"type": "text", "text": "Using the definition of $\\begin{array}{r}{\\mathbf{Q}=\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}+\\mathbf{I}_{n}\\right)^{-1}}\\end{array}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{Z}^{\\top}\\mathbf{Z}}{T d}\\mathbf{Q}=\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{Z}}{T d}+\\mathbf{I}_{n}-\\mathbf{I}_{n}\\right)\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{Z}}{T d}+\\mathbf{I}_{n}\\right)^{-1}=\\mathbf{I}_{n}-\\mathbf{Q}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and one can then let appear $\\tilde{\\mathbf{Q}}$ thanks to the relation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Z}\\mathbf{Q}={\\tilde{\\mathbf{Q}}}\\mathbf{Z},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "that finally gives us: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Q}=\\mathbf{I}_{n}-\\frac{1}{T d}\\mathbf{Z}^{\\top}\\mathbf{Z}\\mathbf{Q}=\\mathbf{I}_{n}-\\frac{1}{T d}\\mathbf{Z}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{Z}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "One can then express: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}^{2}=\\mathbf{I}_{n}-\\frac{2}{T d}\\mathbf{Z}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{Z}+\\frac{1}{(T d)^{2}}\\mathbf{Z}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{Z}\\mathbf{Z}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{Z}}\\\\ &{\\quad=\\mathbf{I}_{n}-\\frac{1}{T d}\\mathbf{Z}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{Z}-\\frac{1}{T d}\\mathbf{Z}^{\\top}\\tilde{\\mathbf{Q}}^{2}\\mathbf{Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Given $i,j\\in[n],i\\neq j$ , let us first estimate (thanks to H\u00f6lder inequality and Lemma 2): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T d}\\mathbb{E}\\left[\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{z}_{j}\\right]=\\frac{1}{T d}\\frac{\\mathbb{E}\\left[\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}_{-i,j}\\mathbf{z}_{j}\\right]}{(1+\\delta_{i})(1+\\delta_{j})}+O\\left(\\frac{1}{\\sqrt{d}}\\right)\\leq O\\left(\\frac{1}{\\sqrt{d}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $\\mathbb{E}[z_{i}]=\\mathbb{E}[z_{j}]=0$ . Now, we consider the case $j=i$ to get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T d}\\mathbb E\\left[\\mathbf z_{i}^{\\top}\\tilde{\\mathbf Q}\\mathbf z_{i}\\right]=\\frac{1}{T d}\\frac{\\mathbb E\\left[\\mathbf z_{i}^{\\top}\\tilde{\\mathbf Q}_{-i}\\mathbf z_{i}\\right]}{(1+\\delta_{i})^{2}}+O\\left(\\frac{1}{\\sqrt d}\\right)=\\frac{1}{T d}\\frac{\\mathrm{tr}\\left(\\mathbf S_{i}\\bar{\\tilde{\\mathbf Q}}\\right)}{(1+\\delta_{i})^{2}}+O\\left(\\frac{1}{\\sqrt d}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As before, we know that $\\begin{array}{r}{\\frac{1}{T d}\\mathbb{E}\\left[\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}\\mathbf{z}_{j}\\right]\\leq O\\left(\\frac{1}{\\sqrt{d}}\\right)}\\end{array}$ if $i\\neq j$ . Considering $i\\in[n]$ , we thus are left to estimate: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T d}\\mathbb{E}\\left[\\mathbf{z}_{i}^{\\top}\\tilde{\\mathbf{Q}}^{2}\\mathbf{z}_{j}\\right]=\\frac{1}{T d}\\frac{\\mathrm{tr}\\left(\\mathbf{S}_{i}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{I}_{n})\\right)}{(1+\\delta_{i})^{2}}+O\\left(\\frac{1}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D Risk Estimation (Proof of Theorem 1) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Test Risk ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The expected value of the MSE of the test data $\\mathbf{x}\\in\\mathbb{R}^{T\\times T d}$ concatenating the feature vector of all the tasks with the corresponding response variable $\\mathbf{y}\\in\\mathbb{R}^{T\\times T q}$ reads as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad=\\frac{1}{T}\\left[\\frac{1}{\\tau}\\overline{{\\gamma\\gamma}}+\\frac{\\gamma\\tau}{T}-\\frac{\\gamma\\tau-1}{T}\\right]\\frac{1}{\\tau^{2}}}\\\\ &{\\quad=\\frac{1}{T}\\left[\\frac{1}{\\tau}\\overline{{\\gamma\\gamma}}\\right]\\left[\\frac{1}{\\tau^{2}}\\overline{{\\gamma}}+\\frac{\\gamma\\tau}{T}\\overline{{\\gamma\\gamma}}+\\frac{\\gamma\\tau}{T}\\overline{{\\gamma}}+\\frac{\\gamma}{[T]}\\right]}\\\\ &{\\quad=\\frac{1}{T}\\left[\\frac{1}{\\tau}\\overline{{\\gamma\\gamma}}+\\frac{\\gamma\\tau^{2}\\overline{{\\gamma}}+2\\tau^{2}\\overline{{\\gamma}}\\tau^{2}}{T^{2}+T}-\\frac{\\gamma\\tau^{2}\\overline{{\\gamma}}+2\\tau\\underline{{\\delta}}}{T^{2}+T}\\epsilon_{1}\\right]}\\\\ &{\\quad=\\frac{1}{T}\\left[\\frac{1}{\\tau}\\overline{{\\gamma\\gamma}}\\frac{\\Gamma(\\frac{1}{T})}{T^{2}+T}-\\frac{2\\tau\\overline{{\\gamma}}\\tau^{2}\\overline{{\\gamma}}\\tau^{2}}{T^{2}+T^{2}+T^{2}}+\\mathrm{er}\\left(\\tau^{2}+\\frac{\\gamma\\tau^{2}}{T}\\right)\\frac{1}{(T^{2}+T^{2}+T^{2})^{2}}\\gamma\\right.}\\\\ &{\\quad\\left.\\frac{1}{T}\\left(\\frac{\\gamma\\tau^{2}\\overline{{\\gamma}}\\tau+2\\tau\\overline{{\\gamma}}\\tau+2\\tau\\overline{{\\gamma}}\\tau^{2}}{T^{2}+T}\\right)\\right]}\\\\ &{\\quad=\\frac{1}{T}\\left[\\frac{1}{\\tau}\\overline{{\\gamma\\gamma}}\\frac{\\Gamma(0)\\tau^{2}\\overline{{\\gamma}}\\tau^{2}}{T^{2}}-\\frac{2\\tau\\overline{{\\gamma}}\\tau^{2}\\overline{{\\gamma}}\\tau^{2}}{T^{2}+T}\\frac{(\\gamma\\tau^{2}+T)(\\mu\\tau-\\tau)(\\tau^{2}+T)}{T^{2}+T}+\\right.}\\\\ &{\\quad\\quad\\left.\\gamma\\tau^{2}\\left[\\frac{1}{\\tau}\\overline{{\\gamma\\gamma}}\\frac{\\gamma\\tau^{2}\\overline{{\\gamma}}\\tau^{2}}{T^{2}}\\frac{\\gamma\\tau^{2}\\overline{{\\gamma}}\\tau^{\n$$The test risk can be further simplified as ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t e s t}^{\\infty}=\\operatorname{tr}\\left(\\Sigma_{N}\\right)+\\frac{\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{A})\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}}{T d}+\\frac{\\operatorname{tr}\\left(\\Sigma_{N}\\bar{\\mathbf{Q}}_{2}\\right)}{T d}+O\\left(\\frac{1}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.2 Train Risk ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we derive the asymptotic risk for the training data. ", "page_idx": 22}, {"type": "text", "text": "Theorem 5 (Asymptotic training risk). Assuming that the training data vectors $\\mathbf{x}_{i}^{(t)}$ and the test data vectors $\\mathbf{x}^{(t)}$ are concentrated random vectors, and given the growth rate assumption (Assumption 2), it follows that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}_{T n}\\mathbf{\\Sigma}_{\\mathrm{tr}}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\bar{\\mathbf{Q}}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)-\\frac{1}{T n}\\mathbf{\\Phi}_{\\mathrm{T}n}^{\\mathrm{T}}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\bar{\\mathbf{Q}}_{2}(\\mathbf{I}_{T d})\\mathbf{A}^{-1/2}\\mathbf{W}\\right)+\\frac{1}{T n}\\mathbf{tr}\\left(\\boldsymbol{\\Sigma}_{N}\\bar{\\mathbf{Q}}_{2}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We aim in this setting of regression, to compute the asymptotic theoretical training risk given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t r a i n}^{\\infty}=\\frac{1}{T n}\\mathbb{E}\\left[\\left\\|\\mathbf{Y}-\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}\\mathbf{Q}\\mathbf{Y}\\right\\|_{2}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the definition of $\\begin{array}{r}{{\\bf Q}=\\left(\\frac{{\\bf Z}^{\\top}{\\bf A}{\\bf Z}}{T d}+{\\bf I}_{T d}\\right)^{-1}}\\end{array}$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}\\mathbf{Q}=\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}+\\mathbf{I}_{T d}-\\mathbf{I}_{T d}\\right)\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{Z}}{T d}+\\mathbf{I}_{T d}\\right)^{-1}=\\mathbf{I}_{T d}-\\mathbf{Q}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging back into the expression of the training risk then leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t r a i n}^{\\infty}=\\frac{1}{T n}\\mathbb{E}\\left[\\mathrm{tr}\\left(\\mathbf{Y}^{\\top}\\mathbf{Q}^{2}\\mathbf{Y}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the definition of the linear generative model and in particular $\\begin{array}{r}{\\mathbf{Y}=\\frac{\\mathbf{Z}^{\\top}\\mathbf{W}}{\\sqrt{T d}}+\\varepsilon}\\end{array}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\mathcal{R}}_{t r a i n}^{\\infty}=\\frac{1}{T n}{\\mathbb{E}}\\left[\\mathrm{tr}\\left(\\frac{1}{\\sqrt{T d}}{\\bf Z}^{\\top}{\\bf W}+\\varepsilon\\right)^{\\top}{\\bf Q}^{2}\\left(\\frac{1}{\\sqrt{T d}}{\\bf Z}^{\\top}{\\bf W}+\\varepsilon\\right)\\right]}}\\\\ {{\\displaystyle=\\frac{1}{T n}\\frac{1}{T d}{\\mathbb{E}}\\left[\\mathrm{tr}\\left({\\bf W}^{\\top}{\\bf Z}{\\bf Q}^{2}{\\bf Z}^{\\top}{\\bf W}\\right)\\right]+\\frac{1}{T n}{\\mathbb{E}}\\left[\\mathrm{tr}\\left(\\varepsilon^{\\top}{\\bf Q}^{2}\\varepsilon\\right)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To simplify this expression, we will introduced the so-called \u201ccoresolvent\u201d defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{Q}}=\\left(\\frac{\\mathbf{A}^{\\frac{1}{2}}\\mathbf{Z}\\mathbf{Z}^{\\top}\\mathbf{A}^{\\frac{1}{2}}}{T d}+\\mathbf{I}_{T d}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Employing the elementary relation ${\\bf A}^{\\frac{1}{2}}{\\bf Z}{\\bf Q}=\\tilde{\\bf Q}{\\bf A}^{\\frac{1}{2}}{\\bf Z}$ , one obtains: ", "page_idx": 22}, {"type": "text", "text": "$\\frac{1}{\\cal T}{\\cal d}{\\bf Q}^{2}{\\bf Z}^{\\top}=\\frac{1}{T d}{\\bf A}^{-\\frac{1}{2}}\\tilde{{\\bf Q}}{\\bf A}^{\\frac{1}{2}}{\\bf Z}{\\bf Q}{\\bf Z}^{\\top}={\\bf A}^{-\\frac{1}{2}}\\tilde{{\\bf Q}}^{2}\\frac{{\\bf A}^{\\frac{1}{2}}{\\bf Z}{\\bf Z}^{\\top}{\\bf A}^{\\frac{1}{2}}}{T d}{\\bf A}^{-\\frac{1}{2}}={\\bf A}^{-\\frac{1}{2}}\\tilde{{\\bf Q}}{\\bf A}^{-\\frac{1}{2}}-{\\bf A}^{-\\frac{1}{2}}\\tilde{{\\bf Q}}^{2}{\\bf A}^{-\\frac{1}{2}},$ Therefore we further get $\\mathbb{S}_{t r a i n}^{\\infty}=\\frac{1}{T n}\\mathbb{E}\\left[\\mathrm{tr}\\left(\\mathbf{\\bar{W}}^{\\top}\\mathbf{A}^{-1/2}\\Tilde{\\mathbf{Q}}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)\\right]-\\frac{1}{T n}\\mathbb{E}\\left[\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\Tilde{\\mathbf{Q}}^{2}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)\\right]+\\frac{1}{T n}\\mathbb{E}\\left[\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\Tilde{\\mathbf{Q}}^{2}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)\\right].$ \u03b5\u22a4Q2\u03b5 ", "page_idx": 22}, {"type": "text", "text": "Using deterministic equivalents in Lemma 1, the training risk then leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{T n}\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)-\\frac{1}{T n}\\mathrm{tr}\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{I}_{T d})\\mathbf{A}^{-1/2}\\mathbf{W}\\right)+\\frac{1}{T n}\\mathrm{tr}\\left(\\mathbf{\\Sigma}_{\\Sigma_{N}}\\bar{\\mathbf{Q}}_{2}\\right)+O\\left(\\left(\\mathbf{W}^{\\top}\\mathbf{A}^{-1/2}\\mathbf{W}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E Interpretation and insights of the theoretical analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Analysis of the test risk ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We recall the test risk as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t e s t}^{\\infty}=\\operatorname{tr}\\left(\\Sigma_{N}\\right)+\\frac{\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{A})\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}}{T d}+\\frac{\\operatorname{tr}\\left(\\Sigma_{N}\\bar{\\mathbf{Q}}_{2}\\right)}{T d}+O\\left(\\frac{1}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The test risk is composed of a signal term of a signal term S = W\u22a4A\u22122 Q\u00af\u02dcT2 d(A)A\u22122 W and a noise term N =tr(\u03a3N Q\u00af2). ", "page_idx": 22}, {"type": "text", "text": "E.2 Interpretation of the signal term ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let\u2019s denote by \u00af\u03a3 = tT=1T d(n1t+dt\u03b4t)2 and $\\begin{array}{r}{\\tilde{\\Sigma}=\\sum_{t=1}^{T}\\frac{c_{0}}{1+\\delta_{t}}\\Sigma^{(t)}}\\end{array}$ . The signal term reads as ", "page_idx": 23}, {"type": "equation", "text": "$$\nS=\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{A})\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the following identity, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{A})\\mathbf{A}^{-\\frac{1}{2}}=\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\mathbf{Q}}\\mathbf{A}^{\\frac{1}{2}}\\left(\\mathbf{I}+\\bar{\\Sigma}\\right)\\mathbf{A}^{\\frac{1}{2}}\\bar{\\mathbf{Q}}\\mathbf{A}^{-\\frac{1}{2}}}\\\\ {=\\left(\\mathbf{A}\\tilde{\\Sigma}+\\mathbf{I}\\right)^{-1}\\left(\\mathbf{I}+\\bar{\\Sigma}\\right)\\left(\\mathbf{A}\\tilde{\\Sigma}+\\mathbf{I}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This finally leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\boldsymbol{S}=\\mathbf{W}^{\\top}\\left(\\mathbf{A}\\tilde{\\boldsymbol{\\Sigma}}+\\mathbf{I}\\right)^{-1}\\left(\\mathbf{I}+\\bar{\\boldsymbol{\\Sigma}}\\right)\\left(\\mathbf{A}\\tilde{\\boldsymbol{\\Sigma}}+\\mathbf{I}\\right)^{-1}\\mathbf{W}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The matrix $\\mathcal{H}=\\left(\\mathbf{A}\\tilde{\\Sigma}+\\mathbf{I}\\right)^{-1}\\left(\\mathbf{I}+\\bar{\\Sigma}\\right)\\left(\\mathbf{A}\\tilde{\\Sigma}+\\mathbf{I}\\right)^{-1}$ is responsible to amplifying the signal $\\mathbf{W}^{\\top}\\mathbf{W}$ in order to let the test risk to decrease more or less. It is is decreasing as function of the number of samples in the tasks $n_{t}$ . Furthermore it is composed of two terms (from the independent training $\\mathbf{W}_{t}^{\\top}\\mathbf{W})$ and the cross term $\\mathbf{W}_{t}^{\\top}\\mathbf{W}_{v}$ for $t\\neq v$ . Both terms decreases as function of the number of samples $n_{t}$ , smaller values of $\\gamma_{t}$ and increasing value of $\\lambda$ . The cross term depends on the matrix $\\Sigma_{t}^{-1}\\Sigma_{v}$ which materializes the covariate shift between the tasks. More specifically, if the features are aligned $\\pmb{\\Sigma}_{t}^{-1}\\pmb{\\Sigma}_{v}=I$ and the cross term is maximal while for bigger Fisher distance between the covariance of the tasks, the correlation is not favorable for multi task learning. To be more specific the off-diagonal term of $\\mathcal{H}$ are responsible for the cross term therefore for the multi tasks and the diagonal elements are responsible for the independent terms. ", "page_idx": 23}, {"type": "text", "text": "To analyze more the element of $\\mathcal{H}$ , let\u2019s consider the case where $\\boldsymbol{\\Sigma}^{(t)}=\\mathbf{I}$ and $\\gamma_{t}=\\gamma$ . In this case the diagonal and non diagonal elements $\\mathbf{D}_{I L}$ and $\\mathbf{C}_{M T L}$ are respectively given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf D}_{I L}=\\frac{(c_{0}(\\lambda+\\gamma)+1)^{2}+c_{0}^{2}\\lambda^{2}}{(c_{0}(\\lambda+\\gamma)+1)^{2}-c_{0}^{2}\\lambda^{2}},\\quad{\\bf C}_{M T L}=\\frac{-2c_{0}\\lambda(c_{0}(\\lambda+\\gamma)+1)}{(c_{0}(\\lambda+\\gamma)+1)^{2}-c_{0}^{2}\\lambda^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Both function are decreasing function of $\\lambda,1/\\gamma$ and $c_{0}$ . ", "page_idx": 23}, {"type": "text", "text": "E.3 Interpretation and insights of the noise terms ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We recall the definition of the noise term $\\mathcal{N}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{N}=\\mathrm{tr}\\left(\\boldsymbol{\\Sigma}_{N}\\left(\\mathbf{A}^{-1}+\\boldsymbol{\\Sigma}\\right)^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now at the difference of the signal term there are no cross terms due to the independence between the noise of the different tasks. In this case on the diagonal elements of $\\left(\\mathbf{A}^{-1}+\\Bar{\\mathbf{\\Omega}}\\right)^{-1}$ matters. This diagonal term is increasing for an increasing value of the sample size, the value of $\\lambda$ . Therefore this term is responsible for the negative transfer. In the specific case where $\\boldsymbol{\\Sigma}^{(t)}=\\mathbf{I}_{d}$ and $\\gamma_{t}=\\gamma$ for all task $t$ , the diagonal terms read as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{N}_{N T}=\\frac{(c_{0}(\\lambda+\\gamma)^{2}+(\\lambda+\\gamma)-c_{0}\\lambda^{2})^{2}+\\lambda^{2}}{((c_{0}(\\lambda+\\gamma)+1)^{2}-c_{0}^{2}\\lambda^{2})^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.4 Optimal Lambda ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The test risk in the particular of identity covariance matrix can be rewritten as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{R}_{t e s t}^{\\infty}=\\mathbf{D}_{I L}\\left(\\|\\mathbf{W}_{1}\\|_{2}^{2}+\\|\\mathbf{W}_{2}\\|_{2}^{2}\\right)+\\mathbf{C}_{M T L}\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{2}+\\mathbf{N}_{N T}\\mathrm{tr}\\mathbf{\\Sigma}_{n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Deriving $\\mathcal{R}_{t e s t}^{\\infty}$ with respect to $\\lambda$ leads after some algebraic calculus to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda^{\\star}=\\frac{n}{d}S N R-\\frac{\\gamma}{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the signal noise ratio is composed of the independent signal to noise ratio and the cross signal to noise ratio $\\begin{array}{r}{S N R=\\frac{\\|\\mathbf{W}_{1}\\|_{2}^{2}+\\mathbf{W}_{2}\\|_{2}^{2}}{\\operatorname{tr}\\pmb{\\Sigma}_{n}}+\\frac{\\mathbf{W}_{1}^{\\top}\\mathbf{W}_{2}}{\\operatorname{tr}\\pmb{\\Sigma}_{n}}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "F Theoretical Estimations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.1 Estimation of the training and test risk ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The different theorems depends on the ground truth W that needs to be estimated through $\\hat{\\bf W}$ . ", "page_idx": 24}, {"type": "text", "text": "To estimate the test risk, one needs to estimate functionals of the form $\\mathbf{W}^{\\top}\\mathbf{M}\\hat{\\mathbf{W}}$ and $\\varepsilon^{\\top}\\mathbf{M}\\varepsilon$ for any matrix M. Using the expression of $\\mathbf{W}=\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Y}$ , we start computing $\\hat{\\mathbf{W}}^{\\top}\\mathbf{M}\\hat{\\mathbf{W}}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{W}}^{\\top}\\mathbf{M}\\mathbf{W}=\\mathbf{Y}^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the generative model for Y = Z\u221a\u22a4W , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\hat{\\mathbf{W}}^{\\top}\\mathbf{M}\\mathbf{W}\\right]=\\mathbb{E}\\left[\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{W}}{\\sqrt{T d}}+\\varepsilon\\right)^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\left(\\frac{\\mathbf{Z}^{\\top}\\mathbf{W}}{\\sqrt{T d}}+\\varepsilon\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{T d}\\mathbb{E}\\left[\\mathbf{W}^{\\top}\\mathbf{Z}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{W}\\right]+\\mathbb{E}\\left[\\varepsilon^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\varepsilon\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Employing the elementary relation ${\\bf A}^{\\frac{1}{2}}{\\bf Z}{\\bf Q}=\\tilde{\\bf Q}{\\bf A}^{\\frac{1}{2}}{\\bf Z}$ , one obtains: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\hat{\\mathbf{W}}^{\\top}\\mathbf{M}\\mathbf{W}\\right]=\\frac{1}{T d}\\mathbb{E}\\left[\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{Z}^{\\top}\\mathbf{Z}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{Z}\\mathbf{Z}^{\\top}\\mathbf{W}\\right]+\\mathbb{E}\\left[\\varepsilon^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\varepsilon\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\left(\\mathbf{I}-\\tilde{\\mathbf{Q}}\\right)\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}}\\left(\\mathbf{I}-\\tilde{\\mathbf{Q}}\\right)\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}\\right]+\\mathbb{E}\\left[\\varepsilon^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\varepsilon\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[\\mathbf{W}^{\\top}\\mathbf{M}\\mathbf{W}\\right]-2\\mathbb{E}\\left[\\mathbf{W}^{\\top}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}\\right]+\\mathbb{E}\\left[\\mathbf{W}^{\\top}\\mathbf{A}^{-\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{-\\frac{1}{2}}\\mathbf{W}\\right]}\\\\ &{\\qquad\\qquad+\\,\\mathbb{E}\\left[\\varepsilon^{\\top}\\mathbf{Q}\\mathbf{Z}^{\\top}\\mathbf{A}\\mathbf{M}\\mathbf{A}\\mathbf{Z}\\mathbf{Q}\\varepsilon\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the deterministic equivalent of Lemma 1, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{N}^{\\top}\\mathbf{M}\\hat{\\mathbf{W}}\\leftrightarrow{\\mathbf{W}}^{\\top}\\mathbf{M}\\mathbf{W}-2{\\mathbf{W}}^{\\top}{\\mathbf{A}}^{-\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{W}+\\mathrm{tr}\\Sigma_{n}\\mathbf{M}({\\mathbf{A}}^{\\frac{1}{2}}\\mathbf{M}{\\mathbf{A}}^{\\frac{1}{2}})+{\\mathbf{W}}^{\\top}{\\mathbf{A}}^{-\\frac{1}{2}}\\tilde{\\mathbf{Q}}_{2}({\\mathbf{A}}^{\\frac{1}{2}}\\mathbf{M}{\\mathbf{A}}^{\\frac{1}{2}})\\mathbf{W}\\leftrightarrow{\\mathbf{W}}^{\\top}{\\mathbf{A}}^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\leftrightarrow{\\mathbf{W}}^{\\top}\\left(\\mathbf{M}-2{\\mathbf{A}}^{-\\frac{1}{2}}\\tilde{\\mathbf{Q}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}+{\\mathbf{A}}^{-\\frac{1}{2}}\\tilde{\\mathbf{Q}}_{2}({\\mathbf{A}}^{\\frac{1}{2}}\\mathbf{M}{\\mathbf{A}}^{\\frac{1}{2}}){\\mathbf{A}}^{-\\frac{1}{2}}\\right)\\mathbf{W}+\\mathrm{tr}\\Sigma_{n}\\tilde{\\mathbf{Q}}_{2}({\\mathbf{A}}^{\\frac{1}{2}}\\mathbf{M}{\\mathbf{A}}^{\\frac{1}{2}})}\\\\ &{\\qquad\\qquad\\leftrightarrow{\\mathbf{W}}^{\\top}\\kappa(\\mathbf{M})\\mathbf{W}+\\mathrm{tr}\\Sigma_{n}\\tilde{\\mathbf{Q}}_{2}({\\mathbf{A}}^{\\frac{1}{2}}\\mathbf{M}{\\mathbf{A}}^{\\frac{1}{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where We define the mapping $\\kappa:\\mathbb{R}^{T d\\times T d}\\rightarrow\\mathbb{R}^{q\\times q}$ as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa(\\mathbf{M})=\\mathbf{M}-2\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}+\\mathbf{A}^{-\\frac{1}{2}}\\bar{\\tilde{\\mathbf{Q}}}_{2}(\\mathbf{A}^{\\frac{1}{2}}\\mathbf{M}\\mathbf{A}^{\\frac{1}{2}})\\mathbf{A}^{-\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "F.2 Estimation of the noise covariance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The estimation of the noise covariance remains a technical challenge in this process. However, when the noise covariance is isotropic, it is sufficient to estimate only the noise variance. By observing that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\lambda\\to0,\\gamma\\to\\infty}\\mathcal{R}_{t r a i n}^{\\infty}=\\sigma^{2}\\frac{\\mathrm{tr}\\mathbf{Q}_{2}}{k n},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we can estimate the noise level from the training risk evaluated at large $\\gamma$ and $\\lambda=0$ . ", "page_idx": 24}, {"type": "text", "text": "G Multivariate Time Series Forecasting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "A version of this work with a stronger focus on time series applications can be found at [20]. ", "page_idx": 24}, {"type": "text", "text": "G.1 Related Work ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Multivariate Time Series Forecasting. Previous work has explored multivariate time series methods to enrich task representations and enhance diversity across time series applications [18]. ", "page_idx": 24}, {"type": "text", "text": "Despite these advancements, random matrix theory, widely applied in multi-task learning, remains underutilized in multivariate time series contexts, particularly for multivariate time series forecasting (MTSF) [52, 54]. MTSF is common in applications like medical data [6], electricity consumption [49], temperatures [27], and stock prices [44]. Various methods, from classical tools [7, 45] and statistical approaches like ARIMA [3, 4] to deep learning techniques [8, 19, 32, 38, 40, 51, 53, 55, 56], have been developed for this task. Some studies prefer univariate models for multivariate forecasting [32], while others introduce channel-wise attention mechanisms [19]. We aim to enhance a univariate model by integrating a regularization technique from random matrix theory and multi-task learning. ", "page_idx": 25}, {"type": "text", "text": "G.2 Architecture and Training Parameters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Architectures without MTL regularization. We follow Chen et al. [8], Nie et al. [32], and to ensure a fair comparison of baselines, we apply the reversible instance normalization (RevIN) of Kim et al. [21]. All the baselines presented here are univariate i.e. no operation is performed by the network along the channel dimension. For the PatchTST baseline [32], we used the official implementation than can be found on Github. The network used in Transformer follows the one in [19], using a single layer Transformer with one head of attention, while RevIN normalization and denormalization are applied respectively before and after the neural network function. The dimension of the model is $d_{\\mathrm{m}}=16$ and remains the same in all our experiments. DLinearU is a single linear layer applied for each channel to directly project the subsequence of historical length into the forecasted subsequence of prediction length. It is the univariate extension of the multivariate DLinear, DLinearM, used in Zeng et al. [53]. The implementation of SAMformer can be found here, and for the iTransformer architecture here. These two multivariate models serve as baseline comparisons. We reported the results found in [19] and [24]. For all of our experiments, we train our baselines PatchTST, DLinearU and Transformer with the Adam optimizer [22], a batch size of 32 for the ETT datasets and 256 for the Weather dataset , and the learning rates summarized in Table 2. ", "page_idx": 25}, {"type": "text", "text": "Architectures with MTL Regularization. We implemented the univariate PatchTST, DLinearU, and Transformer baselines with MTL regularization. Initially, we scale the inputs twice using RevIN normalization. The first scaling is applied to the univariate components, and the second scaling is applied to the multivariate components. For each channel, we then apply our model without MTL regularization. The outputs are concatenated along the channel dimension, and this concatenation is flattened to form a matrix of shape (batch size, $q\\times T)$ , where $q$ is the prediction horizon and $T$ is the number of channels. We then learn a square matrix $W$ of shape $(q\\times\\bar{T})\\times(q\\times T)$ for projection and reshape the result to obtain an output of shape (batch size, $q,T_{\\l}$ ). This method can be applied on top of any univariate model. Our regularized loss has been introduced in 5.4. ", "page_idx": 25}, {"type": "text", "text": "Training parameters. The training/validation/test split is $12/4/4$ months on the ETT datasets and $70\\%/\\bar{2}0\\%/10\\%$ on the Weather dataset. We use a look-back window $d=336$ for PatchTST and $d=512$ for DLinearU and Transformer, using a sliding window with stride 1 to create the sequences. The training loss is the MSE. Training is performed during 100 epochs and we use early stopping with a patience of 5 epochs. For each dataset, baselines, and prediction horizon $H\\doteq\\{9\\bar{6},\\bar{1}9\\bar{2},336,72\\bar{0}\\}$ , each experiment is run 3 times with different seeds, and we display the average of the test MSE over the 3 trials in Table 1. ", "page_idx": 25}, {"type": "table", "img_path": "FFW6rPz48Z/tmp/277e0131dda8bf1d3968db210fc975e7e533299d80a121c44db1dc44af9d0a25.jpg", "table_caption": ["Table 2: Learning rates used in our experiments. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.3 Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conduct our experiments on 3 publicly available datasets of real-world time series, widely used for multivariate long-term forecasting [8, 32, 51]. The 2 Electricity Transformer Temperature datasets ETTh1, and ETTh2 [55] contain the time series collected by electricity transformers from July 2016 to July 2018. Whenever possible, we refer to this set of 2 datasets as ETT. Weather [27] contains the time series of meteorological information recorded by 21 weather indicators in 2020. It should be noted Weather is large-scale datasets. The ETT datasets can be downloaded here while the Weather dataset can be downloaded here. Table 3 sums up the characteristics of the datasets used in our experiments. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "table", "img_path": "FFW6rPz48Z/tmp/93e61e8ae55c0f1e44a9667014f1db442d108ad8493621e0973555ceefe464cd.jpg", "table_caption": ["Table 3: Characteristics of the multivariate time series datasets used in our experiments. "], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "FFW6rPz48Z/tmp/190be017ecdfafbb67fb1c73e776248f5b63ce6ecf1e44dccd43566172d44ffd.jpg", "img_caption": ["G.4 Additional Experiments. ", "(j) Dataset ETTh1, Horizon 720 (k) Dataset ETTh2, Horizon 720 (l) Dataset Weather, Horizon 720 "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 4: Results of our optimization method on different datasets and horizons averaged across 3 different seeds for each gamma and lambda values for the PatchTST baseline ", "page_idx": 27}, {"type": "image", "img_path": "FFW6rPz48Z/tmp/cc628aa1d732f14e73f6dd76cba2a8eddc65acaf9492e57c6a4920b24aa1ff35.jpg", "img_caption": ["(j) Dataset ETTh1, Horizon 720 (k) Dataset ETTh2, Horizon 720 (l) Dataset Weather, Horizon 720 "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 5: Results of our optimization method on different datasets and horizons averaged across 3 different seeds for each gamma and lambda values for the DLinearU baseline ", "page_idx": 28}, {"type": "image", "img_path": "FFW6rPz48Z/tmp/5a3eef2d435408ee3d60033fd400eb0a53c6d106202e0abab2441caeacc75059.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 6: Results of our optimization method on different datasets and horizons averaged across 3 different seeds for each gamma and lambda values for the Transformer baseline ", "page_idx": 29}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "While the study provides valuable insights through its theoretical analysis within a linear framework, it is important to acknowledge its limitations. The linear approach serves as a solid foundation for understanding more complex models, but its practical applications may be constrained. Linear models, though mathematically tractable and often easier to interpret, might not fully capture the intricacies and nonlinear relationships present in real-world data, especially in the context of multivariate time series forecasting. ", "page_idx": 30}, {"type": "text", "text": "To address this limitation, we decided to extend our algorithm\u2019s application to more complex models, specifically within the nonlinear setting of neural networks. This transition aims to evaluate whether the theoretical insights derived from the linear framework hold true empirically when applied to neural networks. As part of this endeavor, an optimal parameter lambda was selected by an oracle, leading to promising results, as detailed in Section 5.4. This oracle-based selection underscores the potential efficacy of our approach when appropriately tuned, even in more complex, nonlinear contexts. ", "page_idx": 30}, {"type": "text", "text": "It is important to note that the limitations are not related to the real-world data itself, as our setting performs well in the context of multi-task regression for real-world data, as shown in Section 5.2. The difficulty arises from transitioning from a linear to a nonlinear model. The results in Section 5.4 are particularly encouraging, demonstrating that our method can improve upon univariate baselines by regularizing with an optimal lambda, as indicated by our oracle. While the oracle provides an upper bound on performance, actual implementation would require robust methods for hyperparameter optimization in non-linear scenarios, which remains an open area for further research. ", "page_idx": 30}, {"type": "text", "text": "By expanding the scope of our theoretical framework to encompass nonlinear models, we pave the way for future work that could focus on the theoretical analysis of increasingly complex architectures ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The datasets used are open-source and all the implementation details are given to reproduce the experimental results. The code will be made available in case of acceptance. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: All details to reproduce experiments are given. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]