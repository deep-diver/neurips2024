[{"figure_path": "1we1V3MAHD/figures/figures_0_1.jpg", "caption": "Figure 1: Motion-aware customized video generation results of MotionBooth. Our method animates a customized object with controllable subject and camera motions.", "description": "This figure shows several examples of videos generated by the MotionBooth model.  Each row represents a different scenario: a dog jumping down stairs, a monster toy walking across Times Square, a cat jumping over a wall, and a cartoon character running in a painted forest. The figure highlights the model's ability to generate videos with specific subject motions and camera movements, demonstrating the method's motion-aware capabilities.", "section": "Abstract"}, {"figure_path": "1we1V3MAHD/figures/figures_2_1.jpg", "caption": "Figure 2: The overall pipeline of MotionBooth. We first fine-tune a T2V model on the subject. This procedure incorporates subject region loss, video preservation loss, and subject token cross-attention loss. During inference, we control the camera movement with a novel latent shift module. At the same time, we manipulate the cross-attention maps to govern the subject motion.", "description": "This figure illustrates the training and inference stages of the MotionBooth model. During training, the model is fine-tuned on a specific subject using three loss functions: subject region loss, video preservation loss, and subject token cross-attention loss.  The inference stage involves using a latent shift module to control camera movement and manipulating cross-attention maps to control subject motion.  The figure visually represents the data flow and key components at each stage.", "section": "3 Method"}, {"figure_path": "1we1V3MAHD/figures/figures_3_1.jpg", "caption": "Figure 3: Case study on subject learning. \u201cRegion", "description": "This figure shows a case study on subject learning. The left column shows the results of a pre-trained text-to-video model. The middle column shows the subject images used for training. The right column shows the results of fine-tuning the model with different loss functions. The top row shows the results of fine-tuning with only the region loss, which results in overfitting to the background. The bottom row shows the results of fine-tuning with both region and video preservation loss, which results in better preservation of the video generation capabilities. indicates subject region loss. \u201cVideo\u201d indicates video preservation loss. The images are extracted from generated videos.", "section": "3.2 Subject Learning"}, {"figure_path": "1we1V3MAHD/figures/figures_4_1.jpg", "caption": "Figure 3: Case study on subject learning. \u201cRegion", "description": "This figure shows a case study on subject learning. It compares the results of pre-trained text-to-video model, subject only learning, region and video loss combined, and region and video loss. The results show that adding subject region loss and video preservation loss leads to better results than using only subject learning.", "section": "3.2 Subject Learning"}, {"figure_path": "1we1V3MAHD/figures/figures_5_1.jpg", "caption": "Figure 5: Illustration of camera movement control through shifting the noised latent.", "description": "This figure illustrates the process of controlling camera movement in MotionBooth's video generation. It involves shifting the noised latent (representing the video's visual information) based on user-specified camera movement ratios (horizontal and vertical).  The process is broken down into three steps: 1. Shift: The latent is shifted according to the input camera movement, creating missing regions in the latent space. 2. Sample Tokens: Tokens are sampled from the original latent space to fill these missing regions using context information (semantically similar regions in the latent). 3. Fill in the Missing Part: The sampled tokens are used to replace the missing parts.  The overall approach is training-free, enabling efficient and versatile camera movement control.", "section": "3.4 Camera Movement Control"}, {"figure_path": "1we1V3MAHD/figures/figures_8_1.jpg", "caption": "Figure 6: Qualitative comparison of customizing objects and controlling their motions.", "description": "This figure shows a qualitative comparison of different methods for customizing objects and controlling their motions in video generation.  The top row shows the input subject and motion. The subsequent rows show the results obtained using various methods, including DreamBooth, CustomVideo, DreamVideo, and the proposed MotionBooth method.  For each method, the generated videos are shown for two different prompts: one involving a cat jumping off stairs, and the other involving a toy riding a bike on a road. The comparison demonstrates the superiority of MotionBooth in terms of subject fidelity, motion accuracy, and overall video quality.", "section": "4.2 Main Results"}, {"figure_path": "1we1V3MAHD/figures/figures_8_2.jpg", "caption": "Figure 7: Qualitative comparison of camera motion control. Lines and points are used to help the readers track the camera movement more easily.", "description": "This figure compares the results of camera movement control using different methods: AnimateDiff, CameraCtrl, and MotionBooth (with Zeroscope and LaVie models).  The results show generated videos with different levels of control over camera movement, demonstrating MotionBooth's superior ability to achieve smooth and precise camera control as indicated by the lines and points guiding the viewer's eyes.", "section": "4.2 Main Results"}, {"figure_path": "1we1V3MAHD/figures/figures_9_1.jpg", "caption": "Figure 8: Human preference study. Our Motion-Booth achieves the best human preference scores in all the evaluation aspects.", "description": "This figure shows the results of a human preference study comparing MotionBooth with three baseline methods (DreamBooth, CustomVideo, and DreamVideo) across four evaluation aspects: motion alignment, camera alignment, subject alignment, and temporal consistency.  The bar chart displays the percentage of times each method was selected as the best for each aspect. MotionBooth consistently outperforms the other methods, achieving the highest preference rate in all four aspects, indicating its superiority in generating high-quality videos that are well-aligned with user input and expectations.", "section": "4.4 Human Preference Study"}, {"figure_path": "1we1V3MAHD/figures/figures_9_2.jpg", "caption": "Figure 9: Failure cases of MotionBooth.", "description": "This figure presents two failure cases of the MotionBooth model.  (a) shows a failure case involving multiple objects, where the subject's appearance merges with another object in the scene. (b) shows a failure case with a challenging motion, where the subject performs an unrealistic action, highlighting limitations in subject separation and motion understanding capabilities.", "section": "4.5 Limitations and Future Work"}, {"figure_path": "1we1V3MAHD/figures/figures_13_1.jpg", "caption": "Figure 10: The evaluation dataset. We present one picture for each subject.", "description": "This figure shows the 26 objects used in the evaluation dataset for MotionBooth.  The images represent a diverse range of subjects, including pets (dogs, cats), plushies (panda, octopus), toys (robot, monster), cartoons, and vehicles. This diversity allows for a thorough assessment of the model's performance across various object categories and visual characteristics.", "section": "Appendix A.1 Implementation Details"}, {"figure_path": "1we1V3MAHD/figures/figures_13_2.jpg", "caption": "Figure 1: Motion-aware customized video generation results of MotionBooth. Our method animates a customized object with controllable subject and camera motions.", "description": "This figure shows the results of MotionBooth, a novel framework for animating customized subjects. The figure demonstrates the ability of MotionBooth to generate videos with precise control over both object and camera movements, even when using only a few images of a specific object. The top row shows the input subject, the subject's motion, and the camera motion. The bottom row shows the generated videos, which accurately reflect the specified inputs. This highlights MotionBooth's ability to efficiently fine-tune a text-to-video model to capture the object's shape and attributes while simultaneously controlling the motions in the generated videos.", "section": "Abstract"}, {"figure_path": "1we1V3MAHD/figures/figures_14_1.jpg", "caption": "Figure 1: Motion-aware customized video generation results of MotionBooth. Our method animates a customized object with controllable subject and camera motions.", "description": "This figure showcases the results of MotionBooth, a novel framework for motion-aware customized text-to-video generation. It highlights the ability of the model to animate a specific object\u2014controlled by a few input images\u2014with precise control over both object and camera movements. The examples displayed show diverse scenarios and subject motions, demonstrating the effectiveness of the approach in generating high-quality, customized videos.", "section": "Abstract"}, {"figure_path": "1we1V3MAHD/figures/figures_14_2.jpg", "caption": "Figure 7: Qualitative comparison of camera motion control. Lines and points are used to help the readers track the camera movement more easily.", "description": "This figure compares the results of camera motion control using three different methods: AnimateDiff, CameraCtrl, and MotionBooth (the proposed method).  Two base T2V models, Zeroscope and LaVie are used. The top row shows results for videos depicting a playful puppy in flowers; the bottom row shows videos of a villa in a garden. The images illustrate how effectively each method controls camera movement based on user-specified camera trajectories.  Lines and points guide the eye to illustrate the camera's movement trajectory, making it easier to compare the differences in camera movement.", "section": "4.2 Main Results"}, {"figure_path": "1we1V3MAHD/figures/figures_16_1.jpg", "caption": "Figure 13: Ablation study on motion control hyperparameters. (a) Ablation study on subject motion control. Only the first frame is shown. Experiments on Zeroscope. (b) Ablation study on latent shift. Experiments on LaVie. A higher \u03c3 means an earlier denoising step.", "description": "This figure presents ablation studies on the hyperparameters used for controlling motion in the MotionBooth model.  Subfigure (a) shows the impact of varying the alpha (\u03b1) parameter and the temporal threshold (\u03c4) on subject motion control, using Zeroscope as the base model. Only the first frame of the generated videos is shown for each configuration.  Subfigure (b) investigates the effect of different latent shift start (\u03c3\u2081) and end (\u03c3\u2082) timesteps during inference, using LaVie as the base model, showing the effects of changing the range of timesteps when the latent shift module is applied.  A higher \u03c3 value indicates an earlier denoising step. The results demonstrate how these hyperparameters influence the model's ability to precisely control both subject and camera movements.", "section": "4.3 Ablation Studies"}, {"figure_path": "1we1V3MAHD/figures/figures_20_1.jpg", "caption": "Figure 1: Motion-aware customized video generation results of MotionBooth. Our method animates a customized object with controllable subject and camera motions.", "description": "This figure showcases the results of the MotionBooth model.  Several examples are shown demonstrating the ability to generate videos of customized objects (e.g., a specific dog, toy monster) with precise control over the object's movement (e.g., jumping, walking) and camera movement (e.g., left, right).  The figure visually demonstrates the model's core capability: animating customized subjects while simultaneously controlling both subject and camera motions.", "section": "Abstract"}]