[{"type": "text", "text": "MotionBooth: Motion-Aware Customized Text-to-Video Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jianzong $\\mathbf{W}\\mathbf{u}^{1,3}$ , Xiangtai $\\mathbf{Li^{2,3}}$ \u2020, Yanhong Zeng3, Jiangning Zhang4, Qianyu Zhou5, Yining $\\mathbf{Li}^{3}$ , Kai Chen3, Yunhai Tong1 1PKU 2S-Lab, NTU 3Shanghai AI Laboratory 4ZJU 5SJTU Project Page: https://jianzongwu.github.io/projects/motionbooth Code: https://github.com/jianzongwu/MotionBooth Email: jzwu@stu.pku.edu.cn, xiangtai94@gmail.com ", "page_idx": 0}, {"type": "image", "img_path": "1we1V3MAHD/tmp/7f8531e832d3cc0d870a99b80ce8fbfdfa6d6992e574961d53090d718941beeb.jpg", "img_caption": ["Figure 1: Motion-aware customized video generation results of MotionBooth. Our method animates a customized object with controllable subject and camera motions. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we present MotionBooth, an innovative framework designed for animating customized subjects with precise control over both object and camera movements. By leveraging a few images of a specific object, we efficiently finetune a text-to-video model to capture the object\u2019s shape and attributes accurately. Our approach presents subject region loss and video preservation loss to enhance the subject\u2019s learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals. Additionally, we propose training-free techniques for managing subject and camera motions during inference. In particular, we utilize cross-attention map manipulation to govern subject motion and introduce a novel latent shift module for camera movement control as well. MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos. Extensive quantitative and qualitative evaluations demonstrate the superiority and effectiveness of our method. Models and codes will be made publicly available. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Generating videos for customized subjects, such as specific scenarios involving a particular dog\u2019s type or appearance, has gained research attention [55, 25, 42]. This customized generation field originated from text-to-image (T2I) generation methods, which learn a subject\u2019s appearance from a few images and generate diverse images of that subject [12, 42, 30]. Following them, subject-driven text-to-video (T2V) generation has seen increasing interest, which has found a wide range of applications in personal shorts or film production [55, 25, 42, 57, 13]. Can you imagine your toy riding along the road from a distance to the camera or your pet dog dancing on the street from the left to the right? However, rendering such lovely imaginary videos is a challenging task. It often involves subject learning and motion injection while maintaining the generative capability to generate diverse scenes. Notably, VideoBooth [25] trains an image encoder to embed the subject\u2019s appearance into the model, generating a short clip of the subject. However, the generated videos often display minimal or missing motion, resembling a \"moving image.\" This approach underutilizes the motion diversity of pre-trained T2V models. Another line of works [57, 61, 13] fine-tunes the customized model on specific videos, requiring motion learning for each specific camera or subject motion type. Their pipelines restrict the type of motion and require fine-tuning a new adapter for each motion type, which is inconvenient and computationally expensive. ", "page_idx": 1}, {"type": "text", "text": "The key lies in the confilct between subject learning and video motion preservation. During subject learning, training on limited images of the specific subject significantly shifts the distribution of the base T2V model, leading to significant degradation (e.g., blurred backgrounds and static video). Therefore, existing methods often need additional motion learning for specific motion control. In this paper, we argue that the base T2V model already has diverse motion prior, and the key is to preserve video capability during subject learning and digging out the motion control during inference. ", "page_idx": 1}, {"type": "text", "text": "To ensure subject-driven video generation with universal and precise motion control, we present MotionBooth, which can perform motion-aware customized video generation. The videos generated by MotionBooth are illustrated in Fig. 1. MotionBooth can take any combination of subject, subject motion, and camera motion as inputs and generate diverse videos, maintaining quality on par with pre-trained T2V models. ", "page_idx": 1}, {"type": "text", "text": "MotionBooth learns subjects without hurting video generation capability, enabling a training-free motion injection for subject-driven video generation. First, during subject learning, we introduce subject region loss and video preservation loss, which enhance both subject fidelity and video quality. In addition, we present a subject token cross-attention loss to connect the customized subject with motion control signals. During inference, we propose training-free techniques to control the camera and subject motion. We directly manipulate the cross-attention maps to control the subject motion. We also propose a novel latent shift module to govern the camera movement. It shifts the noised latent to move the camera pose. Through quantitative and qualitative experiments, we demonstrate the superiority and effectiveness of the proposed motion control methods, and they can be applied to different base T2V models without further tuning. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: 1) We propose a unified framework, MotionBooth, for motion-aware customized video generation. To our knowledge, this is the first framework capable of generating diverse videos by combining customized subjects, subject motions, and camera movements as input. 2) We propose a novel loss-augmented training architecture for subject learning. This includes subject region loss, video preservation loss, and subject token cross-attention loss, significantly enhancing subject fidelity and video quality. 3) We develop innovative, training-free methods for controlling subject and camera motions. Extensive experiments demonstrate that MotionBooth outperforms existing state-of-the-art video generation models. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Text-to-video generation. T2V generation leverages deep learning models to interpret text input and generate corresponding video content. It builds upon earlier breakthroughs in text-to-image generation [44, 41, 19, 21, 37, 48, 60, 62] but introduces more complex dynamics by incorporating motion and time [46, 20, 18, 2, 68, 59]. Recent advancements particularly leverage diffusion-based architectures. Notable models such as ModelScopeT2V [51] and LaVie [54] integrate temporal layers within spatial frameworks. VideoCrafter1 [6] and VideoCrafter2 [7] address the scarcity of video data by utilizing high-quality image datasets. Latte [36] and W.A.L.T [14] adopt Transformers as backbones [49]. VideoPoet [29] explores generating videos autoregressively to produce consistent long videos. Recent Sora [3] excels in generating videos with impressive quality, stable consistency, and varied motion. Despite these advancements, controlling video content through text alone remains challenging, highlighting a continuing need for research into more refined control signals. ", "page_idx": 1}, {"type": "image", "img_path": "1we1V3MAHD/tmp/10c66302c3454b4febe84e36e374053c52a910661f1f4432837eef18c682ed0b.jpg", "img_caption": ["Figure 2: The overall pipeline of MotionBooth. We first fine-tune a T2V model on the subject. This procedure incorporates subject region loss, video preservation loss, and subject token cross-attention loss. During inference, we control the camera movement with a novel latent shift module. At the same time, we manipulate the cross-attention maps to govern the subject motion. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Customized generation. Generating images and videos with customized subjects is attracting growing interest. Most works concentrate on learning a specific subject with a few images from the same subject [23, 8, 10, 43, 47, 39] or specific domains [15, 16, 50]. Textual Inversion [12] proposes to train a new word to capture the feature of an object. In contrast, DreamBooth [42] fine-tunes the whole U-Net, resulting in a better IP preservation ability. Following them, many works explore more challenging tasks, such as customizing multiple objects [30, 55, 33, 5], developing common subject adapter [58, 25, 65, 11, 67], and simultaneously controlling their positions [11, 33]. However, the customization of video models from a few images often results in overfitting. The models fail to incorporate significant motion dynamics. A recent work, DreamVideo [57], addresses this by learning specific motion types from video data. Yet, this method is restricted to pre-defined motion types and lacks the flexibility of text-driven input. In contrast, our work introduces MotionBooth to control both the subject and camera motions without needing pre-defined motion prototypes. ", "page_idx": 2}, {"type": "text", "text": "Motion-aware video generation. Recent works explore incorporating explicit motion control in video generation. This includes camera and object motions. To control camera motion, existing works like AnimateDiff [13], VideoComposer [53], CameraCtrl [17], Direct-A-Video [66], and MotionCtrl [56] design specific modules to encode the camera movement or trajectory. These models usually rely on training on large-scale datasets [1, 9], leading to high computational costs. In contrast, our MotionBooth framework builds a training-free camera motion module that can be easily integrated with any T2V model, eliminating the need for re-training. For object motion control, recent works [63, 31, 32, 24, 66, 4, 69, 27, 22] propose effective methods to manipulate attention values during the inference stage. Inspired by these approaches, we connect subject text tokens to the subject position using a subject token cross-attention loss. This allows for straightforward control over the motion of a customized object by adjusting cross-attention values. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Task formulation. We focus on generating motion-aware videos featured by a customized subject. To customize video subjects, we fine-tune the T2V model on a specific subject. This process can be accomplished with just a few (typically 3-5) images of the same subject. During inference, the fine-tuned model generates motion-aware videos of the subject. The motion encompasses both camera and subject movements, which are freely defined by the user. For camera motion, the user inputs the horizontal and vertical camera movement ratios, denoted as $\\mathbf{c}_{c a m}=[c_{x},c_{y}]$ . For subject motion, the user provides a bounding box sequence $[\\mathbf{B}_{1},\\mathbf{B}_{2},...,\\mathbf{B}_{L}]$ to indicate the desired positions of the subject, where $L$ represents the video length. Each bounding box specifies the x-y coordinates of the top-left and bottom-right points for each frame. By incorporating these conditional inputs, the model is expected to generate videos that include a specific subject, along with predefined camera movements and subject motions. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Overall pipeline. The overall pipeline of MotionBooth is illustrated in Fig. 2. During the training stage, MotionBooth learns the appearance of the given subject by fine-tuning the T2V model. To prevent overfitting, we introduce video preservation loss and subject region loss in Section 3.2. Additionally, we propose a subject token cross-attention (STCA) loss in Section 3.2 to explicitly connect the subject tokens with the subject\u2019s position on cross-attention maps, facilitating the control of subject motion. Camera and subject motion control are performed during the inference stage. We manipulate the cross-attention maps by amplifying the subject tokens and their corresponding regions while suppressing other tokens in Section 3.3. This ensures that the generated subjects appear in the desired positions. By training on the cross-attention map, the STCA loss enhances the subjects\u2019 motion control. For camera movement, we introduce a novel latent shift module to shift the noised latent directly, achieving smooth camera movement in the generated videos in Section 3.4. ", "page_idx": 3}, {"type": "text", "text": "3.2 Subject Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a few images of a subject, previous works have demonstrated that fine-tuning a diffusion model on these images can effectively learn the appearance of the subject [42, 23, 8, 10, 43, 47]. However, two significant challenges remain. First, due to the limited size of the dataset, the model quickly overftis the input images, including their backgrounds, within a few steps. This overfitting of the background impedes the generation of videos with diverse scenes, a problem also noted in previous works [42, 12]. Second, fine-tuning T2V models using images can impair the model\u2019s inherent ability to generate videos, leading to severe background degradation in the generated videos. To illustrate these issues, we ", "page_idx": 3}, {"type": "image", "img_path": "1we1V3MAHD/tmp/172f8204a6a22d1290c9bbc583e060001280891921a76b2551494431605edbfd.jpg", "img_caption": ["Figure 3: Case study on subject learning. \u201cRegion\u201d indicates subject region loss. \u201cVideo\u201d indicates video preservation loss. The images are extracted from generated videos. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "conducted a toy experiment. As depicted in Fig. 3, without any modifications, the model overftis the background to the subject image. To address this, we propose computing the diffusion reconstruction loss solely within the subject region. However, even with this adjustment, the background in the generated videos remains over-smoothed. This degradation likely results from tuning a T2V model exclusively with images, which damages the model\u2019s original weights for video generation. To mitigate this, we propose incorporating video data as preservation data during the training process. Although training with video data but without subject region loss still suffers from overfitting, our approach, MotionBooth, can generate videos with detailed and diverse backgrounds. ", "page_idx": 3}, {"type": "text", "text": "Preliminary. T2V diffusion models learn to generate videos by reconstructing noise in a latent space [42, 30, 55, 12]. The input video is first encoded into a latent representation $\\mathbf{z}_{\\mathrm{0}}$ . Noise $\\epsilon$ is added to this latent representation, resulting in a noised latent $\\mathbf{z}_{t}$ , where $t$ represents the timestamp. This process simulates the reverse process of a fixed-length Markov Chain [41]. The diffusion model $\\epsilon_{\\theta}$ is trained to predict this noise. The training loss, which is a reconstruction loss, is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathbb{E}_{\\mathbf{z},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t,\\mathbf{c}}\\left[||\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathbf{c},t)||_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{c}$ is the conditional input used in classifier-free guidance methods, which can be text or a reference image. During inference, a pure noise ${\\bf z}_{T}$ is gradually denoised to a clean latent $\\mathbf{z}_{\\mathrm{0}}^{\\prime}$ , where $T$ is the length of the Markov Chain. The clean latent is then decoded back into RGB space to generate the video $\\mathbf{X^{\\prime}}$ . ", "page_idx": 3}, {"type": "text", "text": "Subject region loss. To address the challenge of overfitting backgrounds in training images, we propose a subject region loss. The core idea is to calculate the diffusion reconstruction loss exclusively within the subject region, thereby preventing the model from learning the background. Specifically, we first extract the subject mask for each image. This can be done manually or through automatic methods, such as a segmentation model. In practice, we use SAM [28] to collect all the masks. The subject region loss is then calculated as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{s u b}=\\mathbb{E}_{\\mathbf{z},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t,\\mathbf{c}}\\left[||(\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathbf{c}_{i},t))\\cdot\\mathbf{M}||_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where M represents the binary masks for the training images. These masks are resized to the latent space to compute the dot product. $\\mathbf{c}_{i}$ is a fixed sentence in the format \"a [V] [class name],\" where \"[V]\" is a rare token and \"[class name]\" is the class name of the subject [42]. We have found that with the subject region loss, the trained model effectively avoids the background overftiting problem. ", "page_idx": 4}, {"type": "text", "text": "Video preservation loss. Image customization datasets like DreamBooth [42] and CustomDiffusion [30] provide excellent examples of multiple images from the same subject. However, in the customized video generation task, directly fine-tuning the video diffusion model on images leads to significant background degradation. Intuitively, this image-based training process may harm the original knowledge embedded in video diffusion models. To address this, we introduce a video preservation loss designed to maintain video generation knowledge by joint training with video data. Unlike the class-specific preservation data used in previous works [42, 55], we utilize common videos with captions denoted as $\\mathbf{c}_{v}$ . Our experiments in Section 4 demonstrate that common videos are more effective for subject learning and preserving video generation capabilities. The loss function is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v i d}=\\mathbb{E}_{\\mathbf{z},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t,\\mathbf{c}}\\left[||\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},\\mathbf{c}_{v},t)||_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Subject token cross-attention loss. To control the subject\u2019s motion, we directly manipulate the cross-attention maps during inference. Since we introduce a unique token, \u201c[V]\u201d, in the training stage and associate it with the subject, we need to link this special token to the subject\u2019s position within the cross-attention maps. As illustrated in Fig. 4, fine-tuning the model does not effectively connect the unique token to the cross-attention maps. Therefore, we propose a Subject Token Cross-Attention (STCA) loss to guide this process explicitly. First, we extract the crossattention map, A, at the tokens \u201c[V] [class name]\u201d. We then apply a Binary Cross-Entropy Loss to ensure that the corresponding attention map is larger at the subject\u2019s position and smaller outside this region. This process incorporates the subject mask and can be expressed as: ", "page_idx": 4}, {"type": "image", "img_path": "1we1V3MAHD/tmp/6a2cdc3860db5ce9dff90dc7333ca97f373121ff13bc1493d6bf1e5965b89743.jpg", "img_caption": ["(a) Input Images (b) w/o STCA loss (c) w/ STCA loss "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: Case study on subject token cross-attention maps. (b) and (c) are visualization of cross-attention maps on tokens \u201c[V]\u201d and \u201cdog\u201d. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s t c a}=-\\left[\\mathbf{M}\\log(\\mathbf{A})+(1-\\mathbf{M})\\log(1-\\mathbf{A})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "During training, the overall loss function is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathscr{L}=\\mathscr{L}_{s u b}+\\lambda_{1}\\mathscr{L}_{v i d}+\\lambda_{2}\\mathscr{L}_{s t c a},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{1}$ and $\\lambda_{2}$ are hyperparameters that control the weights of the different loss components. ", "page_idx": 4}, {"type": "text", "text": "3.3 Subject Motion Control ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We chose bounding boxes as the motion control signal for subjects because they are easy to draw and manipulate. In contrast, providing object masks for every frame is labor-intensive, requiring consideration of the subject\u2019s shape transformation between frames. In practice, we find that bounding boxes are sufficient for precisely controlling the positions of subjects. Previous works like GLIGEN [31] attempt to control object positions by training an extra condition module with large-scale image data. However, these training methods fix the models and cannot easily align with customized models fine-tuned for specific subjects. Therefore, we adopt an alternative approach that directly edits the cross-attention maps during inference in a training-free manner [66, 27, 4]. This cross-attention editing method is plug-and-play and can be used with any customized model. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In cross-attention layers, the query features $\\mathbf{Q}$ are extracted from the video latent and represent the vision features. The key and value features $\\mathbf{K}$ and $\\mathbf{V}$ are derived from input language tokens. The calculation process of the edited cross-attention layer can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{EditedCrossAttn}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d}}+\\alpha\\mathbf{S}\\right)\\mathbf{V},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d$ is the feature dimension of $\\mathbf{Q}$ and serves as a normalization term. $\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d}}$ is the normalized production between $\\mathbf{Q}$ and $\\mathbf{K}$ , representing the attention scores between vision and language features. We manipulate the production by adding a new term $\\alpha{\\bf S}$ , where S has positive values on the subject region provided in bounding boxes and large negative values outside the desired positions. $\\alpha$ is a hyperparameter to control the editing strength. The editing matrix $\\mathbf{S}$ is set as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{k}[i,j]=\\left\\{{\\underset{0,}{1-\\frac{|\\mathbf{B}_{k}|}{|\\mathbf{Q}|}}},\\right.\\}\\,\\left.{\\mathrm{if~}}i\\in\\mathbf{B}_{k}{\\mathrm{~and~}}j\\in\\mathbf{P}{\\mathrm{~and~}}t\\geq\\tau}\\\\ {0,\\quad\\qquad\\qquad\\qquad\\mathrm{if~}i\\in\\mathbf{B}_{k}{\\mathrm{~and~}}j\\in\\mathbf{P}{\\mathrm{~and~}}t<\\tau}\\\\ {-\\infty,\\quad\\quad\\,\\,\\,\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $i,\\,j.$ , and $k$ indicate the vision token, language token, and frame indexes, respectively. P represents the indexes for subject language tokens in the text prompt. In this work, we choose \u201c[V]\u201d and \u201c[class name]\u201d as subject tokens. The SCTA loss in Section 3.2 binds the two tokens with cross-attention maps. $t$ is the denoising timestamp and $\\tau$ is a hyperparameter defining a timestamp threshold. Since diffusion models tend to form the approximate object layout in earlier denoising steps and refine the details in later steps [63], we apply stronger attention amplification in earlier steps and no amplification in later steps. Note that attention suppression outside the bounding box regions persists throughout the generation. $|\\mathbf{B}_{k}|$ and $|\\mathbf{Q}|$ are the areas of the box and query, respectively. Following previous works [27, 66], smaller boxes should have larger amplifications, and we do not apply any editing on the <start> and <end> tokens. ", "page_idx": 5}, {"type": "text", "text": "Discussion. An important aspect is how the necessary information from other language tokens is integrated into the generated outputs, given that tokens such as verbs and background nouns are assigned minimal values. We propose that this information is extracted through the <start> and <end> tokens. Given that Transformer-based language encoders like CLIP [40] are typically trained on classification tasks, they often encode the overall context of a sentence into these special tokens. Thus, despite the suppression of other tokens during the softmax calculation, the model can still access relevant information about verbs, background elements, and other components necessary for the generation process. To support this explanation, we conducted an experiment in which we examined the softmax outputs using a naive text-to-video pipeline. The results showed that the <start> token consistently held the highest softmax value, close to 1, while the <end> token had the second-largest value. The remaining tokens, including those representing nouns, adjectives, verbs, and conjunctions, were distributed among the remaining softmax values. ", "page_idx": 5}, {"type": "text", "text": "3.4 Camera Movement Control ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Simply editing the cross-attention map can efficiently control the motion of the subject. This suggests that the latent can be considered a \"shrunk image,\" which maintains the same visual geographic distribution as the generated images. For camera movement control, an intuitive approach is to directly shift the noised latent during inference based on the camera movement signal $\\mathbf{c}_{c a m}=[c_{x},c_{y}]$ . The latent shift pipeline is illustrated in Table 2. The key challenge with this idea is fliling in the missing parts caused by the latent shift (the question mark region in Step ", "page_idx": 5}, {"type": "image", "img_path": "1we1V3MAHD/tmp/da4b8b7dc7fcd492f0b01459de54237d8ce2deb5402f10f10a97c67c973ac49c.jpg", "img_caption": ["Figure 5: Illustration of camera movement control through shifting the noised latent. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "1). To address this issue, we propose sampling ", "page_idx": 6}, {"type": "text", "text": "tokens from the original noised latent and using them to fill the gap. This is based on the prior knowledge that when a camera moves in a video, the new scene it captures is semantically close to the previous one. For example, in a video with forest scenes, when the camera pans left, it is highly likely to capture more trees similar to those in the original scene. Another assumption is that in a normally angled video, a visual element is more likely to be semantically close to elements along the same $\\mathbf{X}$ -axis or y-axis rather than other elements. For instance, in the waterfall video in Fig. 5, trees are at the top and bottom, spreading horizontally, while the waterfall spans the middle $\\mathbf{X}$ -axis area. Experimentally, we over that sampling tokens horizontally and vertically provides better initialization and results in smoother video transitions. Randomly sampling tokens degrades the generated video quality. The latent shift process for timestamp $t$ can be formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{x}=\\mathrm{SampleHorizontal}(\\mathbf{z}_{t},\\mathbf{B},c_{x}),}\\\\ &{\\mathbf{h}_{y}=\\mathrm{SampleVertical}(\\mathbf{z}_{t},\\mathbf{B},c_{y}),}\\\\ &{\\mathbf{z}_{\\mathrm{shift}}=\\mathrm{Crop}(\\mathrm{Shift}(\\mathbf{z}_{t},c_{x},c_{y})),}\\\\ &{\\quad\\mathbf{z}_{t}=\\mathrm{Fill}(\\mathbf{z}_{\\mathrm{shift}},\\mathbf{h}_{x},\\mathbf{h}_{y},c_{x},c_{y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{h}_{x}$ and $\\mathbf{h}_{y}$ are sampled tokens along the $\\mathbf{X}$ and y axes, respectively. $\\mathrm{{Crop}(\\cdot)}$ removes the tokens outside the camera view after the shift. $\\mathbf{B}$ is the subject bounding box. We filter out the tokens belonging to the subjects because they are not likely to occur in the new scenes. In addition, to avoid a drastic change in latent in one shift, we spread the latent shift over multiple timestamps, with each step only shifting a small number of tokens. Note that the latent shift needs to be applied after the subject\u2019s approximate layout is fixed but before the video details are completed. We set a pair of hyperparameters $\\sigma_{1}$ and $\\sigma_{2}$ . The latent shift only applies in the timestamp range $[\\sigma_{1},\\sigma_{2}]$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. For customization, we collect a total of 26 objects from DreamBooth [42] and CustomDiffusion [30]. These objects include pets, plushies, toys, cartoons, and vehicles. To evaluate camera and object motion control, we built a dataset containing 40 text-object motion pairs and 40 text-camera motion pairs, ensuring that the camera and object motion patterns are consistent with the text prompts. This dataset evaluates the videos generated for each subject in various scenarios and motions. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We train MotionBooth for 300 steps using the AdamW optimizer, with a learning rate of 5e-2 and a weight decay of 1e-2. We collect 500 preservation videos from the Panda-70M [9] training set, chosen randomly. Each batch consists of one batch for images and one for videos, with batch sizes equal to the number of training images and 1 for images and videos, respectively. The loss weight parameters $\\lambda_{1}$ and $\\lambda_{2}$ are set to 1.0 and 0.01. We use Zeroscope and LaVie as base models. During inference, we perform 50-step denoising using the DDIM scheduler and set the classifier-free guidance scale to 7.5. The generated videos are $576\\mathrm{x}320\\mathrm{x}24$ and $512\\mathrm{x}320\\mathrm{x}16$ for Zeroscope and LaVie, respectively. The training process finishes in around 10 minutes in a single NVIDIA A100 80G GPU. Additional implementation details can be found in Appendix A.1. ", "page_idx": 6}, {"type": "text", "text": "Baselines. Since we are pioneering motion-aware customized video generation, we compare our methods with closely related works, including DreamBooth [42], CustomVideo [55], and DreamVideo [57]. Dreambooth customizes subjects for text-to-image generation. We follow its practice with class preservation images and fine-tune T2V models for generating videos. CustomVideo is a recent video customizing method. We adopt its parameter-efficient training procedure. DreamVideo learns motion patterns from video data. To provide such data, we sample videos from Panda-70M, which are most relevant to the evaluation motions. Since these methods cannot control motions during inference, we apply our camera and object motion control technologies for a fair comparison. Additionally, we compare our camera control method with training-based methods, AnimateDiff [13] and CameraCtrl [17], focusing on camera motion control without subject customization. Since AnimateDiff is trained with only basic camera movement types and cannot take user-defined camera movement $\\mathbf{c}_{c a m}=[c_{x},c_{y}]$ as input, we use the closest basic movement type for evaluation. ", "page_idx": 6}, {"type": "table", "img_path": "1we1V3MAHD/tmp/fe9ebaced2f0c97e482e1f02baee19b7a3e16f93e732e4ad61360dad20a11193.jpg", "table_caption": ["Table 1: Quantitative comparison for motion-aware customized video generation. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "1we1V3MAHD/tmp/030f603ed722b74f1f3a21262e7a46bb685073aa40f744d06be18d2b461ef8b3.jpg", "table_caption": ["Table 2: Quantitative comparison for camera movement control "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Evaluation metrics. We evaluate motion-aware customized video generation from four aspects: region subject fidelity, temporal consistency, camera motion fidelity, and video quality. 1) To ensure the subject is well-preserved and accurately generated in the specified motion, we introduce region CLIP similarity (R-CLIP) and region DINO similarity metrics (R-DINO). These metrics utilize the CLIP [40] and DINOv2 [38] models to compute the similarities between the subject images and frame regions indicated by bounding boxes. Additionally, we use CLIP image-text similarity (CLIP-T) to measure the similarity between entire frames and text prompts. 2) We evaluate temporal consistency by computing CLIP image features between each consecutive frame. 3) We use VideoFlow [45] to predict the optical flow of the generated videos. Then, we calculate the flow error by comparing the predicted flow with the ground-truth camera motion provided in the evaluation dataset. 4) We randomly select 1000 videos from the MSRVTT dataset [64], predict their camera motion sequences with VideoFlow [45], and compute the FVD metric for camera motion control only. ", "page_idx": 7}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantitative results. We conduct quantitative comparisons with baseline models on both motionaware customized video generation and camera movement control. The results for motion-aware customized video generation are shown in Table 1. The results demonstrate that MotionBooth outperforms all baselines on both Zeroscope and LaVie models, indicating that our proposed technologies can be extended to different T2V models. Thanks to the training-free architecture of subject and camera motion control methods, MotionBooth is expected to be adaptable to more open-sourced models in the future, such as Sora [3]. Notably, DreamVideo [57] achieves the second-best scores in T-Cons. and flow error, which aligns with our observation that incorporating video data as auxiliary training data enhances video generation performance. On the other hand, CustomVideo [55] shows inferior performance in R-DINO scores, indicating a poorer ability to generate subjects in given positions. This may be attributed to its approach of only fine-tuning the text embeddings and cross-attention layers of the diffusion models, which is insufficient for learning the subjects. ", "page_idx": 7}, {"type": "text", "text": "For camera movement control, we compare our method with two training-based methods, AnimateDiff [13] and CameraCtrl [17]. The results are shown in Table 2. Remarkably, MotionBooth achieves superior results compared to the two baselines with our training-free latent shift module. Specifically, MotionBooth outperforms the recent method CameraCtrl by 0.617, 0.015, and 0.009 in flow error, CLIP-T, and T-Cons. metrics with Zeroscope, and 0.511, 0.004, and 0.024 for the LaVie model. These results demonstrate that the latent shift method is simple yet effective. ", "page_idx": 7}, {"type": "text", "text": "Qualitative results. The qualitative comparison results for video generation with customized objects and controlled subject motions are presented in Fig. 6. Our observations reveal that MotionBooth excels in subject motion alignment, text prompt alignment, and overall video quality. In contrast, ", "page_idx": 7}, {"type": "image", "img_path": "1we1V3MAHD/tmp/047c5dc76542131ed10fc7a3a83af618b523a0b9af22ce8a3d8b109273594f59.jpg", "img_caption": ["Figure 6: Qualitative comparison of customizing objects and controlling their motions. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "1we1V3MAHD/tmp/200f5df91dca3778dcd6757e1e2f8033c840d05c8bb6f55cee3061bee1ad3525.jpg", "img_caption": ["Figure 7: Qualitative comparison of camera motion control. Lines and points are used to help the readers track the camera movement more easily. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "1we1V3MAHD/tmp/1272380346021cc326fc79d9efc43a5ea1f6ae7c7b973ba05611d9657a393622.jpg", "table_caption": ["Table 3: Ablation study for training technologies. \u201cmask\u201d means subject region loss. \u201cSTCA\u201d means subject token cross-attention loss. \u201cvideo\u201d means video preservation loss. \u201cw/ class video\u201d means utilizing class-specific videos in video preservation loss. The results are evaluated on LaVie. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "DreamBooth and CustomVideo produce videos with vague backgrounds, highlighting that generated backgrounds deteriorate when training is conducted without video data. Additionally, CustomVideo and DreamVideo struggle to capture the subjects\u2019 appearances, likely because their approach tunes only part of the diffusion model, preventing the learning process from fully converging. ", "page_idx": 8}, {"type": "text", "text": "We also conduct qualitative experiments focused on camera movement control, with results shown in Fig. 7. AnimateDiff, limited to basic movements, does not support user-defined camera directions. Although the CameraCtrl method can accept user input, it generates videos with subpar aesthetics and objects that exhibit flash movements. In contrast, our MotionBooth model outperforms both the Zeroscope and Lavie models. The proposed latent method generates videos that adhere to user-defined camera movements while maintaining time consistency and high video quality. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Training technologies. We analyze the technologies proposed during the subject learning stage. The ablation results are shown in Table 3. Clearly, without the proposed modules, the quantitative metrics drop accordingly. These results demonstrate that the proposed subject region loss, STCA loss, and video preservation loss are beneficial for subject learning and generating motion-aware customized videos. Specifically, the R-DINO metric decreases significantly by 0.256 without the subject region loss, highlighting its core contribution in filtering out image backgrounds during training. Additionally, the \"w/ class video\" experiment, which uses class-specific videos instead of randomly sampled common videos, yields worse results. This approach restricts the scenes and backgrounds in class-specific videos, hindering the models\u2019 ability to generalize effectively. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.4 Human Preference Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To evaluate our approach to understanding user preferences, we conducted a user study experiment. We collected 30 groups of videos generated by MotionBooth and baseline methods. We then asked 7 colleagues to select the best videos based on the following criteria: subject motion alignment, camera movement alignment, subject appearance alignment, and temporal consistency. For each group of videos, the annotators selected only the best one. For the subject appearance alignment, the annotators were provided with corresponding subject images. As ", "page_idx": 9}, {"type": "image", "img_path": "1we1V3MAHD/tmp/5a6232375fed1ec0396bda5a0b50608f703b83bb8a0892ff6add054928bc44af.jpg", "img_caption": ["Figure 8: Human preference study. Our MotionBooth achieves the best human preference scores in all the evaluation aspects. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "shown in Fig. 8, MotionBooth was the most preferred method across all models and evaluation aspects, particularly in subject appearance alignment. These results further demonstrate the effectiveness of our method. ", "page_idx": 9}, {"type": "text", "text": "4.5 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Fig. 9, we illustrate several failure cases of MotionBooth. One significant limitation of MotionBooth is its struggle with generating videos involving multiple objects. As shown in Fig. 9(a), the subject\u2019s appearance can sometimes merge with other objects, resulting in visually confusing outputs. This issue might be resolved by incorporating advanced training technologies for multiple subjects. ", "page_idx": 9}, {"type": "text", "text": "Another limitation is the model\u2019s capability to depict certain motions indicated by the text prompt. As depicted in Fig. 9(b), MotionBooth may fail to accurately represent motions that are unlikely to be performed by the subject. For example, it is hard to imagine a scene where a wolf plushie is riding a bike. These failure cases highlight the need for further improvement in the model\u2019s subject separation and motion understanding capabilities to enhance the realism and accuracy of the generated videos. Utilizing more powerful T2V models may eliminate these ", "page_idx": 9}, {"type": "image", "img_path": "1we1V3MAHD/tmp/6cf5cd1af2c3996ea96d716e5e266e77d6763fb28050c638119c4c47070a406d.jpg", "img_caption": ["Figure 9: Failure cases of MotionBooth. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "drawbacks. Future work could focus on refining these aspects to address the current limitations and provide more robust performance in complex scenarios. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces MotionBooth, a novel framework for motion-aware, customized video generation. MotionBooth fine-tunes a T2V diffusion model to learn specific subjects, utilizing subject region loss to focus on the subject area. The training procedure incorporates video preservation data to prevent background degradation. Additionally, an STCA loss is designed to connect subject tokens with the cross-attention map. During inference, training-free technologies are proposed to control both subject and camera motion. Extensive experiments demonstrate the effectiveness and generalization ability of our method. In conclusion, MotionBooth can generate vivid videos with given subjects and controllable subject and camera motions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This work is supported by the National Key Research and Development Program of China (No. 2023YFC3807600). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 3   \n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2   \n[3] Tim Brooks, Bill Peebles, Connor Holmes, Yufei Guo Will DePue, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Sora: Creating video from text, 2024. 3, 8   \n[4] Changgu Chen, Junwei Shu, Lianggangxu Chen, Gaoqi He, Changbo Wang, and Yang Li. Motionzero: Zero-shot moving object control framework for diffusion-based video generation. arXiv preprint arXiv:2401.10150, 2024. 3, 6, 14, 15, 16, 17   \n[5] Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang, Yuwei Zhou, Feilin Han, and Wenwu Zhu. Videodreamer: Customized multi-subject text-to-video generation with disen-mix finetuning. arXiv preprint arXiv:2311.00990, 2023. 3   \n[6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2   \n[7] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 2   \n[8] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv preprint arXiv:2305.03374, 2023. 3, 4   \n[9] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024. 3, 7   \n[10] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. NeurIPS, 2024. 3, 4   \n[11] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. CVPR, 2024. 3   \n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3, 4   \n[13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. 2, 3, 7, 8   \n[14] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 3   \n[15] Yue Han, Jiangning Zhang, Junwei Zhu, Xiangtai Li, Yanhao Ge, Wei Li, Chengjie Wang, Yong Liu, Xiaoming Liu, and Ying Tai. A generalist facex via learning unified facial representation. arXiv preprint arXiv:2401.00551, 2023. 3   \n[16] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face adapter for pre-trained diffusion models with fine-grained id and attribute control. arXiv preprint arXiv:2405.12970, 2024. 3   \n[17] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3, 7, 8   \n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2   \n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2   \n[20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In NeurIPS, 2022. 2   \n[21] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. 2   \n[22] Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024. 3   \n[23] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation. arXiv preprint arXiv:2312.13691, 2023. 3, 4   \n[24] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via masked-diffusion. arXiv preprint arXiv:2312.07509, 2023. 3   \n[25] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In CVPR, 2024. 2, 3   \n[26] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In ICCV, 2023. 8, 14, 16, 18   \n[27] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In ICCV, 2023. 3, 6   \n[28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 5   \n[29] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00e9 Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 3   \n[30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. 2, 3, 4, 5, 7, 14   \n[31] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. 3, 5   \n[32] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091, 2023. 3   \n[33] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. arXiv preprint arXiv:2305.19327, 2023. 3   \n[34] Wan-Duo Kurt Ma, J. P. Lewis, and W. Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. arXiv preprint arXIv:2401.00896, 2024. 14, 16, 17   \n[35] Wan-Duo Kurt Ma, J. P. Lewis, Avisek Lahiri, Thomas Leung, and W. Bastiaan Kleijn. Directed diffusion: Direct control of object placement through attention guidance. In AAAI, 2024. 14, 15, 16, 17   \n[36] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3   \n[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. 2   \n[38] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 8   \n[39] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: A versatile portrait model for fast identity-preserved personalization. arXiv preprint arXiv:2312.06354, 2023. 3   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICCV, 2021. 6, 8   \n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 4   \n[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2, 3, 4, 5, 7, 8, 14   \n[43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023. 3, 4   \n[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. NeurIPS, 2022. 2   \n[45] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. In ICCV, 2023. 8   \n[46] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2   \n[47] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. 3, 4   \n[48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2   \n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 3   \n[50] Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, and Ming-Hsuan Yang. Semflow: Binding semantic segmentation and image synthesis via rectified flow. arXiv preprint arXiv:2405.20282, 2024. 3   \n[51] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2   \n[52] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXIv:2402.01566, 2024. 14, 15   \n[53] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. NeurIPS, 2024. 3   \n[54] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 2, 16   \n[55] Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, and Zhenguo Li. Customvideo: Customizing text-to-video generation with multiple subjects. arXiv preprint arXiv:2401.09962, 2024. 2, 3, 4, 5, 7, 8   \n[56] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641, 2023. 3, 8, 14, 16   \n[57] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. arXiv preprint arXiv:2312.04433, 2023. 2, 3, 7, 8   \n[58] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In ICCV, 2023. 3   \n[59] Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal large language models. CVPR, 2024. 2   \n[60] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, and Dacheng Tao. Towards open vocabulary learning: A survey. T-PAMI, 2024. 2   \n[61] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023. 2   \n[62] Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, and Chen Change Loy. Mosaicfusion: Diffusion models as data augmenters for large vocabulary instance segmentation. arXiv preprint arXiv:2309.13042, 2023. 2   \n[63] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In ICCV, 2023. 3, 6   \n[64] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016. 8   \n[65] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In CVPR, 2023. 3   \n[66] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. arXiv preprint arXiv:2402.03162, 2024. 3, 6   \n[67] Haoyu Zhao, Tianyi Lu, Jiaxi Gu, Xing Zhang, Zuxuan Wu, Hang Xu, and Yu-Gang Jiang. Videoassembler: Identity-consistent video generation with reference entities using diffusion model. arXiv preprint arXiv:2311.17338, 2023. 3   \n[68] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 2   \n[69] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. 3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "1we1V3MAHD/tmp/db3755213a1740402ef150d93524a01947e2ded247fe187733a8e26eedc72d20.jpg", "img_caption": ["Figure 10: The evaluation dataset. We present one picture for each subject. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Overview. The supplementary includes the following sections: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A.1. Implementation details of the experiments.   \n\u2022 Appendix A.2. Discussions about our method and the differences with related methods.   \n\u2022 Appendix A.3. Comparison with more baselines.   \n\u2022 Appendix A.4. Ablation studies.   \n\u2022 Appendix A.5. Social impacts.   \n\u2022 Appendix A.6. More qualitative results. ", "page_idx": 13}, {"type": "text", "text": "Video Demo. We also present a video in a separate supplementary file, which shows the results in video format. ", "page_idx": 13}, {"type": "text", "text": "A.1 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Hyperparameters. For the LaVie model, we set $\\alpha=10.0$ , $\\tau=0.7\\mathbf{T}$ , $\\sigma_{1}=0.8\\mathbf{T}$ , and $\\sigma_{2}=0.6\\mathbf{T}$ . For the Zeroscope model, we set $\\alpha=10.0$ , $\\boldsymbol{\\tau}=0.9\\mathbf{T}$ , $\\sigma_{1}=0.9\\mathbf{T}$ , and $\\sigma_{2}=0.7\\mathbf{T}$ . ", "page_idx": 13}, {"type": "text", "text": "Evaluation dataset. We collect a total of 26 subjects from DreamBooth [42] and CustomDiffusion [30]. We show one image for each subject in Fig. 10. The subjects contain a wide variety of types, including pets, plushie toys, cartoons, and vehicles, which can provide us with a thorough analysis of the model\u2019s effectiveness. ", "page_idx": 13}, {"type": "text", "text": "User study interface. We show the application interface for human preference study in Fig. 11. During user study, we ask the annotators to select the best video based on the question, e.g., \u201cWhich video do you think has the best temporal consistency?\u201d ", "page_idx": 13}, {"type": "text", "text": "Pseudo-code of latent shift. To present the latent shift module more clearly, we show the ", "page_idx": 13}, {"type": "image", "img_path": "1we1V3MAHD/tmp/080998894d642de0ed89f299db8028c8ae5bdf4d5922f7355f4af7d66a4f284a.jpg", "img_caption": ["Figure 11: The application interface for user study. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "pseudo-code of the algorithm in Fig. 14. Our latent shift module can control the camera movement in videos in a training-free manner at minimal costs. ", "page_idx": 13}, {"type": "text", "text": "A.2 Discussions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Differences with related works. Our proposed subject and camera motion control methods differ in several ways from previously established approaches, such as TrailBlazer [34], Directed Diffusion [35], Boximator [52], Motion-Zero [4], MotionCtrl [56], and Text2Video-Zero [26]. ", "page_idx": 13}, {"type": "text", "text": "TrailBlazer [34] and Directed Diffusion [35] use a training-free approach for controlling object motion by manipulating the cross-attention maps. Specifically, TrailBlazer [34] adjusts both spatial and temporal cross-attention by scaling the attention maps with a hyper-parameter of less than 1. Directed Diffusion [35] focuses on image generation in a similar manner. Our approach, in contrast, targets only the spatial cross-attention, setting attention values outside the object\u2019s bounding box to zero. This targeted manipulation simplifies implementation and significantly enhances the performance in generating motion-aware customized videos. ", "page_idx": 13}, {"type": "image", "img_path": "1we1V3MAHD/tmp/f9ab177110eca2dada97130cf0c9840209051364510243a8d2ffcf4b5b38c368.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "1we1V3MAHD/tmp/0d02b27ad97e5ec8496b19547a083f96a0d80581c6f1bac4115ae05ab892935d.jpg", "img_caption": ["(g) Examples of controlling both subject and camera(h) Comparison of latent shift and text guidance to motion. control camera motion. ", "Figure 12: More qualitative results. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Boximator [52] relies on a training-based technique, requiring box coordinates as inputs to a newly trained self-attention layer. In comparison, our method is training-free, thus providing a more accessible and user-friendly solution for controlling subject and camera motion. ", "page_idx": 14}, {"type": "text", "text": "Motion-Zero [4] also operates without additional training, but it utilizes a test-time tuning technique to adjust the latent space using cross-attention map loss during the denoising process. This technique, however, increases the generation time and memory requirements considerably\u2014from approximately 15 seconds to several minutes per video. Additionally, our experiments indicate that Motion-Zero often produces poor-quality outputs, with videos collapsing into unrecognizable elements. This outcome is likely due to the detrimental effects of test-time tuning on parameters and latent distributions in customized scenarios. In contrast, our method directly manipulates the cross-attention map, adding only 0.3 seconds to the generation process and consistently yielding more reliable visual outcomes. ", "page_idx": 14}, {"type": "text", "text": "MotionCtrl [56] uses a training-based approach that requires inputting point trajectories to control camera poses and object motion. Our method does not involve additional training, offering a simpler alternative for subject and camera motion control. ", "page_idx": 15}, {"type": "text", "text": "Text2Video-Zero [26] builds on a pre-trained text-to-image (T2I) model and extends it to video generation by utilizing consistent noise across frames. However, this approach is unsuitable for text-to-video (T2V) models, which typically use different noise for each frame. Additionally, Text2Video-Zero uses latent shifting for overall scene movement and mirrors latent information to flil in missing regions. In comparison, our approach employs random sampling along the x and y axes, resulting in more concise and coherent video generation for camera motion control. ", "page_idx": 15}, {"type": "text", "text": "We conducted quantitative experiments to evaluate the performance of these methods, as reported in Table 2 and Table 4b. Some methods are not included due to the unavailability of their code. Nonetheless, our results indicate that our method generally outperforms the other alternatives. In particular, we observed significant limitations in the test-time tuning strategy of Motion-Zero [4] when applied to customized video generation, which further emphasizes the strengths of our approach. ", "page_idx": 15}, {"type": "text", "text": "Zoom-in/out effect with subject motion control. Our subject motion control technique effectively manages changes in bounding box size, such as gradual enlargement or reduction. Specifically, enlarging the bounding box results in an appropriate scaling of the subject, creating a zoom-in effect in the generated video. Conversely, reducing the bounding box size produces a zoom-out effect. This zoom-in and zoom-out behavior is demonstrated in Fig. 12a, where examples illustrate these effects in response to bounding box adjustments. Fig. 12d shows an example of combining zoom-in and camera motion control, demonstrating the method\u2019s flexibility. ", "page_idx": 15}, {"type": "text", "text": "Large camera motion speeds. We evaluate the performance of our camera motion control technique under varying levels of camera movement intensity. As depicted in Fig. 12b, the method was first tested with a camera movement vector of $[c_{x},c_{y}]\\,=\\,[-0.5,0.45]$ , corresponding to a movement of half the video width to the left and nearly half downward. Under these conditions, our method successfully managed the camera movement, producing accurate results. When the camera motion speed was doubled, the model continued to perform well, demonstrating its ability to handle highspeed scenarios. However, at a speed three times the initial setting, i.e., $[c_{x},c_{y}]=[-1.5,1.35]$ , the method exhibited limitations, resulting in only a downward tiling effect. These findings indicate that while our method can effectively manage camera movements spanning the entire video width, its performance diminishes under extremely high-speed conditions. ", "page_idx": 15}, {"type": "text", "text": "Camera motion control with large foreground objects. We evaluate the camera control capability in scenes containing substantial foreground objects. The results, presented in Fig. 12c, show an experiment involving a scene dominated by a large candy house, which nearly occupies the entire frame. Despite the significant presence of the foreground object, our technique effectively managed to pan the camera to the right, suggesting that the method is capable of handling complex scenes with large foreground elements. This robustness is attributed to the method\u2019s reliance on latent shifts, which simultaneously move both foreground and background elements, thereby ensuring that the presence of large foreground objects does not significantly impair performance. ", "page_idx": 15}, {"type": "text", "text": "Model efficiency. Training our model by fine-tuning a specific subject takes approximately 10 minutes. For LaVie [54], the naive text-to-video (T2V) pipeline takes about 15.0 seconds per video. When incorporating subject control, the inference time is 15.3 seconds per video; with camera control, it increases to 20.6 seconds per video; and when applying both camera and subject control, the inference time is 21.5 seconds per video. ", "page_idx": 15}, {"type": "text", "text": "A.3 Comparison with More Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 4: Comparison with More Baselines. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "(a) Comparison of the latent shift method and text(b) Comparison of subject motion control with more guidance for camera motion control. baselines. ", "page_idx": 15}, {"type": "table", "img_path": "1we1V3MAHD/tmp/0f2d2df5fc6ae9df90adbaeb9de70e845922425ed0788c185669488014dd1286.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "1we1V3MAHD/tmp/854bcc195e7e44b22699583364e5f04c469304872913a849f3572d2a275baf3a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "1we1V3MAHD/tmp/0123ec42c821ff01be5daaea632087b5121ad1e56d027cf2eb270ce2608b3f6d.jpg", "img_caption": ["(a) Ablation study on subject motion control. Only the (b) Ablation study on latent shift. Experiments on first frame is shown. Experiments on Zeroscope. LaVie. A higher $\\sigma$ means an earlier denoising step. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "1we1V3MAHD/tmp/90420e2c87a294af6ed996ed602b423c81c37ddab6062d0d5f0bbfa688a1bfcf.jpg", "table_caption": ["Figure 13: Ablation study on motion control hyperparameters. ", "Table 5: Ablation study for the number of video preservation data. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Comparison with text guidance for camera motion control. we compare our latent shift method with text guidance for controlling camera motion with the results presented in Table 4a and Fig. 12h. Specifically, we utilize simple text prompts, such as \u201ccamera pan right\u201d and \u201ccamera pan up right,\u201d to influence camera movement. The generated videos reflect these instructions to some extent, especially for straightforward motions like panning to the right. However, we observe that such text prompts often lack sufficient specificity, particularly in terms of conveying critical details such as the speed or distance of the camera movement. As a result, this approach yielded sub-optimal outcomes when compared to our proposed method. This indicates that our approach offers a more precise and stable mechanism for controlling camera motion than is possible with text prompts alone. ", "page_idx": 16}, {"type": "text", "text": "Comparison of subject motion control with more baselines. Table 4b presents quantitative experiments comparing our method with several baseline approaches, including TrailBlazer [34], Directed Diffusion [35], and Motion-Zero [4]. The results demonstrate that our approach generally outperforms these alternatives. ", "page_idx": 16}, {"type": "text", "text": "A.4 Ablation Studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Subject motion control hyperparameters. We conduct ablation studies on the hyperparameter for subject motion control, with the results presented in Fig. 13a. We examined the effects of varying the factor $\\alpha$ of $\\mathbf{S}$ and the maximum cross-attention manipulation timestamp $\\tau$ . The findings indicate that increasing $\\alpha$ and extending the controlling steps lead to stronger control. With lower control strengths, the subject does not appear in the desired position, or only part of its body aligns with the intended spot. Conversely, when the control strengths are too high, the generated subjects tend to appear unnaturally square in shape. ", "page_idx": 16}, {"type": "text", "text": "Latent shift hyperparameters. We experiment with the influence of $\\sigma_{1}$ and $\\sigma_{2}$ in the latent shift module. The results are shown in Fig. 13b. The results indicate that applying latent shift in the earlier steps of the denoising process results in incomplete camera movement, as evidenced by the trees in the background. Conversely, shifting the latent in the later steps degrades video quality and introduces artifacts, highlighted by the red boxes in the last row. Empirically, setting $\\sigma_{1}$ and $\\sigma_{2}$ to middle values provides optimal control over camera movement. ", "page_idx": 16}, {"type": "text", "text": "Number of preservation videos. We conduct an ablation study on the number of preservation videos. As shown in Table 5, ranging the preservation videos from 100 to 900 does not bring large changes to the quantitative scores. We conclude that the key is to use video data to preserve the video generation ability of the pre-trained T2V models. The number of video data can be flexible. ", "page_idx": 16}, {"type": "table", "img_path": "1we1V3MAHD/tmp/5c7303c691c5dc56f6b4a9c7f5c535ce5b0affa3cd3a5cbf340d04b01820b589.jpg", "table_caption": ["(a) Ablation of controlling single(b) Ablation of masking the training(c) Ablation of the latent filling motion type. images. method in latent shift. ", "Table 6: More ablation studies. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Controlling Single Motion Type. We conduct an additional experiment focusing specifically on the scenario where only subject motion is controlled, and the results are summarized in Table 6a. Notably, we observe that the inclusion of both subject and camera motion controls leads to a slight decrease in some metrics. For instance, using the Zeroscope model, there is a 0.1 drop in the R-CLIP score and a 0.01 decrease in the T-Cons metric when both types of motion are controlled compared to the scenario where only subject motion is controlled. However, there are also instances where metrics such as R-DINO and CLIP-T show a slight improvement under combined control conditions. We consider this trade-off acceptable within the context of motion-aware customized video generation. Qualitative results in Fig. $12\\mathrm{g}$ show that our method can successfully handle controlling both subject and camera motion. ", "page_idx": 17}, {"type": "text", "text": "Masking the training images. We conduct an ablation study to evaluate the effect of masking the background regions directly in pixel space of the training images, compared to masking the diffusion loss. As illustrated in Fig. 12e, directly masking the training images significantly impairs the model\u2019s capacity to learn the subject effectively. The quantitative metrics shown in Table 6b indicate a marked decrease in performance when the training images are masked. We hypothesize that this decline arises from the unnatural distribution created by the masked images, which disrupts the learning process. Consequently, we conclude that masking the diffusion loss, rather than masking the training images directly, is a more effective strategy for preserving the integrity of the learned representations. ", "page_idx": 17}, {"type": "text", "text": "Latent Filling Methods. We test several latent fliling approaches and present the qualitative results in Fig. 12f and the quantitative results in Table 6c. ", "page_idx": 17}, {"type": "text", "text": "Random Init: This method involves filling the hole with random values. Our experiments revealed that this technique leads to severe artifacts due to the disruption of the natural horizontal and vertical distribution of latent pixels, ultimately degrading the overall visual quality. ", "page_idx": 17}, {"type": "text", "text": "Random Sample: In this approach, values are randomly sampled in the latents. Similar to Random Init, this method produced significant artifacts, leading to poor visual quality in the generated video. ", "page_idx": 17}, {"type": "text", "text": "Loop: This method reuses the moved-out-of-scene values to flil the missing region, thereby creating a looping background effect. While this technique was found to preserve video quality better than the random methods, it introduced a limitation in terms of flexibility for camera movements, resulting in repetitive looping effects. Therefore, it is not suitable for more diverse camera controls. ", "page_idx": 17}, {"type": "text", "text": "Reflect: This approach is employed in Text2Video-Zero [26], where the missing region is filled by reflecting the surrounding content. However, in our Text-to-Video (T2V) scenario, this method collapsed, failing to maintain the desired visual quality. ", "page_idx": 17}, {"type": "text", "text": "The quantitative results presented in Table 6c corroborate these findings. Random Init and Random Sample lead to significant artifacts, whereas the Loop method provides better visual quality but at the cost of limiting camera movement diversity. The Reflect method, despite its success in other applications, did not yield satisfactory results in our T2V context. ", "page_idx": 17}, {"type": "text", "text": "In conclusion, our proposed straight-sample method consistently maintained high visual quality without introducing significant artifacts or limiting camera movement flexibility, demonstrating its superiority in maintaining the desired video fidelity. ", "page_idx": 17}, {"type": "text", "text": "A.5 Social Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Positive societal impacts. MotionBooth allows for precise control over customized subjects and camera movements in video generation, opening new avenues for artists, filmmakers, and content creators to produce unique and high-quality visual content without extensive resources or professional equipment. ", "page_idx": 17}, {"type": "text", "text": "Potential negative societal impacts. The ability to generate realistic customized videos could be misused to create deepfakes, leading to potential disinformation campaigns, privacy violations, and reputational damage. This risk is particularly significant in the context of political manipulation and social media. If the underlying models are trained on biased datasets, the generated content might reinforce harmful stereotypes or exclude certain groups. Ensuring diversity and fairness in training data is crucial to mitigate this risk. ", "page_idx": 18}, {"type": "text", "text": "Mitigation strategies. Developing and adhering to strict ethical guidelines for the use and dissemination of video generation technologies can help mitigate misuse. This includes implementing usage restrictions and promoting transparency about the generated content. ", "page_idx": 18}, {"type": "text", "text": "A.6 More Qualitative Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We show more qualitative results in Fig. 15. ", "page_idx": 18}, {"type": "text", "text": "def shift_latent_one_step(latent, cx, cy, bbox, num_shift_steps): # latent: noised latent for timestamp t # cx: $x$ -axis speed for camera movement # cy: y-axis speed for camera movement # bbox: the bounding box for the customized subject # num_shift_steps: $\\sigma_{1}-\\sigma_{2}$ , the total steps needed to complete latent shift # get the latent shape batch_size, channels, num_frames, height, width $=$ latent.shape ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "# divide the camera speed by num_shift_steps.   \n# for each step, we move a part of the total distance $\\begin{array}{l}{{\\sf S X}\\;=\\;{\\sf\\sf\\sf{C X}}}\\end{array}$ / num_shift_steps   \n$s y\\ =\\ c y\\ /$ num_shift_steps ", "page_idx": 19}, {"type": "text", "text": "for $\\boldsymbol{\\mathsf{f}}$ in range(num_frames): # define latent shift distance for each frame $s f\\times=i n t(s x\\ *\\ f)$ $s f y=i n t(s y\\ *\\ f)$ ", "page_idx": 19}, {"type": "text", "text": "# define a obj_mask to avoid sampling tokens within the subject region obj_mask $=$ torch.ones_like(latent[0,0,f,:,:]) obj_mask[bbox] $=$ False ", "page_idx": 19}, {"type": "text", "text": "# sampling tokens horizontally ", "page_idx": 19}, {"type": "text", "text": "if sfx $!=\\;\\theta$ : $\\mathsf{f i l l}_{-}\\mathsf{x}\\;=$ torch.zeros_like(latent[:,:,f,:,:abs(sfx)]) for i in range(height): included_indices $=$ [x for $\\times$ in range(0, width) if obj_mask[i,x]] sampled_indices $=$ random_choice(included_indices, size $^*=$ abs(sfx) $\\mathsf{f i l l\\_x[:,:,i,:]}\\ =$ latent[:,:,f,i,sampled_indices]   \n# sampling tokens vertically   \nif sfy $!=\\;\\theta$ : $\\tt f i l l\\_y\\ =$ torch.zeros_like(latent[:,:,f,:abs(sfy),:]) for $\\dot{\\sf1}$ in range(width): included_indices $=$ [y for y in range(0, height) if obj_mask[y,j]] sampled_indices $=$ random_choice(included_indices, size=abs(sfy) fill_y[:,:,:,j] $=$ latent[:,:,f,sampled_indices,j] ", "page_idx": 19}, {"type": "text", "text": "# shift the original latent and fill in the hole with sampled tokens if sfx $>\\theta$ : ", "page_idx": 19}, {"type": "text", "text": "temp $=$ latent[:,:,f,:,sfx:] latent[:,:,f,:,:] $=$ torch.cat([temp, fill_x], dim=-1)   \nelif sfx $<~\\theta$ : temp $=$ latent[:,:,f,:,:sfx] latent[:,:,f,:,:] $=$ torch.cat([fill_x, temp], dim=-1)   \nif sfy $>\\theta$ : temp $=$ latent[:,:,f,sfy:,:] latent[:,:,f,:,:] $=$ torch.cat([temp, fill_y], dim=-2)   \nelif sfy $<~\\theta$ : temp $=$ latent[:,:,f,:sfy,:] latent[:,:,f,:,:] $=$ torch.cat([fill_y, temp], dim=-2) ", "page_idx": 19}, {"type": "text", "text": "return latents ", "page_idx": 19}, {"type": "text", "text": "Figure 14: Pseudo-code of the latent shift algorithm. ", "page_idx": 19}, {"type": "image", "img_path": "1we1V3MAHD/tmp/284fe26a683b213428b08432e88553dd78efba11c7110ee490da9caa60ce29c5.jpg", "img_caption": ["Figure 15: More qualitative results of our MotionBooth. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include the main claims in the abstract and introducion. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations of our method in Section 4.5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4 and Appendix A.1 that are needed to reproduce the main experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We are not able to provide the code at submission time. But we are making sure that our code and models will be released publically in the future. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4 and Appendix A.1 that are needed to reproduce the main experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We did not conduct experiments with error bars. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4 and Appendix A.1 that are needed to reproduce the experimental results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conducted in the paper confirm, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the social impacts of MotionBooth in Appendix A.5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We describe the safeguards in Appendix A.5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We use existing assets with properly credited. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We introduce new assets in Section 4 and Appendix A.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not involve crowdsourcing or research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We do not involve crowdsourcing or research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]