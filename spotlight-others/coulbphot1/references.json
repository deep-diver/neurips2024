{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a foundational model for many subsequent advancements in deep learning, including the parameter-efficient fine-tuning methods discussed in the current paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This work demonstrated the effectiveness of the Transformer architecture for image recognition, significantly impacting computer vision and inspiring many visual adaptation tasks in the current paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, demonstrated the power of multimodal learning and connecting image and text data, which is relevant to the visual adaptation focus of this current paper."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-01-01", "reason": "This paper introduced the LoRA method, a key parameter-efficient fine-tuning technique, which forms the basis for several methods compared in the current paper."}, {"fullname_first_author": "Yao Ni", "paper_title": "Chain: Enhancing generalization in data-efficient gans via lipschitz continuity constrained normalization", "publication_date": "2024-06-01", "reason": "This paper, also by one of the current paper's authors, provides a theoretical foundation for enhancing generalization through gradient regularization, a key concept explored and extended in the current work."}]}