[{"figure_path": "181llen2gw/figures/figures_1_1.jpg", "caption": "Figure 1: Bias in VLMs' various downstream tasks. VLMs tend to prefer certain gender for a subject in image or text, while SFID mitigates the bias issue in VLMs.", "description": "This figure shows examples of gender bias in vision-language models (VLMs) for image captioning and image generation tasks.  In the image captioning example, CLIP-CAP captions an image of a woman surfing as \"A woman in a wetsuit surfing on a wave.\"  However, when the same model is used with the SFID debiasing method, the caption becomes more gender-neutral (\"A person on a surfboard in the water.\"). The image generation example shows similar bias where the model generates images of a particular gender even if a neutral prompt is provided.  SFID helps mitigate this bias, as seen in the changes to both the captions and the generated images.", "section": "1 Introduction"}, {"figure_path": "181llen2gw/figures/figures_1_2.jpg", "caption": "Figure 1: Bias in VLMs' various downstream tasks. VLMs tend to prefer certain gender for a subject in image or text, while SFID mitigates the bias issue in VLMs.", "description": "This figure shows examples of gender bias in vision-language models (VLMs) and how the proposed Selective Feature Imputation for Debiasing (SFID) method mitigates this bias.  The top row demonstrates how CLIP-CAP (a VLM) tends to associate specific genders with certain activities (surfing and skiing) in image captioning. The bottom row illustrates the same bias in text-to-image generation with CoDi, where a prompt mentioning \"man\" preferentially generates images of men as nurses. SFID's application improves fairness, resulting in more gender-balanced outputs in both tasks.", "section": "1 Introduction"}, {"figure_path": "181llen2gw/figures/figures_4_1.jpg", "caption": "Figure 2: GradCAM visualization for feature indices sorted by their importance in predicting an attribute (e.g., gender). Highly important features (left) focus on attribute-related characteristics such as face in the image, while the least important features (right) are associated with the background. SFID not only identifies the crucial biased features but also imputes these biased features with ambiguous values derived from low-confidence samples.", "description": "This figure shows GradCAM visualizations to demonstrate how important features are correlated to gender and race biases. The left side displays the top 30 most important features identified by RandomForest, which highlight facial attributes related to gender and race.  The right side shows the least important 10 features, focusing on the background.  SFID uses this information to identify and replace biased features with ambiguous values from low-confidence samples.", "section": "3 Bias Analysis in VLMs"}, {"figure_path": "181llen2gw/figures/figures_5_1.jpg", "caption": "Figure 3: Selective Feature Imputation for Debiasing (SFID) utilizes RandomForest to extract feature importance (jk) identifying bias-related features, and low-confidence samples in the validation set which indicate ambiguous representations. During the inference stage, the extracted feature indices and imputing values (\u03bc\u03b5) from low-confidence samples are imputed into the embedding used in the downstream task.", "description": "This figure illustrates the SFID (Selective Feature Imputation for Debiasing) process.  It shows how RandomForest is used to identify important features correlated with gender bias in a training set of images.  These features are then replaced with values from low-confidence samples (samples that are ambiguous and less likely to exhibit bias) found in a validation set. The process is applied during inference, where important features from the input embedding are replaced with these imputed values before the embedding is used in a downstream task, effectively reducing gender bias.", "section": "4 Proposed Method"}, {"figure_path": "181llen2gw/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison of zero-value imputation, zero-centered Gaussian noise, and low confidence samples. Different colors of points indicate different sensitive attributes. Gray points represent zero-centered Gaussian noise, which is out-of-distribution from the original embedding. SFID utilizes the centroid of low confidence samples (red \u00d7), which remain in-distribution of the original samples.", "description": "This figure compares three different imputation methods: zero-value imputation, zero-centered Gaussian noise, and low-confidence sample imputation.  It shows how these methods affect the distribution of the imputed features in the context of mitigating biases related to sensitive attributes.  The low-confidence sample imputation method, used by SFID, is highlighted as it keeps the imputed features within the original data distribution, unlike the other two methods.", "section": "High-Confidence Imputation in Text-to-Image Generation"}, {"figure_path": "181llen2gw/figures/figures_13_1.jpg", "caption": "Figure 5: Feature importances for gender prediction by RandomForest for each frozen representation.", "description": "The figure shows feature importances for gender prediction by RandomForest for each frozen representation.  The x-axis represents the feature rank, sorted from most to least important. The y-axis represents the feature importance, which indicates how strongly each feature contributes to the prediction of gender. Separate lines are shown for different models and components, including RN50 Image, RN50 Text, ViT-B/32 Image, ViT-B/32 Text, XVLM Image and XVLM Text. The plot shows that the top few features are significantly more important than the others, stabilizing around the top 100 for all components.", "section": "A Details of the Proposed Method"}, {"figure_path": "181llen2gw/figures/figures_14_1.jpg", "caption": "Figure 6: (a) A linear classifier can distinguish the attribute (e.g., gender) from the extracted image embedding of each sample using only the top 2 most important features determined by the pre-trained CLIP model. (b) The DeAR method attempts to fool the attribute classifier by perturbing the features. However, it fails to do so, as the features from the two groups remain distinguishable, as indicated by the comparable accuracy achieved by the attribute classifier. (c) In contrast, the proposed method, SFID, replaces all values in the important features with ambiguous values, effectively obscuring the attribute of the embedding. This method aims to remove bias-related properties from the embedding, which is demonstrated by the resulting very low classification accuracy in classifying the sensitive attribute when using the transformed embeddings. Note that the replaced features still remain within the distribution of the original embedding.", "description": "This figure compares the performance of three different debiasing methods on a binary classification task using only the two most important features. (a) shows that a linear classifier can easily distinguish between two classes in the original embedding. (b) shows that DeAR fails to effectively debias the embedding. (c) shows that SFID successfully removes bias-related information and the classifier cannot distinguish between the two classes.", "section": "C Comparison with Other Debiasing Approaches"}, {"figure_path": "181llen2gw/figures/figures_15_1.jpg", "caption": "Figure 5: Feature importances for gender prediction by RandomForest for each frozen representation.", "description": "This figure shows the feature importances for gender prediction using RandomForest on different frozen representations from various vision-language models (VLMs).  The x-axis represents the feature rank, ordered from most important to least important. The y-axis shows the feature importance score.  Each line represents a different VLM's representation (RN50 Image, RN50 Text, ViT-B/32 Image, ViT-B/32 Text, XVLM Image, XVLM Text).  The plot helps visualize the relative importance of each feature in predicting gender and informs the selection of the top k features to prune during the SFID debiasing process. The stabilization around the top 100 features is a key observation that motivated choosing k=50 in the SFID algorithm.", "section": "A Details of the Proposed Method"}, {"figure_path": "181llen2gw/figures/figures_15_2.jpg", "caption": "Figure 3: Selective Feature Imputation for Debiasing (SFID) utilizes RandomForest to extract feature importance (jk) identifying bias-related features, and low-confidence samples in the validation set which indicate ambiguous representations. During the inference stage, the extracted feature indices and imputing values (\u03bc\u03b5) from low-confidence samples are imputed into the embedding used in the downstream task.", "description": "This figure illustrates the Selective Feature Imputation for Debiasing (SFID) method.  It shows how RandomForest is used to identify important features associated with bias, and how low-confidence samples are used to replace those features, maintaining dimensionality while reducing bias in downstream tasks.", "section": "4 Proposed Method"}]