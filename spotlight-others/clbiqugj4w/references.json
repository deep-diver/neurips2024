{"references": [{"fullname_first_author": "Haoyi Zhou", "paper_title": "Informer: Beyond efficient transformer for long sequence time-series forecasting", "publication_date": "2021-00-00", "reason": "This paper introduces the Informer model, a highly influential architecture for long sequence time series forecasting that is frequently cited and compared against in the current paper."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting", "publication_date": "2021-00-00", "reason": "Autoformer is a significant model in long-term time series forecasting that utilizes decomposition techniques and is used as a comparison model in this paper."}, {"fullname_first_author": "Guokun Lai", "paper_title": "Modeling long-and short-term temporal patterns with deep neural networks", "publication_date": "2018-00-00", "reason": "This paper is a foundational work in deep learning for time series forecasting, providing a basis for many subsequent models and is compared against in this paper."}, {"fullname_first_author": "Yong Liu", "paper_title": "iTransformer: Inverted transformers are effective for time series forecasting", "publication_date": "2024-00-00", "reason": "This paper proposes iTransformer, a state-of-the-art model in long-term time series forecasting that uses an inverted transformer architecture, and is compared against this paper."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023-00-00", "reason": "This paper introduces the PatchTST model, which is a significant model in long-term time series forecasting that uses a patch-based approach, and is compared against in this paper."}]}