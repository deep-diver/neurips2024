[{"figure_path": "VXxj3XZ1X8/figures/figures_2_1.jpg", "caption": "Figure 1: A. Model architecture: The models used in our study are separated into a core learning a non-linear feature representation shared across neurons, and a neuron specific linear readout. In our case, the core is a rotation-equivariant core, i.e. each learnt feature f is analytically rotated n times by 360/n degrees, resulting in f\u00b7 n output channels. We apply batch norm on the learnt feature channels f only and do not learn scale and bias parameters in the core's last layer, as this would interfere with the readout regularization. Two commonly used linear readouts are factorized and Gaussian readout. The factorized readout learns spatial weights accounting for neuron's receptive field position and feature weightings \u2013 the neural embeddings. The Gaussian readout replaced the spatial weights by sampling receptive field positions from a Gaussian and at inference time samples the Gaussian's mean. B. Readout alignment: To yield rotation invariant neural embedding clusters, we align the orientations of the embedding vectors by cyclically rotating the elements in the embedding vectors to minimize the sum of pairwise distances between the rotated embeddings [46].C. Our pipeline: For each model architecture and training configuration we train 3 models with different parameter initialization seeds. We orientation-align their embeddings, cluster them, and eventually compute all model pair-wise ARIS.", "description": "The figure illustrates the model architecture, readout alignment, and analysis pipeline.  Panel A shows the core (shared feature representation) and readout (neuron-specific weights) components of the deep neural network model used. Two types of readouts are detailed: Factorized readout and Gaussian readout. Panel B illustrates the alignment process of embedding vectors to obtain rotation invariance. Panel C outlines the overall data processing pipeline: model training, embedding alignment, clustering, and adjusted Rand index (ARI) calculation for evaluating embedding consistency. ", "section": "Background and related work"}, {"figure_path": "VXxj3XZ1X8/figures/figures_5_1.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the ", "description": "Figure 2 displays the impact of different regularization strengths on the performance and consistency of neuronal embeddings across various model architectures (factorized and Gaussian readouts).  It demonstrates that adaptive regularization offers a balance between predictive performance and consistent clustering of embeddings, outperforming both factorized and uniformly regularized models. The figure shows the performance, t-SNE projections of embeddings, weight distribution histograms, an example of adaptive regularization, and ARI scores for clustering across different models and regularization schemes.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_6_1.jpg", "caption": "Figure 3: Selection of adaptive regularization hyperparameters. A: Validation correlation of model with \u03c3 = 0.1, \u03c3 = 0.5 and different overall regularization strength. For smaller \u03c3 the performance decreases more fast. B: Distribution of \u03b2 values learnt. As expected, for smaller \u03c3 the distribution is closer to LogNorm and as we regularize both embeddings and \u03b2-s, the learnt \u03b2-s are smaller for bigger \u03c3. This also explains, why the performance in panel A decreases slower, as the model is less regularized. C: ARI for different hyperparameters. We see that for \u03c3 = 0.5 ARI for \u03b3 = 10 and \u03b3 = 30 are mostly equivalent and \u03b3 = 50 is slightly worse. The ARIs trend is more visible on \u03c3 = 0.1. It seems like the better the predictive performance, the better is the ARI curve. As the performances for \u03b3 = 10, \u03b3 = 15, and \u03b3 = 20 are identical within std, we choose the least regularized model to introduce less bias.", "description": "This figure displays the results of experiments on selecting optimal hyperparameters for adaptive regularization. Panel A shows the validation correlation for different overall regularization strengths (\u03b3) and standard deviations (\u03c3) of the log-normal prior on neuron-specific regularization coefficients (\u03b2).  Panel B presents the distribution of the learned \u03b2 values for various combinations of \u03b3 and \u03c3. Panel C illustrates the adjusted Rand index (ARI) for different hyperparameter settings across various numbers of clusters, indicating the impact of these choices on the consistency of neuronal embeddings. The results indicate the effectiveness of adaptive regularization in balancing prediction performance and embedding consistency.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_7_1.jpg", "caption": "Figure 4: Pruning. A: Validation score of the pruned models with different regularization. Lines: average across three models. Shaded areas: standard deviation across three models. Colors: model type and regularization (see legend in B). Stars: selected model. Crosses: non-pruned model with 8 channels. B: ARI for non-pruned and models selected after pruning (stars from panel B). Pruning consistently improves consistency of clustering measured by ARI. Adaptive regularization readout pruning is shown in A.11. C: ARI-performance trade-off (20 clusters). Ideally we want high ARI and high performance.", "description": "This figure shows the effect of pruning on model performance and consistency of neuronal embeddings. Panel A displays the validation score of pruned models with different regularization methods, showing that pruning can improve performance while maintaining consistency in clustering. Panel B illustrates how ARI, a measure of consistency in clustering, increases with pruning, indicating that removing less important features enhances the robustness of the embeddings. Panel C presents the trade-off between ARI and validation score, highlighting the best model that balances both measures. The adaptive regularization readout pruning method is a key innovation that improves the balance between performance and consistency.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_8_1.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. Ylognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the \u201cembedding\u201d is a linear vector representing a learned neuron function. Both are weights learned in the readout. In the top, the mask learnt is much smaller, hence, regularizing a neuronal embedding is more important to reduce the L1 penalty, while in the bottom we can keep the neuronal embedding less sparse; its impact is smaller as the mask is bigger. I: Adjusted rand index (ARI) for clustering embeddings using k-means. We take embeddings from models trained with different seeds, cluster them and compute ARI between the clusterings. Note that even clustering the same embeddings twice with different clustering initializations will result in ARI < 1 (see Appendix A.4).", "description": "This figure displays the results of an experiment comparing different model architectures and regularization techniques for predicting neuronal activity in the visual cortex.  Panel A shows the correlation between predicted and observed neural responses, indicating model performance. Panels B-F show t-SNE projections of the learned neuronal embeddings under different regularization strengths, visualizing the structure of the embedding space. Panel G compares the weight distributions for different models. Panel H illustrates the concept of adaptive regularization, demonstrating how it adjusts the regularization strength based on learned receptive field masks and embedding sparsity. Finally, Panel I depicts the adjusted Rand index (ARI) for clustering the embeddings, measuring the consistency of the results across different model fits and random seeds.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_14_1.jpg", "caption": "Figure 6: Regularization strength impact on performance for factorized readouts", "description": "This figure shows the impact of the regularization strength (gamma, \u03b3) on the single-trial validation correlation for factorized readouts.  The x-axis represents the regularization coefficient (\u03b3), and the y-axis represents the single-trial validation correlation.  The plot shows that there is an optimal regularization strength that maximizes the validation correlation.  Too little or too much regularization hurts performance.  The optimal value seems to be around 0.003.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_14_2.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the ", "description": "Figure 2 displays the results of the experiments comparing different regularization strategies for the factorized and Gaussian readouts.  Panel A shows model performance. Panels B-F show the t-SNE projections of the embeddings for different regularization strengths (gamma values). Panel G compares weight distributions before alignment. Panel H illustrates the adaptive regularization scheme, showing how the mask and embedding are regularized differently based on the learned receptive field size. Panel I shows the ARI scores for different regularization and clustering approaches.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_15_1.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the ", "description": "Figure 2 displays the results of an experiment that explores the impact of different regularization techniques on the quality of neuronal embeddings. Panel A shows a comparison of the performance of different model types (factorized vs. Gaussian readout) in terms of correlation between predicted and observed neuronal activity. Panels B-F illustrate the impact of varying the strength of L1 regularization on the resulting embeddings, visualized using t-SNE. Panel G provides a detailed look at the distribution of weights before any alignment, comparing the effects of different regularization methods. Panel H offers a visual explanation of the adaptive regularization strategy used for the factorized mask, illustrating how it adjusts the level of regularization per neuron.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_15_2.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the \u201cembedding\u201d is a linear vector representing a learned neuron function. Both are weights learned in the readout. In the top, the mask learnt is much smaller, hence, regularizing a neuronal embedding is more important to reduce the L1 penalty, while in the bottom we can keep the neuronal embedding less sparse; its impact is smaller as the mask is bigger. I: Adjusted rand index (ARI) for clustering embeddings using k-means. We take embeddings from models trained with different seeds, cluster them and compute ARI between the clusterings. Note that even clustering the same embeddings twice with different clustering initializations will result in ARI < 1 (see Appendix A.4).", "description": "Figure 2 shows the impact of different regularization techniques on the quality of neuronal embeddings obtained from predictive models of the visual cortex.  Panels A-F visualize model performance, embedding projections using t-SNE, weight histograms, an example of adaptive regularization, and adjusted Rand index (ARI) results across various regularization strengths. The results highlight the importance of L1 regularization for obtaining structured embeddings and the benefits of an adaptive regularization scheme that improves embedding consistency for downstream analyses.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_16_1.jpg", "caption": "Figure 10: Optimal Gabors for non-pruned models. Columns are per neuron, rows are per model, for each of three regularization strengths three seeds were trained. We can see that the optimal Gabors selected are somewhat consistent, however, not ideally same. More regularized models also tend to select Gabors with smaller sizes.", "description": "This figure displays the optimal Gabor filters selected by the model for different regularization strengths (gamma=0, 50, 100) and three different random seeds. Each column represents a neuron and each row shows the optimal Gabor for a specific regularization strength and seed. The results suggest a degree of consistency in the selection of optimal Gabor filters across different runs, with more regularized models tending to select smaller Gabor filters.", "section": "A.8 Optimal Gabors"}, {"figure_path": "VXxj3XZ1X8/figures/figures_17_1.jpg", "caption": "Figure 10: Optimal Gabors for non-pruned models. Columns are per neuron, rows are per model, for each of three regularization strengths three seeds were trained. We can see that the optimal Gabors selected are somewhat consistent, however, not ideally same. More regularized models also tend to select Gabors with smaller sizes.", "description": "This figure shows the optimal Gabor filters selected by the model for different neurons and training conditions. Each column represents a single neuron, each row shows the optimal Gabor filter obtained under different regularization strength and random seed. The figure shows that there is some consistency in Gabor filter selection across training conditions, but the selection isn't always identical.", "section": "A.8 Optimal Gabors"}, {"figure_path": "VXxj3XZ1X8/figures/figures_18_1.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the \"embedding\" is a linear vector representing a learned neuron function. Both are weights learned in the readout. In the top, the mask learnt is much smaller, hence, regularizing a neuronal embedding is more important to reduce the L1 penalty, while in the bottom we can keep the neuronal embedding less sparse; its impact is smaller as the mask is bigger. I: Adjusted rand index (ARI) for clustering embeddings using k-means. We take embeddings from models trained with different seeds, cluster them and compute ARI between the clusterings. Note that even clustering the same embeddings twice with different clustering initializations will result in ARI < 1 (see Appendix A.4).", "description": "Figure 2 shows the effects of different regularization strengths on the performance and consistency of neuronal embeddings across different model fits.  It includes visualizations of embeddings using t-SNE, histograms of weights, an example of adaptive regularization, and the adjusted Rand Index (ARI) for evaluating clustering consistency. The results highlight the importance of L1 regularization for obtaining well-structured embeddings and the benefits of an adaptive regularization scheme.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_18_2.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the \"embedding\" is a linear vector representing a learned neuron function. Both are weights learned in the readout. In the top, the mask learnt is much smaller, hence, regularizing a neuronal embedding is more important to reduce the L\u2081 penalty, while in the bottom we can keep the neuronal embedding less sparse; its impact is smaller as the mask is bigger. I: Adjusted rand index (ARI) for clustering embeddings using k-means. We take embeddings from models trained with different seeds, cluster them and compute ARI between the clusterings. Note that even clustering the same embeddings twice with different clustering initializations will result in ARI < 1 (see Appendix A.4).", "description": "This figure shows the results of different regularization methods applied to the model for predicting neuronal activity in the visual cortex.  Panels A-F demonstrate the effect of regularization strength on the model's performance and the clustering of neuronal embeddings. Panel G shows the distribution of weights before and after alignment, while panel H illustrates the adaptive regularization scheme. Finally, panel I presents the adjusted rand index (ARI) values, quantifying the consistency of neuron clustering across multiple model fits. It aims to explore the effect of different regularization approaches on the consistency of neuron representations and clustering results.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_19_1.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the \u201cembedding\u201d is a linear vector representing a learned neuron function. Both are weights learned in the readout. In the top, the mask learnt is much smaller, hence, regularizing a neuronal embedding is more important to reduce the L1 penalty, while in the bottom we can keep the neuronal embedding less sparse; its impact is smaller as the mask is bigger. I: Adjusted rand index (ARI) for clustering embeddings using k-means. We take embeddings from models trained with different seeds, cluster them and compute ARI between the clusterings. Note that even clustering the same embeddings twice with different clustering initializations will result in ARI < 1 (see Appendix A.4).", "description": "This figure displays the results of an experiment comparing different regularization techniques on a model of neuronal activity. It shows model performance (correlation between predicted and observed neuronal responses), t-SNE projections of neuronal embeddings, weight distribution histograms, an example of adaptive regularization, and adjusted Rand Index (ARI) values for clustering embeddings across different models. The results demonstrate that L1 regularization is important for obtaining structured embeddings and that an adaptive regularization scheme improves consistency.", "section": "Results"}, {"figure_path": "VXxj3XZ1X8/figures/figures_19_2.jpg", "caption": "Figure 2: A: Model performance: correlation between predicted and observed neuronal response. B-F: T-SNE projections of the embeddings from differently regularized models. All models used seed = 42. G: The histogram of weights before alignment from different models. Unregularized weights (\u03b3 = 0) are crucially different, while \u03b3 = 50 and \u03b3 = 100 are close to the factorized readout's distribution. \u03b3lognorm = 10 is not as sparse as the factorized one. H: Example of \u2018adaptive\u2018 regularization for the factorized mask. The \u201cmask\u201d is a 2D matrix, selecting a \u201creceptive field\u201d from the latent space and the ", "description": "Figure 2 shows the results of an experiment comparing different regularization methods applied to predictive models of neuronal activity. Panel A shows the performance of the models, measured by the correlation between predicted and observed neuronal responses. Panels B-F display t-SNE projections of the learned neuronal embeddings obtained from models with different regularization strengths. Panel G provides a histogram of the weights from different models, demonstrating the impact of regularization on the sparsity of the weight distribution. Panel H illustrates the mechanism of adaptive regularization, which adjusts the regularization strength per neuron based on the learned mask size. Finally, Panel I shows the adjusted rand index (ARI) obtained by clustering the neuronal embeddings across different models and regularization schemes.", "section": "Results"}]