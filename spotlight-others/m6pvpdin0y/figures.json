[{"figure_path": "m6pVpdIN0y/figures/figures_4_1.jpg", "caption": "Figure 1: Test Accuracy as p increases across different datasets and activation functions averaged over 5 seeds. PSAM with GELU networks more closely follows the behavior of SAM. For ReLU networks and large p, there is a significant difference between PSAM and SAM.", "description": "This figure compares the test accuracy of the original SAM algorithm and Penalty SAM (PSAM) across different datasets and activation functions (GELU and ReLU).  The x-axis represents the hyperparameter \u03c1, which controls the perturbation size in SAM.  The y-axis shows the test accuracy. The figure demonstrates that PSAM with GELU activation functions behaves similarly to the original SAM algorithm, whereas PSAM with ReLU shows a significant performance degradation at larger values of \u03c1.", "section": "4 Explaining the pitfalls of gradient penalties"}, {"figure_path": "m6pVpdIN0y/figures/figures_4_2.jpg", "caption": "Figure 2: Test accuracy as p increases for penalty SAM with parts of the NME ablated (average of 5 seeds). The removal of information from the NME controls the effectiveness of the gradient penalty.", "description": "This figure shows the test accuracy as the hyperparameter p increases for three different datasets (Imagenet, CIFAR-10, and Fashion MNIST).  Three different activation functions are used: ReLU, GELU, and GELU with the activation NME ablated. The results demonstrate that removing information from the NME reduces the effectiveness of the gradient penalty, particularly for ReLU and GELU with ablated NME. This supports the paper's claim that the NME is crucial for understanding the performance of gradient penalty regularization.", "section": "4 Ablating activation NME explains the gap"}, {"figure_path": "m6pVpdIN0y/figures/figures_5_1.jpg", "caption": "Figure 3: Test accuracy as p increases for penalty SAM with parts of the NME replaced synthetically (average of 5 seeds). The addition of information to the NME improves the performance as p increases.", "description": "This figure shows the test accuracy on the ImageNet dataset as the hyperparameter p increases for different activation functions.  The experiment involves adding synthetic information to the Nonlinear Modeling Error (NME) component of the Hessian, specifically for the ReLU activation function. The results indicate that adding synthetic NME improves the performance of the gradient penalty regularization method as p gets larger.  The control groups using ReLU and GeLU without synthetic NME are shown for comparison.", "section": "4 Explaining the pitfalls of gradient penalties"}, {"figure_path": "m6pVpdIN0y/figures/figures_7_1.jpg", "caption": "Figure 4: Test Accuracy as \u03c3\u00b2 increases across different datasets and activation functions averaged over 5 seeds. Large \u03c3\u00b2 reveals a stark contrast between the Gauss-Newton trace penalty, which excludes NME, and methods incorporating it, highlighting the NME's influence.", "description": "This figure compares the test accuracy of three different methods (Gauss-Newton Trace Penalty, Hessian Trace Penalty, and Weight Noise) across three datasets (Imagenet, CIFAR-10, and Fashion MNIST) as the hyperparameter \u03c3\u00b2 increases.  The Gauss-Newton Trace Penalty, which ignores the Nonlinear Modeling Error (NME) component of the Hessian, shows consistently better performance than the Hessian Trace Penalty and Weight Noise, both of which include the NME. This highlights the detrimental effect of minimizing the NME during training and the importance of considering the NME in sharpness regularization.", "section": "5 Minimizing the NME is detrimental"}, {"figure_path": "m6pVpdIN0y/figures/figures_7_2.jpg", "caption": "Figure 5: Test Accuracy as \u03c3\u00b2 increases with both penalties estimated using Hutchinson's estimator with 5 samples (curves are averaged over 2 seeds). Despite the larger number of samples, Hessian trace penalty remains unstable, while Gauss-Newton trace penalty is stable. This suggests the instability is not due the estimator.", "description": "The figure compares the test accuracy of two different penalty methods (Gauss-Newton and Hessian trace penalty) against the noise parameter (\u03c3\u00b2) in weight noise experiments.  The Gauss-Newton penalty shows stable and consistent performance across various noise levels. In contrast, the Hessian penalty shows significantly unstable performance, exhibiting large fluctuations in accuracy.  Despite increasing the number of samples used in the Hutchinson estimator (from one in Figure 4 to five here), the instability of the Hessian penalty persists. This suggests that the instability is not solely due to the limitations of the estimation method, but rather inherent in the nature of the Hessian penalty itself.", "section": "5 Minimizing the NME is detrimental"}, {"figure_path": "m6pVpdIN0y/figures/figures_7_3.jpg", "caption": "Figure 6: Trace penalty over training iterations for different methods averaged over 5 seeds. The trace of the Hessian can be negative due to the NME, while the trace of the Gauss-Newton cannot. Minimizing the Hessian trace causes it to become very negative, which is detrimental to training stability.", "description": "This figure shows the trace of the Hessian and Gauss-Newton matrices over training iterations for two different methods: Hessian Trace Penalty and Gauss-Newton Trace Penalty. The key takeaway is that minimizing the Hessian trace, which includes the Nonlinear Modeling Error (NME), leads to increasingly negative values and instability. In contrast, the Gauss-Newton trace remains stable and close to zero. This illustrates the detrimental impact of minimizing the NME during training.", "section": "5.2 Minimizing the NME is detrimental"}, {"figure_path": "m6pVpdIN0y/figures/figures_13_1.jpg", "caption": "Figure 7: Loss (left) and Nonlinear Modeling Error matrix (NME) norm (right) as a function of 2 parameters in the same hidden layer of an MLP (MSE loss, one datapoint). For ReLU activation model is piecewise multilinear, and piecewise linear for parameters in same layer. Loss is piecewise quadratic for parameters in same layer (left). There is little NME information accessible pointwise and the main features are the boundaries of the piecewise linear regions (blue, right). For \u03b2-GELU, NME magnitude is high only within distance 1/\u03b2 of those boundaries. Therefore the NME encodes information about the utility of switching between piecewise multilinear regions.", "description": "The figure shows the loss landscape and the norm of the Nonlinear Modeling Error (NME) for a two-parameter model with ReLU and \u03b2-GELU activations.  The left panel shows that the ReLU activation results in a piecewise quadratic loss landscape, whereas \u03b2-GELU produces a smoother surface. The right panel visualizes the NME, illustrating how it highlights the boundaries between different linear regions in the ReLU case, providing information about the model's ability to switch between these regions, whereas the \u03b2-GELU NME is largely concentrated near these boundaries.", "section": "A.3 Intuitions about NME with ReLU"}, {"figure_path": "m6pVpdIN0y/figures/figures_17_1.jpg", "caption": "Figure 8: Test Accuracy as \u03b2 increases across different datasets and activation functions averaged over 5 seeds. Large \u03b2 reveals a stark contrast between the Gauss-Newton trace penalty, which excludes NME, and methods incorporating it, highlighting the NME's influence.", "description": "This figure compares the test accuracy of models trained using standard SGD and SGD with a gradient penalty, as the parameter \u03b2 in the \u03b2-GELU activation function is varied. The results are shown for both the Imagenet and CIFAR-10 datasets.  As \u03b2 increases, the \u03b2-GELU activation function approaches the ReLU function. The figure shows that the gradient penalty significantly improves accuracy for smaller \u03b2, but as \u03b2 increases and approaches ReLU, the benefit of the gradient penalty diminishes and even becomes detrimental, highlighting the importance of the NME (Nonlinear Modeling Error) component of the Hessian. The Gauss-Newton trace penalty, which excludes the NME, shows more stable performance across the range of \u03b2 values.", "section": "4.4 Ablating activation NME explains the gap"}, {"figure_path": "m6pVpdIN0y/figures/figures_17_2.jpg", "caption": "Figure 9: Fraction of non-zero activation second derivatives for \u03b2-GELU trained on Imagenet (left) and CIFAR10 (right). At initialization, fraction of non-zeros decreases somewhat with \u03b2 (blue); after training, fraction of non-zeros depends strongly on \u03b2 (orange).", "description": "The figure shows the fraction of non-zero second derivatives of the \u03b2-GELU activation function before and after training on the ImageNet and CIFAR-10 datasets.  It illustrates how the sparsity of the second derivative changes with the \u03b2 parameter, especially after training. The high sparsity for large \u03b2 values contributes to the failure of gradient penalties with ReLU-like activations.", "section": "4.4 Ablating activation NME explains the gap"}, {"figure_path": "m6pVpdIN0y/figures/figures_18_1.jpg", "caption": "Figure 10: Test Accuracy as p increases ablating full NME from the update. We can see ablating the full NME is detrimental to performance.", "description": "This figure shows the impact of ablating the full Nonlinear Modeling Error (NME) from the gradient penalty update rule on the test accuracy.  The experiment uses GELU activation functions. Two lines are shown: one for the standard GELU activation, and one for GELU with the NME component removed from its gradient penalty calculations. The results demonstrate that removing the NME significantly reduces performance across all values of the hyperparameter p.", "section": "C.1 Gradient penalty, Gauss-Newton only"}]