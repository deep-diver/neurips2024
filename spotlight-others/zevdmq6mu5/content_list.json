[{"type": "text", "text": "Curvature Clues: Decoding Deep Learning Privacy with Input Loss Curvature ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deepak Ravikumar Efstathia Soufleri Kaushik Roy ", "page_idx": 0}, {"type": "text", "text": "Department of Electrical and Computer Engineering Purdue University West Lafayette, IN 47907   \n{dravikum, esoufler, kaushik}@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we explore the properties of loss curvature with respect to input data in deep neural networks. Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input. We investigate how input loss curvature varies between train and test sets, and its implications for train-test distinguishability. We develop a theoretical framework that derives an upper bound on the train-test distinguishability based on privacy and the size of the training set. This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature. We validate our theoretical findings through experiments in computer vision classification tasks, demonstrating that input loss curvature surpasses existing methods in membership inference effectiveness. Our analysis highlights how the performance of membership inference attack (MIA) methods varies with the size of the training set, showing that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet. These findings not only advance our understanding of deep neural network behavior but also improve the ability to test privacy-preserving techniques in machine learning.1 2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks are being increasingly trained on sensitive datasets; thus ensuring the privacy of these models is paramount. Membership inference attacks (MIA) have become the standard approach to test a model\u2019s privacy [Murakonda and Shokri, 2020]. These attacks take a trained model and aim to identify if a given example was used in its training. Recent work has linked curvature of loss with respect to input with memorization [Garg et al., 2024] and differential privacy [Dwork et al., 2006, Ravikumar et al., 2024]. Inspired by this line of research, we investigate the properties of input loss curvature and leverage our insights to develop a new membership inference attack. ", "page_idx": 0}, {"type": "text", "text": "Curvature of loss with respect to input (termed input loss curvature) is defined as the trace of the Hessian of loss with respect to the input [Moosavi-Dezfooli et al., 2019, Garg and Roy, 2023]. Prior works that study loss curvature have focused on two lines of research. The first line of research has focused on studying the loss curvature with respect to the weights of the deep neural net [Keskar et al., 2017, Wu et al., 2020, Jiang et al., 2020, Foret et al., 2021, Kwon et al., 2021, Andriushchenko and Flammarion, 2022] to better understand generalization. The second line of research studied the loss curvature with respect to the input (i.e. data) for gaining insight into adversarial robustness ", "page_idx": 0}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/f413f296aaf9208aabd4530394a75828da1eef56ee0635459dc33df7907c12c2.jpg", "img_caption": ["Low Curvature Training Examples From ImageNet ", "High Curvature Training Examples From ImageNet "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Visualizing low and high input curvature samples from a ResNet50 trained on ImageNet. Low input curvature training set images are prototypical and have lots of support in the trainset, while high input curvature train set examples have less support and are atypical. Test set examples lie around the training set images in higher curvature regions. ", "page_idx": 1}, {"type": "text", "text": "[Moosavi-Dezfooli et al., 2019, Fawzi et al., 2018], coresets [Garg and Roy, 2023] and memorization [Garg et al., 2024, Ravikumar et al., 2024], mainly focusing on its properties on the train set. Thus, there is a gap in our understanding of input loss curvature on unseen test examples. ", "page_idx": 1}, {"type": "text", "text": "Input loss curvature on the train set captures the prototypicality of an image. This is visualized in Figure 1 (left) which shows high curvature examples from ImagetNet [Russakovsky et al., 2015] train set. Figure 1 (right) builds intuition as to why this is the case \u2013 a sample with lots of support in the dataset is more likely to lie in low curvature regions while atypical/non-prototypical examples have less support in the training set and thus lie in higher curvature regions [Garg et al., 2024, Ravikumar et al., 2024]. With this established, we make a novel observation that on average the input loss curvature scores for test set examples are higher than train set examples. This is because test samples were not optimized for, hence they lie slightly off the flat minima, in regions of higher input curvature (also visualized in Figure 1). We leverage this insight and develop a theoretical framework that focuses on the distinguishability of train-test input loss curvature scores. Studying this distinguishability leads us to understand performance bounds of membership inference attacks [Shokri et al., 2017, Sablayrolles et al., 2019, Song and Mittal, 2021, Carlini et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "Our theoretical analysis obtains an upper bound on the KL divergence between train-test input loss curvature scores. This provides a ceiling for membership inference attack performance. The analysis also reveals that the upper bound when using input loss curvature scores is dependent on the number of training set examples and the differential privacy parameter \u03f5. This insight helps us understand the conditions under which input loss curvature can be more effective at detecting train vs. test examples. Conditions such as the number of training examples and the privacy guarantees of the model. ", "page_idx": 1}, {"type": "text", "text": "To test our theoretical results we perform black-box (setting where adversaries have access only to model outputs) membership inference attack (MIA) using input loss curvature scores. However, input loss curvature score calculation needs access to model parameters (which are not available in a black box setting). To resolve this issue we propose using a zero-order input loss curvature estimation. Zero-order estimation can calculate input loss curvature without needing access to model parameters. Our input loss curvature-based black-box membership inference attack results show that real datasets contain enough training examples for curvature scores to outperform current stateof-the-art membership inference methods. Our results show that curvature based MIA outperforms prior state-of-the-art techniques on $10\\%$ or larger subsets of CIFAR100 dataset (about 5000 sample training set). ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical Foundation: We provide theoretical analysis to understand the train-test distinguishability with input loss curvature scores, demonstrating that input loss curvature is more ", "page_idx": 1}, {"type": "text", "text": "sensitive and hence better at detecting train vs. test set examples than current state-of-the-art membership inference attacks.   \n\u2022 Adapting Theory To Practice: We propose using zero order input loss curvature estimation to enable black box membership inference attack using input loss curvature scores.   \n\u2022 Better Attack: We conduct experiments to validate our theoretical results. Specifically, we show that input loss curvature enables more effective black box membership inference attacks when compared to existing state-of-the-art techniques. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Membership Inference Attacks are used as a tool to test privacy [Murakonda and Shokri, 2020]. These attacks aim to identify if a particular data point was included in a model\u2019s training dataset [Shokri et al., 2017]. Existing techniques often leverage model outputs like loss values [Shokri et al., 2017, Yeom et al., 2018, Sablayrolles et al., 2019], confidence scores [Carlini et al., 2022] or modified entropy [Song and Mittal, 2021]. Shokri et al. [2017] proposed the used of shadow models which are auxiliary models trained on subsets of the target model\u2019s data to aid in inference. Several modifications to this approach have been proposed. One important addition is the focus on example hardness, where the authors Sablayrolles et al. [2019] proposed an attack that scaled the loss using per example hardness threshold which is estimated by training shadow models. Watson et al. [2022] proposed a similar approach but in the offilne case where they calibrated the example hardness using the average loss of shadow models not trained on the target example. Ye et al. [2022] models various attacks into four categories and performs differential analysis to explain the gaps between them. Both Ye et al. [2022] and Long et al. [2020] consider the entire loss distribution of samples that are not in the training set. However, they face challenges in extrapolating to low false positive rates (FPRs). To address this issue Carlini et al. [2022] propose using a parametric model along with shadow models to improve performance. Orthogonal to these approaches Choquette-Choo et al. [2021] suggests the use of input augmentations during evaluation to improve the performance of the attacks. Similarly Jayaraman et al. [2021] propose the MERLIN attack, which queries the target model multiple times on a sample perturbed with Gaussian noise. ", "page_idx": 2}, {"type": "text", "text": "Input Loss Curvature is defined as the trace of the Hessian of loss with respect to the input [MoosaviDezfooli et al., 2019, Garg and Roy, 2023]. The aim is to measure the sensitivity of the deep neural network to a specific input. In general, loss curvature with respect to the parameters of deep neural nets has received lots of attention [Keskar et al., 2017, Wu et al., 2020, Jiang et al., 2020, Foret et al., 2021, Kwon et al., 2021, Andriushchenko and Flammarion, 2022], specifically due to its role in characterizing the sharpness of the learning objective which is closely connected to generalization. However, loss curvature with respect to the input data has received much less attention. It has been studied in the context of adversarial robustness [Fawzi et al., 2018, Moosavi-Dezfooli et al., 2019], coresets [Garg and Roy, 2023]. It has recently been linked with memorization [Garg et al., 2024, Ravikumar et al., 2024] and privacy [Ravikumar et al., 2024]. The authors in Moosavi-Dezfooli et al. [2019] showed that adversarial training decreases the input loss curvature and provided a theoretical link between robustness and curvature. In an orthogonal direction, Garg and Roy [2023] proposed the use of low input loss curvature examples as training dataset sets called coresets, which they showed to be more data-efficient. However, all of these works have focused on input loss curvature on the trainset. In this paper we focus on input loss curvature and its behavior on test or unseen examples to understand and improve the performance of membership inference attacks. Before we discuss our contributions, we present a few preliminaries, notation, and background needed for this paper ", "page_idx": 2}, {"type": "text", "text": "3 Notation and Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Let us consider a supervised learning problem, where the goal is to learn a mapping from an input space $\\mathcal{X}\\subset\\mathbb{R}^{d}$ to an output space $\\mathcal{V}\\subset\\mathbb{R}$ . The learning is performed using a randomized algorithm $\\boldsymbol{\\mathcal{A}}$ on a training set $S$ . Note that a randomized algorithm employs a degree of randomness as a part of its logic. The training set $S$ contains $m$ elements denoted as $z_{1},\\cdot\\cdot\\cdot\\,,z_{m}$ . Each element $z_{i}\\;=\\;(x_{i},y_{i})$ is drawn from an unknown distribution $\\mathcal{D}$ , where $z_{i}\\;\\in\\;{\\mathcal{Z}},x_{i}\\;\\in\\;{\\mathcal{X}}$ , $y_{i}~\\in~\\mathcal{Y}$ and $\\mathcal{Z}\\,=\\,\\mathcal{X}\\,\\times\\,\\mathcal{Y}$ . Thus, we define the training set $S\\,\\in\\,{\\mathcal{Z}}^{m}$ as $S\\,=\\,\\{z_{1},\\cdot\\cdot\\cdot\\,,z_{m}\\}$ . Another related concept is that of adjacent datasets, which are obtained when the set\u2019s $i^{t h}$ element is removed and ", "page_idx": 2}, {"type": "text", "text": "defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nS^{\\backslash i}=\\{z_{1},\\cdot\\cdot\\cdot,z_{i-1},z_{i+1},\\cdot\\cdot\\cdot,z_{m}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Additionally, the concept of adjacent (or neighboring) datasets is linked to the distance between datasets. The distance between two datasets $S$ and $S^{\\prime}$ , denoted by $\\|S-S^{\\prime}\\|_{1}$ , measures the number of samples that differ between them. The notation $\\|S\\|_{1}$ represents the size of a dataset $S$ . When a randomized learning algorithm $\\boldsymbol{\\mathcal{A}}$ is applied to a dataset $S$ , it produces a hypothesis denoted by $h_{S}^{\\phi}={\\mathcal{A}}(\\phi,S)$ , where $\\phi\\sim\\Phi$ is the random variable associated with the algorithm\u2019s randomness. A cost function $c:\\mathcal{Y}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^{+}$ is used to measure the hypothesis\u2019s performance. The cost of the hypothesis $h$ at a sample $z_{i}$ is also referred to as the loss $\\ell$ at $z_{i}$ , defined as $\\ell(h,z_{i})=c(h(x_{i}),y_{i})$ . The performance of a hypothesis $h$ is measured using risk $R(h)=\\mathbb{E}_{z_{i}}[\\ell(h,z_{i})]$ and approximated using empirical risk $\\begin{array}{r}{R_{e m p}^{\\phantom{\\dagger}}(h)=(1/m)\\sum_{i=1}^{m}\\ell(h,z_{i})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Differential Privacy. Introduced by Dwork et al. [2006], differential privacy is defined as follows: A randomized algorithm $\\boldsymbol{\\mathcal{A}}$ with domain ${\\mathcal{Z}}^{m}$ is considered $\\epsilon$ -differentially private if for any subset $\\mathcal{R}\\subset$ $\\mathrm{Range}(A)$ and for any datasets $S,S^{\\prime}\\in\\mathcal{Z}^{m}$ differing in at most one element (i.e. $||S-S^{\\prime}||_{1}\\leq1\\rangle$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}[h_{S}^{\\phi}\\in\\mathcal{R}]\\leq e^{\\epsilon}\\operatorname*{Pr}_{\\phi}[h_{S^{\\prime}}^{\\phi}\\in\\mathcal{R}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the probability is taken over the randomness of the algorithm $\\boldsymbol{\\mathcal{A}}$ , with $\\phi\\sim\\Phi$ . ", "page_idx": 3}, {"type": "text", "text": "Error Stability of a possibly randomized algorithm $\\boldsymbol{\\mathcal{A}}$ for some $\\beta>0$ is defined as Kearns and Ron [1997]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall i\\in\\{1,\\cdots,m\\},\\,\\,\\left|\\mathbb{E}_{\\phi,z}[\\ell(h_{S}^{\\phi},z)]-\\mathbb{E}_{\\phi,z}[\\ell(h_{S^{\\backslash i}}^{\\phi},z)]\\right|\\,\\,\\leq\\beta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $z\\sim\\mathcal{D}$ and $\\phi\\sim\\Phi$ . ", "page_idx": 3}, {"type": "text", "text": "Generalization. A randomized algorithm $\\boldsymbol{\\mathcal{A}}$ is considered to generalize with confidence $\\delta$ and at a rate of $\\gamma^{\\prime}(m)$ if: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[|R_{e m p}(h,S)-R(h)|\\leq\\gamma^{\\prime}(m)]\\geq\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Uniform Model Bias. The hypothesis $h$ produced by algorithm $\\boldsymbol{\\mathcal{A}}$ to learn the true conditional $h^{*}=\\mathbb{E}[y|x]$ from a dataset $S\\sim\\mathcal{D}^{m}$ has uniform bound on model bias denoted by $\\Delta$ if: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall S\\sim\\mathcal{D}^{m},\\quad\\left|\\mathbb{E}_{\\phi}[R(h_{S}^{\\phi})-R(h^{*})]\\right|\\leq\\Delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\rho$ -Lipschitz Hessian. The Hessian of $\\ell$ is Lipschitz continuous on $\\mathcal{Z}$ , if $\\forall z_{1},z_{2}\\in\\mathcal{Z}$ , and $\\forall h\\in$ $\\mathrm{Range}(A)$ , there exists some $\\rho>0$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{z_{1}}^{2}\\ell(h,z_{1})-\\nabla_{z_{2}}^{2}\\ell(h,z_{2})\\|\\leq\\rho\\|z_{1}-z_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Input Loss Curvature. As defined by Moosavi-Dezfooli et al. [2019], Garg et al. [2024], input loss curvature is the sum of the eigenvalues of the Hessian $H$ of the loss with respect to input $z_{i}$ . This is conveniently expressed using the trace as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Curv}_{\\phi}(z_{i},S)=\\operatorname{tr}(H)=\\operatorname{tr}(\\nabla_{z_{i}}^{2}\\ell(h_{S}^{\\phi},z_{i}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\upsilon$ -adjacency. A dataset $S$ is said to contain $\\upsilon$ -adjacent (read as upsilon-adjacent) elements if it contains two elements $z_{i},z_{j}$ such that $z_{j}~=~z_{i}+\\alpha$ for some $\\alpha~\\in~B_{p}(v)$ (read as $\\upsilon$ -Ball). This condition can be ensured through construction. Consider a dataset ${\\bar{S}}^{\\prime}$ which has no $z_{j}$ s.t $z_{j}\\,=\\,z_{i}+\\alpha;z_{j},z_{i}\\,\\in\\,S^{\\prime}$ . We can construct $S$ such that $S\\,=\\,\\{z\\,|\\,z\\,\\in\\,S^{\\prime}\\}\\cup\\{z_{i}+\\alpha\\}$ for some $z_{i}\\in S^{\\prime},\\alpha\\in B_{p}(v)$ , thereby ensuring $\\upsilon$ -adjacency. ", "page_idx": 3}, {"type": "text", "text": "Membership Inference Threat Model. In a membership inference security game [Yeom et al., 2018, Jayaraman et al., 2021, Carlini et al., 2022], a challenger and an adversary interact to test the privacy of a machine learning model. The process begins with the challenger sampling a training dataset from a distribution and training a model on this data. The challenger then filps a coin to decide whether to select a fresh data point from the distribution, which is not part of the training set, or to choose a data point from the training set. This selected data point is given to the adversary, who has access to the same data distribution and the trained model. The adversary\u2019s task is to determine whether the given data point was part of the training set or not. If the adversary\u2019s guess is correct, the game indicates a successful membership inference attack. Such a game is said to be in a black-box setting when the adversary has access to only the challenger\u2019s output. ", "page_idx": 3}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/cda1faf1b71103bd76a58162f90eec671fd056c962a2a0301826ef166f071182.jpg", "img_caption": ["Marginal Curvature Score Distribution ", "Figure 2: Marginal curvature score histogram for all images in ImageNet when samples in train set vs test set. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/5aa57fa576462f8ba80bb86a4b918f04ef38a3e7c12ef11b6fca3ac98393e0e8.jpg", "img_caption": ["Figure 3: Conditional curvature scores for a single image from ImageNet when using 56 shadow models. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we analyze the distinguishability of train-test samples using KL divergence. We do so for two cases, the first case when using network\u2019s output probability, and second when using input loss curvature. The importance of analyzing network output probabilities stems from its utilization in the current state-of-the-art attack, LiRA [Carlini et al., 2022]. Thus, studying both cases will let us theoretically analyze and compare their performance with input loss curvature. ", "page_idx": 4}, {"type": "text", "text": "Before we begin the analysis, we briefly discuss the conditional and marginal distributions of input loss curvature for train and test examples. Discussing this is important to build intuition for the analysis. Figure 2 visualizes the histogram (proxy for distribution) of input loss curvature for train and test examples from ImageNet [Russakovsky et al., 2015]. Specifically, it plots the log of the input loss curvature $\\log\\bigl(\\mathrm{Curv}_{\\phi}\\bigr)$ obtained on pre-trained ResNet50 [He et al., 2016] models from Feldman and Zhang [2020]. It plots a histogram of these scores, showing the frequency of specific log curvature values. ", "page_idx": 4}, {"type": "text", "text": "A naive membership inference attack would apply a threshold to separate these distributions. However, it is common practice to consider sample specific scores (i.e., conditioned on the target sample). This is visualized in Figure 3 which plots the distribution of curvature scores for a single ImageNet sample, indicating differences when the sample is part of the training set versus when it is a test or unseen sample. The data was generated using models from Feldman and Zhang [2020]. The figure also includes a kernel density estimate (KDE, shown as a dashed line) to better to visualize the underlying distribution. Figure 3 suggests that, similar to Carlini et al. [2022] sample conditional curvature scores can be modeled using a Gaussian parametric model. If we represent the curvature score ${\\mathrm{Curv}}_{\\phi}$ as a random variable, then $\\operatorname{Curv}_{\\phi}\\sim\\mathcal N(\\mu,\\sigma)$ . The probability density function of ${\\mathrm{Curv}}_{\\phi}$ is a function of the randomness of the algorithm $\\phi$ , the dataset $S$ and the $i^{t h}$ sample $z_{i}$ and be denoted by $p_{c}(\\phi,S,z_{i})$ . Similarly, let $p(\\phi,S,z_{i})$ denote the probability density function of the neural network\u2019s output probability, which is also parameterized by the randomness of the algorithm $\\phi$ , the dataset $S$ and the $\\overline{{i}}^{t h}$ sample $z_{i}$ . With this setup we present the following theoretical results on the upper bound on the KL divergence between train and test distribution for the two cases. Theorem 4.1 presents the upper bound on the KL divergence when using the neural network\u2019s output probability scores, and Theorem 4.2 presents the upper bound on the KL divergence when using input loss curvature. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Privacy bounds Train-Test KL Divergence). Assume $\\epsilon$ -differential private algorithm, then the $K L$ divergence between train-test distributions of the neural network\u2019s output probability is upper bound by the differential privacy parameter $\\epsilon$ given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{KL}}\\left(p(\\phi,S,z_{i})\\mid\\mid p(\\phi,S^{\\backslash i},z_{i})\\right)\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Sketch of Proof. Given that the algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\epsilon$ -differentially private, we know that the probability of the output $h_{S}^{\\phi}\\ \\in\\ \\mathcal{R}$ on dataset $S$ is bounded by $e^{\\epsilon}$ times the probability of the same event on the neighboring dataset $S^{\\backslash i}$ , i.e. $\\operatorname*{Pr}_{\\phi}[h_{S}^{\\phi}\\,\\in\\,\\mathcal{R}]\\,\\le\\,e^{\\epsilon}\\operatorname*{Pr}_{\\phi}[h_{S\\backslash i}^{\\phi}\\,\\in\\,\\mathcal{R}]$ . From this inequality it follows that the KL divergence between the output distributions of $\\boldsymbol{\\mathcal{A}}$ on $S$ and $S^{\\backslash i}$ is bounded by $\\mathrm{KL}(p(\\phi,S,z_{i})\\mid\\mid p(\\phi,S^{\\backslash i},z_{i}))\\leq\\epsilon$ . The full proof for Theorem 4.1 is provided in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Dataset Size and Privacy bound Curvature KL Divergence). Let the assumptions of error stability 2, generalization 3, and uniform model bias $^{4}$ hold. Further, assume $0\\,\\leq\\,\\ell\\,\\leq\\,L$ . Let the conditional distribution be parameterized with variance $\\sigma$ . Then, any $\\epsilon$ -differential private algorithm using a dataset of size m with a probability at least $1-\\delta$ satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{D}_{\\mathrm{KL}}(p_{c}(\\phi,S,z_{i})\\;||\\;p_{c}(\\phi,S^{\\backslash i},z_{i}))\\leq\\frac{\\left[L m(1-e^{-\\epsilon})+c\\right]^{2}}{2\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\nc=(4m-1)\\gamma+2(m-1)\\Delta+\\frac{\\rho}{6}\\mathbb{E}[\\|\\alpha\\|^{3}]+L\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Sketch of Proof. We begin by assuming a Gaussian model for the curvature distribution and expand the formula for the $\\mathrm{KL}$ divergence, which leads to expressions involving the mean and standard deviation of the curvature scores. To establish an upper bound for this expression, we prove a lemma concerning the upper bound of the mean curvature, utilizing results from Ravikumar et al. [2024]. This enables us to express the upper bound on the mean curvature in terms of the privacy parameter \u03f5. Subsequently, this result is employed to derive an upper bound on the KL divergence of the curvature scores. The proof for Theorem 4.2 is provided in Appendix A.5. ", "page_idx": 5}, {"type": "text", "text": "Discussion. Theorem 4.1 and Theorem 4.2 provide the upper bound on KL Divergence between train and test distributions of the probability and input loss curvature scores, respectively. They imply the upper limit of MIA performance, and interestingly, the result of Theorem 4.2 suggests the role of the dataset size on MIA performance. However, the relation is not as straightforward as suggested by Equation 8. This is because, in real applications, parameter $\\sigma$ depends on $\\epsilon$ , and so does the loss bound $L$ . In fact, the loss bound is also dependent on the number of samples $m$ [Bousquet and Elisseeff, 2002]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Dataset Size and Curvature MIA Performance). Let the assumptions of error stability 2, generalization 3, and uniform model bias 4 hold. Further, assume $0\\leq\\ell\\leq L$ , and the bounds of Theorem 4.1 and 4.2 are tight. Then, the performance of MIA using curvature scores exceeds that of confidence scores with a probability at least $1-\\delta$ when: ", "page_idx": 5}, {"type": "equation", "text": "$$\nm>{\\frac{({\\sqrt{2\\sigma^{2}\\epsilon}})-c}{L(1-e^{-\\epsilon})}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 suggests that the performance of curvature based MIA will exceed that of probability score based methods when the size of the dataset used to train the target model exceed a certain threshold. Indeed, this is what we observe in practice (see section 6.4). ", "page_idx": 5}, {"type": "text", "text": "Sketch of Proof. The proof compares the upper bounds from Theorem 4.1 and Theorem 4.2, followed by a series of algebraic manipulations. This identifies the conditions under which the performance of curvature scores exceeds that of confidence scores. The full proof can be found in Appendix A.6. ", "page_idx": 5}, {"type": "text", "text": "On the validity of the assumptions. Before presenting our experiments to validate our theory, we briefly discuss the validity of our assumptions in practical settings. Research by Hardt et al. [2016] shows that stochastic gradient methods, such as stochastic gradient descent, achieve small generalization error and exhibit uniform stability. Thus, the assumptions of stability (Equation 2) and generalization (Equation 3) are justified. Model bias is a property of the model, and a uniform bound across different datasets seems reasonable. Note, uniform loss bound (and $L$ independent of $m$ as used by Theorem 4.3) holds true for certain losses and for statistical models and is often assumed in learning theory [Wang et al., 2016]. Lastly, the $\\upsilon$ -adjacency can be ensured through construction. For large datasets this may not be needed, because the size of the ball $B_{p}(v)$ is unconstrained. Hence two samples from the same class that are similar may suffice. Given the size of modern datasets, this assumption is also reasonable. ", "page_idx": 5}, {"type": "text", "text": "5 Zero Order Input Loss Curvature MIA ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To test if input loss curvature based membership inference performs better than existing methods we need an efficient technique to estimate curvature. We are interested in black-box membership inference attacks, where one does not have access to the target network\u2019s parameters. However, current techniques use Hutchinson\u2019s trace estimator [Hutchinson, 1989] to measure input loss curvature such as from Garg and Roy [2023], Garg et al. [2024] or Ravikumar et al. [2024]. This approach needs to evaluate the gradient and hence requires access to model parameters. To solve this issue, we propose using a zero-order estimation technique. Zero-order curvature estimation starts with a finite-difference estimation. Consider a function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , then the Hessian at a given point $z_{i}$ can be estimated as follows: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla^{2}f(z_{i})=n^{2}{\\frac{f(z_{i}+h v+h u)-f(z_{i}-h v+h u)-f(z_{i}+h v-h u)+f(z_{i}-h v-h u)}{4h^{2}}}u v^{\\top}\\,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $h$ is a small increment (a hyper parameter in out case), and $u,v$ are vectors in $\\mathbb{R}^{n}$ . In our case, to get the input loss curvature, we have $f\\;\\leftarrow\\;\\ell(g(x_{i}),y_{i})$ , where $g$ is the neural network, $\\ell$ is the loss function and $z_{i}\\,=\\,(x_{i},y_{i})$ are the image, label pair. The pseudo-code for obtaining input loss curvature score using zero order estimation shown in Algorithm 1 in Appendix A.1. To execute membership inference attack using input loss curvature scores, we propose the following methodology. First, we begin by training shadow models, similar to Shokri et al. [2017], Carlini et al. [2022]. These shadow models are used to obtain empirical estimates of parametric model for the curvature score described in Section 4. During the inference phase, we employ a likelihood ratio between the sample being in the train set vs test set parametric models to identify the membership status of a given sample (see Appendix A.2 for pseudo-code). In addition, we perform a negative log likelihood ratio test. We denote the results of likelihood test as \u2018LR\u2019 and the results of the negative log likelihood ratio test as \u2018NLL\u2019. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. To evaluate our theory, we consider the classification task using standard vision datasets. Specifically, we use the CIFAR10, CIFAR100 [Krizhevsky et al., 2009] and ImageNet [Russakovsky et al., 2015] datasets. ", "page_idx": 6}, {"type": "text", "text": "Architectures. For experiments on ImageNet we use the ResNet50 architecture [He et al., 2016]. For CIFAR10 and CIFAR100, we used the ResNet18 architecture. Details regrading hyperparameters are provided in Appendix A.13. To improve reproducibility, we have open sourced the code at https://github.com/DeepakTatachar/Curvature-Clues. ", "page_idx": 6}, {"type": "text", "text": "Training. For experiments using private models, we trained ResNet18 models with the Opacus library [Yousefpour et al., 2021] using DP-SGD with a maximum gradient norm of 1.0 and a privacy parameter of $\\dot{\\delta}^{'}{=}\\,1\\times10^{-5}$ . Shadow models for CIFAR10 and CIFAR100 were trained on a $50\\%$ subset of the data for 300 epochs. For ImageNet, we used pre-trained models from Feldman and Zhang [2020], trained on a $70\\%$ subset of ImageNet. More details about training and compute resources are provided in Appendix A.13. ", "page_idx": 6}, {"type": "text", "text": "Metrics. To evaluate curvature scores, we use AUROC and balanced accuracy. The Receiver Operating Characteristic (ROC) is the plot of the True Positive Rate (TPR) against the False Positive Rate (FPR). The area under the ROC is called Area Under the Receiver Operating Characteristic (AUROC). AUROC of 1 denotes an ideal detection scheme, since the ideal detection algorithm results in 0 false positive and false negative samples. ", "page_idx": 6}, {"type": "text", "text": "6.2 Membership Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we compare the performance of the proposed input loss curvature based membership inference against prior MIA techniques. ", "page_idx": 6}, {"type": "text", "text": "Setup: We use CIFAR10, CIFAR100 and ImageNet datasets to test the MIA performance. We consider a black-box MIA setup similar to Carlini et al. [2022]. We use ResNet18 for CIFAR10 and CIFAR100, for ImageNet we use the ResNet50 architecture. For all the membership inference attacks, we compute a full ROC curve and report the results. When using shadow models we 64 for CIFAR10 and CIFAR100 and 52 for ImageNet. The AUROC plot for CIFAR100 for various methods are shown in Figure 4. Table 1 reports the average balanced accuracy and AUROC values over three seeds on various MIA methods on the three datasets. The plot in Figure 4 is a log-log plot to emphasize performance of the proposed method at very low false positive rates (see the orange line y-intercept). We also studied the effect of augmentations and the results can be found in Appendix A.7, the takeaway was that adding more augmentations improved performance. Note that the results presented in Table 1 used 2 augmentations (image $^+$ mirror) for our technique and Carlini et al. [2022] for fair comparison. ", "page_idx": 6}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/c74780445c1afe4136ce44a45e4c0a111f0a8a89d1cf7a7cd4c92ba5994bc357.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of the proposed curvature score based MIA with prior methods tested on ImageNet, CIFAR100, and CIFAR10 datasets. Results reported are the mean $\\pm$ std obtained over 3 seeds. For CIFAR10 and CIFAR100 64 shadow models were used and 52 for ImagNet. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results: From Table 1, we see that the proposed method performs better than all existing MIA techniques on both ImageNet and CIFAR datasets. Apart from AUROC and balanced accuracy results, the log-log plot emphasizes the performance at really low FPR (i.e. the y intercept, high TPR at low FPR in Figure 4). Additional results at low FPR are available in Appendix A.8. ", "page_idx": 7}, {"type": "text", "text": "Takeaways: As predicted by Theorem 4.2, the higher KL divergence between train and test curvature score distributions results in superior MIA performance. Further, we observe that while using a negative log likelihood ratio test does better in AUROC and balanced accuracy, the parametric likelihood ratio test does better at low false positive rates as shown in Figure 4. Thus, the proposed use of curvature should be tailored based on use case. Applications that demand high AUROC can use NLL approach, while those that demand high TPR at very low FPR should use the LR technique. ", "page_idx": 7}, {"type": "text", "text": "6.3 Effect of Privacy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we study the effect of differential privacy on MIA performance and test the $\\epsilon$ relation predicted by Theorem 4.2. ", "page_idx": 7}, {"type": "text", "text": "Setup: To study the effect of privacy on MIA attack performance, we use DP-SGD trained models. We use privacy $\\epsilon$ values of 1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50 with $\\delta=1\\times10^{-5}$ . We ", "page_idx": 7}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/88fe398947e03b4fb6ba656be4ad81c02b9e4fde4b10d9b3e180b73ea9907476.jpg", "img_caption": ["Figure 4: Comparing our method against existing Figure 5: Validating the upper bound from Thetechniques at low FPR. The proposed parametric orem 4.2 by fitting the MIA performance (AUCurv LR technique has the highest TPR at very ROC of Curv LR) on DP-SGD trained models low FPR. for various privacy parameters $\\epsilon$ values. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/69d8186ae5d944b7df941859e3a5d67bcc1e929815488572097f348eddc0882e.jpg", "img_caption": ["Figure 6: Visualizing MIA performance as a function of the size of the train set, which is randomly sampled. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/c27c791e228a15d3be42d18e49dd9b3ddb3c730ea23c2ddd778d69384ea4a699.jpg", "img_caption": ["Figure 7: MIA performance as a function of the size of the train set when subsets contain the lowest curvature examples from the entire set. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "plot the result of the proposed curvature based MIA in Figure 5. In Figure 5 we also plots the best fti using $s_{f}(L_{f}(1-e^{-\\bar{\\epsilon}})+c_{f})^{2}$ , where $s_{f},L_{f}$ and $c_{f}$ are fit to the data. ", "page_idx": 8}, {"type": "text", "text": "Results and Takeaways: We see that the theoretical prediction from Theorem 4.2 about MIA performance is well matched. Since Theorem 4.2 provides an upper bound, the results validate the theory. ", "page_idx": 8}, {"type": "text", "text": "6.4 Effect of Dataset Size ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we study the effect of dataset size on MIA attack performance. This lets us test the relationship of MIA performance to $m$ as predicted by Theorem 4.2 and Theorem 4.3. ", "page_idx": 8}, {"type": "text", "text": "Setup: For this experiment, we train models on increasing dataset size on CIFAR100. Specifically, we train multiple seeds on various subsets randomly chosen from the CIFAR100 training set. We also repeated the same by choosing the lowest curvature samples from CIFAR100 and train with the same subset sizes. Next, for each of the models we perform MIA attack and we present the results in Figure 6 (randomly chosen) and Figure 7 (lowest curvature samples). ", "page_idx": 8}, {"type": "text", "text": "Results: From Figure 6, we see that when samples are randomly chosen, the MIA performance decreases as we add more samples. This result is consistent with prior literature [Abadi et al., 2016]. Further as predicted by Theorem 4.3 beyond $30-40\\%$ subset Curv ZO LR out perform prior works and Curv ZO NLL outperforms prior works beyond $10\\%$ subset size. ", "page_idx": 8}, {"type": "text", "text": "To extend this analysis, we train models on curvature based coresets [Garg and Roy, 2023]. These coresets of low input loss curvature samples have been shown to memorize less [Garg et al., 2024, Ravikumar et al., 2024]. Thus we expect MIA accuracy to increase as we increase coreset size. This is exactly what happens and is shown in Figure 7 which plots the AUROC and accuracy of the NLL curvature attack as we increase curvature coreset size. However, the MIA performance is higher for comparable size in Figure 6 and 7. This suggests that curvature based coresets result in more susceptible models, which is also supported by results in Song et al. [2019]. ", "page_idx": 8}, {"type": "text", "text": "Takeaways: We validate Theorem 4.3. We note that beyond a certain dataset size (of about $30-40\\%$ subset of the training set Curv ZO LR out perform prior works and Curv ZO NLL outperforms prior works beyond $10\\%$ subset size) curvature scores outperform probability score based MIA method. While Theorem 4.3 predicts that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets as evident from the results presented. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we explored the properties of input loss curvature on the test set. Specifically, we focus on using input loss curvature to distinguish between train and test examples. We established a theoretical framework deriving an upper bound on train-test KL Divergence based on privacy and training set size. To extend the applicability of input loss curvature computation to a black-box setting, we propose a novel zero-order curvature estimation method. This enables the development of a new cutting-edge black-box membership inference attack (MIA) methodology that leverages input loss curvature. Through extensive experiments on the ImageNet and CIFAR datasets, we demonstrate that our input loss curvature-based MIA method outperforms existing state-of-the-art techniques. Our results corroborate our theoretical predictions regarding the relationship between MIA performance and dataset size. Specifically, we show that beyond a certain dataset size, the effectiveness of curvature scores surpasses other methods. Importantly, we observe that this dataset size condition is frequently met in practical scenarios, as evidenced by our results on the CIFAR100 dataset. These findings advance our understanding of input loss curvature in the context of privacy and help build more secure deep learning models. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by, the Center for the Co-Design of Cognitive Systems (COCOSYS), a DARPA-sponsored JUMP 2.0 center, the Semiconductor Research Corporation (SRC), and the National Science Foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \nM. Andriushchenko and N. Flammarion. Towards understanding sharpness-aware minimization. In International Conference on Machine Learning, pages 639\u2013668. PMLR, 2022.   \nJ. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS \u201924). ACM, Apr. 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf.   \nO. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499\u2013526, 2002.   \nN. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy $(S P)$ , pages 1897\u20131914. IEEE, 2022.   \nC. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot. Label-only membership inference attacks. In International conference on machine learning, pages 1964\u20131974. PMLR, 2021.   \nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\cdot^{=}$ YicbFdNTTy.   \nC. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \nA. Fawzi, S.-M. Moosavi-Dezfooli, P. Frossard, and S. Soatto. Empirical study of the topology and geometry of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3762\u20133770, 2018.   \nV. Feldman and C. Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems, 33:2881\u20132891, 2020.   \nP. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\cdot^{=}$ 6Tm1mposlrM.   \nI. Garg and K. Roy. Samples with low loss curvature improve data efficiency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20290\u201320300, 2023.   \nI. Garg, D. Ravikumar, and K. Roy. Memorization through the lens of curvature of loss function around samples. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id $\\cdot^{=}$ WQbDS9RydY.   \nM. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pages 1225\u20131234. PMLR, 2016.   \nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nM. F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u20131076, 1989.   \nB. Jayaraman, L. Wang, K. Knipmeyer, Q. Gu, and D. Evans. Revisiting membership inference under realistic assumptions. Proceedings on Privacy Enhancing Technologies, 2021(2), 2021.   \nY. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id $\\cdot^{=}$ SJgIPJBFvH.   \nM. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation. In Proceedings of the tenth annual conference on Computational learning theory, pages 152\u2013162, 1997.   \nN. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\cdot^{=}$ H1oyRlYgg.   \nA. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images, 2009.   \nJ. Kwon, J. Kim, H. Park, and I. K. Choi. Asam: Adaptive sharpness-aware minimization for scaleinvariant learning of deep neural networks. In International Conference on Machine Learning, pages 5905\u20135914. PMLR, 2021.   \nY. Long, L. Wang, D. Bu, V. Bindschaedler, X. Wang, H. Tang, C. A. Gunter, and K. Chen. A pragmatic approach to membership inferences on machine learning models. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P), pages 521\u2013534. IEEE, 2020.   \nH. Mehta, A. Thakurta, A. Kurakin, and A. Cutkosky. Large scale transfer learning for differentially private image classification. arXiv preprint arXiv:2205.02973, 2022.   \nS.-M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, and P. Frossard. Robustness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9078\u20139086, 2019.   \nS. K. Murakonda and R. Shokri. Ml privacy meter: Aiding regulatory compliance by quantifying the privacy risks of machine learning. arXiv preprint arXiv:2007.09339, 2020.   \nY. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.   \nD. Ravikumar, E. Soufleri, A. Hashemi, and K. Roy. Unveiling privacy, memorization, and input curvature links. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=4dxR7awO5n.   \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.   \nA. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. J\u00e9gou. White-box vs black-box: Bayes optimal strategies for membership inference. In International Conference on Machine Learning, pages 5558\u20135567. PMLR, 2019.   \nR. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy $(S P)$ , pages 3\u201318. IEEE, 2017.   \nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \nL. Song and P. Mittal. Systematic evaluation of privacy risks of machine learning models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2615\u20132632, 2021.   \nL. Song, R. Shokri, and P. Mittal. Privacy risks of securing machine learning models against adversarial examples. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 241\u2013257, 2019.   \nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_ files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \nY.-X. Wang, J. Lei, and S. E. Fienberg. Learning with differential privacy: Stability, learnability and the sufficiency and necessity of erm principle. Journal of Machine Learning Research, 17(183): 1\u201340, 2016.   \nL. Watson, C. Guo, G. Cormode, and A. Sablayrolles. On the importance of difficulty calibration in membership inference attacks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\cdot$ 3eIrli0TwQ.   \nD. Wu, S.-T. Xia, and Y. Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33:2958\u20132969, 2020.   \nJ. Ye, A. Maddi, S. K. Murakonda, V. Bindschaedler, and R. Shokri. Enhanced membership inference attacks against machine learning models. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pages 3093\u20133106, 2022.   \nS. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF), pages 268\u2013282. IEEE, 2018.   \nA. Yousefpour, I. Shilov, A. Sablayrolles, D. Testuggine, K. Prasad, M. Malek, J. Nguyen, S. Ghosh, A. Bharadwaj, J. Zhao, G. Cormode, and I. Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Zero Order Curvature Estimation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We present the pseudo-code for zero-order curvature score calculation below. Please note that Algorithm 1 shows the input loss curvature calculation for one example; however, it can be easily implemented for batch data. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 1 Pseudo-code for Zero Order Input Loss Curvature Estimation   \n1: Input: $z_{i}=(x_{i},y_{i})$ (image, label data), $\\ell$ (loss function), $n_{i t e r}$ (hyperparameter: number of   \niterations), $h$ (hyperparameter: small increment), $g$ (neural network)   \n2: Output: curv_estimate (zero order input loss curvature estimate of $z_{i}$ )   \n3: $c u r v\\gets0$   \n4: for $i\\gets1$ to $n_{i t e r}$ do   \n5: $v\\sim$ Rademacher   \n6: $u\\sim$ Rademacher   \n7: $f\\leftarrow\\ell(g(x_{i}),y_{i})$   \n8: $\\nabla^{2}f(z_{i})\\gets$ Use Equation 11   \n9: $c u r v\\gets c u r v+\\mathrm{tr}(\\nabla^{2}f(z_{i}))$   \n10: end for   \n11: curv_estimate \u2190curv/niter   \n12: return curv_estimate ", "page_idx": 12}, {"type": "text", "text": "A.2 Curvature based MIA ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we present the detailed step by step description and pseudo code for the curvature based MIA (refer to Algorithm 2) ", "page_idx": 12}, {"type": "text", "text": "1. Initialization (Lines 3-4): Two empty sets, $c u r v_{i n}$ and $c u r v_{o u t}$ , are initialized. These sets will store the curvature scores for the shadow models trained with and without the target example $z_{i}$ . ", "page_idx": 12}, {"type": "text", "text": "Shadow Model Training (Lines 5-12): For a specified number of iterations N, the algorithm performs the following steps: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Sampling Shadow Dataset (Line 6): A shadow dataset $\\mathcal{D}_{a t t a c k}$ is sampled from the data distribution $\\mathcal{D}$ . \u2022 Training IN Shadow Model (Line 7): A shadow model $g_{i n}^{n}$ is trained on the shadow dataset augmented with the target example, $\\mathcal{D}_{a t t a c k}\\cup\\{z_{i}\\}$ . The curvature score for this model, denoted as cur $v_{i n}^{n}$ , is computed using Algorithm 1. \u2022 Collecting IN Curvature Scores (Line 8): The curvature score for the training set $\\mathcal{D}_{a t t a c k}\\cup\\{z_{i}\\}$ of this model $c u r v_{i n}^{n}$ is added to the set $c u r v_{i n}$ . \u2022 Training OUT Shadow Model (Line 9): Another shadow model $g_{o u t}^{n}$ is trained on the shadow dataset excluding the target example, $\\mathcal{D}_{a t t a c k}\\setminus\\{z_{i}\\}$ . The curvature score for this model, denoted as $c u r v_{o u t}^{n}$ , is also computed using Algorithm 1. \u2022 Collecting OUT Curvature Scores (Line 10): The curvature score for the training set $\\bar{D}_{a t t a c k}\\,\\bar{\\backslash}\\,\\{z_{i}\\}$ of this model $c u r v_{o u t}^{n}$ is added to the set $c u r v_{o u t}$ . ", "page_idx": 12}, {"type": "text", "text": "3. Model Parameter Estimation (Lines 13-16): After training the shadow models, the algorithm calculates the mean $(\\mu_{i n},\\mu_{o u t})$ and variance $(\\sigma_{i n}^{2},\\sigma_{o u t}^{2})$ of the collected curvature scores for both the \u2018IN\u2019 and \u2018OUT\u2019 models. ", "page_idx": 12}, {"type": "text", "text": "4. Target Model Query (Line 17): The curvature score for the target model $g_{t a r g e t}$ with respect to the target example $z_{i}$ , denoted as $c u r v_{t a r g e t}$ , is computed using Algorithm 1. 5. Likelihood Ratio Test (Line 18-19): The likelihood ratio $P_{i n}$ is calculated. This ratio compares the probability of the observed curvature score under the \u2018IN\u2019 distribution against the \u2018OUT\u2019 distribution and is returned. For the \u2018NLL\u2019 version $P_{i n}$ us given by: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{i n}^{N L L}=\\log\\left[P\\left(c u r v_{t a r g e t}\\,|\\,\\mathcal{N}(\\mu_{o u t},\\sigma_{o u t}^{2})\\right)\\right]-\\log\\left[P\\left(c u r v_{t a r g e t}\\,|\\,\\mathcal{N}(\\mu_{i n}\\sigma_{i n}^{2})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Algorithm 2 Membership Inference Attack using Input Loss Curvature Scores ", "page_idx": 13}, {"type": "text", "text": "Require: Target model $g_{t a r g e t}$ , target example $z_{i}=(x_{i},y_{i})$ , data distribution $\\mathcal{D}$   \n1: $c u r v_{i n}\\gets\\{\\}$   \n2: $c u r v_{o u t}\\gets\\{\\}$ ", "page_idx": 13}, {"type": "text", "text": "3: for $n$ in $N$ do \u25b7Sample a shadow dataset $\\triangleright$ Train in shadow model ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "4: $\\begin{array}{r l}&{\\mathcal{D}_{a t t a c k}\\leftarrow\\mathcal{D}}\\\\ &{g_{i n}^{n}\\leftarrow\\mathrm{Train\\;on}\\;\\mathcal{D}_{a t t a c k}\\cup\\{z_{i}\\}.}\\\\ &{c u r v_{i n}^{n}\\leftarrow\\mathrm{Use\\;Algorithm}\\;1}\\\\ &{c u r v_{i n}\\leftarrow c u r v_{i n}\\cup c u r v_{i n}^{n}}\\\\ &{g_{o u t}^{n}\\leftarrow\\mathrm{Train\\;on}\\;\\mathcal{D}_{a t t a c k}\\setminus\\{z_{i}\\}.}\\\\ &{c u r v_{o u t}^{n}\\leftarrow\\mathrm{Use\\;Algorithm}\\;1}\\\\ &{c u r v_{o u t}\\leftarrow c u r v_{o u t}\\cup c u r v_{o u t}^{n}}\\end{array}$   \n5:   \n6:   \n7:   \n8:   \n9:   \n10: ", "page_idx": 13}, {"type": "text", "text": "\u25b7Train out shadow model ", "page_idx": 13}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "12: $\\mu_{i n}\\gets\\operatorname*{mean}(c u r v_{i n})$   \n13: $\\mu_{o u t}\\gets\\operatorname*{mean}(c u r v_{o u t})$   \n14: $\\sigma_{i n}^{2}\\gets\\mathrm{var}(c u r v_{i n})$ i2n   \n15: $\\sigma_{o u t}^{2}\\gets\\mathrm{var}(c u r v_{o u t})$   \n16: $c u r v_{t a r g e t}\\gets[$ Use Algorithm 1 with $g_{t a r g e t}$ and $z_{i}$ . $P_{i n}=\\frac{P\\left(c u r v_{t a r g e t}\\mid\\mathcal{N}(\\mu_{i n},\\sigma_{i n}^{2})\\right)}{P\\left(c u r v_{t a r g e t}\\mid\\mathcal{N}(\\mu_{o u t},\\sigma_{o u t}^{2})\\right)}$   \n18: return $P_{i n}$ ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider an $\\epsilon{-}\\mathrm{DP}$ algorithm and $S,S^{\\backslash i}$ such that $||S-S^{\\setminus i}||=1$ . Next let $\\mathcal{R}\\subset\\mathrm{Range}(A)$ such that $\\mathcal{R}=\\{h\\mid h(x_{i})=y_{i}\\}$ . Since $\\boldsymbol{\\mathcal{A}}$ is $\\epsilon$ -differentially private, then it follows from the definition of differential privacy in Equation 1 that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}[h_{S}^{\\phi}\\in\\mathcal{R}]\\leq e^{\\epsilon}\\operatorname*{Pr}_{\\phi}[h_{S\\setminus i}^{\\phi}\\in\\mathcal{R}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\implies p(\\phi,S,z_{i})\\leq e^{\\epsilon}p(\\phi,S^{\\backslash i},z_{i})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $p(\\phi,S,z_{i})=\\mathrm{Pr}_{\\phi}[h_{S}^{\\phi}\\in\\mathcal{R}]$ and denotes the probability density function of the neural network\u2019s output probability. Now, we use the definition of KL divergence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}\\left(p(\\phi,S,z_{i})\\mid\\mid p(\\phi,S^{\\setminus i},z_{i})\\right)=\\mathbb{E}_{\\phi}\\left[\\log\\left(\\frac{p(\\phi,S,z_{i})}{p\\left(\\phi,S^{\\setminus i},z_{i}\\right)}\\right)\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\mathbb{E}_{\\phi}\\left[\\log e^{\\epsilon}\\right]\\quad\\mathrm{From~Equation~}14}\\\\ &{\\mathrm{D}_{\\mathrm{KL}}\\left(p(\\phi,S,z_{i})\\mid\\mid p(\\phi,S^{\\setminus i},z_{i})\\right)\\leq\\epsilon\\quad\\mathbf{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.4 Lemma A.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (Upper bound on Input Loss Curvature). Let $\\boldsymbol{\\mathcal{A}}$ be a randomized algorithm which is $\\epsilon$ -differentially private and the assumptions of error stability 2, generalization $^3$ , and uniform model bias 4 hold. Further, assume $0\\leq\\ell\\leq L$ . Then for two adjacent datasets $S,S^{\\backslash i}\\sim\\mathcal{D}$ with $a$ probability at least $1-\\delta$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\phi}[\\mathrm{Curv}_{\\phi}(z_{i},S^{\\backslash i})]\\leq L(m(1-e^{-\\epsilon})+1)+c_{1}}\\\\ &{\\qquad c_{1}=(4m-1)\\gamma+2(m-1)\\Delta+\\displaystyle\\frac{\\rho}{6}\\mathbb{E}[\\|\\alpha\\|^{3}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma A.1 For this proof, we use results from Nesterov and Polyak [2006], we restate it here for convenience ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. If Lipschitz assumption 5 on the Hessian of $\\ell$ holds from Nesterov and Polyak [2006] we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\ell(h,z_{1})-\\ell(h,z_{2})-\\langle\\nabla\\ell(h,z_{2}),z_{1}-z_{2}\\rangle-\\langle\\nabla^{2}\\ell(h,z_{2})(z_{1}-z_{2}),z_{1}-z_{2}\\rangle|\\leq\\frac{\\rho}{6}|z_{1}-z_{2}|^{3}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Lemma A.2 we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\frac{\\rho}{6}|z_{1}-z_{2}|^{3}\\leq\\ell(h,z_{1})-\\ell(h,z_{2})-\\langle\\nabla\\ell(h,z_{2}),z_{1}-z_{2}\\rangle-\\langle\\nabla^{2}\\ell(h,z_{2})(z_{1}-z_{2}),z_{1}-z_{2}\\rangle}\\\\ {\\ell(h,z_{1})-\\ell(h,z_{2})-\\langle\\nabla\\ell(h,z_{2}),z_{1}-z_{2}\\rangle-\\langle\\nabla^{2}\\ell(h,z_{2})(z_{1}-z_{2}),z_{1}-z_{2}\\rangle\\leq\\frac{\\rho}{6}|z_{1}-z_{2}|^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This gives us a lower bound on $\\ell(h,z_{1})$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n-\\frac{\\rho}{6}|z_{1}-z_{2}|^{3}+\\ell(h,z_{2})+\\langle\\nabla\\ell(h,z_{2}),z_{1}-z_{2}\\rangle+\\langle\\nabla^{2}\\ell(h,z_{2})(z_{1}-z_{2}),z_{1}-z_{2}\\rangle\\leq\\ell(h,z_{1})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consider $z_{j}\\,\\in\\,S$ such that $z_{i}\\,=\\,z_{j}\\,+\\,\\alpha$ for some $j\\neq i$ where $\\alpha\\,\\in\\,B_{p}(v)$ such that $\\mathbb{E}[\\alpha]\\,=\\,0$ and $\\mathbb{E}[\\alpha^{T}\\alpha]~=~1$ . Using the lower bound in Lemma A.2 from Ravikumar et al. [2024] with $z_{1}=z_{i},z_{2}=z_{j}$ we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\phi}[\\ell(h_{S^{\\setminus i}}^{\\phi},z_{j})]-\\mathbb{E}_{\\phi}[\\ell(h_{S}^{\\phi},z_{i})]\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the result from Lemma A.2 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\displaystyle\\frac{\\rho}{6}\\|\\alpha\\|^{3}+\\mathbb{E}_{\\phi}\\big[\\ell(h_{S^{\\setminus i}}^{\\phi},z_{i})\\big]+\\mathbb{E}_{\\phi}\\big[\\langle\\nabla\\ell(h_{S^{\\setminus i}}^{\\phi},z_{i}),\\alpha\\rangle\\big]}&{}\\\\ {+\\mathbb{E}_{\\phi}\\big[\\langle\\nabla^{2}\\ell(h_{S^{\\setminus i}}^{\\phi},z_{i})\\alpha,\\alpha\\rangle\\big]-\\mathbb{E}_{\\phi}\\big[\\ell(h_{S}^{\\phi},z_{i})\\big]}&{\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking Expectation over $\\alpha$ and since the mean of $\\alpha=0$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\phi}[\\ell(h_{S^{\\setminus i}}^{\\phi},z_{i})]+\\mathbb{E}_{\\alpha,\\phi}[\\langle\\nabla\\ell(h_{S^{\\setminus i}}^{\\phi},z_{i}),\\alpha\\rangle]+\\mathbb{E}_{\\alpha,\\phi}[\\langle\\nabla^{2}\\ell(h_{S^{\\setminus i}}^{\\phi},z_{i})\\alpha,\\alpha\\rangle]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}_{\\phi}[\\ell(h_{S}^{\\phi},z_{i})]\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta+\\frac{\\rho}{6}\\|\\alpha\\|^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that we can change the order of expectation using Fubini\u2019s theorem ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underline{{\\hat{p}}}_{\\diamond^{1}},z_{i})]+\\mathbb{E}_{\\phi,\\alpha}[\\langle\\nabla^{2}\\ell(h_{S^{\\setminus4}}^{\\phi},z_{i})\\alpha,\\alpha\\rangle]-\\mathbb{E}_{\\phi}[\\ell(h_{S}^{\\phi},z_{i})]\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta+\\displaystyle\\frac{\\rho}{6}\\|\\alpha\\|^{3}}\\\\ &{[\\ell(h_{S^{\\setminus4}}^{\\phi},z_{i})]+\\mathbb{E}_{\\phi}[\\mathrm{tr}(\\nabla^{2}\\ell(h_{S^{\\setminus4}}^{\\phi},z_{i}))]-\\mathbb{E}_{\\phi}[\\ell(h_{S}^{\\phi},z_{i})]\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta+\\displaystyle\\frac{\\rho}{6}\\|\\alpha\\|^{3}}\\\\ &{\\nabla^{2}\\ell(h_{S^{\\setminus4}}^{\\phi},z_{i}))]\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta+\\displaystyle\\frac{\\rho}{6}\\mathbb{E}[\\|\\alpha\\|^{3}]+\\mathbb{E}_{\\phi}[\\ell(h_{S}^{\\phi},z_{i})]-\\mathbb{E}_{\\phi}[\\ell(h_{S^{\\setminus4}}^{\\phi},z_{j})]}\\\\ &{(\\nabla^{2}\\ell(h_{S}^{\\phi},z_{i}))]\\leq m\\beta+(4m-1)\\gamma+2(m-1)\\Delta+\\displaystyle\\frac{\\rho}{6}\\mathbb{E}[\\|\\alpha\\|^{3}]+L}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Ravikumar et al. [2024] Lemma 5.2 we have $\\beta\\le L(1-e^{-\\epsilon})$ . Thus we have the upper bound: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\phi}[\\mathrm{Curv}_{\\phi}(z,S^{\\backslash i})]\\leq L\\big(m(1-e^{-\\epsilon})+1\\big)+c_{1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nc_{1}=(4m-1)\\gamma+2(m-1)\\Delta+\\frac{\\rho}{6}\\mathbb{E}[\\|\\alpha\\|^{3}]\\quad,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This proof uses results from Lemma A.1 provided above. For the proof of Theorem 4.2 let $p_{c}(\\phi,S,z_{i})$ be the curvature probability mass function, then using the Gaussian model discussed in Section 4 we model it as a Gaussian. Thus, we can write ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{c}(\\phi,S,z_{i})=\\frac{1}{\\sigma_{S}\\sqrt{2\\pi}}\\,e^{-\\frac{(\\phi-\\mu_{S})^{2}}{2\\sigma_{S}^{2}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now consider the $\\mathrm{D}_{\\mathrm{KL}}(p_{c}(\\phi,S,z_{i})\\mid\\mid p_{c}(\\phi,S^{\\backslash i},z_{i}))$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{D}_{\\mathrm{KL}}\\left(p_{c}(\\phi,S,z_{i})\\mid|\\,p_{c}(\\phi,S^{\\backslash i},z_{i})\\right)=\\operatorname{\\mathbb{E}}_{\\phi}\\left[\\log\\left(\\frac{p_{c}(\\phi,S,z_{i})}{p_{c}(\\phi,S^{\\backslash i},z_{i})}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname{\\mathbb{E}}_{\\phi}\\left[-\\frac{(\\phi-\\mu_{S})^{2}}{2\\sigma^{2}}+\\frac{(\\phi-\\mu_{S}\\operatorname{v}_{i})^{2}}{2\\sigma^{2}}\\right]\\quad\\mathrm{Assume}\\,\\sigma=\\sigma_{S}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname{\\mathbb{E}}_{\\phi}\\left[\\frac{-\\,\\phi^{2}\\,-\\,\\mu_{S}^{2}\\,+\\,2\\phi\\mu_{S}\\,+\\,\\phi^{2}\\,+\\,\\mu_{S,\\cup i}^{2}\\,-\\,2\\phi\\mu_{S}\\operatorname{v}_{i}^{\\prime}}{2\\sigma^{2}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname{\\mathbb{E}}_{\\phi}\\left[\\frac{\\mu_{S,\\cup i}^{2}\\,-\\,\\mu_{S}^{2}}{2\\sigma^{2}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\mu_{S,\\cup i}^{2}\\,-\\,\\mu_{S}^{2}}{2\\sigma^{2}}}\\end{array}\n$$$\\sigma=\\sigma_{S\\setminus i}=\\sigma_{S}$ ", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To upper bound Equation 29, we need an upper bound on $\\mu_{S}\\backslash i$ and a lower bound on $\\mu_{S}$ . The lower bound on $\\mu_{S}^{2}=0$ ", "page_idx": 15}, {"type": "text", "text": "For ease of notation let\u2019s define $\\beta_{m a x}:=L(m(1-e^{-\\epsilon})+1)$ . Notice that the upper bound of per sample $\\mu_{S^{\\setminus i}}=\\mathbb{E}_{\\phi}[\\mathrm{Curv}_{\\phi}(z,S^{\\setminus i})]$ . Using the result from Lemma A.1 in Equation 29 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}(p_{c}(\\phi,S,z_{i})\\mid\\mid p_{c}(\\phi,S^{\\setminus i},z_{i}))\\leq\\frac{(\\beta_{m a x}+c_{1})^{2}-0^{2}}{2\\sigma^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{(\\beta_{m a x}+c_{1})^{2}}{2\\sigma^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\left[L\\left(m\\left(1-e^{-\\epsilon}\\right)+1\\right)+c_{1}\\right]^{2}}{2\\sigma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.6 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Assuming the bounds of Theorem 4.1 and Theorem 4.2 are tight, then using curvature score will outperform neural network probability scores if: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{D}_{\\mathrm{KL}}(p_{c}(\\phi,S,z_{i})\\mid\\mid p_{c}(\\phi,S^{\\setminus i},z_{i}))>\\mathrm{D}_{\\mathrm{KL}}(p(\\phi,S,z_{i})\\mid\\mid p(\\phi,S^{\\setminus i},z_{i}))}&{}\\\\ {\\frac{\\left[L(m(1-e^{-\\epsilon})+1)+c_{1}\\right]^{2}}{2\\sigma^{2}}>\\epsilon}&{}\\\\ {\\frac{\\left[L m(1-e^{-\\epsilon})+c\\right]^{2}}{2\\sigma^{2}}>\\epsilon}&{;c=c_{1}+L}\\\\ {L^{2}(1-e^{-\\epsilon})^{2}m^{2}+2L c(1-e^{-\\epsilon})m+c^{2}-2\\sigma^{2}\\epsilon>0}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the quadratic formula we have the roots given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{={\\frac{-2L c(1-e^{-\\epsilon})\\pm{\\sqrt{4L^{2}c^{2}(1-e^{-\\epsilon})^{2}-4L^{2}(1-e^{-\\epsilon})^{2}(c^{2}-2\\sigma^{2}\\epsilon)}}}{2L^{2}(1-e^{-\\epsilon})^{2}}}}\\\\ &{={\\frac{-\\,2L c\\pm{\\sqrt{4L^{2}c^{2}-4L^{2}(c^{2}-2\\sigma^{2}\\epsilon)}}}{2L^{2}(1-e^{-\\epsilon})}}}\\\\ &{={\\frac{-\\,L c\\pm{\\sqrt{L^{2}c^{2}-L^{2}(c^{2}-2\\sigma^{2}\\epsilon)}}}{L^{2}(1-e^{-\\epsilon})}}}\\\\ &{={\\frac{-\\,c\\pm{\\sqrt{c^{2}-(c^{2}-2\\sigma^{2}\\epsilon)}}}{L(1-e^{-\\epsilon})}}}\\\\ &{={\\frac{-\\,c\\pm{\\sqrt{c^{2}-(c^{2}-2\\sigma^{2}\\epsilon)}}}{L(1-e^{-\\epsilon})}}}\\\\ &{={\\frac{-\\,c\\pm{\\sqrt{2\\sigma^{2}\\epsilon}}}{L(1-e^{-\\epsilon})}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assuming both roots are real, the larger root is $\\frac{\\sqrt{2\\sigma^{2}\\epsilon}-c}{L(1-e^{-\\epsilon})}$ Thus when $m>{\\frac{\\sqrt{2\\sigma^{2}\\epsilon}-c}{L(1-e^{-\\epsilon})}}$ Equation 33 is satisfied. \u25a0 ", "page_idx": 16}, {"type": "text", "text": "A.7 Input Augmentations Ablation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we present the results of using 1 augmentation (only the image) vs 2 augmentations (image $^+$ mirror) in Tables 2, 3 and 4 for ImageNet, CIFAR100 and CIFAR10 respectively. For these results, we used 64 shadow models for CIFAR10 and CIFAR100 and 52 shadow models for ImageNet. ", "page_idx": 16}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/a95bf26e6f73ce3218847837d9af88aa9c0d8c8b22117097420b3192d30ce5c9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/93539904bca0f109d3542cdc165e8b8ba07ee4b728cc4be583b69333981ce8a3.jpg", "table_caption": ["Table 2: Comparison of the proposed curvature score-based MIA with prior methods on ImageNet dataset with 1 and 2 augmentations. Results reported are the mean $\\pm$ std obtained over 3 seeds. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/c131a48d884a00cb495ed2739be3c45411b10cb29b2e0706e256dee422054f1f.jpg", "table_caption": ["Table 3: Comparison of the proposed curvature score-based MIA with prior methods on CIFAR100 dataset with 1 and 2 augmentations. Results reported are the mean $\\pm$ std obtained over 3 seeds. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.8 Low FPR Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the performance of curvature based MIA compared with prior works using TPR (true positive rate) at very low FPR percentages. The results are presented when using 1 augmentation (only the image) and 2 augmentations (imag $\\varsigma\\,\\mathrm{e}+\\mathrm{m}$ irror) in Table 5 and Figure 8. For these results, we use 64 shadow models for all the results in Table 5 (for methods that use shadow models). The results for Carlini et al. [2022] are a little lower than the one reported by Carlini et al. [2022] in Table I of their paper. However, note that the authors of Carlini et al. [2022] report the results with 256 shadow models while we report it using 64 shadow models. This highlights the benefits of curvature based approach, it achieves similar performance to Carlini et al. [2022] with 256 shadow models but with only 64 shadow models thus needs significantly less shadow models to achieve similar performance. ", "page_idx": 16}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/d0bcabf5f9c712083ce67e93e69b3589e828ff6cc9479ec8d6a75c21f4527e8f.jpg", "table_caption": ["Takeaways: The parametric curvature LR method has the best TPR at very low FPR and makes efficient use of shadow models. "], "table_footnote": ["Table 5: Comparison of TPR $\\mathcal{\\varrho}\\ 0.1\\%$ FPR and TPR $\\mathcal{Q}\\ 0.01\\%$ FPR for CIFAR100 dataset with 1 and 2 augmentations. Results reported are the mean $\\pm$ std obtained over 3 seeds and using 64 shadow models. "], "page_idx": 17}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/03ef992a686613867f42d43a552986f6503575c3c5f43e49b0aa01c57fdd021f.jpg", "img_caption": ["Figure 8: TPR at low FPR visualizing MIA performance as a function of the size of the train set, which is randomly sampled. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "ZEVDMQ6Mu5/tmp/cd74907504c1d5c6a454c595a825e1d37c5e2c8397955e479f6447a942a61f8a.jpg", "img_caption": ["Figure 9: Low TPR performance comparison on CIFAR100 of ZO estimation (Curv ZO, blackbox) vs. Hutchinson (Curv, white-box). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.9 ZO Estimation Performance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the results of comparing the proposed zero-order curvature estimation with Hutchinson\u2019s trace estimator-based curvature calculation. It is important to note that the trace estimator proposed by Garg et al. [2024] requires access to the model\u2019s parameters, thus it is a white-box attack. In contrast, the proposed zero-order (ZO) estimation operates as a black-box attack. ", "page_idx": 17}, {"type": "text", "text": "The comparative results are presented in Table 6. Additionally, Figure 9 illustrates the True Positive Rate (TPR) vs. False Positive Rate (FPR) for the ZO estimation and Hutchinson\u2019s trace estimator, utilizing a log-log plot to emphasize performance at low FPR. ", "page_idx": 17}, {"type": "text", "text": "Takeaways. From Table 6 we see that zero-order approach to curvature estimation results in notable differences in performance. Specifically, when examining AUROC and balanced accuracy, particularly on the CIFAR-10 dataset, a gap of several percentage points can be observed between the two methods. From Figure 9, we observe that the proposed LR technique performs similarly between Hutchinson\u2019s trace estimator [Garg et al., 2024] and the proposed ZO estimation. However, the NLL method exhibits much greater sensitivity to estimation errors, as demonstrated by a significant performance drop at low FPR levels. ", "page_idx": 17}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/aed25d480c4994e7b5a2ffad6a6029ea05e84c1a8a1697088ab2ce623e4d4aaf.jpg", "table_caption": [], "table_footnote": ["Table 6: Performance Metrics for CIFAR100 and CIFAR10 of zero order estimation (Curv ZO, black-box) vs. Hutchinson trace estimation (Curv, white-box) [Garg et al., 2024]. "], "page_idx": 18}, {"type": "text", "text": "A.10 Performance Across Architectures ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we present the results of evaluating the proposed zero-order MIA on different neural network architectures. We selected four architectures: Inception [Szegedy et al., 2015] (specifically, the smaller Inception architecture used by Feldman and Zhang [2020]), VGG11 [Simonyan and Zisserman, 2014], VGG11 with batch normalization (denoted as VGG11BN), and finally, a ViT-B16 vision transformer [Vaswani et al., 2017, Dosovitskiy et al., 2021]. The results are provided in Table Table 7. Please note that for all of the results we used ResNet18 shadow models and 2 augmentations. ", "page_idx": 18}, {"type": "text", "text": "Takeaways. The proposed techniques outperform the previous state-of-the-art method by Carlini et al. [2022] across all architectures. Additionally, the lower performance on the Vision Transformer (ViT) model can be attributed to transfer learning. Since ViTs do not perform well when trained from scratch on CIFAR-100, we used an ImageNet-pretrained model, which was fine-tuned on CIFAR-100. This led to significant privacy benefits, which aligns with the findings of Mehta et al. [2022]. ", "page_idx": 18}, {"type": "table", "img_path": "ZEVDMQ6Mu5/tmp/e82a13234af08b67adb0a6e732de3ee77f2aa02b864981a52c17e7b17d8c3c79.jpg", "table_caption": ["Table 7: MIA performance metrics for different architectures and methods on CIFAR100 dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.11 Broader Impact ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The development and analysis of input loss curvature in deep neural networks presented in this work have significant implications for both the academic and practical fields of machine learning and privacy. By introducing a novel black-box membership inference attack that leverages input loss curvature, this research advances the state-of-the-art in privacy testing for machine learning models. ", "page_idx": 19}, {"type": "text", "text": "From a societal perspective, the ability to detect and mitigate membership inference attacks is crucial for maintaining user privacy in machine learning applications. This work helps pave the way for more robust privacy-preserving techniques, ensuring that sensitive information is protected against unauthorized inference. Furthermore, the insights gained from the relationship between input loss curvature and memorization can guide the development of more secure machine learning models, which is particularly important as these models are increasingly deployed in sensitive domains such as healthcare, finance, and personal data processing. ", "page_idx": 19}, {"type": "text", "text": "Additionally, this research highlights the potential of using subsets of training data as a defense mechanism against membership inference attacks. By identifying that training on certain sized subsets can improve resistance to these attacks, this work offers practical guidance for model training practices that can enhance privacy without significantly compromising performance. ", "page_idx": 19}, {"type": "text", "text": "A.12 Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this paper we presented the a theoretical analysis and experimental evidence for improved MIA using input loss curvature. The theoretical and empirical evidence also shows that certain sized subsets of the training set may provide defense against membership inference attacks. However, as mentioned in the paper, the MIA performance improvement occurs only above a certain training dataset size below which the non-parametric model of \u2018Curv NLL\u2019 works better and is not explained by the theoretical analysis. Similar to techniques that use shadow models such as by Shokri et al. [2017], Carlini et al. [2022] we need to train shadow models, which can be computationally expensive. Further, the method described requires more queries at minimum $4\\times$ more than Carlini et al. [2022]. We believe these limitations can be addressed by follow up research. ", "page_idx": 19}, {"type": "text", "text": "A.13 Reproducibility Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section we present additional details for reproducing our results. Additionally we have released our code here https://github.com/DeepakTatachar/Curvature-Clues. ", "page_idx": 19}, {"type": "text", "text": "Training. For experiments that use private models, we use the Opacus library [Yousefpour et al., 2021] to train ResNet18 models for 20 epochs till the privacy budget is reached. We use DP-SGD [Abadi et al., 2016] with the maximum gradient norm set to 1.0 and privacy parameter $\\delta=1\\times10^{-5}$ . The initial learning rate was set to 0.001. The learning rate is decreased by 10 at epochs 12 and 16 with a batch size of 128. ", "page_idx": 19}, {"type": "text", "text": "For shadow model training on CIFAR10 and CIFAR100 we trained on $50\\%$ randomly sampled subset of the data for 300 epochs with a batch size of 512 for CIFAR100 and 256 for CIFAR10. We used SGD optimizer with the initial learning rate set to 0.1, weight decay of $1\\times10^{-4}$ and momenutm of 0.9. The learning rate was decayed by 0.1 at $180^{t h}$ and $240^{t h}$ epoch. For ImageNet we used pre-trained models from Feldman and Zhang [2020] as shadow models which were trained on a $70\\%$ subset of ImageNet. For both CIFAR10 and CIFAR100 datasets, we used the following sequence of data augmentations for training: resize $(32\\times32)$ , random crop, and random horizontal flip, this is followed by normalization. ", "page_idx": 19}, {"type": "text", "text": "Testing. During testing we used resize followed by normalization. We used two augmentations the original image and its mirror. The number of augmentations used are specified in the corresponding experiment section. When using pre-trained models from Feldman and Zhang [2020] we validated the accuracy of the models before performing experiments. ", "page_idx": 19}, {"type": "text", "text": "Compute Resources. All of the experiments were performed on a heterogeneous compute cluster consisting of 9 1080Ti\u2019s, 6 2080Ti\u2019s and 4 A40 NVIDIA GPUs, with a total of 100 CPU cores and a combined 1.2 TB of main system memory. However, the results can be replicated with a single GPU with 11GB of VRAM. ", "page_idx": 19}, {"type": "text", "text": "Hyperparameters. Our code uses 2 hyper parameters for zero-order input loss curvature estimation. The $n_{i t e r}$ and $h$ in Algorithm 1. We used $n_{i t e r}=10\\$ and $h=0.001$ . To improve reproducibility, we have provided the code in the supplementary material. ", "page_idx": 20}, {"type": "text", "text": "A.14 Licenses for Assets Used ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For each of the assets used we present the licenses below we also have provided the correct citation in the main paper as well as here for convenience. ", "page_idx": 20}, {"type": "text", "text": "1. ImageNet [Russakovsky et al., 2015]: Terms of access available at https://image-net. org/download.php   \n2. CIFAR10 [Krizhevsky et al., 2009]: Unknown / No license provided   \n3. CIFAR100 [Krizhevsky et al., 2009]: Unknown / No license provided   \n4. Pre-trained ImageNet models and code: We used pre-trained ImageNet models from Feldman and Zhang [2020] which is licensed under Apache 2.0 https://github.com/ google-research/heldout-influence-estimation/blob/master/LICENSE.   \n5. Baseline methods: We re-implemented the baseline methods hence is provided with along with our code which is distributed under the MIT License.   \n6. Opacus [Yousefpour et al., 2021]: Licensed under Apache 2.0, https://github.com/ pytorch/opacus/blob/main/LICENSE.   \n7. Pytorch [Ansel et al., 2024]: Custom BSD-style license available at https://github. com/pytorch/pytorch/blob/main/LICENSE.   \n8. ResNet Model Architecture [He et al., 2016]: MIT license available at https://github. com/kuangliu/pytorch-cifar/blob/master/LICENSE ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract clearly outlines the exploration of input loss curvature in deep neural networks and the development of a theoretical framework for train-test distinguishability. It introduces a novel black-box membership inference attack (MIA) utilizing input loss curvature and the use of subsets of training data as a mechanism to defend against shadow model based MIA. These claims are substantiated through theoretical analysis and experimental validation, as presented in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Appendix A.12, where we note the increased computation requirement imposed by zero-order curvature calculation and other limitations of the proposed methods. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: All the assumptions are stated in the Theorems and discussed prior to their utilization in the main paper. The complete proof has also been provided in the Appendix of the paper, specifically Appendix A.3, A.5 and A.6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include all the training details for the models in Appendix A.13. We also provide the pseudo-code for the proposed technique in Appendix A.2 along with all the hyper parameter details (see Appendix A.13). Further, we also provide the code in the supplementary material. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 22}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The datasets used are publicly available and are the standard datasets used for vision tasks. Our code is included in the supplementary material and will be released publicly after the conference deadline. The code replicates the results of our experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The training details are reported in Section 6.1 of the paper and more details are provided in Appendix A.13. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All reported results are obtained by averaging over 3 seeds and reported with mean and standard deviation values. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: For the experiments, we used a heterogeneous cluster whose details are mentioned in Appendix A.13. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper conforms to the code of ethics, we discus potential impacts of this research paper in Appendix A.11. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a detailed discussion about the broader impact of the research presented in the paper in Appendix A.11. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not release data or models thus this paper does not pose such risks. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the detailed list of assets used and the terms under which they were licensed in Appendix A.14. We have respected the licensing terms of the original owners. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We release the code for reproducing the results under the MIT license and details of its usage are documented in Appendix A.13 and the supplementary material README file. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]