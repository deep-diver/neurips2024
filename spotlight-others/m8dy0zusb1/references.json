{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Benchmarking neural network robustness to common corruptions and perturbations", "publication_date": "2019-03-15", "reason": "This paper introduces ImageNet-C, a benchmark dataset for evaluating robustness to common image corruptions, which is heavily used in this paper's experiments."}, {"fullname_first_author": "Ekin D. Cubuk", "paper_title": "Autoaugment: Learning augmentation policies from data", "publication_date": "2018-05-09", "reason": "This paper proposes AutoAugment, a data augmentation method that automatically learns optimal augmentation policies, which is compared to the proposed method in this paper."}, {"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2021-01-01", "reason": "This paper introduces SAM, a training method that enhances model generalization by minimizing sharpness, which is closely related to and compared with the proposed method."}, {"fullname_first_author": "Jungmin Kwon", "paper_title": "ASAM: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks", "publication_date": "2021-01-01", "reason": "This paper introduces ASAM, an improved version of SAM that addresses limitations of SAM for ReLU networks, which is theoretically and empirically compared to the proposed method."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduces Vision Transformers (ViTs), a novel neural network architecture used in this paper's experiments to demonstrate the effectiveness of the proposed method on different architectures."}]}