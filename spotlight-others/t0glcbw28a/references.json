{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models and their use in few-shot learning, a key technique leveraged in the Alchemist system."}, {"fullname_first_author": "Shuohang Wang", "paper_title": "Want to reduce labeling cost? GPT-3 can help", "publication_date": "2021-08-17", "reason": "This paper directly addresses the cost of data annotation using large language models, a core problem that Alchemist aims to solve more efficiently."}, {"fullname_first_author": "Alexander J Ratner", "paper_title": "Data programming: Creating large training sets, quickly", "publication_date": "2016-12-01", "reason": "This paper introduces the concept of data programming, a weak supervision technique that Alchemist adapts to synthesize programs for labeling, which underpins the cost-effectiveness of Alchemist."}, {"fullname_first_author": "Alexander Ratner", "paper_title": "Snorkel: Rapid training data creation with weak supervision", "publication_date": "2017-01-01", "reason": "This paper presents the Snorkel framework, a core component of Alchemist used for aggregating noisy labels from multiple sources, improving the overall accuracy and robustness of labeling."}, {"fullname_first_author": "Jiacheng Ye", "paper_title": "Zerogen: Efficient zero-shot learning via dataset generation", "publication_date": "2022-02-22", "reason": "This paper explores efficient zero-shot learning through dataset generation, an approach related to Alchemist's program generation for efficient labeling, addressing a similar challenge of reducing the need for expensive human annotation."}]}