[{"figure_path": "JDAQwysFOc/figures/figures_5_1.jpg", "caption": "Figure 1: RUM can (in closed form), whereas the Weisfeiler-Lehman (WL) isomorphism test and WL-equivalent GNNs cannot, distinguish these two graphs\u2014an illustration of Example 8.1.", "description": "This figure demonstrates the difference in expressiveness between RUM and the Weisfeiler-Lehman (WL) test.  It shows two graphs that are indistinguishable by the WL test and WL-equivalent Graph Neural Networks (GNNs), but which RUM can easily distinguish. This highlights RUM's ability to capture more subtle structural information within graphs, exceeding the limitations of convolutional GNNs.", "section": "4 Theory: RUM as a joint remedy"}, {"figure_path": "JDAQwysFOc/figures/figures_5_2.jpg", "caption": "Figure 2: RUM alleviates over-smoothing. Dirichlet energy (E) on Cora [47] graph plotted against L, the number of steps or layers.", "description": "This figure shows how the Dirichlet energy, a measure of the dissimilarity between node representations in a graph, changes over multiple layers (or steps) of message passing for various graph neural network (GNN) architectures.  The x-axis represents the number of layers (L), while the y-axis shows the Dirichlet energy (E).  As the number of layers increases, traditional convolutional GNNs (GCN, GAT, GCNII, Res) suffer from over-smoothing, meaning that the Dirichlet energy decreases exponentially, and node representations become similar, hindering the ability to distinguish between nodes effectively.  RUM, in contrast, exhibits significantly less over-smoothing, maintaining a higher Dirichlet energy across multiple layers, indicating better preservation of node differences.", "section": "4.2 RUM alleviates over-smoothing and over-squashing"}, {"figure_path": "JDAQwysFOc/figures/figures_8_1.jpg", "caption": "Figure 3: Impact of number of samples and walk length. Test classification accuracy of Cora [47] with varying numbers of samples and walk length.", "description": "This figure shows a 3D surface plot illustrating the relationship between the number of samples used to estimate node representations, the length of random walks, and the resulting test classification accuracy on the Cora dataset. The x-axis represents the number of samples, the y-axis represents the length of random walks, and the z-axis represents the test accuracy. The plot reveals that increasing either the number of samples or the walk length generally improves the classification accuracy, up to a certain point where diminishing returns set in.", "section": "5 Experiments"}, {"figure_path": "JDAQwysFOc/figures/figures_8_2.jpg", "caption": "Figure 4: RUM is faster than convolutional GNNs on GPU. Inference time over the Cora [47] graph on CPU and CUDA devices, respectively, plotted against L, the number of message-passing steps or equivalently the length of random walks. Numbers in the bracket indicate the number of sampled random walks drawn.", "description": "This figure compares the inference time of RUM with GCN and GAT on the Cora dataset. The x-axis represents the number of message-passing steps (L) or the length of random walks. The y-axis represents the inference time in seconds. The figure shows that RUM is faster than GCN and GAT, especially on GPU. The numbers in the bracket indicate the number of sampled random walks used for each RUM configuration.", "section": "5 Experiments"}, {"figure_path": "JDAQwysFOc/figures/figures_8_3.jpg", "caption": "Figure 5: Long-range neighborhood matching training accuracy \u2191 [14] with 32 unit models.", "description": "The figure shows the accuracy of different Graph Neural Network (GNN) models on a long-range neighborhood matching task.  The task involves predicting the label of a node based on its attributes and those of a node located far away in the graph. The x-axis represents the \"problem radius,\" which is the distance between the nodes whose attributes need to be matched. The y-axis is the accuracy of the prediction. The results indicate that RUM outperforms other GNN models, especially when the problem radius is large, highlighting RUM's ability to capture long-range dependencies.", "section": "Experiments"}, {"figure_path": "JDAQwysFOc/figures/figures_8_4.jpg", "caption": "Figure 6: Robustness analysis. Accuracy \u2191 on Cora [47] dataset with % fictitious edges added to the graph.", "description": "This figure demonstrates the robustness of the RUM model against adversarial attacks.  The x-axis represents the percentage of randomly added fictitious edges to the Cora graph, simulating noise or corruption in the graph structure. The y-axis shows the classification accuracy achieved by different GNN models (RUM, GCN, GAT, and GRAND). The plot visually shows how the accuracy of each model degrades as the level of perturbation increases.  The RUM model exhibits higher robustness than the other GNNs, showing a smaller decrease in accuracy even with a high percentage of added edges.", "section": "5 Experiments"}]