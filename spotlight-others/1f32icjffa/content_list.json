[{"type": "text", "text": "Schr\u00f6dinger Bridge Flow for Unpaired Data Translation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Valentin De Bortoli \u2217 Iryna Korshunova \u2217 Andriy Mnih Arnaud Doucet Google DeepMind Google DeepMind Google DeepMind Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another. Generative modeling techniques like Generative Adversarial Networks (GANs) and Denoising Diffusion Models (DDMs) have been successfully adapted to solve such transport problems, resulting in CycleGAN and Bridge Matching respectively. However, these methods do not approximate Optimal Transport (OT) maps, which are known to have desirable properties. Existing techniques approximating OT maps for high-dimensional data-rich problems, such as DDM-based Rectified Flow and Schr\u00f6dinger Bridge procedures, require fully training a DDM-type model at each iteration, or use mini-batch techniques which can introduce significant errors. We propose a novel algorithm to compute the Schr\u00f6dinger Bridge, a dynamic entropyregularised version of OT, that eliminates the need to train multiple DDM-like models. This algorithm corresponds to a discretisation of a flow of path measures, which we call the Schr\u00f6dinger Bridge Flow, whose only stationary point is the Schr\u00f6dinger Bridge. We demonstrate the performance of our algorithm on a variety of unpaired data translation tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of finding a map to transport one probability distribution to another one has numerous applications in machine learning. In particular, it is at the core of generative modeling where the idea is to transform a noise distribution into the data distribution, and is also central to transfer learning tasks such as image-to-image translation. For discrete probability distributions, it is possible to compute the Optimal Transport (OT) map but this is computationally expensive (Peyr\u00e9 et al., 2019). By showing that an entropy-regularised version of OT, the Entropic OT (EOT), could be computed much more efficiently using the Sinkhorn algorithm, Cuturi (2013) has enabled transport ideas to be used in numerous applications (Ge et al., 2021; Zhou et al., 2022). However, the computational complexity of Sinkhorn algorithm is quadratic in the sample size, which makes its application to very large datasets impractical. Mini-batch versions have been proposed, see e.g. (Genevay et al., 2018), but tend to introduce significant errors in high dimensions (Sommerfeld et al., 2019). ", "page_idx": 0}, {"type": "text", "text": "In the context of generative modeling, Denoising Diffusion Models (DDMs) (Song et al., 2021a; Ho et al., 2020) have shown impressive performance in a variety of domains. DDMs define a forward process progressively noising the data, and sample generation is achieved by approximating the timereversal of this diffusion. In order to leverage the iterative refinement properties of DDMs in the OT setting, methods exploiting the equivalence between the static versions of (E)OT and their dynamic counterparts (Benamou and Brenier, 2000; L\u00e9onard, 2014) have been developed. A procedure to approximate the dynamic OT is considered by Liu et al. (2023b), while techniques to approximate the dynamic equivalent to EOT, the Schr\u00f6dinger Bridge (SB), have been proposed in (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022; Peluchetti, 2023; Shi et al., 2023). These techniques are expensive however, as they require training multiple DDM-type models. Mini-batch versions of OT and Sinkhorn (Pooladian et al., 2023; Tong et al., 2024b) combined with bridge or flow matching have also been proposed to approximate the OT path and SB, but they optimise a minibatch OT objective that can introduce significant errors in high dimensions: the error in Wasserstein-1 distance is of order $O(B^{-1/(2d)})$ , where $d$ is the dimension of the problem and $B$ the minibatch size, see (Sommerfeld et al., 2019, Corollary 1). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel approach to computing the SB. Similarly to Iterative Markovian Fitting (IMF) and its practical implementation, Diffusion Schr\u00f6dinger Bridge Matching (DSBM) (Shi et al., 2023; Peluchetti, 2023), it leverages the fact that the SB is the only Markov process with prescribed marginals at the endpoints which is in the reciprocal class of the Brownian motion, i.e. it has the same bridge as the Brownian motion (L\u00e9onard, 2014); see Section 2 for more details on Markov processes and the reciprocal class. Compared to DSBM, our approach is easier to implement as it does not require caching samples, alternating between optimising two different losses, and, optionally, uses one neural network instead of two. In Section 3, we start by introducing a flow of path measures whose time-discretisation yields a family of algorithms called $\\alpha$ -IMF and presented in Section 4. Notably, we show that $\\alpha$ -IMF converges to the Schr\u00f6dinger Bridge for any $\\bar{\\alpha}\\in(0,1]$ . Additionally, for a special value of the discretisation stepsize $\\alpha=1$ , we recover the IMF procedure (Peluchetti, 2023; Shi et al., 2023), while $\\alpha<1$ corresponds to online versions of IMF. We implement a parametric version of the $\\alpha$ -IMF as an online DSBM procedure, called $\\alpha$ -DSBM. We illustrate the efficiency of our approach in unpaired image-to-image translation settings in Section 6. ", "page_idx": 1}, {"type": "text", "text": "Notation. We denote the space of path measures by $\\mathscr{P}(\\mathscr{C})$ , i.e. $\\mathcal{P}(\\mathcal{C})=\\mathcal{P}(\\mathrm{C}([0,1],\\mathbb{R}^{d}))$ , where $\\mathrm{C}([0,1],\\ensuremath{\\mathbb{R}}^{d})$ is the space of continuous functions from $[0,1]$ to $\\mathbb{R}^{d}$ . The subset of Markov path measures associated with a diffusion of\u221a the form $\\mathrm{d}\\mathbf{X}_{t}=v_{t}(\\mathbf{X}_{t})\\mathrm{d}t+\\sigma_{t}\\mathrm{d}\\mathbf{B}_{t}$ , with $\\sigma,v$ locally Lipschitz, is denoted $\\mathcal{M}$ . For $\\mathbb{Q}$ induced by $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ , with $\\varepsilon>0$ and $(\\mathbf{B}_{t})_{t\\geq0}$ a $d$ -dimensional Brownian motion, the reciprocal class of $\\mathbb{Q}$ is denoted $\\mathcal{R}(\\mathbb{Q})$ , see Definition 2.1. For any $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ , we denote by $\\mathbb{P}_{t}$ its marginal distribution at time $t$ , $\\mathbb{P}_{s,t}$ the joint distribution at times $s,t,\\mathbb{P}_{s\\mid t}$ the conditional distribution at time $s$ given the state at time $t$ , and $\\mathbb{P}_{|0,1}\\in\\mathcal{P}(\\mathcal{C})$ the distribution of the path on time interval $(0,1)$ given its endpoints; e.g. $\\mathbb{Q}_{|0,1}$ is a scaled Brownian bridge. Unless specified otherwise, all gradient operators $\\nabla$ are w.r.t. the variable $x_{t}$ with time index $t$ . Given probability spaces $(\\mathsf{X},{\\mathcal{X}})$ and $(\\mathsf{Y},\\mathcal{Y})$ , a Markov kernel $\\mathrm{K}:\\,\\mathsf{X}\\!\\times\\!\\mathcal{Y}\\!\\to\\![0,1]$ , and a probability measure $\\mu$ defined on $\\mathcal{X}$ , we write $\\mu\\mathrm{K}$ for the probability measure on $\\boldsymbol{\\wp}$ such that for any $\\mathsf{A}\\in\\mathcal{V}$ we have $\\begin{array}{r}{\\mu\\mathrm{K}(\\mathsf{A})=\\int_{\\mathsf{X}}\\mathrm{K}(x,\\mathsf{A})\\mathrm{d}\\mu(x)}\\end{array}$ . In particular, for any joint distribution $\\Pi_{0,1}$ over $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , we denote the mixture of bridges measure as $\\Pi\\,=\\,\\Pi_{0,1}{\\bar{\\mathbb{P}}}_{|0,1}\\,\\in\\,{\\mathcal{P}}({\\mathcal{C}})$ , which is short for $\\begin{array}{r}{\\Pi(\\cdot)\\,=\\,\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\mathbb{P}_{|0,1}\\!\\left(\\cdot|x_{0},x_{1}\\right)\\!\\mathrm{d}\\Pi_{0,1}\\!\\left(x_{0},x_{1}\\right)}\\end{array}$ . Finally, we define the Kullback\u2013Leibler (KL) divergence between two probability measures $\\pi_{0},\\pi_{1}\\in\\mathcal{P}(\\mathsf{X})$ as $\\begin{array}{r}{\\mathrm{KL}(\\pi_{0}|\\pi_{1})=\\int_{\\mathsf X}\\log((\\mathrm{d}\\pi_{0}/\\mathrm{d}\\pi_{1})(x))\\bar{\\mathrm{d}}\\pi_{0}(x)}\\end{array}$ if $\\pi_{0}$ is absolutely continuous w.r.t. $\\pi_{1}$ and $\\mathrm{KL}(\\pi_{0}|\\pi_{1})=+\\infty$ otherwise. ", "page_idx": 1}, {"type": "text", "text": "2 Optimal Transport and Schr\u00f6dinger Bridge ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Unpaired Transfer and Optimal Transport. Given unpaired data samples from $\\pi_{0}$ and $\\pi_{1}$ , where $\\pi_{0},\\pi_{1}$ are two distributions on $\\mathbb{R}^{d}$ , we are interested in designing a transport map from $\\pi_{0}$ to $\\pi_{1}$ . This corresponds to an unpaired data transfer task. We can formulate this problem as finding a distribution $\\Pi$ on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ with marginals $\\Pi_{0}=\\pi_{0}$ and $\\Pi_{1}=\\pi_{1}$ so that if $\\mathbf{X}_{0}\\sim\\pi_{0}$ then $\\mathbf{X}_{1}|\\mathbf{X}_{0}\\overset{\\cdot}{\\sim}\\Pi_{1|0}(\\cdot|\\mathbf{X}_{0})$ satisfies ${\\bf X}_{1}\\sim\\pi_{1}$ . Among an infinite number of such so-called coupling distributions $\\Pi$ , we are here interested in finding the Entropic Optimal Transport (EOT) coupling $\\Pi^{\\star}$ defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Pi^{\\star}=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})}\\left\\{\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\frac12\\|x-y\\|^{2}\\mathrm{d}\\Pi(x,y)-\\varepsilon\\mathrm{H}(\\Pi)\\ ;\\ \\Pi_{0}=\\pi_{0},\\ \\Pi_{1}=\\pi_{1}\\right\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathrm{H}(\\Pi)$ is the differential entropy of $\\Pi$ and $\\varepsilon>0$ is a regularisation hyperparameter (Peyr\u00e9 et al., 2019). For $\\varepsilon=0$ , we recover the standard OT. ", "page_idx": 1}, {"type": "text", "text": "In order to leverage the recent advances in generative modeling, and in particular the concept of iterative refinement central to DDMs, we turn to a dynamic formulation of EOT known as the Schr\u00f6dinger Bridge problem (L\u00e9onard, 2014). It is defined as follows: find $\\mathbb{P}^{\\star}\\in\\mathcal{P}(\\mathcal{C})$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{\\star}=\\operatorname*{argmin}_{\\mathbb{P}\\in\\mathcal{P}(\\mathcal{C})}\\{\\mathrm{KL}\\big(\\mathbb{P}|\\mathbb{Q}\\big)\\;;\\;\\mathbb{P}_{0}=\\pi_{0},\\;\\mathbb{P}_{1}=\\pi_{1}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with $\\mathbb{Q}\\,\\in\\,\\mathcal{P}(\\mathcal{C})$ induced by a scaled $d$ -dimensional Brownian motion $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ . The term dynamic here refers to the fact that (2) is defined on path measures, i.e. on (stochastic) processes, in contrast to the static problem (1) which is defined on measures on the space $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ . In Section 3, we show that solving (2) is equivalent to optimising the vector field of a stochastic process using objectives similar to the ones of bridge matching (Peluchetti, 2021; Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023a). Under mild assumptions, it can be shown that $\\mathbb{P}_{0,1}^{\\star}=\\Pi^{\\star}$ , see e.g. (L\u00e9onard, 2014; Pavon et al., 2021). Hence solving (1) reduces to solving (2). Once we have found $\\mathbb{P}^{\\star}$ associated with $(\\mathbf{X}_{t}^{\\star})_{t\\in[0,1]}$ , we can sample from $\\mathbb{P}^{\\star}$ by first sampling $\\mathbf{X}_{0}^{\\star}\\sim\\pi_{0}$ and then sampling the trajectory $(\\mathbf{X}_{t}^{\\star})_{t\\in(0,1]}$ which yields $(\\mathbf{X}_{0}^{\\star},\\mathbf{X}_{1}^{\\star})\\sim\\Pi^{\\star}$ . ", "page_idx": 2}, {"type": "text", "text": "Reciprocal and Markov projections. To introduce our methodology, it is necessary to recall the notions of reciprocal and Markov projections. We refer to Shi et al. (2023) for more details. For practitioners, a more intuitive explanation of these projections is given in Appendix E. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Reciprocal projection): $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ is in the reciprocal class ${\\mathcal{R}}(\\mathbb{Q})$ of $\\mathbb{Q}\\mathbf{\\Psi}i f\\mathbb{P}=$ $\\mathbb{P}_{0,1}\\mathbb{Q}_{|0,1}$ . We define the reciprocal projection of $\\mathbb{P}\\in\\mathcal{P}(\\mathcal{C})$ as $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathbb{P})=\\mathbb{P}_{0,1}\\mathbb{Q}_{|0,1}$ . We will write $\\mathrm{proj}_{\\mathcal{R}}$ instead of proj ${\\mathcal{R}}(\\mathbb{Q})$ to simplify notation. ", "page_idx": 2}, {"type": "text", "text": "In other words, $\\mathbb{P}$ is in the reciprocal class of $\\mathbb{Q}$ if the conditional distribution of a path given its endpoints is identical under $\\mathbb{P}$ and $\\mathbb{Q}$ , see (R\u0153lly, 2013). Sampling from the reciprocal projection of $\\mathbb{P}$ can be achieved by sampling a path $(\\mathbf{X}_{t})_{t\\in[0,1]}$ from $\\mathbb{P}$ , keeping only the values of the endpoints, say $\\mathbf{X}_{0},\\mathbf{X}_{1}$ , and then sampling a new value for the bridge $(\\mathbf{X}_{t})_{t\\in(0,1)}$ from $\\mathbb{Q}_{|0,1}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (Markov projection): Assume that $\\mathbb{Q}$ is induced by $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ for $\\varepsilon>0$ . Then, when it is well-defined, for any $\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})$ , the Markovian projection $\\mathbb{M}=\\operatorname{proj}_{\\mathcal{M}}(\\mathbb{P})\\in\\mathcal{M}$ is the path measure induced by the diffusion $(\\mathbf{X}_{t}^{\\star})_{t\\in[0,1]}$ with for any $t\\in[0,1]$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}^{\\star}=v_{t}^{\\star}(\\mathbf{X}_{t}^{\\star})\\mathrm{d}t+\\sqrt{\\varepsilon}\\mathrm{d}\\mathbf{B}_{t},\\qquad v_{t}^{\\star}(x_{t})=\\left(\\mathbb{E}_{\\mathbb{P}_{1\\mid t}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t}\\right)/(1-t),\\qquad\\mathbf{X}_{0}^{\\star}\\sim\\mathbb{P}_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In practice, implementing a Markovian projection requires solving a regression problem to approximate $\\mathbb{E}_{\\mathbb{P}_{1\\mid t}}[\\mathbf{X}_{1}^{\\textit{\\footnotesize-}}|\\mathbf{\\Lambda}\\mathbf{X}_{t}=\\mathbf{\\Lambda}^{\\!\\scriptscriptstyle-}\\!\\!\\!x_{t}]$ , similar to the one appearing in bridge matching and flow matching. One key property of the Markovian projection is that $\\mathbb{M}_{t}=\\mathbb{P}_{t}$ for all $t\\in[0,1]$ , i.e. the Markovian projection preserves the marginals; see (Peluchetti, 2021) for instance. ", "page_idx": 2}, {"type": "text", "text": "Iterative Markovian Fitting. Leveraging the reciprocal and Markovian projections, Peluchetti (2023) and Shi et al. (2023) concurrently introduced IMF. Starting from $\\hat{\\mathbb{P}}^{0}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ , a measure where endpoints are sampled independently from $\\pi_{0}$ and $\\pi_{1}$ and then interpolated using a (scaled) Brownian bridge, they define a sequence of path measures $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ where $\\mathbb{P}^{n}=$ $\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})$ and ${\\hat{\\mathbb{P}}}^{n+1}\\;=\\;\\operatorname{proj}_{\\mathcal{R}}(\\mathbb{P}^{n})$ . This ensures that $\\mathbb{P}_{0}^{n}~=~\\pi_{0}$ , $\\mathbb{P}_{1}^{n}~=~\\pi_{1}$ for all $n$ , and it can be shown that the sequence $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ converges to the SB, see (Peluchetti, 2023, Theorem 2). The practical implementation of this algorithm proposed by Shi et al. (2023) is called DSBM. Implementing DSBM poses challenges, as each Markovian projection requires training a neural network to approximate the relevant conditional expectations by minimising a bridge matching loss. Furthermore, in practice, generated model samples are stored in a cache in order to train the next iterations of DSBM. This introduces additional hyperparameters that require tuning. In Section 3 we propose $\\alpha$ -IMF, an algorithm which can be interpreted as the discretisation of a flow of path measures. This leads to $\\alpha$ -DSBM, an algorithm that is computationally much more efficient than DSBM as it does not rely on a Markovian projection at each step. ", "page_idx": 2}, {"type": "text", "text": "3 Schr\u00f6dinger Bridge flow ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We will now introduce a flow of path measures $(\\mathbb{P}^{s})_{s\\geq0}$ , and show that the time-discretisation of this flow with an appropriate stepsize $\\alpha\\in(0,1]$ yields a family of procedures called $\\alpha$ -IMF, which all converge to the Schr\u00f6dinger Bridge. While $\\alpha\\,=\\,1$ yields the classical IMF, $\\alpha\\in(0,1)$ yields an incremental version of IMF. In Section 4 we show that $\\alpha$ -IMF can be implemented as an online version of DSBM. ", "page_idx": 2}, {"type": "text", "text": "3.1 A flow of path measures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $(\\mathbb{P}^{s},\\hat{\\mathbb{P}}^{s})_{s\\geq0}$ be a flow of path measures defined for any $s\\geq0$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbb{P}}^{0}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1},\\quad\\partial_{s}\\hat{\\mathbb{P}}^{s}=\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{s}))-\\hat{\\mathbb{P}}^{s},\\quad\\mathbb{P}^{s}=\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{s}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "1F32iCJFfa/tmp/ebcbbbe83c30713c75b2df8a3cd2e045301571c640c16d1acda103be2987b75a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "which we assume is well-defined. Note that for any $\\begin{array}{l}{{s}}\\end{array}\\geq\\ 0,\\ \\mathbb{P}^{s}$ is Markov while $\\hat{\\mathbb{P}}^{s}$ is in the reciprocal class of $\\mathbb{Q}$ . Crucially, the only fixed point of (3) is the SB. Indeed, let $\\bar{\\mathbb P}$ be a fixed point of $(\\mathbb{P}^{s})_{s\\geq0}$ in (3). Then, we have that $\\bar{\\mathbb{P}}\\ =\\ \\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\bar{\\mathbb{P}}))$ . Hence, we get $\\bar{\\mathbb{P}}=\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\cdot\\cdot\\cdot(\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\bar{\\mathbb{P}})))\\cdot\\cdot\\cdot\\cdot))$ ). Hence, under mild assumptions, $\\bar{\\mathbb P}$ is a limit point of IMF and therefore $\\bar{\\mathbb P}$ is the SB $\\mathbb{P}^{\\star}$ given by (2), see (Peluchetti, 2023, Theorem 2). ", "page_idx": 3}, {"type": "text", "text": "Next, for any $\\alpha\\in(0,1]$ , we define the following discretisation of (3) called $\\alpha$ -IMF: ", "page_idx": 3}, {"type": "text", "text": "Figure 1: Illustration of the SB Flow and comparison with IMF. $\\mathbb{P}^{\\star}$ is the SB, $({\\hat{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ the IMF sequence and $(\\hat{\\mathbb{P}}^{s})_{s\\geq0}$ the flow we consider. See Appendix B for the analysis of this example. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{P}}^{n+1}=(1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\mathbb{P}^{n}=\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})$ . Note that for any $n\\in\\ensuremath{\\mathbb{N}},\\,\\hat{\\mathbb{P}}^{n}\\in$ ${\\mathcal{R}}(\\mathbb{Q})$ . This recovers the IMF procedure (Shi et al., 2023; Peluchetti, 2023) when $\\alpha=1$ . Using the definition of the sequence $({\\hat{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ , it is possible to analyse the sequence $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ using the properties of the KL divergence as well as the Pythagorean identities derived in (Shi et al., 2023; Peluchetti, 2023). We first introduce some assumptions on the Schr\u00f6dinger Bridge problem. We recall that the differential entropy of a probability measure $\\pi$ is given by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{H}(\\pi)=-\\int_{\\mathbb{R}^{d}}\\mathrm{log}((\\mathrm{d}\\pi/\\mathrm{d}\\mathrm{Leb})(x))\\mathrm{d}\\pi(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "if $\\pi$ admits a den\u221asity with respect to the Lebesgue measure and $+\\infty$ otherwise. Recall that $\\mathbb{Q}$ is associated with $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ and assume that $\\mathbb{Q}_{0}=\\mathrm{Leb}$ . Let $\\pi_{0},\\pi_{1}\\in\\mathcal{P}(\\mathbb{R}^{d})$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\|x\\|^{2}\\mathrm{d}\\pi_{i}(x)<+\\infty,\\qquad\\mathrm{H}(\\pi_{i})<+\\infty,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $i\\in\\{0,1\\}$ . Under these assumptions, we can use the characterisation of the SB as the only path measure that preserves $\\pi_{0},\\pi_{1}$ , and is both Markov and in the reciprocal class of $\\mathbb{Q}$ (see e.g. (L\u00e9onard, 2014, Theorem 2.12)). We get the following result. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Convergence of $\\alpha$ -IMF): Let $\\alpha\\in(0,1]$ and $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ defined by (4). Under mild assumptions, we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathbb{P}^{n}=\\mathbb{P}^{\\star}}\\end{array}$ , where $\\mathbb{P}^{\\star}$ is the solution of the Schr\u00f6dinger Bridge problem (2). ", "page_idx": 3}, {"type": "text", "text": "3.2 Discretisation and non-parametric loss ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We show here that $\\alpha$ -IMF is associated with an incremental version of DSBM for $\\alpha\\in(0,1)$ . ", "page_idx": 3}, {"type": "text", "text": "Iterative Markovian Fitting. For any $v:\\,[0,1]\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ , we introduce the loss function ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(v,\\mathbb{P})=\\int_{0}^{1}\\mathcal{L}_{t}(v_{t},\\mathbb{P})\\mathrm{d}t=\\int_{0}^{1}\\int_{(\\mathbb{R}^{d})^{3}}\\Big\\lVert v_{t}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\Big\\rVert^{2}\\mathrm{d}\\mathbb{P}_{0,1}(x_{0},x_{1})\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1})\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we recall that $\\mathbb{Q}$ is induced by $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ for some $\\varepsilon>0$ . This loss was already considered in (Peluchetti, 2021; Lipman et al., 2023; Liu et al., 2023a; Liu, 2022; Shi et al., 2023). We also define the path measure $\\mathbb{P}_{v}\\in\\mathcal{P}(\\mathcal{C})$ associated with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm d\\mathbf X_{t}=v_{t}(\\mathbf X_{t})\\mathrm d t+\\sqrt{\\varepsilon}\\mathrm d\\mathbf B_{t},\\qquad\\mathbf X_{0}\\sim\\pi_{0}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consider first the sequence $(v^{n})_{n\\in\\mathbb{N}}$ defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\nv^{n+1}=\\operatorname{argmin}_{v}\\mathcal{L}(v,\\mathbb{P}_{v^{n}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using Definition 2.2, we have that $\\mathbb{P}_{v^{n+1}}=\\mathrm{proj}_{\\mathcal{M}}(\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}_{v^{n}}))$ , which corresponds to $\\mathbb{P}^{n+1}$ in the IMF sequence. Therefore we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathbb{P}_{v^{n}}=\\mathbb{P}^{\\star}}\\end{array}$ under mild assumptions (Peluchetti, 2023, Theorem 2). ", "page_idx": 4}, {"type": "text", "text": "Functional gradient descent. We now introduce a relaxation of (7), where, instead of considering the argmin, we update the vector field with one gradient step. To define this relaxation, we recall that for a functional $F:\\,\\mathcal{F}\\rightarrow\\mathbb{R}$ , where $\\mathcal{F}$ is an appropriate function space, its functional derivative (Courant and Hilbert, 2008) with reference measure $\\mu$ is denoted $\\nabla_{\\mu}F$ and is given for any $\\phi\\in{\\mathcal{F}}$ , when it exists, by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{lim}_{\\gamma\\to0}(F(f+\\gamma\\phi)-F(f))/\\gamma=\\int\\langle\\nabla_{\\mu}F(f)(x),\\phi(x)\\rangle\\mathrm{d}\\mu(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Initialised with $v_{t}^{0}(x)=(\\mathbb{E}_{\\hat{\\mathbb{P}}_{1\\mid t}^{0}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x]-x)/(1-t)$ , where $\\hat{\\mathbb{P}}^{0}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ , we now introduce a sequence of vector fields $(v^{n})_{n\\in\\mathbb{N}}$ . This corresponds to training a bridge matching model (see e.g. Liu et al. (2023a); Albergo et al. (2023)), giving $\\mathbb{P}_{v^{0}}=\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{0})$ . Then for $n\\in\\mathbb N$ , let ", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{t}^{n+1}(x)=v_{t}^{n}(x)-\\delta_{n}\\nabla_{\\mu^{n}}\\mathcal{L}_{t}(v_{t}^{n},\\mathbb{P}_{v^{n}})(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\delta_{n}>0$ and $\\mu^{n}\\in\\mathcal{P}(\\mathcal{C})$ . The parameters $(\\delta_{n},\\mu^{n})_{n\\in\\mathbb{N}}$ will be made explicit in Proposition 3.2. We emphasize that, in contrast to the IMF procedure, in the online update (8) we do not need to solve a Markovian projection problem at every step; instead we simply take a gradient step on the loss (5). ", "page_idx": 4}, {"type": "text", "text": "Connection with $\\alpha$ -IMF. The following proposition shows that $(\\mathbb{P}_{v^{n}})_{n\\in\\mathbb{N}}$ defined by (8) is associated with $\\alpha$ -IMF defined in (4). ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2 (Non-parametric updates are $\\alpha$ -IMF): Let $\\alpha\\in(0,1],$ , $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ as in (4), $\\delta_{n}=\\alpha$ and $\\mu^{n}=(1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{n})$ . Then, under mild assumptions, we have $\\mathbb{P}_{v^{n}}=\\mathbb{P}^{n}$ for all $n\\in\\mathbb N$ . ", "page_idx": 4}, {"type": "text", "text": "Combining Theorem 3.1 to Proposition 3.2, we get that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathbb{P}_{v^{n}}=\\mathbb{P}^{\\star}}\\end{array}$ , i.e. the non-parametric procedure converges to the SB. ", "page_idx": 4}, {"type": "text", "text": "4 $\\alpha$ -Diffusion Schr\u00f6dinger Bridge Matching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "From DSBM to $\\alpha$ -DSBM. In Section 3, we introduced $\\alpha$ -IMF, a scheme which defines a sequence of path measures converging to the SB for all $\\alpha\\in(0,1]$ . For $\\alpha=1$ , this corresponds to the IMF, whose practical DSBM implementation (Shi et al., 2023) requires repeatedly solving an expensive minimisation problem (7). In contrast, for $\\alpha<1$ we are only required to take one (non-parametric) gradient step to update the vector field, see (8). This suggests the following practical implementation of $\\alpha$ -IMF, called $\\alpha$ -DSBM: First, pretrain a bridge matching model so that for $t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $v_{t}^{\\theta}(x)\\,=\\,(\\mathbb{E}_{\\hat{\\mathbb{P}}_{1\\mid t}^{0}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}\\,=\\,x]\\,-\\,x)/(1\\,-\\,t)$ , where $\\hat{\\mathbb{P}}^{0}\\,=\\,(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ . Then, perform the parametric version of the update (8): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{~\\gamma~}\\!\\gets\\!\\alpha\\!-\\!\\alpha\\nabla_{\\theta}\\mathrm{L}(\\theta,\\mathbb{P}_{\\bar{\\theta}});\\,\\mathrm{L}(\\theta,\\mathbb{P})=\\int_{0}^{1}\\int_{(\\mathbb{R}^{d})^{3}}\\Big\\lVert v_{t}^{\\theta}(x_{t})\\!-\\!\\frac{x_{1}-x_{t}}{1-t}\\Big\\rVert^{2}\\mathrm{d}\\mathbb{P}_{0,1}(x_{0},x_{1})\\mathrm{d}\\mathbb{Q}_{[0,1}(x_{t}|x_{0},x_{1})\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{P}_{\\bar{\\theta}}$ is a stop-gradient version of $\\mathbb{P}_{v^{\\theta}}$ . In Appendix D.2, we give a theoretical justification for this parametric equivalent of (5) and (8) by showing that, as $\\alpha\\rightarrow0$ , the update on the velocity fields $v^{\\dot{\\theta}}$ given by (9) corresponds to a direction of descent for the non-parametric loss (8) on average. Once again, we emphasize that if we replace the gradient step in (9) with the minimisation $\\theta\\gets\\bar{\\mathrm{argmin}}_{\\theta}\\mathrm{L}(\\bar{\\theta},\\mathbb{P}_{\\bar{\\theta}})$ , we recover DSBM. ", "page_idx": 4}, {"type": "text", "text": "Bidirectional online procedure. As with DSBM, directly implementing (9) leads to error quickly accumulating, see Appendix I for details. One way to circumvent this error accumulation issue is to consider a bidirectional procedure, in which we train both a forward and a backward model. This is possible because the Markovian projection coincides for forward and backward path measures, see (Shi et al., 2023, Proposition 9). This suggests considering the loss $\\begin{array}{r}{\\mathcal{L}(v^{\\diamond},v^{\\bullet},\\mathbb{P}^{\\diamond},\\mathbb{P}^{\\bullet})=\\int_{0}^{1}\\mathcal{L}_{t}(v_{t}^{\\diamond},v_{t}^{\\epsilon},\\mathbb{P}^{\\diamond},\\mathbb{P}^{\\bullet})\\mathrm{d}t}\\end{array}$ , which is an extension of (5), where ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t}(v_{t}^{\\flat},v_{t}^{\\star},\\mathbb{P}^{\\flat},\\mathbb{P}^{\\bullet})=\\displaystyle\\int_{(\\mathbb{R}^{d})^{3}}\\left\\|v_{t}^{\\flat}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\right\\|^{2}\\mathrm{d}\\mathbb{P}_{0,1}^{\\epsilon}(x_{0},x_{1})\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\int_{(\\mathbb{R}^{d})^{3}}\\left\\|v_{1-t}^{\\star}(x_{t})-\\frac{x_{0}-x_{t}}{t}\\right\\|^{2}\\mathrm{d}\\mathbb{P}_{0,1}^{\\flat}(x_{0},x_{1})\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly to (6), we define $\\mathbb{P}_{v^{\\ast}},\\mathbb{P}_{v^{\\ast}}$ , associated with $(\\mathbf{X}_{t})_{t\\in[0,1]}$ and $(\\mathbf{Y}_{1-t})_{t\\in[0,1]}$ respectively, which are defined by forward and backward SDEs ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\mathbf{f}\\mathbf{w}\\mathbf{d})\\colon\\mathbf{d}\\mathbf{X}_{t}=v_{t}^{*}(\\mathbf{X}_{t})\\mathbf{d}t+\\sqrt{\\varepsilon}\\mathbf{d}\\mathbf{B}_{t},\\ \\mathbf{X}_{0}\\sim\\pi_{0},\\ (\\mathbf{b}\\mathbf{w}\\mathbf{d})\\colon\\mathbf{d}\\mathbf{Y}_{t}=v_{t}^{*}(\\mathbf{Y}_{t})\\mathbf{d}t+\\sqrt{\\varepsilon}\\mathbf{d}\\mathbf{B}_{t},\\ \\mathbf{Y}_{0}\\sim\\pi_{1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly to (8), we define non-parametric updates for any $n\\in\\ensuremath{\\mathbb{N}},\\,t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n(v_{t}^{n+1,\\ast}(x),v_{t}^{n+1,\\ast}(x))=\\big(v_{t}^{n,\\ast}(x),v_{t}^{n,\\ast}(x)\\big)-\\delta_{n}\\nabla_{\\mu^{n}}\\mathcal{L}_{t}\\left(v_{t}^{n,\\ast}(x),v_{t}^{n,\\ast}(x),\\mathbb{P}_{v^{n,\\ast}},\\mathbb{P}_{v^{n,\\ast}}\\right)(x).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We have the following proposition which ensures our bidirectional procedure is still valid and that the results of Proposition 3.2 still hold. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.1 (Bidirectional updates): Let $\\alpha\\in(0,1]$ . For any $n\\in\\mathbb{N}$ , define $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ by (4). Then, under mild assumption and assuming that $\\delta_{n}=\\alpha$ and $\\mu^{n}=(1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{n}),$ , we have that for any $n\\in\\mathbb N$ , $\\mathbb{P}_{v^{n,\\ast}}=\\mathbb{P}_{v^{n,\\ast}}=\\mathbb{P}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "In Appendix I, we show that in the Gaussian setting the bidirectional procedure (4.1) does not accumulate error when the vector field is approximated, while the unidirectional one (8) does. ", "page_idx": 5}, {"type": "text", "text": "Vector field parameterisation. Contrary to existing procedures (Shi et al., 2023; Peluchetti, 2023; Liu, 2022), we do not parameterise $v^{\\rightarrow}$ and $v^{\\leftarrow}$ using two separate networks. Instead, we consider an additional input $s\\in\\{\\bar{0},1\\}$ such that $v_{\\theta}(1,\\cdot)\\approx v^{\\rightarrow}$ and $v_{\\theta}(\\bar{0},\\cdot)\\approx v^{\\leftarrow}$ . This allows us to substantially reduce the number of parameters in the model. The conditioning on $s$ in the network is detailed in Appendix K. Before stating our full algorithm in Algorithm 1, we introduce a batched parametric version of (10). For ease of notation, we write $\\operatorname{Interp}_{t}$ for the operation corresponding to sampling from $\\mathbb{Q}_{t|0,1}$ , i.e. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Interp}_{t}(\\mathbf{X}_{0},\\mathbf{X}_{1},\\mathbf{Z})=(1-t)\\mathbf{X}_{0}+t\\mathbf{X}_{1}+\\sqrt{\\varepsilon(1-t)t}\\mathbf{Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We are now ready to introduce the batched parametric version of (10). For a given batch of inputs $\\underline{{\\mathbf{X}}}_{0}^{1:B}$ and $\\mathbf{X}_{1}^{1:B}$ , timesteps $t\\sim\\mathrm{Unif}([0,1])^{\\otimes B}$ , and $\\mathbf{X}_{t}=\\operatorname{Interp}_{t}(\\mathbf{X}_{0},\\mathbf{X}_{1},\\mathbf{Z})$ with $\\mathbf{Z}\\sim{\\mathcal{N}}(0,{\\bar{\\mathrm{Id}}})^{\\otimes B}$ , we compute the empirical forward and backward losses as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ell^{\\rightarrow}(\\theta;t,{\\bf X}_{1},{\\bf X}_{t})=\\frac{1}{B}\\sum_{i=1}^{B}\\|v_{\\theta}\\left(1,t^{i},{\\bf X}_{t}^{i}\\right)-\\left({\\bf X}_{1}^{i}-{\\bf X}_{t}^{i}\\right)/(1-t^{i})\\|^{2},}}\\\\ {{\\displaystyle\\ell^{\\leftarrow}(\\theta;t,{\\bf X}_{0},{\\bf X}_{t})=\\frac{1}{B}\\sum_{i=1}^{B}\\Big\\|v_{\\theta}\\left(0,1-t^{i},{\\bf X}_{t}^{i}\\right)-\\left({\\bf X}_{0}^{i}-{\\bf X}_{t}^{i}\\right)/t^{i}\\Big\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We present the resulting $\\alpha$ -DSBM in Algorithm 1. Note that in this algorithm, we maintain an Exponential Moving Average (EMA) of model parameters, as is common in diffusion models (Nichol and Dhariwal, 2021). During the finetuning stage, when we generate samples to use as model\u2019s inputs, we then have a choice of sampling using the EMA or non-EMA parameters. At test time, we always sample using the EMA parameters, as it is known to improve the visual quality (Song and Ermon, 2020). In Algorithm 1, we specify $\\alpha\\in(0,1]$ as a stepsize parameter. In practice, we use Adam (Kingma and Ba, 2015) for optimization, thus the choice of $\\alpha$ is implicit and adaptive throughout the training. To emphasize the importance of the parameter $\\alpha$ , we sweep over its value with an explicit solver SGD in a toy setting, see Appendix K.2. We refer to Appendix $\\mathbf{K}$ for more details on our experimental setup. ", "page_idx": 5}, {"type": "text", "text": "1: Input: datasets $\\pi_{0}$ and $\\pi_{1}$ , entropic regularisation $\\varepsilon$ , number of pretraining and finetuning steps   \n$N_{\\mathrm{pretraining}}$ and $N_{\\mathrm{finetuning}}$ , batch size $B$ and half batch size $\\bar{b}=B/2$ , EMA decay $\\gamma$ , initial   \nparameters $\\theta$ and initial EMA parameters $\\theta^{\\mathrm{EMA}}=\\theta$ , $\\alpha\\in(0,1]$   \n2: for $n\\in\\{1,\\ldots,N_{\\mathrm{pretraining}}\\}$ do   \n3: Sample $(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim(\\pi_{0}\\otimes\\pi_{1})^{\\otimes B}$   \n4: Sample $t\\sim\\mathrm{Unif}([0,1])^{\\otimes B}$ and $\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id})^{\\otimes B}$ and compute $\\mathbf{X}_{t}=\\operatorname{Interp}_{t}(\\mathbf{X}_{0},\\mathbf{X}_{1},\\mathbf{Z})$   \n5: Update $\\theta$ with a gradient step o n $\\begin{array}{r}{\\frac{1}{2}\\left[\\ell^{\\diamond}\\left(t^{1:b},\\mathbf{X}_{1}^{1:b},\\mathbf{X}_{t}^{1:b}\\right)+\\ell^{\\bullet}\\left(t^{b+1:B},\\mathbf{X}_{0}^{b+1:B},\\mathbf{X}_{t}^{b+1:B}\\right)\\right]}\\end{array}$   \n6: Update EMA parameters: $\\theta^{\\mathrm{EMA}}=\\gamma\\theta^{\\mathrm{EMA}}+(1-\\gamma)\\theta$   \n7: end for   \n8: for $n\\in\\{1,\\ldots,N_{\\mathrm{finetuning}}\\}$ do   \n9: Sample $(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim(\\pi_{0}\\otimes\\pi_{1})^{\\otimes b}$   \n10: Sample $\\hat{\\mathbf{X}}_{1}$ solving forward SDE (11)-(fwd) with $v_{\\theta^{\\mathrm{EMA}}}(1,\\cdot)$ or $v_{\\theta}(1,\\cdot)$ starting from $\\mathbf{X}_{0}$   \n11: Sample $\\hat{\\mathbf{X}}_{0}$ solving backward SDE (11)-(bwd) with $v_{\\theta^{\\mathrm{EMA}}}(0,\\cdot)$ or $v_{\\theta}(0,\\cdot)$ starting from $\\mathbf{X}_{1}$   \n12: Sample $t^{\\rightarrow}\\sim\\mathrm{Unif}([0,1])^{\\otimes b}$ and $\\mathbf{Z}^{\\rightarrow}\\sim\\mathcal{N}(0,\\operatorname{Id})^{\\otimes b}$ and compute $\\mathbf{X}_{t}^{\\rightarrow}=\\mathrm{Interp}_{t^{\\rightarrow}}(\\hat{\\mathbf{X}}_{0},\\mathbf{X}_{1},\\mathbf{Z}^{\\rightarrow})$   \n13: Sample $t^{\\leftarrow}\\sim\\mathrm{Unif}([0,1])^{\\otimes b}$ and $\\mathbf{Z}^{\\leftarrow}\\sim\\mathcal{N}(0,\\operatorname{Id})^{\\otimes b}$ and compute $\\mathbf{X}_{t}^{\\leftarrow}=\\mathrm{Interp}_{t^{\\leftarrow}}(\\mathbf{X}_{0},\\hat{\\mathbf{X}}_{1},\\mathbf{Z}^{\\leftarrow})$   \n14: Update $\\theta$ with a gradient step on $\\begin{array}{r}{\\frac{1}{2}\\left[\\ell^{>}(t^{>},\\mathbf{X}_{1},\\mathbf{X}_{t}^{>})+\\ell^{*}(t^{\\leftarrow},\\mathbf{X}_{0},\\mathbf{X}_{t}^{\\leftarrow})\\right]}\\end{array}$ and stepsize $\\alpha$   \n15: Update EMA parameters: $\\theta^{\\mathrm{EMA}}=\\bar{\\gamma}\\theta^{\\mathrm{EMA}}+(1-\\gamma)\\theta$   \n16: end for ", "page_idx": 6}, {"type": "text", "text": "17: Output: $(\\theta,\\theta^{\\mathrm{EMA}})$ parameters of the finetuned model ", "page_idx": 6}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Solving Schr\u00f6dinger Bridge problems. Schr\u00f6dinger Bridges (Schr\u00f6dinger, 1932) have been thoroughly studied through the lens of probability theory (L\u00e9onard, 2014) and stochastic control (Dai Pra, 1991; Chen et al., 2021). They recently found applications in generative modeling and related fields leveraging recent advances in diffusion models (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022). Extensions of these methods to other machine learning problems and modalities were studied in (Shi et al., 2022; Thornton et al., 2022; Liu et al., 2022; Chen et al., 2023; Tamir et al., 2023). Shi et al. (2023); Peluchetti (2023) concurrently introduced the DSBM algorithm which relies on a new procedure called IMF, while the DSB algorithm introduced in (De Bortoli et al., 2021) is based on the standard Iterative Proportional Fitting (IPF) scheme. Neklyudov et al. (2023a,b); Liu et al. (2022) generalise DSBM to arbitrary cost functions, albeit at the expense of having to learn the reciprocal projection which is no longer given by a Brownian bridge. These new methodologies translate to improved numerics when compared to their IPF counterparts, but they remain reliant on alternating between the optimisation of two losses. Finally, we note that the Schr\u00f6dinger Bridge flow and the $\\alpha$ -IMF procedure can be linked to the Sinkhorn flow recently introduced by Karimi et al. (2024), see Appendix H.1 for a detailed discussion. ", "page_idx": 6}, {"type": "text", "text": "Sampling-free methodologies. Sampling-free methodologies have been proposed to solve OT related objectives. In (Liu et al., 2023a; Somnath et al., 2023; Diefenbacher et al., 2024; Cao et al., 2024), the authors perform one step of DSBM, i.e. only consider the pretraining stage of our algorithm. While the obtained bridge might enjoy transport properties, it does not solve an OT problem. In another line of work, Pooladian et al. (2023); Tong et al. (2024a,b); Eyring et al. (2024) have proposed simulation-free methods to minimise OT objectives. However, they target not the OT problem, but a minibatch version of it which coincides with OT only in the limit of infinite batch size, see (Pooladian et al., 2023, Theorem 4.2). Other sampling-free methods to solve the Schr\u00f6dinger Bridge problem include Kim et al. (2024); Gushchin et al. (2024b) both of which rely on adversarial losses to solve the OT problem. In (De Bortoli et al., 2021; Vargas et al., 2021; Liu et al., 2022; Shi et al., 2023; Peluchetti, 2023) the adversarial objective is dropped and instead the procedure requires alternating objectives during training and is not sampling-free. We also highlight the line of work of Korotin et al. (2024); Gushchin et al. (2024a) in which the Schr\u00f6dinger Bridge potentials are parameterised with mixtures of Gaussians, allowing for fast training in small dimensions. Finally, recently Deng et al. (2024) introduced a variation on Schr\u00f6dinger Bridge for generative modeling, which while still not sampling-free, does not require learning a forward process. ", "page_idx": 6}, {"type": "image", "img_path": "1F32iCJFfa/tmp/f2919bef386d1550ef6ae5dfc03bc3d6f3af5bb141ec29ad4ca7452159781329.jpg", "img_caption": ["Figure 2: Evolution of the covariance during online and iterative DSBM finetuning for forward and backward networks. The finetuning starts after 10K steps of training a bridge matching model. For the iterative case, we alternate between forward and backward updates with varying frequencies, i.e. changing after 1K, 2.5K and 5K steps. Left: Gaussian with scalar covariance matrix. Right: Gaussian with full covariance matrix. We compute the normFrob between $\\mathrm{C}_{\\star}$ and its estimate using Bridge Matching (Base), $\\alpha$ -DSBM (Online), and DSBM (Iterative with $@_{\\mathrm{XK}}$ training steps per model fit) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we illustrate the efficiency of $\\alpha$ -DSBM on different tasks. In Section 6.1, we compare $\\alpha$ -DSBM to DSBM in a Gaussian setting where the EOT coupling is tractable and show that $\\alpha$ -DSBM recovers the solution faster than DSBM. In Section 6.2, we illustrate the scalability of our method through a range of unpaired image translation experiments. ", "page_idx": 7}, {"type": "text", "text": "6.1 Gaussian case ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare $\\alpha$ -DSBM to D\u221aSBM in the Gauss\u221aian setting where $\\pi_{0}=\\mathcal{N}(0,\\sigma_{0}^{2}\\mathrm{Id})$ , $\\pi_{1}=\\mathcal{N}(0,\\sigma_{1}^{2}\\mathrm{Id})$ and $\\mathbb{Q}$ is associated with $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ with $\\sqrt{\\varepsilon}=0.5$ . In this case, the EOT coupling is $\\mathcal{N}(0,\\Sigma_{\\star})$ , with $\\Sigma_{\\star}$ given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Sigma_{\\star}=\\left(\\sigma_{\\star}^{2}\\mathrm{Id}\\quad\\sigma_{\\star}^{2}\\mathrm{Id}\\right),\\mathrm{~where~}\\sigma_{\\star}^{2}=(1/2)((\\sigma_{0}^{2}\\sigma_{1}^{2}+\\varepsilon^{2})^{1/2}-\\varepsilon),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with Id being a $d\\times d$ identity matrix. We consider $d=50$ , $\\sigma_{0}=\\sigma_{1}=1$ , resulting in $\\sigma_{\\star}^{2}\\approx0.88$ . To showcase the robustness of $\\alpha$ -DSBM, we consider the initial coupling $\\mathbb{P}_{0,1}$ , where $(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim\\mathbb{P}_{0,1}$ , $\\mathbf{X}_{0}\\sim\\mathcal{N}(0,\\mathrm{Id})$ , $\\mathbf{X}_{1}=-\\mathbf{X}_{0}$ , and let $\\hat{\\mathbb{P}}^{0}=\\mathbb{P}_{0,1}\\mathbb{Q}_{|0,1}$ . In this setting, the base model, i.e. bridge matching, significantly underestimates the true covariance $\\sigma_{\\star}^{2}$ , as shown in Section 6.1. Additionally, the figure illustrates that online finetuning approaches the true solution faster than the original iterative DSBM finetuning. For the latter, we can set how often we alternate between updating the forward and backward networks, and as this frequency increases, the behaviour approaches that of the online finetuning. ", "page_idx": 7}, {"type": "text", "text": "Full covariance Gaussian case. Let $\\pi_{0}=\\mathcal{N}(\\mu_{0},\\Sigma_{0})$ , $\\pi_{1}=\\mathcal{N}(\\mu_{1},\\Sigma_{1})$ with $\\begin{array}{r}{\\Sigma_{i}=\\operatorname{Id}+\\frac{1}{2}Z_{i}Z_{i}^{\\top}}\\end{array}$ for $i\\,\\in\\,\\{0,1\\}$ and $Z_{0},Z_{1}$ independent $d\\times d$ matrices with unit Gaussian entries. We also set $\\mu_{0}=\\mu_{1}=0$ . We consider the Entropic Optimal Transport (EOT) with regularization $\\sigma=0.5$ and $d=3$ , given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Pi={\\mathcal N}(\\mu_{\\star},\\Sigma_{\\star}),\\quad\\Sigma_{\\star}=\\left(\\Sigma_{0}^{}\\quad\\mathrm{C}_{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $\\mathrm{C_{\\star}}\\,=\\,\\frac{1}{2}\\mathrm{[}\\Sigma_{0}^{1/2}\\mathrm{D}_{\\star}\\Sigma_{0}^{-1/2}\\,-\\,\\sigma^{2}\\mathrm{Id]}$ , with $\\mathrm{D}_{\\star}\\,=\\,(4\\Sigma_{0}^{1/2}\\Sigma_{1}\\Sigma_{0}^{1/2}+\\sigma^{4}\\mathrm{Id})^{1/2}$ . Let normFrob $=$ $\\|\\mathrm{A-B}\\|_{\\mathrm{Fro}}7\\|\\mathrm{A}\\|_{\\mathrm{Fro}}$ be the normalized Frobenius distance between matrices A and B. The results are presented in Section 6.1 and confirm those presented in the original manuscript considering a diagonal covariance. ", "page_idx": 7}, {"type": "image", "img_path": "1F32iCJFfa/tmp/cf3385fdfc8532e8e306df767c2bd8becfee19dfcfb078b5e1f57d1d1e2be39a.jpg", "img_caption": ["Figure 3: Left: FID and Mean Squared Distance (MSD) on EMNIST to MNIST translation before and after finetuning with different values of $\\varepsilon$ . Right: AFHQ-64 samples after the finetuning. For both, we use a bidirectional model with online finetuning. More results are in Appendix K.3 and K.4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 Image datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Similarly to Shi et al. (2023), we apply our method to image translation problems, such as MNIST digits to EMNIST letters (LeCun and Cortes, 2010; Cohen et al., 2017), Wild to Cat domains from the Animal Faces-HQ (AFHQ) dataset (Choi et al., 2020), downsampled to $64\\times64$ and $256\\times256$ resolutions and CelebA $64\\times64$ . ", "page_idx": 8}, {"type": "text", "text": "The whole training procedure can be framed as a two-stage process: first, we train a base model on the true data samples, performing bridge matching (Peluchetti, 2021; Albergo and Vanden-Eijnden, 2023; Lipman et al., 2023; Liu et al., 2023a), and then we finetune this model. We compare models that combine different vector field parameterisations (two networks vs. one bidirectional net), finetuning methods (iterative vs. online), and sample generation strategies during the finetuning stage. ", "page_idx": 8}, {"type": "text", "text": "Following the established practice (Choi et al., 2020), we evaluate our models using FID (Heusel et al., 2017) for visual quality, and mean squared distance (MSD) or LPIPS (Zhang et al., 2018) for alignment. It is important to note that for image translation tasks at hand, FID scores are not ideal, as FID was designed for natural RGB images, which is not the case for MNIST. It is also not well suited for small sample sizes as it is the case with AFHQ, where the test set in each domain has fewer than 500 examples. Thus quantitative results in Table 1 should be interpreted cautiously, and we recommend a visual inspection of samples to complement these quantitative measures, especially for the AFHQ models. Samples from the models along with the training and evaluation protocols are given in Appendix K. ", "page_idx": 8}, {"type": "text", "text": "Compared to the iterative DSBM, our online finetuning $\\alpha$ -DSBM reduces the number of tunable hyperparameters, i.e. inner and outer iterations, refresh rate and the size of the cache for storing generated samples. This simplifies implementation and makes the algorithm more practical. The primary remaining hyperparameter, the variance of a Brownian motion $\\varepsilon$ , requires careful tuning as it influences the trade-off between the visual quality and alignment, as was also observed in Shi et al. (2023). An appropriate $\\varepsilon$ needs to balance the two: setting $\\varepsilon$ too low results in poor visual quality, while high values of $\\varepsilon$ cause poorly aligned and oversmoothed samples. Figure 3 illustrates how FID and MSD metrics vary with $\\varepsilon$ for the case of MNIST. Additionally, it demonstrates the impact of $\\varepsilon$ on the generated samples for the AFHQ-64 model. ", "page_idx": 8}, {"type": "text", "text": "We run $\\alpha$ -DSBM on CelebA with image size $64\\times64$ with $\\sigma=2.0$ . We do not change the training hyper-parameters compared to AFHQ. Visual results are reported in Figure 5 and Figure 6. In Figure 5, we show the influence of $\\sigma$ during the pretraining. The visual quality of the transfer is much lower for $\\sigma=0$ than for $\\sigma=2.0$ . The case $\\sigma=0$ corresponds to the first step of Rectified Flow (i.e. Flow Matching). Given the poor quality of the samples, we do not perform finetuning with $\\sigma=0$ . In Figure 6, we compare the visual quality and alignment of DSBM and $\\alpha$ -DSBM after 4000 training steps, corresponding to two outer DSBM iterations. In this case DSBM is trained with a bidirectional network and both procedures consist of finetuning the pretrained model obtained with $\\sigma=2.0$ . We note that the alignment is better in the case of $\\alpha$ -DSBM. ", "page_idx": 8}, {"type": "table", "img_path": "1F32iCJFfa/tmp/76d79fd56491c1085a786bc6f3b086852eb835f5ea2109bb81cbaeebcfbbfcc2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 1: Results of image translation between EMNIST and MNIST, and AFHQ $64\\!\\times\\!64$ between Wild and Cat domains. DSBM\\* results are from Shi et al. (2023). Our reimplementation of DSBM corresponds to row (a). For MNIST and AFHQ models, we used $\\varepsilon=1$ and $\\varepsilon=0.75^{2}$ , respectively. Each finetuning run was done with 5 random seeds, and we report mean scores $\\pm$ standard deviation. ", "page_idx": 9}, {"type": "image", "img_path": "1F32iCJFfa/tmp/654a3e983ba3b46a1c78ec06d66e1778c7f07768609553ae00f8bb250694335c.jpg", "img_caption": ["Figure 4: Online DSBM transfer results on AFHQ $256\\times256$ dataset between Cat and Wild domains. Top row\u2014initial samples, bottom row\u2014transferred samples. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we have introduced $\\alpha$ -Diffusion Schr\u00f6dinger Bridge Matching ( $\\alpha$ -DSBM), a new methodology to solve Entropic Optimal Transport problems. $\\alpha$ -DSBM is an improved version of DSBM, which does not require training multiple DDM-type models. We have shown that a non-parametric version of this method recovers the Schr\u00f6dinger Bridge (SB). In addition, $\\alpha$ -DSBM is easier to implement than existing SB methodologies while exhibiting similar performance. We illustrated the efficiency of our algorithm on a variety of unpaired transfer tasks. ", "page_idx": 9}, {"type": "text", "text": "While $\\alpha$ -DSBM solves one of the most critical limitations of DBSM, namely the alternative optimisation, several issues remain to be addressed in order for the method to scale comparably to generative DDMs. In particular, the method is not sampling-free, as during training it requires sampling from the model from the previous iteration to obtain the training data for the current iteration. While it seems difficult to derive a completely sampling-free method to solve SB problems without resorting to the Minibatch OT approximation, there is still room for improvement. ", "page_idx": 9}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Translation Female $\\rightarrow$ Male on CelebA. Left: pretraining with $\\sigma\\,=\\,0$ . Right: pretraining with $\\sigma=2.0$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "1F32iCJFfa/tmp/9397f44b68c6dc650f4a5dc0ad14cf268437a73979fa5f601f20a01171740f24.jpg", "img_caption": ["Figure 6: Translation Female $\\rightarrow$ Male on CelebA. Left: output after finetuning with DSBM. Right: output after $\\alpha$ -DSBM finetuning. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Albergo, M. S., Boff,i N. M., and Vanden-Eijnden, E. (2023). Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797.   \nAlbergo, M. S. and Vanden-Eijnden, E. (2023). Building normalizing flows with stochastic interpolants. International Conference on Learning Representations.   \nAmbrosio, L., Gigli, N., and Savar\u00e9, G. (2008). Gradient Flows in Metric Spaces and in the Space of Probability Measures. Lectures in Mathematics ETH Z\u00fcrich. Birkh\u00e4user Verlag, Basel, second edition.   \nBauschke, H. H. and Kruk, S. G. (2004). Reflection-projection method for convex feasibility problems with an obtuse cone. Journal of Optimization Theory and Applications, 120(3):503\u2013531.   \nBenamou, J.-D. and Brenier, Y. (2000). A computational fluid mechanics solution to the Monge\u2013 Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393.   \nBlack, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. (2023). Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301.   \nBrekelmans, R. and Neklyudov, K. (2023). On Schr\u00f6dinger bridge matching and expectation maximization. In NeurIPS 2023 Workshop Optimal Transport and Machine Learning.   \nBrock, A., Donahue, J., and Simonyan, K. (2019). Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations.   \nCao, Z., Wu, X., and Deng, L.-J. (2024). Neural shr\u00f6dinger bridge matching for pansharpening. arXiv preprint arXiv:2404.11416.   \nChen, T., Liu, G.-H., Tao, M., and Theodorou, E. (2023). Deep momentum multi-marginal Schr\u00f6dinger bridge. Advances in Neural Information Processing Systems.   \nChen, T., Liu, G.-H., and Theodorou, E. A. (2022). Likelihood training of Schr\u00f6dinger bridge using forward-backward SDEs theory. In International Conference on Learning Representations.   \nChen, Y., Georgiou, T. T., and Pavon, M. (2021). Optimal transport in systems and control. Annual Review of Control, Robotics, and Autonomous Systems, 4.   \nChen, Y., Goldstein, M., Hua, M., Albergo, M. S., Boff,i N. M., and Vanden-Eijnden, E. (2024). Probabilistic forecasting with stochastic interpolants and F\u00f6llmer processes. arXiv preprint arXiv:2403.13724.   \nChoi, Y., Uh, Y., Yoo, J., and Ha, J.-W. (2020). Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.   \nChong, M. J. and Forsyth, D. (2020). Effectively unbiased FID and inception score and where to find them. In IEEE/CVF Conference on Computer Vision and Pattern Recognition.   \nCohen, G., Afshar, S., Tapson, J., and van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. arXiv preprint arXiv:1702.05373.   \nCourant, R. and Hilbert, D. (2008). Methods of Mathematical Physics: Partial Differential Equations. John Wiley & Sons.   \nCuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems.   \nDai Pra, P. (1991). A stochastic control approach to reciprocal diffusion processes. Applied Mathematics and Optimization, 23(1):313\u2013329.   \nDaras, G., Dagan, Y., Dimakis, A., and Daskalakis, C. (2023a). Consistent diffusion models: Mitigating sampling drift by learning to be consistent. In Advances in Neural Information Processing Systems.   \nDaras, G., Shah, K., Dagan, Y., Gollakota, A., Dimakis, A., and Klivans, A. (2023b). Ambient diffusion: Learning clean distributions from corrupted data. In Advances in Neural Information Processing Systems.   \nDe Bortoli, V., Hutchinson, M., Wirnsberger, P., and Doucet, A. (2024). Target score matching. arXiv preprint arXiv:2402.08667.   \nDe Bortoli, V., Liu, G.-H., Chen, T., Theodorou, E. A., and Nie, W. (2023). Augmented bridge matching. arXiv preprint arXiv:2311.06978.   \nDe Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion Schr\u00f6dinger bridge with applications to score-based generative modeling. In Advances in Neural Information Processing Systems.   \nDe Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. (2021). A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366\u20133385.   \nDeng, W., Luo, W., Tan, Y., Bilo\u0161, M., Chen, Y., Nevmyvaka, Y., and Chen, R. T. (2024). Variational Schr\u00f6dinger diffusion models. arXiv preprint arXiv:2405.04795.   \nDhariwal, P. and Nichol, A. Q. (2021). Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems.   \nDiefenbacher, S., Liu, G.-H., Mikuni, V., Nachman, B., and Nie, W. (2024). Improving generative model-based unfolding with Schr\u00f6dinger bridges. Physical Review D, 109(7):076011.   \nDupuis, P. and Ellis, R. S. (2011). A Weak Convergence Approach to the Theory of Large Deviations. John Wiley & Sons.   \nEyring, L., Klein, D., Uscidda, T., Palla, G., Kilbertus, N., Akata, Z., and Theis, F. (2024). Unbalancedness in neural Monge maps improves unpaired domain translation. In International Conference on Learning Representations.   \nFan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P., Ghavamzadeh, M., Lee, K., and Lee, K. (2024). Reinforcement learning for fine-tuning text-to-image diffusion models. In Advances in Neural Information Processing Systems.   \nGe, Z., Liu, S., Li, Z., Yoshie, O., and Sun, J. (2021). Ota: Optimal transport assignment for object detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition.   \nGenevay, A., Peyre, G., and Cuturi, M. (2018). Learning generative models with Sinkhorn divergences. In Artificial Intelligence and Statistics.   \nGushchin, N., Kholkin, S., Burnaev, E., and Korotin, A. (2024a). Light and optimal schr\u00f6dinger bridge matching. arXiv preprint arXiv:2402.03207.   \nGushchin, N., Kolesov, A., Korotin, A., Vetrov, D. P., and Burnaev, E. (2024b). Entropic neural optimal transport via diffusion processes. In Advances in Neural Information Processing Systems.   \nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems.   \nHo, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems.   \nHoogeboom, E., Heek, J., and Salimans, T. (2023). Simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning.   \nHu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. (2021). LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.   \nHudson, D. A., Zoran, D., Malinowski, M., Lampinen, A. K., Jaegle, A., McClelland, J. L., Matthey, L., Hill, F., and Lerchner, A. (2023). Soda: Bottleneck diffusion models for representation learning. arXiv preprint arXiv:2311.17901.   \nKarimi, M. R., Hsieh, Y.-P., and Krause, A. (2024). Sinkhorn flow as mirror flow: A continuous-time framework for generalizing the Sinkhorn algorithm. In International Conference on Artificial Intelligence and Statistics.   \nKarras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems.   \nKim, B., Kwon, G., Kim, K., and Ye, J. C. (2024). Unpaired image-to-image translation via neural schr\u00f6dinger bridge. In International Conference on Learning Representations.   \nKingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In International Conference on Learning Representations.   \nKorotin, A., Gushchin, N., and Burnaev, E. (2024). Light Schr\u00f6dinger bridge. In International Conference on Learning Representations.   \nLeCun, Y. and Cortes, C. (2010). MNIST handwritten digit database.   \nLee, K., Liu, H., Ryu, M., Watkins, O., Du, Y., Boutilier, C., Abbeel, P., Ghavamzadeh, M., and Gu, S. S. (2023). Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192.   \nL\u00e9onard, C. (2014). A survey of the Schr\u00f6dinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems-A, 34(4):1533\u20131574.   \nL\u00e9onard, C., R\u0153lly, S., Zambrini, J.-C., et al. (2014). Reciprocal processes. a measure-theoretical point of view. Probability Surveys, 11:237\u2013269.   \nLipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. (2023). Flow matching for generative modeling. In International Conference on Learning Representations.   \nLiu, G.-H., Chen, T., So, O., and Theodorou, E. A. (2022). Deep generalized Schr\u00f6dinger bridge. In Advances in Neural Information Processing Systems.   \nLiu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., Nie, W., and Anandkumar, A. (2023a). I2sb: image-to-image Schr\u00f6dinger bridge. In International Conference on Machine Learning.   \nLiu, Q. (2022). Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577.   \nLiu, X., Gong, C., and Liu, Q. (2023b). Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations.   \nMasip, S., Rodriguez, P., Tuytelaars, T., and van de Ven, G. M. (2023). Continual learning of diffusion models with generative distillation. arXiv preprint arXiv:2311.14028.   \nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533.   \nNeal, R. M. and Hinton, G. E. (1998). A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in Graphical Models, pages 355\u2013368. Springer.   \nNeklyudov, K., Brekelmans, R., Severo, D., and Makhzani, A. (2023a). Action matching: Learning stochastic dynamics from samples. In International Conference on Machine Learning.   \nNeklyudov, K., Brekelmans, R., Tong, A., Atanackovic, L., Liu, Q., and Makhzani, A. (2023b). A computational framework for solving Wasserstein Lagrangian flows. arXiv preprint arXiv:2310.10649.   \nNichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International Conference on Machine Learning.   \nParisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter, S. (2019). Continual lifelong learning with neural networks: A review. Neural networks, 113:54\u201371.   \nPavon, M., Trigila, G., and Tabak, E. G. (2021). The data-driven Schr\u00f6dinger bridge. Communications on Pure and Applied Mathematics, 74:1545\u20131573.   \nPeluchetti, S. (2021). Non-denoising forward-time diffusions. https://openreview.net/forum? id=oVfIKuhqfC.   \nPeluchetti, S. (2023). Diffusion bridge mixture transports, Schr\u00f6dinger bridge problems and generative modeling. Journal of Machine Learning Research, 24:1\u201351.   \nPerez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville, A. C. (2018). Film: Visual reasoning with a general conditioning layer. In AAAI.   \nPeyr\u00e9, G., Cuturi, M., et al. (2019). Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607.   \nPooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y., and Chen, R. T. (2023). Multisample flow matching: Straightening flows with minibatch couplings. In International Conference on Learning Representations.   \nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. In Advances in Neural Information Processing Systems.   \nR\u0153lly, S. (2013). Reciprocal processes: a stochastic analysis approach. In Modern Stochastics and Applications, pages 53\u201367. Springer.   \nRonneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Navab, N., Hornegger, J., Wells, W. M., and Frangi, A. F., editors, Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, pages 234\u2013241, Cham. Springer International Publishing.   \nSchr\u00f6dinger, E. (1932). Sur la th\u00e9orie relativiste de l\u2019\u00e9lectron et l\u2019interpr\u00e9tation de la m\u00e9canique quantique. Annales de l\u2019Institut Henri Poincar\u00e9, 2(4):269\u2013310.   \nShi, Y., De Bortoli, V., Campbell, A., and Doucet, A. (2023). Diffusion Schr\u00f6dinger bridge matching. In Advances in Neural Information Processing Systems.   \nShi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. (2022). Conditional simulation using diffusion Schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence.   \nSmith, J. S., Hsu, Y.-C., Zhang, L., Hua, T., Kira, Z., Shen, Y., and Jin, H. (2023). Continual diffusion: Continual customization of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027.   \nSommerfeld, M., Schrieber, J., Zemel, Y., and Munk, A. (2019). Optimal transport: Fast probabilistic approximation with exact solvers. Journal of Machine Learning Research, 20(105):1\u201323.   \nSomnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R., Krause, A., and Bunne, C. (2023). Aligned diffusion Schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence.   \nSong, J., Meng, C., and Ermon, S. (2021a). Denoising diffusion implicit models. In International Conference on Learning Representations.   \nSong, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems.   \nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021b). Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations.   \nSu, X., Song, J., Meng, C., and Ermon, S. (2023). Dual diffusion implicit bridges for image-to-image translation. In International Conference on Learning Representations.   \nTamir, E., Trapp, M., and Solin, A. (2023). Transport with support: Data-conditional diffusion bridges. Transactions on Machine Learning Research.   \nThornton, J., Hutchinson, M., Mathieu, E., De Bortoli, V., Teh, Y. W., and Doucet, A. (2022). Riemannian diffusion Schr\u00f6dinger bridge. arXiv preprint arXiv:2207.03024.   \nTong, A., Fatras, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., and Bengio, Y. (2024a). Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research.   \nTong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G., and Bengio, Y. (2024b). Simulation-free Schr\u00f6dinger bridges via score and flow matching. In International Conference on Artificial Intelligence and Statistics.   \nVargas, F., Padhy, S., Blessing, D., and N\u00fcsken, N. (2024). Transport meets variational inference: Controlled Monte Carlo diffusions. In International Conference on Learning Representations.   \nVargas, F., Thodoroff, P., Lamacraft, A., and Lawrence, N. (2021). Solving Schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134.   \nYang, K., Tao, J., Lyu, J., Ge, C., Chen, J., Li, Q., Shen, W., Zhu, X., and Li, X. (2023). Using human feedback to fine-tune diffusion models without any reward model. arXiv preprint arXiv:2311.13231.   \nZaj a\u02dbc, M., Deja, K., Kuzina, A., Tomczak, J. M., Trzci\u00b4nski, T., Shkurti, F., and Mi\u0142o\u00b4s, P. (2023). Exploring continual learning of diffusion models. arXiv preprint arXiv:2303.15342.   \nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. (2018). The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 586\u2013595.   \nZhou, T., Wang, W., Konukoglu, E., and Van Gool, L. (2022). Rethinking semantic segmentation: A prototype view. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix organisation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The supplementary material is organised as follows. First in Appendix B, we analyze an Euclidean counterpart to the $\\alpha$ -IMF sequence identified in Section 3 and the associated flow. In Appendix C, we show that the Markovian projection can be recovered as the parameterisation of the vector field that minimises the accumulation of errors, extending the results of Chen et al. (2024). Theoretical results are gathered in Appendix D. In particular in Appendix D.1 we show that the proposed non-parametric method coincides with the $\\alpha$ -IMF and prove the convergence of the $\\alpha$ -IMF. In Appendix D.2, we show the connection between the non-parametric and the parametric updates. In Appendix E, we provide more background on DSBM and propose an extension of the DSBM methodology. Consistency losses similar to (Daras et al., 2023b; De Bortoli et al., 2024) are proposed in Appendix F. Model stitching procedures are described in Appendix G. We comment on extended related work in Appendix H. In particular we draw connections with Sinkhorn flows (Karimi et al., 2024), Reinforcement Learning policies, Expectation-Maximisation schemes following (Brekelmans and Neklyudov, 2023) and comment on finetuning of diffusion models. In Appendix I, we investigate the accumulation of bias in a Gaussian setting and compare forward-forward and forward-backward methods. In Appendix J, we derive the preconditioning of loss following the principles of (Karras et al., 2022) in the case of bridge matching. Additional results and experimental details are presented in Appendix K. ", "page_idx": 15}, {"type": "text", "text": "B Euclidean flow and iterative procedure ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we study a simplified counterpart of the Schr\u00f6dinger flow and of DSBM in a Euclidean setting. The goal of this section is to draw some conclusions in the Euclidean case which also remain true empirically when analyzing the Schr\u00f6dinger Bridge problem. ", "page_idx": 15}, {"type": "text", "text": "We consider the set $\\mathsf{A}_{1}\\,=\\,\\{(x,y)\\,\\in\\,\\mathbb{R}^{2}\\;:\\;y\\,\\geq\\,x\\}$ and the set $\\mathsf{A}_{2}\\,=\\,\\{(x,y)\\,\\in\\,\\mathbb{R}^{2}\\;:\\;y\\,\\leq\\,0\\}$ . Loosely speaking, one can identify $\\mathsf{A}_{1}$ with the reciprocal class $\\mathcal{R}(\\mathbb{Q})$ and $\\mathsf{A}_{2}$ with the set of Markov path measures $\\mathrm{proj}_{\\mathcal{M}}$ . In that case, we have that for any $(x,\\dot{y})\\in\\mathbb{R}^{2}$ , $\\mathrm{proj}_{\\mathsf{A_{1}}}((x,y))=$ $((x+y)/2,(x+y)/2)$ if $(\\dot{x},\\dot{y})\\not\\in\\mathsf{A}_{1}$ and otherwise, $\\mathrm{proj}_{\\mathsf{A}_{1}}((x,y))\\,=\\,(x,y)$ . In addition, we have that for any $(x,y)\\in\\mathbb{R}^{2}$ , $\\mathrm{proj}_{\\mathsf{A}_{2}}((x,y))=(x,0)$ if $(x,y)\\notin{\\mathsf{A}}_{2}$ and pro $\\boldsymbol{\\dot{\\vert}}_{\\mathrm{A_{2}}}((x,y))=(x,y)$ otherwise. We consider the following flow $(x_{t},y_{t})_{t\\geq0}$ given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}(x_{t},y_{t})=\\mathrm{proj}_{\\mathsf{A}_{1}}(\\mathrm{proj}_{\\mathsf{A}_{2}}((x_{t},y_{t})))-(x_{t},y_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $(x_{0},y_{0})\\notin\\mathsf{A}_{1}$ and $(x_{0},y_{0})\\notin\\mathsf{A_{2}}$ . Denote $T$ the explosion time of $\\left({{x}_{t}},{{y}_{t}}\\right)$ , i.e. for any $t\\geq T$ we have that $(x_{t},y_{t})=\\infty$ , where $\\mathbb{R}^{2}\\cup\\{\\infty\\}$ is the one-point compactification of $\\mathbb{R}^{2}$ . Finally, denote $\\tau\\leq T$ such that for any $t\\in[0,\\tau]$ , $(x_{t},y_{t})\\not\\in{\\mathsf{A}}_{1}$ and $\\bar{(x_{t},y_{t})}\\not\\in\\bar{\\mathsf{A}}_{2}$ . Then, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}(x_{t},y_{t})=(-x_{t}/2,x_{t}/2-y_{t}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we have that $x_{t}~~=~~x_{0}\\exp[-t/2]$ for any $t\\ \\ \\in\\ \\ [0,\\tau]$ and $\\begin{array}{r c l}{y_{t}}&{=}&{x_{0}\\exp[-t/2]\\ +}\\end{array}$ $(x_{0}\\exp[-t/2])^{2}(y_{0}\\mathrm{~-~}x_{0})/x_{0}^{2}$ . Therefore, we get that $\\tau\\,=\\,T\\,=\\,+\\infty$ and we have that for any $t\\geq0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{t}=x_{0}\\exp[-t/2],\\qquad y_{t}=x_{t}+x_{t}^{2}(y_{0}-x_{0})/x_{0}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, $((x_{t},y_{t}))_{t\\geq0}$ converges exponentially fast to $(0,0)$ with rate $1/2$ . ", "page_idx": 15}, {"type": "text", "text": "We now investigate the rate of convergence of the alternate projection scheme, i.e. the Euclidean equivalent of DSBM. We define $((x_{n},\\bar{y}_{n}))_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n(x_{n+1},y_{n+1})=\\operatorname{proj}_{\\mathsf{A}_{1}}(\\operatorname{proj}_{\\mathsf{A}_{2}}((x_{n},y_{n})))=(x_{n}/2,0).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we get that $x_{n}=x_{0}2^{-n}$ and therefore $((x_{n},y_{n}))_{n\\in\\mathbb{N}}$ converges exponentially fast to $(0,0)$ .   \nNote that this procedure corresponds to a discretisation of the flow $((x_{t},y_{t}))_{n\\in\\mathbb{N}}$ with stepsize $\\alpha=1$ . ", "page_idx": 15}, {"type": "text", "text": "More generally, we define for any $\\alpha\\in(0,1],((x_{n}^{\\alpha},y_{n}^{\\alpha}))_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n(x_{n+1}^{\\alpha},y_{n+1}^{\\alpha})=\\alpha\\mathrm{proj}_{\\mathsf{A}_{1}}(\\mathrm{proj}_{\\mathsf{A}_{2}}((x_{n}^{\\alpha},y_{n}^{\\alpha})))+(1-\\alpha)(x_{n}^{\\alpha},y_{n}^{\\alpha}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we get that $x_{n}=x_{0}2^{-n}$ and therefore $((x_{n},y_{n}))_{n\\in\\mathbb{N}}$ converges exponentially fast to $(0,0)$ . It can be shown that for any $n\\in\\mathbb{N}$ , $x_{n}^{\\alpha}=x_{0}^{\\alpha}(1-\\alpha/2)^{n}$ and in addition, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\ny_{n}^{\\alpha}=(1-\\alpha)^{n}y_{0}^{\\alpha}+\\alpha x_{0}^{\\alpha}\\sum_{k=0}^{n-1}(1-\\alpha)^{k}(1-\\alpha/2)^{n-k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we get that ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{n}=(1-\\alpha)^{n}y_{0}^{\\alpha}+2(1-(1-\\alpha/(2-\\alpha))^{n})(1-\\alpha/2)^{n}x_{0}^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now analyse the complexity of the different discretisations assuming that the cost of discretising the flow with stepsize $\\alpha\\,\\in\\,(0,1]$ is $C^{\\alpha}$ . In that case in order to reach the threshold value $\\varepsilon$ , i.e. $|x_{n}^{\\alpha}|\\leq\\varepsilon$ , we get a total cost $C_{n}^{\\alpha}=O(\\log(1/\\varepsilon)C^{\\alpha}/\\log(1/(1-\\alpha/2))$ ), where we have neglected the terms that do not depend on $\\log(1/\\varepsilon)$ . Hence, if $C^{\\alpha}$ is constant then the choice $\\alpha=1$ is the best possible one in the range $\\alpha\\in(0,1]$ . Otherwise, one has to consider the ratio $C^{\\alpha}/\\log(1/(1-\\alpha/2))$ , where the lower is the better. The flow procedure and the iterative based one are presented in Figure 1. ", "page_idx": 16}, {"type": "text", "text": "Based on this simplified Euclidean experiment, we draw some conclusions which also remain true in our setting, see Appendix K for more experimental details. First, we have that different discretisations of the flow yield different convergence rates. Large stepsizes incur faster convergence. This suggests to choose $\\alpha=1$ . However, if the cost of choosing $\\alpha=1$ is too high then one might turn to alternative schemes with $\\alpha\\in(0,1)$ assuming that $C^{\\alpha}<\\bar{C}^{1}$ in that case. To draw a parallel with our setting, in the case of DSBM (case $\\alpha=1$ ), we need to solve the projection subproblem at each step which incurs a great cost. On the other hand, one step of the online algorithm only requires sampling once from the model and performing one gradient step. ", "page_idx": 16}, {"type": "text", "text": "C Minimisation of errors and Markovian projection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For a given non-Markovian (stochastic) interpolant process (see definition below), there exist an infinite number of Markov processes admitting the same marginals (Albergo and Vanden-Eijnden, 2023). In this section, when it is well-defined, we show that the Markovian projection corresponds to the process which minimises an error measure (defined further) in case one has access to the oracle of $\\bar{x_{t}}\\mapsto\\mathbb{E}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]$ . ", "page_idx": 16}, {"type": "text", "text": "Stochastic Interpolant. We first start by recalling the framework of Albergo and Vanden-Eijnden (2023). Consider a coupling $\\Pi$ between $\\pi_{0}$ and $\\pi_{1}$ , one builds a (stochastic) flow between $\\pi_{0}$ and $\\pi_{1}$ using the following interpolation procedure ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}=\\mathrm{Interp}_{t}(\\mathbf{X}_{0},\\mathbf{X}_{1},\\mathbf{Z})=\\alpha_{t}\\mathbf{X}_{0}+\\beta_{t}\\mathbf{X}_{1}+\\gamma_{t}\\mathbf{Z},\\quad(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim\\Pi,\\quad\\mathbf{Z}\\sim\\mathcal{N}(0,\\mathrm{Id}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha_{1}=\\beta_{0}=\\gamma_{0}=\\gamma_{1}=0$ and $\\alpha_{0}=\\beta_{1}=1$ . This defines a non-Markovian process. We denote by $\\pi_{t}$ the induced unconditional distribution of $\\mathbf{X}_{t}$ . Let us now consider the Markov process $(\\mathbf{X}_{t}^{\\varepsilon})$ given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}{\\bf X}_{t}^{\\varepsilon}=\\mathbb{E}\\left[\\dot{\\alpha}_{t}{\\bf X}_{0}+\\dot{\\beta}_{t}{\\bf X}_{1}+(\\dot{\\gamma}_{t}-\\varepsilon_{t}^{2}/(2\\gamma_{t})){\\bf Z}\\mid{\\bf X}_{t}={\\bf X}_{t}^{\\varepsilon}\\right]+\\varepsilon_{t}\\mathrm{d}{\\bf B}_{t},\\qquad{\\bf X}_{0}^{\\varepsilon}\\sim\\boldsymbol{\\pi}_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(\\mathbf{B}_{t})_{t\\in[0,1]}$ is a $d$ -dimensional Brownian motion and $\\varepsilon_{t}$ is an additional hyperparameter. It can then be shown that $(\\mathbf{X}_{t}^{\\varepsilon})_{t\\in[0,1]}$ satisfies that ${\\bf X}_{t}^{\\varepsilon}\\sim\\pi_{t}$ for all $t\\in[0,1]$ ; see e.g. (Albergo et al., 2023, Theorem 2.8, Corollary 2.10). Hence $(\\mathbf{X}_{t}^{\\varepsilon})_{t\\in[0,1]}$ is a (stochastic) flow mapping $\\pi_{0}$ onto $\\pi_{1}$ . Note that $(\\mathbf{X}_{t}^{\\varepsilon})_{t\\in[0,1]}$ in (14) can be rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\sf{I X}}_{t}^{\\varepsilon}=(\\dot{\\alpha}_{t}/\\alpha_{t}){\\bf{X}}_{t}^{\\varepsilon}+\\mathbb{E}\\left[(\\dot{\\beta}_{t}-\\beta_{t}\\dot{\\alpha}_{t}/\\alpha_{t}){\\bf{X}}_{1}+(\\dot{\\gamma}_{t}-\\gamma_{t}\\dot{\\alpha}_{t}/\\alpha_{t}-\\varepsilon_{t}^{2}/(2\\gamma_{t})){\\bf{Z}}\\mid{\\bf{X}}_{t}={\\bf{X}}_{t}^{\\varepsilon}\\right]+\\varepsilon_{t}\\mathrm{{d}}{\\bf{B}}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the specific case where $\\alpha_{t}=1-t$ , $\\beta_{t}=t$ and $\\gamma_{t}=\\sigma_{0}\\sqrt{t(1-t)}$ then (15) becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf X}_{t}=\\mathrm{Interp}_{t}({\\bf X}_{0},{\\bf X}_{1},{\\bf Z})=(1-t){\\bf X}_{0}+t{\\bf X}_{1}+\\sigma_{0}\\sqrt{t(1-t)}{\\bf Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This corresponds to the marginal distribution of the bridge associated with $(\\sigma_{0}\\mathbf{B}_{t})_{t\\in[0,1]}$ . In this case, (15) becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}^{\\varepsilon}=\\mathbb{E}\\left[(\\mathbf{X}_{1}-\\mathbf{X}_{t})/(1-t)|\\mathbf{X}_{t}=\\mathbf{X}_{t}^{\\varepsilon}\\right]\\mathrm{d}t+\\sqrt{2}\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $\\varepsilon_{t}^{2}=(2\\gamma_{t})(\\dot{\\gamma}_{t}-\\gamma_{t}\\dot{\\alpha}_{t}/\\alpha_{t})=2\\sigma_{0}^{2}$ . In Proposition C.1, we will show that this choice of $(\\varepsilon_{t})_{t\\in[0,1]}$ is optimal in some sense. ", "page_idx": 16}, {"type": "text", "text": "Consider $(\\hat{\\mathbf{X}}_{t}^{\\varepsilon})_{t\\in[0,1]}$ given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{\\mathbf{X}}_{t}^{\\varepsilon}=(\\dot{\\alpha}_{t}/\\alpha_{t})\\hat{\\mathbf{X}}_{t}^{\\varepsilon}+(\\dot{\\beta}_{t}-\\beta_{t}\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbb{E}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=\\hat{\\mathbf{X}}_{t}^{\\varepsilon}]+(\\dot{\\gamma}_{t}-\\gamma_{t}\\dot{\\alpha}_{t}/\\alpha_{t}-\\varepsilon_{t}^{2}/(2\\gamma_{t}))\\hat{\\mathbf{Z}}(t,\\hat{\\mathbf{X}}_{t}^{\\varepsilon})+\\varepsilon_{t}\\mathrm{d}\\mathbf{B}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\hat{\\mathbf{X}}_{0}^{\\varepsilon}\\sim\\pi_{0}$ and where $\\hat{\\mathbf{Z}}(t,x)$ is an approximation of $\\mathbb{E}[\\mathbf{Z}|\\mathbf{X}_{t}=x]$ . We have the following result. ", "page_idx": 17}, {"type": "text", "text": "Proposition C.1 (Optimality and stochastic interpolant): Denote $\\mathbb{P}^{\\varepsilon}$ , respectively $\\hat{\\mathbb{P}}^{\\varepsilon}$ the path measures associated with $(\\mathbf{X}_{t}^{\\varepsilon})_{t\\in[0,1]}$ and $(\\hat{\\mathbf{X}}_{t})_{t\\in[0,1]}$ respectively. Consider $\\ell(\\varepsilon)=\\mathrm{KL}(\\mathbb{P}^{\\varepsilon}|\\hat{\\mathbb{P}}^{\\varepsilon})$ . Let $\\varepsilon^{\\star}=\\mathrm{argmin}_{\\varepsilon}\\ell(\\varepsilon)$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\varepsilon_{t}^{\\star}\\right)^{2}=2\\gamma_{t}\\dot{\\gamma}_{t}-2\\gamma_{t}^{2}\\dot{\\alpha}_{t}/\\alpha_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In particular, $i f\\alpha_{t}=t,\\,\\beta_{t}=1-t$ and $\\gamma_{t}=\\sigma_{0}\\sqrt{t(1-t)},$ , then $\\varepsilon_{t}=\\sqrt{2}\\sigma_{0}$ . The value $(\\varepsilon_{t}^{\\star})_{t\\in[0,1]}$ corresponds to Markovian projection when it is well defined. ", "page_idx": 17}, {"type": "text", "text": "Proof. We have that for any ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mathbb{P}^{\\varepsilon}|\\hat{\\mathbb{P}}^{\\varepsilon})=\\int_{0}^{1}\\frac{1}{\\varepsilon_{t}^{2}}(\\dot{\\gamma}_{t}-\\gamma_{t}\\dot{\\alpha}_{t}/\\alpha_{t}-\\varepsilon_{t}^{2}/(2\\gamma_{t}))^{2}\\mathbb{E}[\\Delta_{t}]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Delta_{t}\\:=\\:\\lVert\\hat{\\mathbf{Z}}(t,\\mathbf{X}_{t}^{\\varepsilon})\\,-\\,\\mathbb{E}[\\mathbf{Z}\\mid\\mathbf{X}_{t}\\,=\\,\\mathbf{X}_{t}^{\\varepsilon}]\\rVert^{2}$ and the expectation is w.r.t. $\\mathbb{P}^{\\varepsilon}$ . We have that $\\mathrm{KL}(\\mathbb{P}^{\\varepsilon}|\\hat{\\mathbb{P}}^{\\varepsilon})=0$ if $\\varepsilon=\\varepsilon^{\\star}$ , which concludes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proposition C.1 is related to (Chen et al., 2024, Section 3.4). Therein it is noticed that, in the case of Augmented Bridge matching (De Bortoli et al., 2023), the choice of $\\varepsilon_{t}$ does not affect the joint distribution of $(\\mathbf{X}_{0}^{\\varepsilon},\\mathbf{X}_{1}^{\\varepsilon})$ . The authors then select $\\left(\\varepsilon_{t}\\right)$ so as to minimise an approximation error. They show that, in that case, they recover the F\u00f6llmer process. ", "page_idx": 17}, {"type": "text", "text": "We now show that Proposition C.1 can be further strengthened to establish that $\\varepsilon^{\\star}$ is also the optimal value if we interpolate between $\\pi_{s}$ and $\\pi_{1}$ , or $\\pi_{0}$ and $\\pi_{s}$ , for any $s\\in[0,1]$ and $\\pi_{s}$ the distribution of $\\mathbf{X}_{s}$ . Consider in this context for any $s,t\\in[0,1]$ with $t\\geq s$ , $\\gamma_{t}/\\gamma_{s}\\,\\geq\\,\\alpha_{t}\\,\\geq\\,\\alpha_{s}$ the following interpolation model. ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf X}_{t}=(\\alpha_{t}/\\alpha_{s}){\\bf X}_{s}+(\\beta_{t}-\\alpha_{t}\\beta_{s}/\\alpha_{s}){\\bf X}_{1}+\\sqrt{\\gamma_{t}^{2}-\\alpha_{t}^{2}\\gamma_{s}^{2}/\\alpha_{s}^{2}}{\\bf Z},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\bf X}_{s}\\sim\\pi_{s}$ , ${\\bf X}_{1}\\sim\\pi_{1}$ and $\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id})$ . Assume that $\\alpha_{t}=1\\!-\\!t$ , $\\beta_{t}=t$ and $\\gamma_{t}=\\sigma_{0}\\sqrt{t(1-t)}$ for any $t\\in[0,1]$ then (16) corresponds to the Brownian bridge associated with $(\\sigma_{0}\\mathbf{B}_{t})_{t\\in[0,1]}$ with endpoints $\\mathbf{X}_{s}$ at time $s$ and $\\mathbf{X}_{1}$ at time 1. We have the following proposition. ", "page_idx": 17}, {"type": "text", "text": "Proposition C.2 (Stochastic interpolant with intermediate time points): Define $(\\mathbf{X}_{t,s}^{\\varepsilon})_{t\\in[s,1]}$ given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathbf{X}_{t,s}^{\\varepsilon}=(\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbf{X}_{t,s}^{\\varepsilon}+\\mathbb{E}\\left[(\\dot{\\beta}_{t}-\\beta_{t}\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=\\mathbf{X}_{t,s}^{\\varepsilon}\\right]\\qquad\\qquad\\qquad(1,\\varepsilon)}\\\\ &{\\qquad\\qquad+\\mathbb{E}\\left[(\\dot{\\gamma}_{t,s}-\\gamma_{t,s}\\dot{\\alpha}_{t}/\\alpha_{t}-\\varepsilon_{t,s}^{2}/(2\\gamma_{t,s}))\\mathbf{Z}\\mid\\mathbf{X}_{t}=\\mathbf{X}_{t,s}^{\\varepsilon}\\right]+\\varepsilon_{t,s}\\mathrm{d}\\mathbf{B}_{t},\\quad\\mathbf{X}_{s,s}^{\\varepsilon}\\sim\\pi_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\gamma_{t,s}=\\sqrt{\\gamma_{t}^{2}-\\alpha_{t}^{2}\\gamma_{s}^{2}/\\alpha_{s}^{2}}$ . Then for any $t\\in[s,1]$ , $\\mathbf{X}_{t,s}^{\\varepsilon}$ and $\\mathbf{X}_{t}$ defined by (16) have the same distribution. ", "page_idx": 17}, {"type": "text", "text": "Proof. We let $s\\in[0,1]$ and $\\mathbf{X}_{1},\\mathbf{X}_{s}\\in\\mathbb{R}^{d}$ . From (16), we have directly that for any $t\\in[s,1]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=[(\\dot{\\alpha}_{t}/\\alpha_{s})\\mathbf{X}_{s}+(\\dot{\\beta}_{t}-\\dot{\\alpha}_{t}\\beta_{s}/\\alpha_{s})\\mathbf{X}_{1}+\\dot{\\gamma}_{t,s}\\mathbf{Z}]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id})$ . In addition, rearranging (16), we also have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf X}_{s}=(\\alpha_{s}/\\alpha_{t}){\\bf X}_{t}-(\\alpha_{s}\\beta_{t}/\\alpha_{t}-\\beta_{s}){\\bf X}_{1}-\\gamma_{t,s}(\\alpha_{s}/\\alpha_{t}){\\bf Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, by combining these two expressions, we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}=[(\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbf{X}_{t}+(\\dot{\\beta}_{t}-\\dot{\\alpha}_{t}\\beta_{t}/\\alpha_{s})\\mathbf{X}_{1}+(\\dot{\\gamma}_{t,s}-\\gamma_{t,s}(\\dot{\\alpha}_{t}/\\alpha_{t}))\\mathbf{Z}]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows that $(\\mathbf{X}_{t,s})_{t\\in[s,1]}$ given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathbf{X}_{t,s}=(\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbf{X}_{t,s}+(\\dot{\\beta}_{t}-\\beta_{t}\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbb{E}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=\\mathbf{X}_{t,s}]}\\\\ &{\\qquad\\qquad+\\left(\\dot{\\gamma}_{t,s}-\\gamma_{t,s}\\dot{\\alpha}_{t}/\\alpha_{t}\\right)\\!\\mathbb{E}[\\mathbf{Z}\\mid\\mathbf{X}_{t}=\\mathbf{X}_{t,s}],\\quad\\mathbf{X}_{s,s}\\sim\\boldsymbol{\\pi}_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is such that for any $t\\in[s,1]$ the same distribution as $\\mathbf{X}_{t}$ defined by (16). Then, we conclude similarly to (Albergo et al., 2023, Theorem 2.8, Corollary 2.10). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "We now consider the following approximate version of (17) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}{\\hat{\\mathbf{X}}}_{t,s}^{\\varepsilon}=(\\dot{\\alpha}_{t}/\\alpha_{t})\\hat{\\mathbf{X}}_{t,s}^{\\varepsilon}+(\\dot{\\beta}_{t}-\\beta_{t}\\dot{\\alpha}_{t}/\\alpha_{t})\\mathbb{E}\\left[{\\mathbf{X}}_{1}\\mid\\mathbf{X}_{t}=\\hat{\\mathbf{X}}_{t,s}^{\\varepsilon}\\right]}\\\\ &{\\qquad\\qquad+\\left(\\dot{\\gamma}_{t,s}-\\gamma_{t,s}\\dot{\\alpha}_{t}/\\alpha_{t}-\\varepsilon_{t,s}^{2}/(2\\gamma_{t,s})\\right)\\hat{\\mathbf{Z}}(t,\\mathbf{X}_{t,s}^{\\varepsilon})+\\varepsilon_{t,s}\\mathrm{d}\\mathbf{B}_{t},\\quad\\mathbf{X}_{s,s}^{\\varepsilon}\\sim\\boldsymbol{\\pi}_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly to Proposition C.1 we consider the best choice of $\\varepsilon$ to minimise the interpolation cost. ", "page_idx": 18}, {"type": "text", "text": "Proposition C.3 (Optimality and stochastic interpolant): Let $s\\in[0,1]$ . Denote $\\mathbb{P}^{\\varepsilon}$ , respectively $\\hat{\\mathbb{P}}^{\\varepsilon}$ , the path measure associated with $(\\mathbf{X}_{t,s}^{\\varepsilon})_{t\\in[0,1]}$ , respectively $(\\hat{\\mathbf{X}}_{t,s})_{t\\in[0,1]}$ . Consider $\\ell(\\varepsilon)=$ $\\mathrm{KL}(\\mathbb{P}^{\\varepsilon}|\\hat{\\mathbb{P}}^{\\varepsilon})$ . Let $\\varepsilon^{\\star}=\\mathrm{argmin}_{\\varepsilon}\\ell(\\varepsilon)$ . Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\varepsilon_{t}^{\\star}\\right)^{2}=2\\gamma_{t}\\dot{\\gamma}_{t}-2\\gamma_{t}^{2}\\dot{\\alpha}_{t}/\\alpha_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, $\\varepsilon^{\\star}$ does not depend on $s\\in[0,1]$ and for every $s_{1},s_{2}\\in[0,1]$ with $s_{1}\\leq s_{2}$ , we have that $(\\mathbf{X}_{t,s_{1}}^{\\varepsilon})_{t\\in[s_{2},1]}$ and $(\\mathbf{X}_{t,s_{2}}^{\\varepsilon})_{t\\in[s_{2},1]}$ coincide. ", "page_idx": 18}, {"type": "text", "text": "Proof. Similarly to Proposition C.1, we get first that for any $s,t\\in[0,1]$ with $s\\leq t$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varepsilon_{t,s}^{\\star}=2\\gamma_{t,s}\\dot{\\gamma}_{t,s}-2\\gamma_{t,s}^{2}\\dot{\\alpha}_{t}/\\alpha_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Second, we have that for any $s,t\\in[0,1]$ with $s\\leq t$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\dot{\\gamma}_{t,s}\\gamma_{t,s}=\\dot{\\gamma}_{t,s}^{2}=2\\dot{\\gamma}_{t}\\gamma_{t}-2\\dot{\\alpha}_{t}\\alpha_{t}\\gamma_{s}^{2}/\\alpha_{s}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Third, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma_{t,s}^{2}\\dot{\\alpha}_{t}/\\alpha_{t}=\\gamma_{t}^{2}\\dot{\\alpha}_{t}/\\alpha_{t}-\\dot{\\alpha}_{t}\\alpha_{t}\\gamma_{s}^{2}/\\alpha_{s}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining (18), (19) and (20), we can conclude. ", "page_idx": 18}, {"type": "text", "text": "D Theoretical results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we prove the main theoretical results of the paper. In Appendix D.1, we first prove the convergence of the $\\alpha$ -IMF sequence, i.e. we prove Theorem 3.1. Second, we show that the non-parametric updates (8) correspond to the $\\alpha$ -IMF sequence, i.e. we prove Proposition 3.2. In Appendix D.2, we link the non-parametric updates to the parametric updates. ", "page_idx": 18}, {"type": "text", "text": "D.1 Non-parametric sequence and convergence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{C})$ be associated with $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ , where $(\\mathbf{B}_{t})_{t\\in[0,1]}$ is a $d$ -dimensional Brownian motion and $\\varepsilon>0$ . In this section, we abuse notation and denote $\\mathcal{P}(\\mathcal{C})$ the set of path measures which are not\u221a necessarily probability path measures. In particular, we will consider $\\mathbb{Q}\\in{\\mathcal{P}}({\\mathcal{C}})$ associated with $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ with $\\mathbb{Q}_{0}=\\mathrm{Leb}$ . In that case, the Kullback\u2013Leibler divergence is still well-defined and we refer to (L\u00e9onard, 2014) for more details. We recall that we have defined $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ for any $n\\in\\mathbb N$ and $\\alpha\\in(0,1]$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}^{n}=\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}),\\qquad\\hat{\\mathbb{P}}^{n+1}=(1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In addition, for any $n\\in\\ensuremath{\\mathbb{N}},\\,t\\in[0,1)$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ we have defined ", "page_idx": 18}, {"type": "equation", "text": "$$\nv_{t}^{n+1}(x)=v_{t}^{n}(x)-\\delta_{n}\\nabla_{\\mu^{n}}\\mathcal{L}_{t}(v_{t}^{n},\\mathbb{P}_{v^{n}})(x),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{L}}_{t}(v_{t},{\\mathbb{P}})=\\displaystyle\\frac{1}{2}\\int_{({\\mathbb{R}}^{d})^{3}}\\Big\\|v_{t}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\Big\\|^{2}\\mathrm{d}{\\mathbb{P}}_{0,1}(x_{0},x_{1})\\mathrm{d}{\\mathbb{Q}}_{t\\mid0,1}(x_{t}\\vert x_{0},x_{1})}}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{1}{2}\\int_{({\\mathbb{R}}^{d})^{3}}\\Big\\|v_{t}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\Big\\|^{2}\\mathrm{d}\\mathrm{proj}_{{\\mathcal{R}}}({\\mathbb{P}})_{1,t}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We define $(\\mathbb{P}_{v^{n}})_{n\\in\\mathbb{N}}$ associated with (27), where for any suitable vector field $v$ , $\\mathbb{P}_{v}$ is associated with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=v_{t}(\\mathbf{X}_{t})\\mathrm{d}t+\\sqrt{\\varepsilon}\\mathrm{d}\\mathbf{B}_{t},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(\\mathbf{B}_{t})_{t\\in[0,1]}$ is a $d$ -dimensional Brownian motion. ", "page_idx": 19}, {"type": "text", "text": "In order to rigorously prove Proposition D.1 detailed further, we introduce $\\mathcal{P}_{2}(\\mathscr{C})$ , such that $\\mathbb{P}\\in\\mathscr{P}_{2}(\\mathscr{C})$ if $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ and for ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{(\\mathbb{R}^{d})^{2}}\\{\\|x_{0}\\|^{2}+\\|x_{1}\\|^{2}\\}\\mathrm{d}\\mathbb{P}_{0,1}(x_{0},x_{1})<+\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that if $\\mathbb{P}\\in\\mathscr{P}_{2}(\\mathscr{C})$ then we have that for any $t\\in[0,1]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\|x_{t}\\|^{2}\\mathrm{dproj}_{\\mathcal{R}}(\\mathbb{P})_{t}<+\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition, we recall that $\\phi\\in\\operatorname{L}^{2}(\\mu)$ for $\\mu\\in\\mathcal{P}(\\mathbb{R}^{d})$ if $\\phi:\\,\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\|\\phi(x)\\|^{2}\\mathrm{d}\\mu(x)<+\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{A}_{2}=\\{(\\phi,\\mathbb{P})\\ :\\ \\mathbb{P}\\in\\mathcal{P}_{2}(\\mathscr{C}),\\ \\phi\\in\\mathrm{L}^{2}(\\mathbb{P})\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then for any $t\\in[0,1)$ , we define $\\mathcal{L}_{t}:\\,\\mathsf{A}_{2}\\to\\mathbb{R}$ given for any $(v,\\mathbb{P})\\in\\mathsf{A}_{2}$ by (21). ", "page_idx": 19}, {"type": "text", "text": "Proposition D.1 (Non-parametric updates are $\\alpha$ -IMF): Let $\\alpha\\in(0,1],$ , $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ as in (4), $\\delta_{n}=\\alpha$ and $\\mu^{n}=(1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{n})$ . Assume that for any $n\\in\\mathbb{N},\\mathbb{P}_{v^{n}}$ is well-defined. Then, for any $n\\in\\mathbb N$ , $\\mathbb{P}_{v^{n}}=\\mathbb{P}^{n}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. First, we have that for any $t\\in[0,1)$ , $v,\\mathbb{P}\\in\\mathsf{A}_{2}$ and $\\phi\\in\\mathrm{L}^{2}(\\mathbb{P}_{t})$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t}(v_{t}+\\varepsilon\\phi,\\mathbb{P})=\\mathcal{L}_{t}(v_{t},\\mathbb{P})+\\varepsilon\\int_{(\\mathbb{R}^{d})^{3}}\\langle\\phi(x_{t}),v_{t}(x_{t})-\\frac{x_{1}-x_{1}}{1-t}\\rangle\\mathrm{d}\\mathbb{P}_{0,1}(x_{0},x_{1})\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1})}\\\\ &{\\qquad\\qquad\\qquad+(\\varepsilon^{2}/2)\\int_{(\\mathbb{R}^{d})^{3}}\\|\\phi(x_{t})\\|^{2}\\mathrm{d}\\mathbb{P}_{0,1}(x_{0},x_{1})\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1})}\\\\ &{\\qquad=\\mathcal{L}_{t}(v_{t},\\mathbb{P})+\\varepsilon\\int_{(\\mathbb{R}^{d})^{2}}\\langle\\phi(x_{t}),v_{t}(x_{t})}\\\\ &{\\qquad\\qquad-\\left(\\int_{\\mathbb{R}^{d}}x_{1}\\mathrm{d}\\mathrm{proj}_{\\mathbb{R}}(\\mathbb{P})_{1\\mid t}(x_{1}|x_{t})-x_{t}\\right)/(1-t)\\rangle\\mathrm{d}\\mathrm{proj}_{\\mathbb{R}}(\\mathbb{P})_{t}(x_{t})}\\\\ &{\\qquad\\qquad+(\\varepsilon^{2}/2)\\int_{\\mathbb{R}^{d}}\\|\\phi(x_{t})\\|^{2}\\mathrm{proj}_{\\mathbb{R}}(\\mathbb{P})_{t}(x_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mu}\\mathcal{L}_{t}(v_{t},\\mathbb{P}_{v})(x_{t})=\\left(v_{t}(x_{t})-\\left(\\mathbb{E}_{\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P})}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t}\\right)/(1-t)\\right)(\\mathrm{dproj}_{\\mathcal{R}}(\\mathbb{P}_{v})_{t}/\\mathrm{d}\\mu_{t})(x_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Assume that for some $n\\,\\in\\,\\mathbb{N}$ we have that for any $t\\,\\in\\,[0,1)$ and $\\boldsymbol{x}_{t}\\,\\in\\,\\mathbb{R}^{d}$ , we have $v_{t}^{k}(x_{t})\\stackrel{!}{=}$ $\\left(\\mathbb{E}_{\\widehat{\\mathbb{P}}^{k}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t}\\right)/(1-t)$ . We are going to show that for any $t\\in[0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , we have $v_{t}^{n+1}(x_{t})=\\left(\\mathbb{E}_{\\hat{\\mathbb{P}}^{n+1}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t}\\right)/(1-t)$ . For any $t\\in[0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , we denote ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\delta}_{t}^{n}(\\boldsymbol{x}_{t})=\\delta_{n}(\\mathrm{dproj}_{\\mathcal{R}}(\\mathbb{P}^{n})_{t}/\\mathrm{d}\\mu_{t}^{n})(\\boldsymbol{x}_{t}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since we have that $\\delta_{n}=\\alpha$ and $\\mu^{n}=(1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{n})$ , we obtain for any $t\\in[0,1]$ and xt \u2208Rd ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\delta}_{t}^{n}(x_{t})=\\alpha(\\mathrm{dproj}_{\\mathcal{R}}(\\mathbb{P}^{n})_{t}/\\mathrm{d}((1-\\alpha)\\hat{\\mathbb{P}}_{t}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{n})_{t})(x_{t}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\bar{\\delta}_{t}^{n}(x_{t})=(1-\\alpha)(\\mathrm{d}\\hat{\\mathbb{P}}_{t}^{n}/\\mathrm{d}((1-\\alpha)\\hat{\\mathbb{P}}_{t}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{n})_{t})(x_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, combining (8) with (23), (24), (22), we get that for any $t\\in[0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{v_{t}^{n+1}(x_{t})=(1-\\tilde{\\delta}_{t}^{n}(x_{t}))v_{t}^{n}(x_{t})}&{}\\\\ &{\\qquad+\\tilde{\\delta}_{t}^{n}(x_{t})(\\mathbb{R}_{p\\cap p_{t}(\\mathbb{R}^{n})}[\\Phi_{1}]\\left(\\mathbf{X}_{t}=x_{t}\\right)-x_{t})\\left/(1-t\\right)}\\\\ &{=(1-\\tilde{\\delta}_{t}^{n}(x_{t}))\\left(\\mathbb{E}_{\\bar{p}_{n}\\mid\\mathbf{X}_{t}}\\left|\\mathbf{X}_{t}-x_{t}\\right|-x_{t}\\right)/(1-t)}\\\\ &{\\qquad+\\tilde{\\delta}_{t}^{n}(x_{t})\\left(\\mathbb{E}_{\\mathbb{R}^{n}\\cap p_{t}(\\mathbb{R}^{n})}[\\mathbf{X}_{1}]\\left(\\mathbf{X}_{t}=x_{t}\\right|-x_{t}\\right)/(1-t)}\\\\ &{=(1-\\tilde{\\delta}_{t}^{n}(x_{t}))\\left(\\int_{\\mathbb{R}^{n}}x_{t}\\mathrm{d}\\hat{\\mathbf{r}}_{1}^{\\mathbb{H}}(x_{t})[x_{t}]_{t}-x_{t}\\right)/(1-t)}\\\\ &{\\qquad+\\tilde{\\delta}_{t}^{n}(x_{t})\\left(\\int_{\\mathbb{R}^{n}}x_{t}\\mathrm{d}\\hat{\\mathbf{r}}_{2}\\mathrm{e}(\\mathbb{P}^{n})_{1\\mid t}(x_{1}\\vert x_{t})-x_{t}\\right)/(1-t)}\\\\ &{\\qquad+\\tilde{\\delta}_{t}^{n}(x_{t})\\left(\\int_{\\mathbb{R}^{n}}x_{t}\\mathrm{d}\\hat{\\mathbf{r}}_{2}\\mathrm{e}(\\mathbb{P}^{n})_{1\\mid t}(x_{1}\\vert x_{t})-x_{t}\\right)/(1-t)}\\\\ &{=\\int_{\\mathbb{R}^{n}}(x_{1}-x_{t})/(1-t)\\mathrm{d}[(1-\\tilde{\\delta}_{t}^{n}(x_{t}))\\hat{\\mathbf{r}}_{1}^{\\mathbb{H}}_{t}+\\tilde{\\delta}_{t}^{n}(x_{t})\\mathrm{proj}_{\\mathbb{R}}(\\mathbb{P}^{n})_{1\\mid t}](x_{1}\\vert x_{t})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we have that for any $t\\in[0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , $v_{t}^{n+1}(x_{t})=\\left(\\mathbb{E}_{\\hat{\\mathbb{P}}^{n+1}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t}\\right)/(1-\\$ $t^{'}$ ). Since, for any $t\\in[0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , $v_{t}^{0}(x_{t})=\\left(\\mathbb{E}_{\\hat{\\mathbb{P}}^{0}}[{\\bf X}_{1}\\mid{\\bf X}_{t}=x_{t}]-x_{t}\\right/(1-t)$ by definition, we get that for any $n\\in\\mathbb{N}$ , $t\\in[0,1)$ and $x_{t}\\in\\mathbb{R}^{d}$ , $v_{t}^{n}(x_{t})\\,=\\,\\left(\\mathbb{E}_{\\hat{\\mathbb{P}}^{n}}[{\\bf X}_{1}\\mid{\\bf X}_{t}=x_{t}]-x_{t}\\right)/(1-t)$ . Using, Definition 2.2, we get that $\\mathbb{P}_{v^{n}}=\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})$ , which concludes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Before stating our convergence theorem, we show the following result which is a direct consequence of (L\u00e9onard, 2014, Theorem 2.12) and (L\u00e9onard et al., 2014, Theorem 2.14). We recall that the differential entropy of a probability measure $\\pi$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{H}(\\pi)=-\\int_{\\mathbb{R}^{d}}\\mathrm{log}((\\mathrm{d}\\pi/\\mathrm{d}\\mathrm{Leb})(x))\\mathrm{d}\\pi(x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "if $\\pi$ admits a density with respect to the Lebesgue measure and $+\\infty$ otherwise. ", "page_idx": 20}, {"type": "text", "text": "L\u221aemma D.2 (Characterisation of Schr\u00f6dinger Bridge): Recall that $\\mathbb{Q}$ is associated with $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ and assume that $\\mathbb{Q}_{0}=$ Leb. Let $\\pi_{0},\\pi_{1}\\in\\check{\\mathcal{P}}(\\mathbb{R}^{d})$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\|x\\|^{2}\\mathrm{d}\\pi_{i}(x)<+\\infty,\\qquad\\mathrm{H}(\\pi_{i})<+\\infty,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $i\\in\\{0,1\\}$ . Let $\\mathbb{P}^{\\star}$ such that $\\mathbb{P}^{\\star}$ is Markov, $\\mathbb{P}^{\\star}\\in\\mathcal{R}(\\mathbb{Q}),\\mathbb{P}_{0}^{\\star}=\\pi_{0}$ and $\\mathbb{P}_{1}^{\\star}=\\pi_{1}$ . Then $\\mathbb{P}^{\\star}$ is the Schr\u00f6dinger Bridge, i.e. the unique solution to (2). ", "page_idx": 20}, {"type": "text", "text": "Proof. First, we have that $\\mathbb{Q}_{0,1}$ is equivalent to Leb $\\otimes$ Leb. Indeed, we have that for any $x_{0},x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\mathrm{d}\\mathbb{Q}_{0,1}/\\mathrm{d}(\\mathrm{Leb}\\otimes\\mathrm{Leb}))(x_{0},x_{1})=(2\\pi\\varepsilon)^{-d/2}\\exp[-\\|x_{0}-x_{1}\\|^{2}/(2\\varepsilon)].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, we have that for any $t\\,\\in\\,(0,1)$ and $x_{t}\\,\\in\\,\\mathbb{R}^{d},\\,\\mathbb{Q}_{0,1|t}(\\cdot|x_{t})$ is equivalent to Leb $\\otimes$ Leb. Indeed, we have that for any $t\\in(0,1)$ and $x_{0},x_{t},x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{d}\\mathbb{Q}_{0,1\\mid t}(\\cdot|x_{t})/\\mathrm{d}(\\mathrm{Leb}\\otimes\\mathrm{Leb}))(x_{0},x_{1})=(2\\pi\\varepsilon t)^{-d/2}\\exp[-\\|x_{0}-x_{t}\\|^{2}/(2\\varepsilon t)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\ (2\\pi\\varepsilon(1-t))^{-d/2}\\exp[-\\|x_{t}-x_{1}\\|^{2}/(2\\varepsilon(1-t))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, for any $t\\in(0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , $\\mathbb{Q}_{0,1|t}(\\cdot|x_{t})$ is equivalent to $\\mathbb{Q}_{0,1}$ . Since $\\mathbb{P}^{\\star}$ is Markov and $\\mathbb{P}^{\\star}\\in\\mathcal{R}(\\mathbb{Q})$ we get that there exist $\\varphi_{0}^{\\circ}$ and $\\varphi_{1}^{\\star}$ which are Lebesgue measurable such that for any $\\omega\\in{\\mathcal{C}}$ we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\mathrm{d}\\mathbb{P}^{\\star}/\\mathrm{d}\\mathbb{Q})(\\omega)=\\varphi_{0}^{\\circ}(\\omega_{0})\\varphi_{1}^{\\star}(\\omega_{1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Second we verify that the conditions (i)-(vii) of (L\u00e9onard, 2014, Theorem 2.12) are satisfied. First, $\\mathbb{Q}$ is Markov and hence (i) is satisfied. Then, (ii) is satisfied since for any $t\\in(0,1)$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , ", "page_idx": 20}, {"type": "text", "text": "$\\mathbb{Q}_{0,1|t}(\\cdot|x_{t})$ is equivalent to $\\mathbb{Q}_{0,1}$ . We have that $\\mathbb{Q}_{0}=\\mathbb{Q}_{1}=\\mathrm{Leb}$ and (iii) is satisfied. We have that for any $x_{0},x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{d}\\mathbb{Q}_{0,1}/\\mathrm{d}(\\mathrm{Leb}\\otimes\\mathrm{Leb}))(x_{0},x_{1})=(2\\pi\\varepsilon)^{-d/2}\\exp[-\\|x_{0}-x_{1}\\|^{2}/(2\\varepsilon)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq(2\\pi\\varepsilon)^{-d/2}\\exp[-\\|x_{0}\\|^{2}/\\varepsilon-\\|x_{1}\\|^{2}/\\varepsilon].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, (iv) is satisfied and we let $A:\\,\\mathbb{R}^{d}\\to\\mathbb{R}_{+}$ be given for any $x\\in\\mathbb{R}^{d}$ by $A(x)=\\|x\\|^{2}/\\varepsilon$ . In addition, we have that for any $x_{0},x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{(\\mathbb{R}^{d})^{2}}\\exp[-\\|x_{0}\\|^{2}/\\varepsilon-\\|x_{1}\\|^{2}/\\varepsilon]\\mathrm{d}\\mathbb{Q}_{0,1}(x_{0},x_{1})<+\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, (v) is satisfied and we let $B:\\ \\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ given for any $x\\in\\mathbb{R}^{d}$ by $B(x)\\,=\\,\\|x\\|^{2}/\\varepsilon$ . By assumption (vi) and (vii) are satisfied. We conclude the proof upon using (L\u00e9onard, 2014, Theorem 2.12-(b)) and (25). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We are now ready to state our main convergence result. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.3 (Convergence of $\\alpha$ -IMF): Let $\\alpha\\in(0,1]$ and $(\\mathbb{P}^{n},\\hat{\\mathbb{P}}^{n})_{n\\in\\mathbb{N}}$ defined by (4). Under mild assumptions, we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathbb{P}^{n}=\\mathbb{P}^{\\star}}\\end{array}$ , where $\\mathbb{P}^{\\star}$ is the solution of the Schr\u00f6dinger Bridge problem (2). ", "page_idx": 21}, {"type": "text", "text": "Proof. Using the convexity of the Kullback\u2013Leibler divergence with respect to its first argument (see e.g. (Dupuis and Ellis, 2011)), the data processing inequality (see e.g. (Ambrosio et al., 2008, Lemma 9.4.5)), the fact that the Schr\u00f6dinger Bridge is Markov and in the reciprocal class of $\\mathbb{Q}$ (see e.g. (L\u00e9onard, 2014, Theorem 2.12) and (L\u00e9onard et al., 2014, Theorem 3.2)), and the Pythagorean theorem for the Markovian projection (Shi et al., 2023, Lemma 6), we have that for any $n\\in\\mathbb N$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\hat{\\mathbb{P}}^{n+1}|\\mathbb{P}^{\\star})=\\mathrm{KL}((1-\\alpha)\\hat{\\mathbb{P}}^{n}+\\alpha\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))|\\mathbb{P}^{\\star})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq(1-\\alpha)\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})+\\alpha\\mathrm{KL}(\\mathrm{proj}_{\\mathcal{R}}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))|\\mathbb{P}^{\\star})}\\\\ &{\\quad\\quad\\quad\\quad\\leq(1-\\alpha)\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})+\\alpha\\mathrm{KL}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})_{0,1}|\\mathbb{P}_{0,1}^{\\star})}\\\\ &{\\quad\\quad\\quad\\leq(1-\\alpha)\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})+\\alpha\\mathrm{KL}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})|\\mathbb{P}^{\\star})}\\\\ &{\\quad\\quad\\quad\\leq(1-\\alpha)\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})+\\alpha\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})-\\alpha\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))\\leq\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})-\\mathrm{KL}(\\hat{\\mathbb{P}}^{n+1}|\\mathbb{P}^{\\star}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{n\\in\\mathbb{N}}{\\mathrm{KL}}(\\hat{\\mathbb{P}}^{n}|{\\mathrm{proj}}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))\\leq2{\\mathrm{KL}}(\\hat{\\mathbb{P}}^{0}|\\mathbb{P}^{\\star})<+\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So we obtain $\\begin{array}{r l r}{\\operatorname*{lim}_{n\\rightarrow+\\infty}\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))}&{=}&{0}\\end{array}$ . In addition, using (26) we have that $\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathbb{P}^{\\star})\\,\\le\\,\\mathrm{KL}(\\hat{\\mathbb{P}}^{0}|\\mathbb{P}^{\\star})\\,<\\,+\\infty$ for all $\\textit{n}\\in\\mathbb{N}$ . Using (Shi et al., 2023, Lemma 6), we also get that $\\mathrm{KL}(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})|\\mathbb{P}^{\\star})\\,\\le\\,\\mathrm{KL}(\\hat{\\mathbb{P}}^{0}|\\mathbb{P}^{\\star})\\,<\\,+\\infty$ for any $\\textit{n}\\in\\mathbb{N}$ . Hence both the sequences $({\\hat{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ and $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}=(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))_{n\\in\\mathbb{N}}$ are relatively compact in $\\mathcal{P}(\\mathcal{C})$ . Let $\\bar{\\mathbb{P}}\\in\\mathcal{P}(\\mathcal{C})$ be an adherent point to the sequence $({\\hat{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ and $\\varphi:\\mathbb{N}\\rightarrow\\mathbb{N}$ increasing such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathbb{P}^{\\varphi(n)}=\\bar{\\mathbb{P}}}\\end{array}$ . Similarly, let $\\phi:\\mathbb{N}\\rightarrow\\mathbb{N}$ increasing such that $(\\phi(n))_{n\\in\\mathbb{N}}$ is a subsequence of $(\\varphi(n))_{n\\in\\mathbb{N}}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow+\\infty}\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n})=\\bar{\\mathbb{P}}^{\\prime}}\\end{array}$ , with $\\bar{\\mathbb{P}}^{\\prime}$ and adherent point to the sequence $(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))_{n\\in\\mathbb{N}}$ . Using the lower semi-continuity of the Kullback-Leibler divergence in both arguments (Dupuis and Ellis, 2011), we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\bar{\\mathbb{P}}|\\bar{\\mathbb{P}}^{\\prime})\\leq\\operatorname*{lim}_{n\\to+\\infty}\\mathrm{KL}(\\hat{\\mathbb{P}}^{\\phi(n)}|\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{\\phi(n)}))=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the set of Markov measures and the set of reciprocal measures w.r.t. $\\mathbb{Q}$ are both closed, we have that $\\bar{\\mathbb P}$ is Markov and in the reciprocal class of $\\mathbb{Q}$ . Since we also have that $\\bar{\\mathbb{P}}_{0}=\\pi_{0}$ and $\\bar{\\mathbb{P}}_{1}=\\pi_{1}$ , we get that $\\bar{\\mathbb{P}}=\\mathbb{P}^{\\star}$ using Appendix D.1. Since every adherent point of $({\\hat{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ is $\\mathbb{P}^{\\star}$ , we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\hat{\\mathbb{P}}^{n}=\\mathbb{P}^{\\star}}\\end{array}$ . Similarly, using that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow+\\infty}\\mathrm{KL}(\\hat{\\mathbb{P}}^{n}|\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))=0}\\end{array}$ and again the lower semi-continuity of the Kullback\u2013Leibler divergence in both arguments, we get that every adherent point of $(\\mathrm{proj}_{\\mathcal{M}}(\\hat{\\mathbb{P}}^{n}))_{n\\in\\mathbb{N}}$ is $\\mathbb{P}^{\\star}$ . Hence, we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathbb{P}^{n}=\\mathbb{P}^{\\star}}\\end{array}$ , which concludes the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.2 From parametric to non-parametric. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we show that the parametric updates considered in (9) are a preconditioned version of the non-parametric updates considered in (8). We first recall the non-parametric loss ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}(v,\\mathbb{P})=\\int_{0}^{1}\\mathcal{L}_{t}(v_{t},\\mathbb{P})\\mathrm{d}t=\\frac{1}{2}\\int_{0}^{1}\\int_{(\\mathbb{R}^{d})^{2}}\\Big\\|v_{t}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\Big\\|^{2}\\mathrm{d}\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P})_{t,1}(x_{t},x_{1})\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the parametric loss ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{L}(\\theta,\\mathbb{P})=\\frac{1}{2}\\int_{0}^{1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\Big\\lVert v_{t}^{\\theta}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\Big\\rVert^{2}\\mathrm{dproj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathbb{P})_{t,1}(x_{t},x_{1})\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The non-parametric sequence $(v^{n})_{n\\in\\mathbb{N}}$ is given by (27), i.e. we have for any $n\\in\\ensuremath{\\mathbb{N}},\\,t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nv_{t}^{n+1}(x)=v_{t}^{n}(x)-\\delta_{n}\\nabla_{\\mu^{n}}\\mathcal{L}_{t}(v_{t}^{n},\\mathbb{P}_{v^{n}})(x).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly the sequence of parametric updates is given for any $n\\in\\ensuremath{\\mathbb{N}},\\,t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\theta_{n+1}=\\theta_{n}-\\alpha\\nabla_{\\theta}\\mathrm{L}(\\theta_{n},\\mathbb{P}^{\\bar{\\theta}_{n}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We recall that $\\mathbb{P}^{\\bar{\\theta}_{n}}$ is a stop gradient version of $\\mathbb{P}_{v^{\\bar{\\theta}_{n}}}$ . We are going to show that on average the parametric algorithm yields a direction of descent for the non-parametric loss. We assume that the set of parameters $\\Theta$ is an open subset of $\\mathbb{R}^{p}$ for some $p\\in\\mathbb N$ . For any $t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ we assume that $\\theta\\mapsto v_{t}^{\\theta}(x)$ is twice continuously differentiable and denote $\\mathrm{D}_{\\theta}v_{t}^{\\theta}\\bar{(}x)\\in\\mathbb{R}^{d\\times p}$ its Jacobian and $\\mathrm{D}_{\\theta}^{2}v_{t}^{\\theta}(x)$ its Hessian. For any $\\theta\\in\\Theta$ , we denote ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{\\theta}=\\nabla_{\\theta}\\mathrm{L}(\\theta,\\mathbb{P}^{\\bar{\\theta}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We show the following result. ", "page_idx": 22}, {"type": "text", "text": "Proposition D.4 (Velocity field parametric update): Assume that there exists $C>0$ such that for any $\\theta\\in\\Theta$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{0}^{1}(1-s)\\mathrm{D}_{\\theta}^{2}v_{s}^{\\theta-\\alpha s h_{\\theta}}(x)(h_{\\theta},h_{\\theta})\\mathrm{d}s<C,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $h_{\\theta}=\\nabla_{\\theta}\\mathrm{L}(\\theta,\\mathbb{P}^{\\bar{\\theta}})$ . We have that for any $n\\in\\mathbb{N}$ , $t\\in[0,1)$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{t}^{\\theta_{n+1}}(x)=v_{t}^{\\theta_{n}}(x)}\\\\ &{\\quad-\\,\\alpha\\mathrm{D}_{\\theta}v_{t}^{\\theta_{n}}(x)\\displaystyle\\int_{0}^{1}\\int_{\\mathbb R^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(\\tilde{x})^{\\top}\\nabla_{\\mu^{n}}\\mathcal{L}_{s}(v^{\\theta_{n}},\\mathbb{P}_{v^{\\theta_{n}}})(\\tilde{x})\\mathrm{d}\\mu_{s}^{n}(\\tilde{x})\\mathrm{d}s+o(\\alpha),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mu^{n}=(1-\\alpha)\\mathbb{P}^{n}+\\alpha\\mathbb{P}_{v^{\\theta_{n}}}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. First, we have that for any $\\mu\\in{\\mathcal{P}}({\\mathcal{C}})$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathrm{L}(\\theta,\\mathbb{P}^{\\bar{\\theta}})=\\int_{0}^{1}\\int_{\\mathbb{R}^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(x_{s})^{\\top}(v_{s}^{\\theta}(x_{s})-(x_{1}-x_{s})/(1-s))\\mathrm{d}\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{\\bar{\\theta}})_{s,1}(x_{s},x_{1})\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, using (22), we get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathrm{L}(\\theta,\\mathbb{P}^{\\bar{\\theta}})=\\int_{0}^{1}\\int_{\\mathbb{R}^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(x_{s})^{\\top}\\nabla_{\\mu}\\mathcal{L}_{s}(v_{s}^{\\theta},\\mathbb{P}^{\\bar{\\theta}})(x_{s})\\mathrm{d}\\mu_{s}(x_{s})\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\theta\\in\\Theta$ and denote $\\theta^{\\prime}=\\theta-\\alpha\\nabla_{\\theta}\\mathrm{L}(\\theta,\\mathbb{P}^{\\bar{\\theta}})$ . Using a Taylor expansion, we get that for any $\\theta\\in\\Theta$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{t}^{\\theta^{\\prime}}(x)=v_{t}^{\\theta}(x)-\\alpha\\mathrm{D}_{\\theta}v_{t}^{\\theta}(x)\\displaystyle\\int_{0}^{1}\\int_{\\mathbb R^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(x_{s})^{\\top}\\nabla_{\\mu}\\mathcal{L}_{s}(v_{s}^{\\theta},\\mathbb{P}^{\\bar{\\theta}})(x_{s})\\mathrm{d}\\mu_{s}(x_{s})\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad+\\alpha^{2}\\displaystyle\\int_{0}^{1}(1-s)\\mathrm{D}_{\\theta}^{2}v_{s}^{\\theta-\\alpha s h_{\\theta}}(x)(h_{\\theta},h_{\\theta})\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the functional gradient is not applied on the second coordinate, we can drop the stop gradient operator and therefore we have for any $\\theta\\in\\Theta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{t}^{\\theta^{\\prime}}(x)=v_{t}^{\\theta}(x)-\\alpha\\mathrm{D}_{\\theta}v_{t}^{\\theta}(x)\\displaystyle\\int_{0}^{1}\\int_{\\mathbb R^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(x_{s})^{\\top}\\nabla_{\\mu}\\mathcal{L}_{s}(v_{s}^{\\theta},\\mathbb{P}_{v^{\\theta}})(x_{s})\\mathrm{d}\\mu_{s}(x_{s})\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad+\\alpha^{2}\\displaystyle\\int_{0}^{1}(1-s)\\mathrm{D}_{\\theta}^{2}v_{s}^{\\theta-\\alpha s h_{\\theta}}(x)(h_{\\theta},h_{\\theta})\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining this result with (28), we conclude the proof. ", "page_idx": 23}, {"type": "text", "text": "The corresponding update on the velocity field is given for any $n\\in\\mathbb N$ , $t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ by ", "page_idx": 23}, {"type": "equation", "text": "$$\nd_{t}^{n}(x)=-\\alpha\\mathrm{D}_{\\theta}v_{t}^{\\theta_{n}}(x)\\int_{0}^{1}\\int_{\\mathbb R^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(\\tilde{x})^{\\top}\\nabla_{\\mu^{n}}\\mathcal{L}_{s}(v^{\\theta_{n}},\\mathbb{P}_{v^{\\theta_{n}}})(\\tilde{x})\\mathrm{d}\\mu_{s}^{n}(\\tilde{x})+o(\\alpha).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We immediately have the following corollary. ", "page_idx": 23}, {"type": "text", "text": "Proposition D.5 (Parametric direction of descent): For any $n\\in\\mathbb{N}$ , if ", "text_level": 1, "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\int_{\\mathbb{R}^{d}}\\mathrm{D}_{\\theta}v_{s}^{\\theta}(\\tilde{x})^{\\top}\\nabla_{\\mu^{n}}\\mathcal{L}_{s}(v^{\\theta_{n}},\\mathbb{P}_{v^{\\theta_{n}}})(\\tilde{x})\\mathrm{d}\\mu_{s}^{n}(\\tilde{x})\\neq0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\alpha\\to0}\\int_{0}^{1}\\int_{\\mathbb{R}^{d}}\\langle\\nabla_{\\mu^{n}}\\mathcal L_{t}(v^{\\theta_{n}},\\mathbb{P}_{v^{\\theta_{n}}})(x),d_{t}^{n}(x)\\rangle\\mathrm{d}\\mu_{t}^{n}(x)\\leq0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E Background material on DSBM and extensions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we recall some basics on Markovian and reciprocal projections in Appendix E.1. We explain the link between the concept of iterative refinement and Schr\u00f6dinger Bridges in Appendix E.2. Then, we briefly present Diffusion Schr\u00f6dinger Bridge Matching (DSBM) (Shi et al., 2023) in Appendix E.3 and propose some new extensions in Appendix E.4. ", "page_idx": 23}, {"type": "text", "text": "E.1 Markov and reciprocal projections in practice ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we recall the definition of the reciprocal and Markov projection. We provide more details on how these different projections can be performed and illustrate them on simple examples. ", "page_idx": 23}, {"type": "text", "text": "Markov projection. First, we recall the definition of the Markovian projection. ", "page_idx": 23}, {"type": "text", "text": "Definition E.1 (Markov projection): Assume that $\\mathbb{Q}$ is induced by $(\\sqrt{\\varepsilon}\\mathbf{B}_{t})_{t\\in[0,1]}$ for $\\varepsilon>0$ . Then, when it is well-defined, for any $\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})$ , the Markovian projection $\\mathbb{M}=\\operatorname{proj}_{\\mathcal{M}}(\\mathbb{P})\\in\\mathcal{M}$ is the path measure induced by the diffusion ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}^{\\star}=v_{t}^{\\star}(\\mathbf{X}_{t}^{\\star})\\mathrm{d}t+\\sqrt{\\varepsilon}\\mathrm{d}\\mathbf{B}_{t},\\qquad v_{t}^{\\star}(x_{t})=\\left(\\mathbb{E}_{\\mathbb{P}_{1\\mid t}}\\left[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}\\right]-x_{t}\\right)/(1-t),\\qquad\\mathbf{X}_{0}^{\\star}\\sim\\mathbb{P}_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In Figure 7 and Figure 8, we illustrate the effect of the Markovian projection, following the example of (Liu, 2022). We consider two distributions $\\pi_{0}$ and $\\pi_{1}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi_{0}=\\frac{1}{2}\\mathcal{N}([-2,-2],\\mathrm{Id})+\\frac{1}{2}\\mathcal{N}([-2,2],\\mathrm{Id}),\\quad\\pi_{1}=\\frac{1}{2}\\mathcal{N}([2,-2],\\mathrm{Id})+\\frac{1}{2}\\mathcal{N}([2,2],\\mathrm{Id}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In Figure 7, we display samples from the distributions $\\pi_{0}$ and $\\pi_{1}$ as well as trajectories from the path measure $\\mathbb{P}=\\bar{(}\\pi_{0}\\'\\otimes\\pi_{1}\\')\\mathbb{Q}_{|0,1}$ . Practically, this means that we sample $\\mathbf{X}_{0}\\sim\\pi_{0}$ and ${\\bf X}_{1}\\sim\\pi_{1}$ independently and then consider a Brownian bridge between $\\mathbf{X}_{0}$ and $\\mathbf{X}_{1}$ . The SDE associated with the Brownian bridge with scale $\\varepsilon>0$ is given for any $t\\in[0,1]$ by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=(\\mathbf{X}_{1}-\\mathbf{X}_{t})/(1-t)\\mathrm{d}t+\\sqrt{\\varepsilon}\\mathrm{d}\\mathbf{B}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that the measure $\\mathbb{P}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ is in the reciprocal class, i.e. $\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})$ . ", "page_idx": 23}, {"type": "image", "img_path": "1F32iCJFfa/tmp/55932b4d51de5d39f0f4ef347e018940b93dd76d7cd7dbd393ad84b28ad75b05.jpg", "img_caption": ["Figure 7: Samples from the original distributions $\\pi_{0}$ (left) and $\\pi_{1}$ (right) are shown in red, while sample paths from $\\mathbb{P}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ are shown in blue. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Next in Figure 8, we display samples from the distributions $\\pi_{0}$ and $\\pi_{1}$ as well as trajectories from the path measure $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{M}}(\\mathbb{P})$ . Note that in Figure 8, contrary to Figure 7, we observe less crossings between the trajectories. Indeed in the limit case where $\\varepsilon\\rightarrow0$ the Markov measures $\\mathbb{P}^{\\star}$ is an ODE with regular coefficients and therefore admits a unique solution for every starting point in the space so no crossing is possible. In particular, note that most of the trajectories starting from the upper-left Gaussian end at the upper-right Gaussian. Similarly, most of the trajectories starting from the lower-left Gaussian end at the lower-right Gaussian. ", "page_idx": 24}, {"type": "image", "img_path": "1F32iCJFfa/tmp/f8f4653981f82bc50dc26d843f4b137acf7718e3086a08dec2a4554ef8a6e5c9.jpg", "img_caption": ["Figure 8: Samples from the original distributions are shown in red, while sample paths from $\\mathbb{M}=\\mathrm{proj}_{\\mathcal{M}}(\\mathbb{P})$ are shown in blue. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "In practice, computing the Markov projection involves finding the optimal drift $v_{t}^{\\star}$ . This optimal drift is the minimizer of a regression problem, see (Shi et al., 2023) for more details. Hence, computing the Markovian projection requires training a neural network to define a vector field. ", "page_idx": 24}, {"type": "text", "text": "Reciprocal projection. First, we recall the definition the reciprocal projection. ", "page_idx": 24}, {"type": "text", "text": "Definition E.2 (Reciprocal projection): $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ is in the reciprocal class ${\\mathcal{R}}(\\mathbb{Q})$ of $\\mathbb{Q}\\;i f\\mathbb{P}=$ $\\mathbb{P}_{0,1}\\mathbb{Q}_{|0,1}$ . We define the reciprocal projection of $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ as $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathbb{P})=\\mathbb{P}_{0,1}\\mathbb{Q}_{|0,1}$ . We will write $\\mathrm{proj}_{\\mathcal{R}}$ instead of proj ${\\mathcal{R}}(\\mathbb{Q})$ to simplify notation. ", "page_idx": 24}, {"type": "text", "text": "To sample from $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P})$ , we only need to sample $(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim\\mathbb{P}_{0,1}$ and then to sample from the Brownian bridge conditioned on $(\\mathbf{X}_{0},\\mathbf{X}_{1})$ . This means that in order to sample $\\mathbf{X}_{t}^{\\star}\\sim\\mathbb{P}_{t}^{\\star}$ , we only need to sample $(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim\\mathbb{P}_{0,1}$ and then compute ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X}_{t}=(1-t)\\mathbf{X}_{0}+t\\mathbf{X}_{1}+\\sqrt{\\varepsilon t(1-t)}\\mathbf{Z},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id})$ . In particular, sampling from $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P})$ does not require training any neural network. However, in practice, in order to obtain samples $(\\mathbf{X}_{0},\\mathbf{\\dot{X}}_{1})\\sim\\mathbb{P}$ , we have that $\\mathbb{P}$ is associated with an SDE and therefore obtaining $(\\mathbf{X}_{0},\\mathbf{X}_{1})$ requires unrolling the SDE associated with $\\mathbb{P}$ . In Algorithm 1, the measure $\\mathbb{P}$ is associated with an SDE with parametric drift $v^{\\theta}$ . ", "page_idx": 25}, {"type": "text", "text": "In Figure 9, we continue our study of the example of (Liu, 2022) that we used to explain the concept of Markovian projection. We consider the path measure $\\mathbb{M}$ obtained as the Markov projection of $\\mathbb{P}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ . In Figure 9, we display samples from the distributions $\\pi_{0}$ and $\\pi_{1}$ as well as trajectories from the path measure $\\mathbb{P}^{\\star}=\\mathbb{M}_{0,1}\\mathbb{Q}_{|0,1}$ . In order to sample from $\\mathbb{P}^{\\star}$ we first sample $(\\mathbf{X}_{0},\\mathbf{X}_{1})\\sim\\mathbb{M}_{0,1}$ . This involves unrolling the SDE associated with $\\mathbb{M}$ . Once we have access to samples $(\\mathbf{X}_{0},\\mathbf{X}_{1})$ , we draw trajectories from the Brownian bridge following the SDE (29). We can also sample from any time $t$ without having to unroll the SDE (29) by simply sampling from (30). This is what is done in Algorithm 1. ", "page_idx": 25}, {"type": "image", "img_path": "1F32iCJFfa/tmp/fb0fbe8296f3a046f94c114d6b598cbbdda01873ef9778a3d0a4a2089ffe1faf.jpg", "img_caption": ["Figure 9: Samples from the original distributions are shown in red, while sample paths from $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{M})$ are shown in blue. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "E.2 Iterative refinement and Schr\u00f6dinger Bridge ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The Schr\u00f6dinger Bridge problem (2) can be solved leveraging techniques from diffusion models and bridge matching. De Bortoli et al. (2021); Vargas et al. (2021) consider an alternating projection algorithm, corresponding to a dynamic version of the celebrated Sinkhorn algorithm. Peluchetti (2023); Shi et al. (2023) introduce the Iterative Markovian Fitting procedure which corresponds to perform an alternating projection algorithm on the class of Markov processes and the reciprocal class of the Brownian motion. It can be shown that the solution of this iterative algorithm converges to the Schr\u00f6dinger Bridge under mild assumptions, see (Peluchetti, 2023; Shi et al., 2023). We highlight that in the case where $\\varepsilon\\rightarrow0$ then DSBM is equivalent to the Rectified Flow algorithm (Liu et al., 2023b). One of the main limitation of those previously introduced procedures which provably converge to the solution of the Schr\u00f6dinger Bridge problem is that they rely on these expensive iterative solvers and requires to consider two networks, one parameterising the forward process $\\pi_{0}\\to\\pi_{1}$ and one parameterising the backward $\\pi_{1}\\rightarrow\\pi_{0}$ . ", "page_idx": 25}, {"type": "text", "text": "E.3 Diffusion Schr\u00f6dinger Bridge Matching ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Diffusion Schr\u00f6dinger Bridge Matching corresponds to the practical implementation of the Iterative Markovian Fitting procedure proposed in Shi et al. (2023); Peluchetti (2023). The IMF procedure alternates between projecting on the Markov class $\\mathcal{M}$ and the reciprocal class $\\mathcal{R}_{\\mathbb{Q}}$ . In what follows, we denote $\\mathbb{M}^{n+1}\\,\\stackrel{\\cdot}{=}\\,\\bar{\\mathbb{P}}^{2n+1}\\,\\stackrel{\\bullet}{\\in}\\,\\mathcal{M}$ and $\\Pi^{n}=\\mathbb{P}^{2n}\\,\\in\\,\\mathcal{R}(\\mathbb{Q})$ . We also recall that $\\mathbb{Q}$ is a (rescaled) Brownian motion associated with $(\\sigma_{0}\\mathbf{B}_{t})_{t\\in[0,1]}$ and that therefore sampling from $\\mathbb{Q}_{|0,1}(\\cdot|x_{0},x_{1})$ corresponds to sampling from ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm d\\mathbf X_{t}=(x_{1}-\\mathbf X_{t})/(1-t)\\mathrm d t+\\sigma_{0}\\mathrm d\\mathbf B_{t},\\qquad\\mathbf X_{0}=x_{0}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We recall that the main computational bottleneck of the DSBM lies in the approximation of the Markovian projection. Indeed, using (Shi et al., 2023, Definition 1, Proposition 2), we have that ", "page_idx": 25}, {"type": "text", "text": "$\\mathbb{M}^{\\star}=\\mathrm{proj}_{\\mathcal{M}}(\\Pi)$ is associated with the process ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm d\\mathbf X_{t}=(\\mathbb E_{\\Pi}[\\mathbf X_{1}\\mid\\mathbf X_{t}]-\\mathbf X_{t})/(1-t)\\mathrm d t+\\sigma_{0}\\mathrm d\\mathbf B_{t}\\qquad\\mathbf X_{0}\\sim\\boldsymbol\\pi_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We also have using (Shi et al., 2023, Proposition 2) that $\\mathbb{M}^{\\star}$ can be approximated using $\\mathbb{M}^{\\theta^{\\star}}$ given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathbf{X}_{t}=v_{\\theta^{\\star}}(t,\\mathbf{X}_{t})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\mathbf{X}_{0}\\sim\\pi_{0},}\\\\ &{\\theta^{\\star}=\\operatorname*{argmin}_{\\theta\\in\\Theta}\\displaystyle\\int_{0}^{1}\\mathbb{E}_{\\Pi_{t,1}}[\\|(\\mathbf{X}_{1}-\\mathbf{X}_{t})/(1-t)-v_{\\theta}(t,\\mathbf{X}_{t})\\|^{2}]\\mathrm{d}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\{v_{\\theta}~:~\\theta\\in\\Theta\\}$ is a parametric family of functions, usually given by a neural network. ", "page_idx": 26}, {"type": "text", "text": "Hence, since we can approximate proj $\\mathcal{R}(\\mathbb{Q})^{\\left(\\mathbb{M}\\right)}$ and $\\mathrm{proj}_{\\mathcal{M}}(\\Pi)$ we can approximate the IMF procedure. This is the DSBM algorithm introduced in Shi et al. (2023); Peluchetti (2023). We describe the first few iterations. Let $\\Pi^{\\breve{0}}=\\Pi_{0,1}^{0}\\mathbb{Q}_{|0,1}$ where $\\Pi_{0}^{0}=\\pi_{0}$ , $\\Pi_{1}^{0}=\\pi_{1}$ . Learn $\\mathbb{M}^{1}\\approx\\operatorname{proj}_{\\mathcal{M}}(\\Pi^{0})$ given by (31) with $v_{\\theta^{\\star}}$ given by (32). Next, sample from $\\Pi^{1}=\\mathrm{proj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathbb{M}^{1})=\\mathbb{M}_{0,1}^{1}\\mathbb{Q}_{|0,1}$ by sampling from $\\mathbb{M}_{0,1}^{1}$ and reconstructing the bridge $\\mathbb{Q}_{|0,1}$ . Upon iterating the previous procedure, we obtain a sequence $(\\Pi^{n},\\mathbb{M}^{n+1})_{n\\in\\mathbb{N}}$ . To mitigate the bias accumulation problem caused by approximating only the forward process, we alternate between a forward Markovian projection and a backward Markovian projection. We give more details on the advantage of using a forward-backward parameterisation instead of a forward-forward in Appendix I. This procedure is valid using (Shi et al., 2023, Proposition 9). The optimal backward process is approximated with ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{Y}_{t}=v_{\\phi^{\\star}}(1-t,\\mathbf{Y}_{t})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\mathbf{Y}_{0}\\sim\\boldsymbol{\\pi}_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\phi^{\\star}=\\operatorname*{argmin}_{\\phi\\in\\Phi}\\int_{0}^{1}\\mathbb{E}_{\\Pi_{0,t}}[\\|\\big({\\bf X}_{0}-{\\bf X}_{t}\\big)/t-v_{\\phi}(t,{\\bf X}_{t})\\|^{2}]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We recall the full DSBM algorithm in Algorithm 2. ", "page_idx": 26}, {"type": "text", "text": "Algorithm 2 Diffusion Schr\u00f6dinger Bridge Matching   \n1: Input: Joint distribution $\\Pi_{0,1}^{0}$ , tractable bridge $\\mathbb{Q}_{|0,1}$ , number of outer iterations $N\\in\\mathbb N$ .   \n2: Let \u03a00 = \u03a000,1Q|0,1.   \n3: for $n\\in\\{0,\\ldots,N-1\\}$ do   \n4: Learn $v_{\\phi^{\\star}}$ using (34) with $\\Pi=\\Pi^{2n}$ .   \n5: Let $\\mathbb{M}^{2n+1}$ be given by (33).   \n6: Let $\\Pi^{2n+1}=\\bar{\\mathbb{M}}_{0,1}^{2n+1}\\dot{\\mathbb{Q}}_{|0,1}$ .   \n7: Learn $v_{\\theta^{\\star}}$ using (5) with $\\begin{array}{r}{\\Pi=\\Pi^{2n+1}}\\end{array}$ .   \n8: Let $\\mathbb{M}^{2n+2}$ be given by (31).   \n9: Let $\\Pi^{2n+2}=\\bar{\\mathbb{M}}_{0,1}^{2n+2}\\dot{\\mathbb{Q}}_{|0,1}$ .   \n10: end for   \n11: Output: $v_{\\theta^{\\star}},v_{\\phi^{\\star}}$ ", "page_idx": 26}, {"type": "text", "text": "E.4 A Reflection-projection extension ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "First, we consider a reflection-projection method similar to the one investigated in Bauschke and Kruk (2004). We recall that the DSBM algorithm is associated with a sequence $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}^{n+1/2}=\\mathrm{proj}_{\\mathcal{M}}(\\mathbb{P}^{n}),\\qquad\\mathbb{P}^{n+1}=\\mathrm{proj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathbb{P}^{n+1/2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In a reflection-projection scheme, one of the projection is replaced by a reflection. As noted in Bauschke and Kruk (2004), this can yield faster convergence rates in practice. We consider the sequence $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}^{n+1/2}=\\mathrm{{proj}}_{\\mathcal{M}}(\\mathbb{P}^{n}),\\qquad\\mathbb{P}^{n+1}=\\mathrm{{proj}}_{\\mathcal{R}(\\mathbb{Q})}(2\\mathbb{P}^{n+1/2}-\\mathbb{P}^{n}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In what follows, we make the assumption that $2\\mathbb{P}^{n+1/2}-\\mathbb{P}^{n}$ is a probability measure, even though it is not clear if this path measure is non-negative. However, even by making this strong assumption, we show that we can recover DSBM in Algorithm 4. By considering a relaxation of the reflectionprojection scheme. ", "page_idx": 26}, {"type": "text", "text": "First, note that for any $n\\in\\ensuremath{\\mathbb{N}},\\ensuremath{\\mathbb{P}}_{|0}^{n}$ is Markov, see De Bortoli et al. (2023) for instance. Hence, we assume that $\\mathbb{P}^{n}$ is associated with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm d\\mathbf X_{t}^{n}=v_{t,0}^{n}(\\mathbf X_{t},\\mathbf X_{0})\\mathrm d t+\\sigma_{0}\\mathrm d\\mathbf B_{t},\\qquad\\mathbf X_{0}\\sim\\boldsymbol\\pi_{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Estimating Pn+1. First, we compute vtn,0+1 assuming that we can sample from $\\mathbb{P}^{n}$ and $\\mathbb{P}^{n+1/2}$ . Since $\\mathbb{P}^{n+1}$ is in the reciprocal class, we have that $\\mathbb{P}^{n+1}$ is associated with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=(\\mathbb{E}_{\\mathbb{P}_{1\\mid0,t}^{n+1}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t},\\mathbf{X}_{0}]-\\mathbf{X}_{t})/(1-t)\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We refer to De Bortoli et al. (2023) for a proof of this fact. Hence, using (35), we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{\\Upsilon}_{t,0}^{n+1}=\\mathrm{argmin}_{v}\\int_{0}^{1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\|v_{t,0}(t,x_{t},x_{0})-(x_{1}-x_{t})/(1-t)\\|^{2}\\mathrm{d}\\mathbb{P}_{0,t,1}^{n+1}(x_{0},x_{t},x_{1})}}\\\\ &{}&{\\mathrm{\\Upsilon}=\\mathrm{argmin}_{v}2\\int_{0}^{1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\|v_{t,0}(t,x_{t},x_{0})-(x_{1}-x_{t})/(1-t)\\|^{2}\\mathrm{d}\\mathbb{P}_{0,1}^{n+1/2}\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1})}\\\\ &{}&{\\mathrm{\\Upsilon}-\\int_{0}^{1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\|v_{t,0}(t,x_{t},x_{0})-(x_{1}-x_{t})/(1-t)\\|^{2}\\mathrm{d}\\mathbb{P}_{0,1}^{n}\\mathrm{d}\\mathbb{Q}_{t\\mid0,1}(x_{t}|x_{0},x_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we turn to the estimation of $\\mathbb{P}^{n+3/2}$ . ", "page_idx": 27}, {"type": "text", "text": "Estimating $\\mathbb{P}^{n+3/2}$ . Next, we assume that for any $n\\in\\mathbb{N},\\mathbb{P}^{n+1/2}$ is associated with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}{\\mathbf{X}}_{t}^{n}=v_{t}^{n}({\\mathbf{X}}_{t})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}{\\mathbf{B}}_{t},\\qquad{\\mathbf{X}}_{0}\\sim\\pi_{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using (36), we have that $v_{t}^{n+1}$ is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nv_{t}^{n+1}=\\operatorname{argmin}_{v}\\int_{0}^{1}\\int_{\\mathbb R^{d}\\times\\mathbb R^{d}}\\|v_{t}(t,x_{t})-v_{t,0}^{n+1}(t,x_{t},x_{0})\\|^{2}\\mathrm{d}\\mathbb P_{0,t}^{n+1}(x_{0},x_{t}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We note also that using (Shi et al., 2023, Proposition 2) and the fact $\\mathbb{P}^{n}$ is in the reciprocal class of $\\mathbb{Q}$ , we also have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{v_{t}^{n+1}=\\operatorname*{argmin}_{v^{2}}\\displaystyle\\int_{0}^{1}\\int_{\\mathbb R^{d}\\times\\mathbb R^{d}\\times\\mathbb R^{d}}\\|v_{t}(t,x_{t})-(x_{1}-x_{t})/(1-t)\\|^{2}\\mathrm{d}\\mathbb P_{0,1}^{n+1/2}\\mathrm{d}\\mathbb Q_{t\\mid0,1}(x_{t}|x_{0},x_{1})}\\\\ {-\\displaystyle\\int_{0}^{1}\\int_{\\mathbb R^{d}\\times\\mathbb R^{d}\\times\\mathbb R^{d}}\\|v_{t}(t,x_{t})-(x_{1}-x_{t})/(1-t)\\|^{2}\\mathrm{d}\\mathbb P_{0,1}^{n}\\mathrm{d}\\mathbb Q_{t\\mid0,1}(x_{t}|x_{0},x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, assuming that we can sample from $\\mathbb{P}^{n}$ and $\\mathbb{P}^{n+1/2}$ then we can estimate $v_{t,0}^{n+1}$ and $v_{t}^{n+1}$ , i.e. sample from $\\mathbb{P}^{n+1}$ and $\\mathbb{P}^{n+3/2}$ . Note that the losses (37) and (38) only differ by the conditioning with respect to the initial condition $x_{0}$ and therefore the optimisation can be conducted in parallel. We are now able to propose the following projection-reflection algorithm, see Algorithm 3. ", "page_idx": 27}, {"type": "text", "text": "Algorithm 3 Reflection Diffusion Schr\u00f6dinger Bridge Matching ", "page_idx": 28}, {"type": "text", "text": "1: Input: Vector field and conditional vector field $v_{t}^{0}$ and $\\boldsymbol{v}_{t,0}^{0}$ , noise level $\\sigma_{0}$ and associated bridge   \n$\\mathbb{Q}_{|0,1}$ , number of outer iterations $N\\in\\mathbb{N}$ , batch size $B$   \n2: for $n\\in\\{0,\\ldots,N-1\\}$ do   \n3: while not converged do   \n45:: SSaammppllee X10:B $\\mathbf{X}_{0}^{1:B}\\overset{\\cdot}{\\sim}\\pi_{0}^{\\otimes B}$ $\\mathbf{X}_{1}^{1:B}$ $\\mathrm{d}\\mathbf{X}_{t}^{1:B}=v_{t}^{n}(\\mathbf{X}_{t}^{1:B})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t}$   \n6: Sample $\\hat{\\mathbf{X}}_{1}^{1:B}$ using $\\mathrm{d}\\hat{\\mathbf{X}}_{t}^{1:B}=v_{t,0}^{n}(\\hat{\\mathbf{X}}_{t}^{1:B},\\mathbf{X}_{0}^{1:B})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t}$   \n7: $\\begin{array}{r}{\\mathcal{L}=\\int_{0}^{1}[\\sum_{k=1}^{B}\\|v_{t}(\\mathbf{X}_{t}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|_{\\cdot}^{2}-(1/2)\\sum_{k=1}^{B}\\|v_{t}(\\mathbf{X}_{t}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}]\\mathrm{d}t\\quad\\quad.}\\end{array}$   \n8: $\\begin{array}{r l}&{\\rightharpoonup-J_{0}\\downarrow\\sum_{k=1}\\|\\boldsymbol{v}_{t}\\{\\bf{\\alpha}\\cdot}\\alpha_{t}\\jmath=\\frac{1-t}{1-t}\\parallel-\\mathrm{\\alpha_{1}}\\mathrm{\\alpha_{1}}^{t}\\downarrow\\alpha_{t}\\jmath\\int\\angle_{k=1}\\|\\boldsymbol{v}_{t}\\{\\bf{\\alpha}\\cdot}\\alpha_{t}\\jmath=\\frac{1-t}{1-t}\\parallel\\mathrm{\\alpha_{1}}^{t\\mathrm{u}t}}\\\\ &{\\mathcal{L}_{0}=\\int_{0}^{1}[\\sum_{k=1}^{B}\\|\\boldsymbol{v}_{t,0}(\\mathbf{X}_{t}^{k},\\mathbf{X}_{0}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}-(1/2)\\sum_{k=1}^{B}\\|\\boldsymbol{v}_{t,0}(\\mathbf{X}_{t}^{k},\\mathbf{X}_{0}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}]\\mathrm{d}t}\\end{array}$   \n9: $\\begin{array}{r l}&{v_{t}^{n+1}=\\mathrm{Gradientstep}(\\mathcal{L})}\\\\ &{v_{t,0_{\\mathrm{-}}}^{n+1}=\\mathrm{Gradientstep}(\\mathcal{L}_{0})}\\end{array}$   \n10:   \n11: end while   \n12: end for   \n13: Output: vtN+1, vtN,0+ ", "page_idx": 28}, {"type": "text", "text": "Note that in Algorithm 3 we only consider the optimisation of a forward process but similarly to Algorithm 2, one can construct a forward backward extension to alleviate some of the bias accumulation problems. Finally, we can interpolate between DSBM and this new reflection algorithm and DSBM by introducing an hyperparameter $\\alpha\\geq0$ and consider the following extension given in Algorithm 4 ", "page_idx": 28}, {"type": "text", "text": "Algorithm 4 Reflection Diffusion Schr\u00f6dinger Bridge Matching ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1: Input: Vector field and conditional vector field $v_{t}^{0}$ and $\\boldsymbol{v}_{t,0}^{0}$ , noise level $\\sigma_{0}$ and associated bridge   \n$\\mathbb{Q}_{|0,1}$ , number of outer iterations $N\\in\\mathbb{N}$ , batch size $B$   \n2: for $n\\in\\{0,\\ldots,N-1\\}$ do   \n34:: SSaammppllee $\\mathbf{X}_{0}^{1:B}\\overset{\\cdot}{\\sim}\\pi_{0}^{\\otimes B}$   \n$\\mathbf{X}_{1}^{1:B}$ $\\mathrm{d}\\mathbf{X}_{t}^{1:B}=v_{t}^{n}(\\mathbf{X}_{t}^{1:B})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t}$   \n6: $\\begin{array}{r l}&{\\mathrm{Sample~}\\hat{\\mathbf{X}}_{1}^{1:B}\\mathrm{~using~d}\\hat{\\mathbf{X}}_{t}^{1:B}=v_{t,0}^{n}(\\hat{\\mathbf{X}}_{t}^{1:B},\\mathbf{X}_{0}^{1:B})\\mathrm{d}t+\\sigma_{0}\\mathrm{d}\\mathbf{B}_{t}}\\\\ &{\\mathcal{L}=\\int_{0}^{1}[\\sum_{k=1}^{B}\\|v_{t}(\\mathbf{X}_{t}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}-\\alpha\\sum_{k=1}^{B}\\|v_{t}(\\mathbf{X}_{t}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}]\\mathrm{d}t}\\\\ &{\\mathcal{L}_{0}=\\int_{0}^{1}[\\sum_{k=1}^{B}\\|v_{t,0}(\\mathbf{X}_{t}^{k},\\mathbf{X}_{0}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}-\\alpha\\sum_{k=1}^{B}\\|v_{t,0}(\\mathbf{X}_{t}^{k},\\mathbf{X}_{0}^{k})-\\frac{\\mathbf{X}_{1}^{k}-\\mathbf{X}_{t}^{k}}{1-t}\\|^{2}]\\mathrm{d}t}\\\\ &{v_{t}^{n+1}=\\mathrm{Gradientstep}(\\mathcal{L})}\\\\ &{v_{t,0,\\ast}^{n+1}=\\mathrm{Gradientstep}(\\mathcal{L}_{0})}\\end{array}$   \n7:   \n8:   \n9:   \n10:   \n11: end while   \n12: end for   \n13: Output: $v_{t}^{N+1},v_{t,0}^{N+1}$ ", "page_idx": 28}, {"type": "text", "text": "Using different values of $\\alpha\\geq0$ in Algorithm 4, we recover different existing algorithms. If $\\alpha=1$ , we recover DSBM Shi et al. (2023). Finally, if $\\alpha=1/2$ , we recover the reflection algorithm Algorithm 3. ", "page_idx": 28}, {"type": "text", "text": "F Consistency in Schr\u00f6dinger Bridge ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The idea of training both the forward and the backward jointly was mentioned in (Shi et al., 2023, Section G). However, it was still assumed that, while being trained jointly, the forward and backward vector fields were obtained using an argmin operation, see (Shi et al., 2023, Equation (43), (44)). In addition, in (Shi et al., 2023, Section G) a consistency loss was proposed in order to enforce that the forward and backward processes match, see (Shi et al., 2023, Equation (49)). In this section, we leverage new results from Daras et al. (2023a); De Bortoli et al. (2024) in order to enforce the internal consistency of the model. ", "page_idx": 28}, {"type": "text", "text": "First, note that for any $(\\mathbf{X}_{t})_{t\\in[0,1]}$ associated with $\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})$ we have for any $0\\leq t_{0}\\leq t\\leq t_{1}\\leq1$ that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}=\\frac{t-t_{0}}{t_{1}-t_{0}}\\mathbf{X}_{t_{1}}+\\frac{t_{1}-t}{t_{1}-t_{0}}\\mathbf{X}_{t_{0}}+\\sigma_{t_{0},t,t_{1}}\\mathbf{Z},\\qquad\\mathbf{Z}\\sim\\mathcal{N}(0,\\mathrm{Id}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{t_{0},t,t_{1}}=\\sqrt{\\frac{(t-t_{0})(t_{1}-t)}{t_{1}-t_{0}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $p_{t}$ be the density of $\\mathbf{X}_{t}$ with respect to the Lebesgue measure, we have that for any $0\\leq t_{0}\\leq t\\leq$ $t_{1}\\leq1$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{\\dot{d}}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\star(x_{t})=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}(2\\pi\\sigma_{t_{0},t,t_{1}}^{2})^{-d/2}\\exp\\Biggl(-\\frac{\\|x_{t}-\\frac{t-t_{0}}{t_{1}-t_{0}}x_{t_{1}}-\\frac{t_{1}-t}{t_{1}-t_{0}}x_{t_{0}}\\|^{2}}{2\\sigma_{t_{0},t,t_{1}}^{2}}\\Biggr)p_{t_{0}}(x_{t_{0}})p_{t_{1}}(x_{t_{1}})\\mathrm{d}x_{t_{0}}\\mathrm{d}x_{t_{1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using the change of variable $x_{t_{0}}\\rightarrow x_{t_{0}}+x_{t}$ and $x_{t_{1}}\\rightarrow x_{t_{1}}+x_{t}$ we get that for any $0\\leq t_{0}\\leq t\\leq$ $t_{1}\\leq1$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla\\log p_{t}(x_{t})=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\{\\nabla\\log p_{t_{0}}(x_{t_{0}})+\\nabla\\log p_{t_{1}}(x_{t_{1}})\\}p_{t_{0},t_{1}|t}(x_{t_{0}},x_{t_{1}}|x_{t})\\mathrm{d}x_{t}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This identity for the score has already been presented in a bridge matching context in (De Bortoli et al., 2024, Section 3.3). Let $\\mathbb{P}\\,\\in\\,\\mathcal{R}(\\mathbb{Q})$ then we have that $\\mathrm{proj}_{\\mathcal{M}}(\\mathbb{P})$ is such that for any $t\\ \\in$ $[0,1]$ , $\\mathrm{\\proj}_{\\mathcal{M}}(\\mathbb{P})_{t}\\,=\\,\\mathbb{P}_{t}$ , see (Shi et al., 2023, Proposition 2). We have that for any $t\\,\\in\\,[0,1]$ , $v_{t}^{\\rightarrow}(x)+v_{1-t}^{\\leftarrow}(x)=\\sigma_{0}^{2}\\nabla\\log p_{t}(x)$ . Combining this result and (39), this suggests considering the following consistency loss ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{cons}_{(t_{0},t,t_{1})}(\\theta)=\\mathbb{E}[\\|v_{\\theta}(t,1,\\mathbf{X}_{t})+v_{\\theta}(1-t,0,\\mathbf{X}_{t})}&{{\\scriptstyle(40)}}\\\\ &{\\quad}&{\\quad\\quad-\\,v_{\\theta}(t_{0},1,\\mathbf{X}_{t})-v_{\\theta}(1-t_{0},0,\\mathbf{X}_{t_{0}})-v_{\\theta}(t_{1},1,\\mathbf{X}_{t_{1}})-v_{\\theta}(1-t_{1},0,\\mathbf{X}_{t_{1}})\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly to (13), we can consider an empirical version of (40). ", "page_idx": 29}, {"type": "text", "text": "G Model stitching ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Algorithm 1, the finetuning stage requires a pretrained bridge matching model interpolating between $\\pi_{0}$ and $\\pi_{1}$ (lines 2-7). However, for large datasets with complex distributions $\\pi_{0}$ and $\\pi_{1}$ , e.g. ImageNet, training this bridge model from scratch can be computationally expensive. To improve efficiency, we can leverage existing diffusion models targeting $\\pi_{0}$ and $\\pi_{1}$ . Specifically, we assume access to generative models transferring between ${\\mathcal{N}}(0,{\\mathrm{Id}})$ and $\\pi_{0}$ , and between ${\\mathcal{N}}(0,{\\mathrm{Id}})$ and $\\pi_{1}$ . In the rest of this section, we show how one can adapt Algorithm 1 to this setting. We then comment on the link between the proposed algorithm and Dual Diffusion Implicit Bridges (Su et al., 2023). ", "page_idx": 29}, {"type": "text", "text": "Setting. For simplicity, assume that we have two pretrained diffusion models for $\\pi_{0}$ and $\\pi_{1}$ . We describe our procedure for $\\pi_{0}$ . Consider a forward process of the form $\\mathbf{X}_{t}\\,=\\,\\mathbf{X}_{0}\\,+\\,\\sigma_{t}\\mathbf{Z}$ , with $\\mathbf{Z}\\sim\\mathcal{N}(0,\\mathrm{Id})$ , where $\\sigma_{t}$ is a hyperparameter. Note that we could have considered an interpolant of the form $\\mathbf{X}_{t}=\\alpha_{t}\\mathbf{X}_{0}+\\sigma_{t}\\mathbf{Z}$ instead, see Song et al. (2021a) for instance. ", "page_idx": 29}, {"type": "text", "text": "We assume that the model $\\mathbf{X}_{t}=\\mathbf{X}_{0}+\\sigma_{t}\\mathbf{Z}$ is associated with the forward diffusion model ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=g_{t}\\mathrm{d}\\mathbf{B}_{t},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we assume that $g_{t}\\geq0$ for all $t\\in[0,1]$ . Note that we have that for any $\\begin{array}{r}{t\\in[0,1],\\sigma_{t}^{2}=\\int_{0}^{t}g_{s}^{2}\\mathrm{d}s}\\end{array}$ In particular, we have that for any $s,t\\in[0,1]$ with $s\\leq t$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbf{X}_{t}=\\mathbf{X}_{s}+\\sqrt{\\sigma_{t}^{2}-\\sigma_{s}^{2}}\\mathbf{Z},}&{{}}&{\\mathbf{Z}\\sim\\mathcal{N}(0,\\mathrm{Id}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Our goal is to solve the following Entropic Optimal Transport problem ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Pi^{\\star}=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(\\mathbb{R}^{2d})}\\Big\\{\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}c(x,y)\\mathrm{d}\\Pi(x,y)-\\varepsilon\\mathrm{H}(\\Pi)\\,;\\,\\Pi_{0}=\\pi_{0},\\,\\Pi_{1}=\\pi_{1}\\Big\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\varepsilon>0$ is some entropic regularisation. We assume that $\\varepsilon>0$ is fixed and assume that there exists $t^{\\prime}\\in[0,1]$ such that $\\bar{\\sigma}_{t^{\\prime}}^{2}=\\bar{\\varepsilon}/2$ . We now consider a dynamic version of (42) with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{\\star}=\\operatorname*{argmin}_{\\mathbb{P}\\in\\mathcal{P}(\\mathcal{C})}\\{\\mathrm{KL}\\big(\\mathbb{P}|\\mathbb{Q}\\big)\\;;\\;\\mathbb{P}_{0}=\\pi_{0},\\;\\mathbb{P}_{t^{\\prime}}=\\pi_{1}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathbb{Q}$ is associated with $(\\mathbf{X}_{t})_{t\\in[0,t^{\\prime}]}$ (41). Note that contrary to the setting presented in the main paper, here we do not consider the integration between time 0 and 1 but between time 0 and $t^{\\prime}$ . It can be shown that for any $t\\in[0,t^{\\prime}]$ , $(\\mathbf{X}_{t})_{t\\in[0,t^{\\prime}]}$ associated with $\\mathbb{Q}_{t|0,t^{\\prime}}$ is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X}_{t}=\\mathrm{Interp}_{t}(\\mathbf{X}_{0},\\mathbf{X}_{t^{\\prime}},\\mathbf{Z})=\\left(1-\\frac{\\sigma_{t}^{2}}{\\sigma_{t^{\\prime}}^{2}}\\right)\\mathbf{X}_{0}+\\frac{\\sigma_{t}^{2}}{\\sigma_{t^{\\prime}}^{2}}\\mathbf{X}_{t^{\\prime}}+\\sigma_{t}\\sqrt{1-\\frac{\\sigma_{t}^{2}}{\\sigma_{t^{\\prime}}^{2}}}\\mathbf{Z},\\quad\\mathbf{Z}\\sim\\mathcal{N}(0,\\mathrm{Id}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Solving (43) is equivalent to solving (42). We now propose an algorithm to solve (43). It corresponds to the finetuning stage of Algorithm 1 with a specific initialisation, similar to DSBM-IPF in Shi et al. (2023). ", "page_idx": 30}, {"type": "text", "text": "By $v_{\\phi}$ , we denote a DDM model associated with $\\pi_{1}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}=v_{\\phi}(t,\\mathbf{X}_{t})\\mathrm{d}t+g_{t}\\mathrm{d}\\mathbf{B}_{t},\\quad\\mathbf{X}_{0}\\sim\\mathcal{N}(0,\\mathrm{Id}),\\quad\\mathbf{X}_{1}\\sim\\pi_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similarly, $v_{\\theta}$ denotes a diffusion model associated with $\\pi_{0}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{Y}_{t}=v_{\\theta}(t,\\mathbf{Y}_{t})\\mathrm{d}t+g_{t}\\mathrm{d}\\mathbf{B}_{t},\\quad\\mathbf{Y}_{0}\\sim\\mathcal{N}(0,\\mathrm{Id}),\\quad\\mathbf{Y}_{1}\\sim\\boldsymbol{\\pi}_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In analogy to Equation (11), the two equations above correspond to the forward and backward SDEs. For a given batch of inputs $\\mathbf{X}_{0}^{1:B}$ and $\\mathbf{X}_{1}^{1:B}$ , timesteps $t\\sim\\mathrm{Unif}([0,t^{\\prime}])^{\\otimes B}$ , and interpolations ${\\bf X}_{t}^{\\leftarrow}$ and $\\mathbf{X}_{t}^{\\rightarrow}$ , we compute the empirical forward and backward losses as the following modification of Equation (13): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ell^{*}(\\phi;t,\\mathbf{X}_{1},\\mathbf{X}_{t}^{*})=\\frac{1}{B}\\sum_{i=1}^{B}\\left\\|v_{\\phi}\\left(t^{i},\\mathbf{X}_{t}^{*i}\\right)-\\left(\\mathbf{X}_{1}^{i}-\\mathbf{X}_{t}^{*i}\\right)/\\sigma_{t}^{i}\\right\\|^{2}},}}\\\\ {{\\displaystyle\\ell^{*}(\\theta;t,\\mathbf{X}_{0},\\mathbf{X}_{t}^{*})=\\frac{1}{B}\\sum_{i=1}^{B}\\|v_{\\theta}\\left(t^{i},\\mathbf{X}_{t}^{*i}\\right)-\\left(\\mathbf{X}_{0}^{i}-\\mathbf{X}_{t}^{*i}\\right)/\\sqrt{\\sigma_{t^{\\prime}}^{2}-\\sigma_{t^{\\prime}}^{2}})\\|^{2}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Algorithm 5 corresponds to an online version of DSBM-IPF (Shi et al., 2023) with the initialisation given by two generative models. In Algorithm 5, we finetune the trained vector fields to solve the interpolation task. At inference time, the SDE associated with vector field $v_{\\theta}$ interpolates between $\\pi_{1}\\rightarrow\\pi_{0}$ , while the SDE associated with the vector field $v_{\\phi}$ interpolates between $\\pi_{0}\\to\\pi_{1}$ . ", "page_idx": 30}, {"type": "text", "text": "Algorithm 5 $\\alpha$ -Diffusion Schr\u00f6dinger Bridge Matching for DDM finetuning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1: Input: datasets $\\pi_{0}$ and $\\pi_{1}$ , number finetuning steps $N_{\\mathrm{finetuning}}$ , batch size $B$ , DDM parameters   \n$\\phi$ and $\\theta$ .   \n2: for $n\\in\\{1,\\ldots,N_{\\mathrm{finetuning}}\\}$ do   \n3: Sample ${\\bf X}_{1})\\sim(\\bar{\\pi_{0}}\\stackrel{\\leftarrow}{\\otimes}\\pi_{1})^{\\otimes B},t\\sim\\mathrm{Unif}([0,1]),{\\bf Z}^{1:B}\\sim\\mathcal{N}(0,\\mathrm{Id})^{\\otimes B}$   \n4: Sample $\\hat{\\mathbf X}_{t^{\\prime}}^{\\leftarrow}$ by solving (44) starting from $\\mathbf{X}_{0}$   \n5: Sample $\\hat{\\mathbf{X}}_{t^{\\prime}}^{\\rightarrow}$ by solving (45) starting from $\\mathbf{X}_{1}$   \n6: Sample $t^{\\leftarrow}\\sim\\mathrm{Unif}([0,t^{\\prime}])^{\\otimes B}$ , $\\mathbf{Z}^{\\leftarrow}\\sim\\mathcal{N}(0,\\operatorname{Id})^{\\otimes B}$ , and compute $\\mathbf{X}_{t}^{\\leftarrow}=\\mathrm{Interp}_{t^{\\leftarrow}}(\\mathbf{X}_{0},\\hat{\\mathbf{X}}_{t^{\\prime}}^{\\leftarrow},\\mathbf{Z}^{\\leftarrow})$   \n7: Sample $t^{\\rightarrow}\\sim\\mathrm{Unif}([0,t^{\\prime}])^{\\otimes B}$ , $\\mathbf{Z}^{\\rightarrow}\\sim\\mathcal{N}(0,\\mathrm{Id})^{\\otimes B}$ , and compute $\\mathbf{X}_{t}^{\\rightarrow}=\\mathrm{Interp}_{t^{\\rightarrow}}(\\mathbf{X}_{1},\\hat{\\mathbf{X}}_{t^{\\prime}}^{\\rightarrow},\\mathbf{Z}^{\\rightarrow})$   \n8: Update $\\theta$ with gradient step on $\\ell^{\\leftarrow}(\\theta;t^{\\leftarrow},{\\bf X}_{0},{\\bf X}_{t}^{\\leftarrow})$   \n9: Update $\\phi$ with gradient step on $\\ell^{\\rightarrow}(\\phi;t^{\\rightarrow},{\\bf X}_{1},{\\bf X}_{t}^{\\rightarrow})$ ", "page_idx": 30}, {"type": "text", "text": "10: end for ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "11: Output: $\\theta,\\phi$ parameters of the finetuned models ", "page_idx": 30}, {"type": "text", "text": "Our model stitching approach is related to Dual Diffusion Implicit Bridges (DDIB) (Su et al., 2023), which uses pretrained diffusion models, but without further finetuning. As highlighted in Shi et al. (2023), DDIB is inferior to DSBM in terms of quality and alignement of the samples. ", "page_idx": 30}, {"type": "text", "text": "H Extended related work ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We highlight links between our proposed flow and Sinkhorn flows in Appendix H.1. We draw connection between our practical approach and Reinforcement Learning in Appendix H.2. We discuss how $\\alpha$ -IMF is related to (incremental) Expectation-Maximisation in Appendix H.3. Finally, we discuss how our algorithm can be seen as an instance of continual learning in Appendix H.5. ", "page_idx": 31}, {"type": "text", "text": "H.1 Links with Sinkhorn flow ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we discuss the links between our approach and the Sinkhorn flow introduced by Karimi et al. (2024). We start by recalling how Sinkhorn flows are defined and then discuss how they are related to our approach. ", "page_idx": 31}, {"type": "text", "text": "$\\gamma$ -Sinkhorn and Sinkhorn flows. We first consider the static EOT problem and recall the Sinkhorn procedure, also called Iterative Proportional Fitting. We define a sequence of coupling $(\\bar{\\Pi}^{n},\\Pi^{n})_{n\\in\\mathbb{N}}$ , i.e. for any $n\\in\\mathbb{N},\\Pi^{n}\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ . We let $\\Pi^{0}=\\mathbb{Q}_{0,1}$ and we consider for any $n\\in\\mathbb N$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Pi^{n}=\\operatorname*{argmin}\\{\\mathrm{KL}(\\Pi\\mid\\Bar\\Pi^{n})\\,:\\,\\Pi\\in\\mathcal{P}(\\mathbb R^{d}\\times\\mathbb R^{d}),\\,\\Pi_{0}=\\pi_{0}\\},}\\\\ &{\\Bar{\\Pi}^{n+1}=\\operatorname*{argmin}\\{\\mathrm{KL}(\\Pi\\mid\\Pi^{n})\\,:\\,\\Pi\\in\\mathcal{P}(\\mathbb R^{d}\\times\\mathbb R^{d}),\\,\\Pi_{1}=\\pi_{1}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In Karimi et al. (2024), the authors generalise (46) by introducing an extra hyperparameter $\\gamma\\in(0,1]$ and defining ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Pi^{n}=\\mathrm{argmin}\\{\\mathrm{KL}(\\Pi\\mid\\bar{\\Pi}^{n})\\,:\\,\\Pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\,\\Pi_{1}=\\pi_{1}\\},}\\\\ &{\\bar{\\Pi}^{n+1}=\\mathrm{argmin}\\{\\gamma\\mathrm{KL}(\\Pi\\mid\\Pi^{n})+(1-\\gamma)\\mathrm{KL}(\\Pi\\mid\\bar{\\Pi}^{n})\\,:\\,\\Pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\,\\Pi_{0}=\\pi_{0}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using (Karimi et al., 2024, Lemma 2), we have that for any $\\gamma\\in(0,1]$ , any $n\\in\\mathbb N$ and any $x_{0},x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\mathrm{d}\\bar{\\Pi}^{n}/\\mathrm{d}\\mathbb{Q}_{0,1})(x_{0},x_{1})=\\exp[f_{\\gamma}^{n}(x_{0})+g_{\\gamma}^{n}(x_{1})],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with $f_{\\gamma}^{0}=g_{\\gamma}^{0}=0$ and for any $n\\in\\mathbb{N},\\gamma\\in(0,1]$ and $x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\ng_{\\gamma}^{n+1}(x_{1})=g_{\\gamma}^{n}(x_{1})-\\gamma\\log(\\mathrm{d}\\bar{\\Pi}_{1}^{n}/\\mathrm{d}\\pi_{1})(x_{1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In addition, using (Karimi et al., 2024, Equation (9)) we have that for any $n\\in\\mathbb{N}$ , $\\gamma\\in\\,(0,1]$ and x0 \u2208Rd ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{\\gamma}^{n}(x_{0})=-\\log\\left(\\int_{\\mathbb{R}^{d}}\\exp[g_{\\gamma}^{n}(x_{1})-(1/(2\\varepsilon))\\|x_{0}-x_{1}\\|^{2}]\\mathrm{d}\\pi_{1}(x_{1})\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "When letting $\\gamma\\to0$ , (48) and (49) suggest to consider for any $s\\geq0,x_{0},x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\bar{\\Pi}^{s}/\\mathrm{d}\\mathbb{Q}_{0,1})(x_{0},x_{1})=\\exp[f^{s}(x_{0})+g^{s}(x_{1})],\\qquad\\Pi^{s}=\\mathrm{argmin}\\{\\mathrm{KL}(\\Pi\\mid\\bar{\\Pi}^{s})~:~\\Pi,~\\Pi_{1}=\\pi_{1}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where for any $s\\geq0$ , $x_{1}\\in\\mathbb{R}^{d}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\partial_{s}g^{s}(x_{1})=-\\log(\\mathrm{d}\\bar{\\Pi}_{1}^{s}/\\mathrm{d}\\pi_{1})(x_{1}),\\qquad\\partial_{s}f^{s}(x_{0})=\\int_{\\mathbb{R}^{d}}\\log(\\mathrm{d}\\bar{\\Pi}_{1}^{s}/\\mathrm{d}\\pi_{1})(x_{1})\\mathrm{d}\\bar{\\Pi}^{s}(x_{1}|x_{0}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Comparison with Schr\u00f6dinger Bridge flows. In order to compare our approach with the one of Karimi et al. (2024), we start by rewriting the $\\gamma$ -Sinkhorn algorithm defined by (47). To do so, we introduce the projection on the measures with fixed marginal. ", "page_idx": 31}, {"type": "text", "text": "Definition H.1 (Projection on marginals): Let $\\Pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ and $\\pi_{0}\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d})$ , we define pro $|_{0,\\pi_{0}}(\\Pi)$ as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{proj}_{0,\\pi_{0}}(\\Pi)=\\mathrm{argmin}\\{\\mathrm{KL}(\\tilde{\\Pi}\\mid\\Pi)\\,:\\,\\tilde{\\Pi}\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\tilde{\\Pi}_{0}=\\pi_{0}\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, for any $\\Pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ and $\\pi_{1}\\in\\mathcal{P}(\\mathbb{R}^{d})$ , we define proj $_1,\\pi_{1}\\left(\\Pi\\right)$ as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{proj}_{1,\\pi_{1}}(\\Pi)=\\mathrm{argmin}\\{\\mathrm{KL}(\\tilde{\\Pi}\\mid\\Pi)\\,:\\,\\tilde{\\Pi}\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\tilde{\\Pi}_{1}=\\pi_{1}\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "table", "img_path": "1F32iCJFfa/tmp/9e04b4d2000fa383af84d3b35a9eeb052637bd6580aec20de3106ae862f00a8e.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison between $\\gamma$ -Sinkhorn and $\\scriptstyle\\overline{{\\gamma-\\mathrm{IMF}}}$ . "], "page_idx": 32}, {"type": "text", "text": "With these definitions, we have that for any $n\\in\\mathbb N$ , $\\bar{\\Pi}^{n+1}=\\mathrm{proj}_{0,\\pi_{0}}(\\mathrm{proj}_{1,\\pi_{1}}(\\bar{\\Pi}^{n}))$ , with $({\\bar{\\Pi}}^{n})_{n\\in\\mathbb{N}}$ the original Sinkhorn sequence defined by (46). Similarly, we have that the original Iterative Markovian Fitting (IMF) sequence $({\\hat{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ as defined in (4) with $\\alpha=1$ satisfies for any $n\\in\\mathbb N$ , $\\hat{\\mathbb{P}}^{n+1}=\\mathrm{proj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathrm{proj}_{\\mathcal{M}}(\\mathbb{P}^{n}))$ . The analogy between the Sinkhorn iterates and the IMF sequence was already highlighted in Shi et al. (2023); Peluchetti (2023) and further studied in Brekelmans and Neklyudov (2023). We know show that similarly, we can draw an analogy between the sequences defined in (4) with $\\alpha\\,\\in\\,(0,1)$ and the sequences obtained in $\\gamma$ -Sinkhorn. To do so, we start by introducing for any $\\Pi,\\tilde{\\Pi}\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\mathrm{IPF}}(\\Pi,\\tilde{\\Pi})=\\mathrm{KL}(\\Pi\\,|\\mathrm{proj}_{1,\\pi_{1}}(\\tilde{\\Pi})),\\qquad R^{\\mathrm{IPF}}(\\Pi,\\tilde{\\Pi})=\\mathrm{KL}(\\Pi\\,|\\tilde{\\Pi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "With this notation, we can now rewrite (47) for any $n\\in\\mathbb N$ as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{\\Pi}^{n+1}=\\operatorname*{argmin}\\{\\mathcal{L}^{\\mathrm{IPF}}(\\Pi,\\bar{\\Pi}^{n})+((1-\\gamma)/\\gamma)R^{\\mathrm{IPF}}(\\Pi,\\bar{\\Pi}^{n})\\,\\,:\\,\\,\\Pi\\in\\mathcal{P}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\,\\,\\Pi_{0}=\\pi_{0}\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, we are going to see that (50) is linked with the discretisation of the path measure flow described in (4). Recall that for any suitable $v$ , we define the path measure $\\mathbb{P}_{v}$ associated with ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm d\\mathbf X_{t}=v_{t}(\\mathbf X_{t})\\mathrm d t+\\sqrt\\varepsilon\\mathrm d\\mathbf B_{t},\\qquad\\mathbf X_{0}\\sim\\pi_{0}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}(v,\\mathbb{P})=\\int_{0}^{1}\\mathcal{L}(v_{t},\\mathbb{P})\\mathrm{d}t=\\int_{0}^{1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\left\\|v_{t}(x_{t})-\\frac{x_{1}-x_{t}}{1-t}\\right\\|^{2}\\mathrm{d}\\mathrm{proj}_{\\mathcal{R}(\\mathbb{Q})}(\\mathbb{P})_{t,1}(x_{t},x_{1})\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, for any $\\mu\\in{\\mathcal{P}}({\\mathcal{C}})$ , we define ", "page_idx": 32}, {"type": "equation", "text": "$$\nR_{\\mu}(\\mathbb{P}_{v},\\mathbb{P}_{\\tilde{v}})=\\int_{0}^{1}\\int_{\\mathbb{R}^{d}}\\|v_{t}(x_{t})-\\tilde{v}_{t}(x_{t})\\|^{2}\\mathrm{d}\\mu_{t}(x_{t})\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Next, we define the sequence of path measures $({\\bar{\\mathbb{P}}}^{n})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{\\mathbb{P}}^{n+1}=\\operatorname{argmin}\\{\\mathcal{L}(\\mathbb{P},\\bar{\\mathbb{P}}^{n})+(1/\\alpha)R_{\\mu^{n}}(\\Pi,\\bar{\\mathbb{P}}^{n})\\,:\\,\\mathbb{P}=\\mathbb{P}_{v},\\mathrm{~for~some~}v\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, if we denote $(v^{n})_{n\\in\\mathbb{N}}$ the sequence such that for any $n\\in\\mathbb N$ , $\\bar{\\mathbb{P}}^{n}=\\mathbb{P}_{v^{n}}$ then we have that for any $n\\in\\mathbb N$ , $t\\in[0,1]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{t}^{n+1}(x)=v_{t}^{n}(x)-\\delta\\nabla_{\\mu^{n}}\\mathcal{L}_{t}(v^{n+1},\\Bar{\\mathbb{P}}^{n})(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall that $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ given by (4) is associated with $(v^{n})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\ensuremath{\\mathbb{N}},\\,t\\in[0,1]$ and x \u2208Rd ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{t}^{n+1}(x)=v_{t}^{n}(x)-\\delta\\nabla_{\\mu}^{n}\\mathcal{L}_{t}(v^{n},\\mathbb{P}^{n})_{t}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "see Proposition 3.2. Therefore, the only difference between (52) and (51) is that (52) is an explicit update whereas (51) is an implicit update. We summarise the differences between $\\gamma$ -Sinkhorn and the discretisation we introduce in Table 2. ", "page_idx": 32}, {"type": "text", "text": "H.2 Links with Reinforcement Learning ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we draw some connection between Algorithm 1 and self-play in Reinforcement Learning. In particular, we introduce a generalisation of Algorithm 1 which uses the concept of replay buffer commonly used in Reinforcement learning, see Mnih et al. (2015) for instance. ", "page_idx": 32}, {"type": "text", "text": "We first present a generalisation of Algorithm 1 called Replay Buffer Diffusion Schr\u00f6dinger Bridge Matching Algorithm 6. We define a buffer $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ as a collection of samples $\\{(\\mathbf{X}_{0}^{k},\\mathbf{X}_{1}^{k})\\}_{k=1}^{\\check{N}}$ , where ", "page_idx": 32}, {"type": "text", "text": "$N\\;\\in\\;\\mathbb{N}$ is the size of the buffer equipped with two functions Add and Sample. We have that Add : $\\Omega\\times\\sqcup_{k\\in\\mathbb{N}}(\\mathbb{R}^{2d})^{k}\\times(\\mathbb{R}^{2d})^{N^{\\star}}\\vec{\\rightarrow}^{\\star}(\\mathbb{R}^{2d})^{N}$ , where $\\Omega$ is a probability space. In practice Add takes a rand om number (the function can be stochastic), any number of proposed samples as well as the current buffer. As an output Add returns the updated buffer. We also define Sample : $\\Omega\\times\\mathbb{N}\\times(\\mathbb{R}^{2d})^{N}\\to\\bigcup_{k\\in\\mathbb{N}}(\\mathbb{R}^{2d})^{k}$ . This function takes a random number (the function can be stochastic), a natural number $k$ representing the number of samples to return as well as the current buffer. As an output Sample returns a batch of $k$ samples from the buffer. ", "page_idx": 33}, {"type": "text", "text": "Algorithm 6 Replay Buffer Diffusion Schr\u00f6dinger Bridge Matching ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1: Input: $\\pi_{0},\\,\\pi_{1}$ , $\\varepsilon$ (entropic regularisation), $N_{\\mathrm{pretraining}}$ (number of pretraining steps), Nfinetuning   \n(number of finetuning steps), $B$ (batch size), $\\gamma$ (EMA parameter), $\\theta$ (initial parameters), $B^{\\mathrm{fwd}}$   \n(forward buffer), $\\beta^{\\mathrm{bwi}}$ (backward buffer)   \n2: $\\overset{\\cdot}{\\theta}=\\theta$   \n3: for $n\\in\\{0,\\ldots,N_{\\mathrm{pretraining}}\\}$ do   \n4: Sample $(\\mathbf{X}_{0}^{1:B},\\mathbf{\\dot{X}}_{1}^{1:B})\\sim(\\pi_{0}\\otimes\\pi_{1})^{\\otimes B},t\\sim\\operatorname{Unif}([0,1]),\\mathbf{Z}^{1:B}\\sim\\mathcal{N}(0,\\operatorname{Id})^{\\otimes B}$   \n5: Compute $\\mathbf{X}_{t}^{1:B}=\\mathrm{Interp}_{t}(\\mathbf{X}_{0}^{1:B},\\mathbf{X}_{1}^{1:B},\\mathbf{Z}^{1:B})$ using (12)   \n6: Update $\\theta$ with gradient step on $\\ell_{t}^{B}$ , $\\bar{\\theta}=(1-\\acute{\\gamma})\\bar{\\theta}+\\gamma\\theta$   \n7: end for   \n8: for $n\\in\\{0,\\ldots,N_{\\mathrm{finetuning}}\\}$ do   \n9: if $n\\equiv0[n_{\\mathrm{refresh}}]$ then   \n10: Sample $(\\hat{\\mathbf{X}}_{0}^{1:B},\\hat{\\mathbf{Y}}_{0}^{1:B})\\sim(\\pi_{0}\\otimes\\pi_{1})^{\\otimes B},t\\sim\\mathrm{Unif}([0,1]),\\mathbf{Z}^{1:B}\\sim\\mathcal{N}(0,\\mathrm{Id})^{\\otimes B}$   \n11: Sample $(\\mathbf{X}_{1}^{1:B},\\mathbf{Y}_{1}^{1:B})$ using (11) with initialisation $(\\hat{\\mathbf{X}}_{0}^{1:B},\\hat{\\mathbf{Y}}_{0}^{1:B})$   \n12: $\\mathcal{B}^{\\mathrm{fwd}}=\\mathrm{Add}((\\hat{\\mathbf{X}}_{0}^{1:B},\\mathbf{X}_{1}^{1:B}),\\mathcal{B}^{\\mathrm{fwd}})$   \n13: $\\mathcal{B}^{\\mathrm{bwd}}=\\mathrm{Add}((\\mathbf{Y}_{1}^{B},\\hat{\\mathbf{Y}}_{0}^{1:B}),\\mathcal{B}^{\\mathrm{bwd}})$   \n14: end if   \n15: $(\\hat{\\mathbf{X}}_{0}^{1:B},\\mathbf{X}_{1}^{1:B})=\\operatorname{Sample}(B,\\mathcal{B}^{\\mathrm{fwd}})$   \n16: $(\\mathbf{Y}_{1}^{1:B},\\hat{\\mathbf{Y}}_{0}^{1:B})=\\operatorname{Sample}(B,\\mathcal{B}^{\\mathrm{bwd}})$   \n17: Compute $\\mathbf{X}_{t}^{1:B}=\\mathrm{Interp}_{t}(\\hat{\\mathbf{X}}_{0}^{1:B},\\mathbf{X}_{1}^{1:B},\\mathbf{Z}^{1:B})$ using (12)   \n18: Compute $\\mathbf{Y}_{1-t}^{1:B}=\\mathrm{Interp}_{t}(\\mathbf{Y}_{1}^{1:B},\\hat{\\mathbf{Y}}_{0}^{1:B},\\mathbf{Z}^{1:B})$ using (12)   \n19: Update $\\theta$ with gradient step on $\\ell_{t}^{B}$ , $\\bar{\\theta}=(1-\\gamma)\\bar{\\theta}+\\gamma\\theta$   \n20: end for   \n21: Output: $(\\theta,{\\bar{\\theta}})$ parameters of the finetuned model ", "page_idx": 33}, {"type": "text", "text": "In Algorithm 6, we allow for more flexibility than the online procedure by leveraging the concept of replay buffer originally introduced in Reinforcement Learning Mnih et al. (2015). The concept of replay buffer has been used previously in Schr\u00f6dinger Bridge works, with the notion of cache where every $n_{\\mathrm{refresh}}$ steps a cache is emptied and fliled with new samples. If $n_{\\mathrm{refresh}}=1$ , $N=B$ for both $B^{\\mathrm{fwd}}$ and $B^{\\mathrm{fwd}}$ we have that for any $\\omega\\in{\\Omega}$ and $(\\mathbf{X}_{0}^{1:B},\\mathbf{X}_{1}^{1:B})\\mathbf{\\widehat{\\xi}}\\in(\\mathbb{R}^{2d})^{\\bar{B}}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{Add}(\\omega,(\\mathbf{X}_{0}^{1:B},\\mathbf{X}_{1}^{1:B}),\\mathcal{B})=(\\mathbf{X}_{0}^{1:B},\\mathbf{X}_{1}^{1:B})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{Sample}(\\omega,B,({\\bf X}_{0}^{1:B},{\\bf X}_{1}^{1:B}))=({\\bf X}_{0}^{1:B},{\\bf X}_{1}^{1:B}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This means that the Add simply fills the buffer with the new samples while Sample just return the whole current buffer. In that case we recover Algorithm 1. For more general update rules, the replay buffers $B^{\\mathrm{fwd}}$ and $B^{\\mathrm{bwd}}$ allow us to collect previous samples and therefore to keep a memory of the past experiences. In future work, we plan to investigate popular choice in experience replay and their impact on the performance of Algorithm 6. ", "page_idx": 33}, {"type": "text", "text": "H.3 Links with Expectation Maximisation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we make a connection between DSBM and the Expectation Maximisation (EM) algorithm, and show that the discretisation of the Schr\u00f6dinger Flow proposed in Algorithm 1 corresponds to some incremental version of an idealised algorithm, as discussed in Neal and Hinton (1998). We would like to emphasize that the link between the EM algorithm and Diffusion Schr\u00f6dinger Bridge based methodologies was already highlighted by Vargas et al. (2024); Brekelmans and Neklyudov (2023). Below, we follow the framework of Brekelmans and Neklyudov (2023) and recall the following definitions. ", "page_idx": 33}, {"type": "text", "text": "Definition H.2 (Projections and maximisations): Let A be a subset of $\\mathscr{P}(\\mathscr{C})$ . Then, for any $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ , when it is well-defined, we define its E-projection on A as $\\mathbb{P}^{\\star}=\\operatorname{argmin}_{\\mathbb{Q}\\in\\mathsf{A}}\\mathrm{KL}(\\mathbb{Q}\\mid\\mathbb{P})$ . Similarly, for any $\\mathbb{P}\\in{\\mathcal{P}}({\\mathcal{C}})$ , when it is well-defined, we define its M-projection on A as $\\mathbb{P}^{\\star}=$ $\\mathrm{argmin}_{\\mathbb{Q}\\in\\mathsf{A}}\\mathrm{KL}(\\mathbb{P}\\mid\\mathbb{Q})$ . ", "page_idx": 34}, {"type": "text", "text": "In Brekelmans and Neklyudov (2023), the authors choose M-projection because this corresponds to the Maximisation step in an EM algorithm while the $\\mathrm{E}$ -projection corresponds to the expectation step in the EM algorithm. In Brekelmans and Neklyudov (2023), the authors highlight that the Iterative Proportional Fitting procedure is a Expectation-Expectation procedure, i.e. the alternating projections are both $\\mathrm{E}$ -projections. In contrast, the Iterative Markovian Fitting procedure is a MaximisationMaximisation procedure, i.e. the alternating projections are both M-projections. In particular, we can define the following sequence of path measures $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ , where for any $n\\in\\mathbb N$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{n+1/2}=\\operatorname*{argmin}_{\\mathbb{P}\\in\\operatorname{proj}_{M}}\\mathrm{KL}(\\mathbb{P}^{n}\\mid\\mathbb{P}),\\qquad\\mathbb{P}^{n+1}=\\operatorname*{argmin}_{\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})}\\mathrm{KL}(\\mathbb{P}^{n+1/2}\\mid\\mathbb{P}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In addition, we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{n+1/2}=\\mathbb{P}_{v_{*}^{n+1}},\\qquad v_{\\star}^{n+1}=\\operatorname*{argmin}_{v}\\mathcal{L}(v,\\mathbb{P}^{n}),\\qquad\\mathbb{P}^{n+1}=\\operatorname*{argmin}_{\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})}\\mathrm{KL}(\\mathbb{P}_{v_{*}^{n+1}}\\mid\\mathbb{P}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "since we have that $\\mathbb{P}^{n}=\\mathbb{P}^{v_{\\star}^{n}}$ . Hence, our online procedure Algorithm 1, which corresponds to the discretisation of the flow of path measures (3) can be rewritten as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}^{n+1/2}=\\mathbb{P}_{v_{\\star}^{n+1}},\\quad v_{\\star}^{n+1}=\\operatorname{Gradientstep}(\\mathcal{L}(v,\\mathbb{P}^{n}),\\quad\\mathbb{P}^{n+1}=\\operatorname{argmin}_{\\mathbb{P}\\in\\mathcal{R}(\\mathbb{Q})}\\mathrm{KL}(\\mathbb{P}_{v_{\\star}^{n+1}}\\mid\\mathbb{P}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, our proposed algorithm can be seen as an incremental version of the MaximisationMaximisation algorithm associated with DSBM instead of an incremental version of the ExpectationMaximisation algorithm discussed in (Neal and Hinton, 1998). ", "page_idx": 34}, {"type": "text", "text": "H.4 Links with finetuning of diffusion models ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Algorithm 1 can be seen as a method to finetune bridge matching. Finetuning of diffusion models and flow matching procedures is an active research area. Most of the existing methodologies optimise for an external cost after a pretraining phase. These procedures rely on Reinforcement Learning strategies (Lee et al., 2023; Black et al., 2023; Fan et al., 2024). Recently Direct Preference optimisation (DPO) (Rafailov et al., 2024) has been applied to the finetuning of diffusion models in (Yang et al., 2023; Rafailov et al., 2024). Our approach departs from these works as the objective we minimise is given by the EOT cost. However all of these approaches involve some level of self-play, i.e. are not simulation free. ", "page_idx": 34}, {"type": "text", "text": "H.5 Links with continual learning ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Continual learning develops techniques to train models when the dataset changes during the training, usually to solve different tasks De Lange et al. (2021); Parisi et al. (2019); Zaj a\u02dbc et al. (2023). In the context of diffusion models, continual learning has been investigated in Masip et al. (2023); Zaj a\u02dbc et al. (2023); Smith et al. (2023). In (Masip et al., 2023), the authors consider a weighted loss between a diffusion model loss and a distillation loss which ensures some consistency between the model being trained and the previous task model. Similarly to our approach this distillation loss is not simulation-free but, contrary to our loss, the clean samples are not obtained by unrolling the diffusion model but by applying a one-step prediction operator. In (Zaj a\u02dbc et al., 2023), consider different replay buffer techniques to train continual diffusion models and observe that experience replay with a small coefficient can bring improvements. Finally, in (Smith et al., 2023), the authors consider the continual training of a text-to-image diffusion model with LoRA (Hu et al., 2021). ", "page_idx": 34}, {"type": "text", "text": "I Forward-Forward, Forward-Backward and accumulation of error ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we investigate how error accumulates in the context of DSBM. In practice, we observe similar conclusions in the case of the online version of DSBM. We compare two methods: one which only trains a forward model and one which trains a forward and a backward model. ", "page_idx": 34}, {"type": "text", "text": "In\u221a what follows, we assume that $\\pi_{0}=\\pi_{1}={\\mathcal N}(0,\\mathrm{Id})$ , we also assume that $\\mathbb{Q}$ is associated with $(\\sqrt{2}\\mathbf{B}_{t})_{t\\in[0,1]}$ . We recall that for any $t\\in[0,1]$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}=(1-t)\\mathbf{X}_{0}+t\\mathbf{X}_{1}+{\\sqrt{2t(1-t)}}\\mathbf{Z},\\qquad\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We are going to consider to approximate schemes to implement IMF. ", "page_idx": 35}, {"type": "text", "text": "Forward-forward. First, we consider the following sequence of path measures $(\\mathbb{P}^{n})_{n\\in\\mathbb{N}}$ . We set $\\mathbb{P}^{0}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ . For any $n\\in\\mathbb N$ , we define $\\mathbb{P}^{2n+2}=\\mathbb{P}_{0,1}^{2n+1}\\mathbb{Q}_{|0,1}$ , i.e. $\\mathbb{P}^{2n+2}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{2n+1})$ In addition, we define $\\mathbb{P}^{2n+1}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P}^{2n})$ such that $\\mathbb{P}^{2n+1}$ is associated with $(\\mathbf{X}_{t})_{t\\in[0,1]}$ where for any $t\\in[0,1]$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}=\\{(\\mathbb{E}_{\\mathbb{P}_{1\\mid t}^{2n}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}]-\\mathbf{X}_{t})/(1-t)+\\varepsilon\\mathbf{X}_{t}\\}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\mathbf{X}_{0}\\sim\\pi_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with $\\varepsilon\\in\\mathbb R$ . Recall that if we define ${\\bar{\\mathbb{P}}}^{2n+1}=\\mathrm{proj}_{\\mathcal{M}}(\\mathbb{P}^{2n})$ we have that for any $t\\in[0,1]$ , $\\bar{\\mathbb{P}}^{2n+1}$ is associated with $(\\mathbf{X}_{t})_{t\\in[0,1]}$ where for any $t\\in[0,1]$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=(\\mathbb{E}_{\\mathbb{P}_{1\\mid t}^{2n}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}]-\\mathbf{X}_{t})/(1-t)\\mathrm{d}t+{\\sqrt{2}}\\mathrm{d}\\mathbf{B}_{t}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, $\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}$ corresponds to making an error of order $x\\mapsto\\varepsilon x$ on the estimated velocity field. Doing so, we now longer have that for any $n\\in\\mathbb N$ , $\\mathbb{P}_{1}^{n}=\\pi_{1}$ . In what follows, we are going to show how the error accumulates for the sequence P0n,1. ", "page_idx": 35}, {"type": "text", "text": "Before stating Proposition I.1, we introduce $f:\\,\\mathbb{R}^{4}\\to\\mathbb{R}$ such that for any $c_{0,0},c_{1,1},c_{0,1}>0$ and $t\\in[0,1]$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(c_{0,0},c_{1,1},c_{0,1},t)=[-(1-t)c_{0,0}+t c_{1,1}+(1-2t)c_{0,1}-2t]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad/[(1-t)^{2}c_{0,0}+t^{2}c_{1,1}+2t(1-t)c_{0,1}+2t(1-t)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We define $\\begin{array}{r}{F(c_{0,0},c_{1,1},c_{0,1},\\varepsilon,t)=2\\int_{0}^{t}f(c_{0,0},c_{1,1},c_{0,1},s)\\mathrm{d}s+2\\varepsilon t.}\\end{array}$ Finally, we define ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\mathrm{cov}}(c_{0,0},c_{1,1},c_{0,1},\\varepsilon)=\\exp[\\frac{1}{2}F(c_{0,0},c_{1,1},c_{0,1},\\varepsilon,1)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as well as ", "page_idx": 35}, {"type": "equation", "text": "$$\nf_{\\mathrm{var}}(c_{0,0},c_{1,1},c_{0,1},\\varepsilon)=\\exp[\\frac{1}{2}F(c_{0,0},c_{1,1},c_{0,1},\\varepsilon,1)](1+2\\int_{0}^{1}\\exp[-F(c_{0,0},c_{1,1},c_{0,1},\\varepsilon,s)]\\mathrm{d}s).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proposition I.1 (Forward-Forward updates): For any $\\begin{array}{l l l}{n}&{\\in}&{\\mathbb N,}\\end{array}$ , we have that $\\mathbb{P}_{0,1}^{2n+1}\\;=\\;$ ${\\mathcal{N}}(0,\\Sigma^{n+1}\\mathrm{Id})$ where ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Sigma^{n+1}=\\binom{\\mathrm{Id}}{c_{0,1}^{n+1}\\mathrm{Id}}\\;\\;\\;c_{0,1}^{n+1}\\mathrm{Id}\\bigg)\\,,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and for any $n\\in\\mathbb N$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{c_{1,1}^{n+1}=f_{\\mathrm{var}}(1,c_{1,1}^{n},c_{0,1}^{n},\\varepsilon),}\\\\ {c_{0,1}^{n+1}=f_{\\mathrm{cov}}(1,c_{1,1}^{n},c_{0,1}^{n},\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let $\\mathbb{P}=(\\mathbb{P}_{0,1})\\mathbb{Q}_{|0,1}$ where $\\mathbb{P}_{0,1}$ is a Gaussian random variable with zero mean and covariance matrix $\\Sigma\\in\\mathbb{R}^{2d\\times2d}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Sigma=\\left(\\!\\!\\begin{array}{c c}{{\\mathrm{Id}}}&{{c_{0,1}\\mathrm{Id}}}\\\\ {{c_{0,1}\\mathrm{Id}}}&{{c_{1,1}\\mathrm{Id}}}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where Id is the $d$ -dimensional identity matrix and we assume that $c_{0,1},c_{1,1}~>~0$ . We denote $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P})$ . We have that $\\mathbb{P}_{1|t}$ is a Gaussian random variable with zero mean. We now compute its covariance matrix. First, we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{X}_{t}\\mathbf{X}_{1}^{\\top}]=(1-t)\\mathbb{E}[\\mathbf{X}_{0}\\mathbf{X}_{1}^{\\top}]+t\\mathbb{E}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]=[(1-t)c_{0,1}+t c_{1,1}]\\mathrm{Id}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We also have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[{\\mathbf{X}}_{t}{\\mathbf{X}}_{t}^{\\top}]=(1-t)^{2}\\mathbb{E}[{\\mathbf{X}}_{0}{\\mathbf{X}}_{0}^{\\top}]+t(1-t)(\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{0}^{\\top}]+\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{0}^{\\top}])+t^{2}\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{1}^{\\top}]+2t(1-t)\\mathrm{Id}}\\\\ &{~~~~~~~~~~~~~~~=[(1-t)^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}+2t(1-t)]\\mathrm{Id}}\\\\ &{~~~~~~~~~~~~~~~=[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}]\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, we get that for any $t\\in[0,1]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}_{1\\mid t}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]=([(1-t)c_{0,1}+t c_{1,1}]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}])x_{t}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, we have that for any $t\\in[0,1]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{E}_{\\mathbb{P}_{1\\mid t}}\\big[{\\bf X}_{1}\\mid{\\bf X}_{t}=x_{t}\\big]-x_{t}=([(1-t)c_{0,1}+t c_{1,1}]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}]-1)x_{t}}\\\\ &{\\quad=([(1-t)c_{0,1}+t c_{1,1}-1+t^{2}-t^{2}c_{1,1}-2t(1-t)c_{0,1}]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}])x_{t}}\\\\ &{\\quad=([(1-t)(1-2t)c_{0,1}+t(1-t)c_{1,1}-1+t^{2}]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}])x_{t}}\\\\ &{\\quad=(1-t)([(1-2t)c_{0,1}+t c_{1,1}-1-t]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}])x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "So it follows that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(\\mathbb{E}_{\\mathbb{P}_{1\\mid t}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t})/(1-t)}\\\\ &{\\qquad\\qquad=([(1-2t)c_{0,1}+t c_{1,1}-1-t]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}])x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that if we set $c_{0,1}=c^{2}$ and $c_{1,1}=1$ , we recover (Shi et al., 2023, Lemma 13) with $\\sigma=2$ . Denote $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P})$ . Combining (54) and (53) we get that $\\mathbb{P}^{\\star}$ is associated with $(\\mathbf{X}_{t})_{t\\in[0,1]}$ such that for any $t\\in[0,1]$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}=\\{([(1-2t)c_{0,1}+t c_{1,1}-1-t]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}])+\\varepsilon\\}\\mathbf{X}_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, we get that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathfrak{C}_{t}=\\exp\\bigl[\\frac{1}{2}G(t,c_{0,1},c_{1,1},\\varepsilon)\\bigr]\\mathbf{X}_{0}+\\Bigl(2\\int_{0}^{t}\\exp[-G(s,c_{0,1},c_{1,1},\\varepsilon)\\bigr]\\mathrm{d}s\\exp[G(t,c_{0,1},c_{1,1},\\varepsilon)]\\Bigr)^{1/2}\\mathbf{Z},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id})$ is independent from $\\mathbf{X}_{0}$ and for any $t\\in[0,1],c_{0,1},c_{1,1},\\varepsilon>0$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nG(t,c_{0,1},c_{1,1},\\varepsilon)=2\\int_{0}^{t}[(1-2t)c_{0,1}+t c_{1,1}-1-t]/[1-t^{2}+t^{2}c_{1,1}+2t(1-t)c_{0,1}]\\mathrm{d}t+2\\varepsilon t.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In addition, we define ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{\\mathrm{cov}}\\big(c_{0,1},c_{1,1},\\varepsilon\\big)=\\exp[G(1,c_{0,1},c_{1,1},\\varepsilon)],}\\\\ &{g_{\\mathrm{var}}\\big(c_{0,1},c_{1,1},\\varepsilon\\big)=\\exp[G(1,c_{0,1},c_{1,1},\\varepsilon)]\\Big(1+2\\int_{0}^{1}\\exp[-G(t,c_{0,1},c_{1,1},\\varepsilon)]\\mathrm{d}t\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[{\\mathbf{X}}_{0}{\\mathbf{X}}_{1}^{\\top}]=g_{\\mathrm{cov}}(c_{0,1},c_{1,1},\\varepsilon)\\mathrm{Id},\\qquad\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{1}^{\\top}]=g_{\\mathrm{var}}(c_{0,1},c_{1,1},\\varepsilon)\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, since for any $n\\in\\mathbb N$ , we have that $\\mathbb{P}^{2n+1}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P}^{2n})$ and $\\mathbb{P}^{2n+2}=\\mathbb{P}_{0,1}^{2n+1}\\mathbb{Q}_{|0,1}$ , we define $(c_{0,1}^{n},c_{1,1}^{n})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{2n}}[\\mathbf{X}_{0}\\mathbf{X}_{1}^{\\top}]=c_{0,1}^{n}\\mathrm{Id},\\qquad\\mathbb{E}_{\\mathbb{P}^{2n}}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]=c_{1,1}^{n}\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that for any $n\\in\\mathbb{N}$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{2n+1}}[\\mathbf{X}_{0}\\mathbf{X}_{1}^{\\top}]=c_{0,1}^{n+1}\\mathrm{Id},\\qquad\\mathbb{E}_{\\mathbb{P}^{2n+1}}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]=c_{1,1}^{n+1}\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $\\mathbb{P}^{0}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ we get that $c_{0,1}^{0}=0$ and $c_{1,1}=1$ . We have that for any $n\\in\\mathbb N$ ", "page_idx": 36}, {"type": "equation", "text": "$$\nc_{0,1}^{n+1}=g_{\\mathrm{cov}}(c_{0,1}^{n},c_{1,1}^{n},\\varepsilon),\\qquad c_{1,1}^{n+1}=g_{\\mathrm{var}}(c_{1,1}^{n},c_{1,1}^{n},\\varepsilon),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 36}, {"type": "text", "text": "Forward-backward. Next, we consider the following sequences of path measures $(\\mathbb{P}^{n,\\to})_{n\\in\\mathbb{N}}$ and $(\\mathbb{P}^{n,\\leftarrow})_{n\\in\\mathbb{N}}$ $\\mathbb{P}^{0,\\diamond}=\\mathbb{P}^{0,\\bullet}=(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ $n\\in\\mathbb N$ , nwd $\\mathbb{P}^{2n+2,\\diamond}=\\mathbb{P}_{0,1}^{2n+1,\\bullet}\\mathbb{Q}_{|0,1}$ $\\mathbb{P}^{2n+2,\\leftarrow}=\\mathbb{P}_{0,1}^{2n+1,\\rightarrow}\\mathbb{Q}_{|0,1}$ $\\mathbb{P}^{2n+2,\\rightarrow}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{2n+1,\\leftarrow})$ $\\mathbb{P}^{2n+2,\\epsilon}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{2n+1,\\rightarrow})$ addition, we define $\\mathbb{P}^{2n+1,\\to}=\\mathrm{proj}_{\\mathcal{M}}^{\\ensuremath{\\varepsilon},\\Phi}(\\mathbb{P}^{2n,\\star})$ such that for any $t\\in[0,1]$ , $\\mathbb{P}^{2n+1,\\rightarrow}$ is associated with $(\\mathbf{X}_{t})_{t\\in[0,1]}$ where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{X}_{t}=\\{(\\mathbb{E}_{\\mathbb{P}_{1|t}^{2n,\\star}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}]-\\mathbf{X}_{t})/(1-t)+\\varepsilon\\mathbf{X}_{t}\\}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\mathbf{X}_{0}\\sim\\pi_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with $\\varepsilon\\in\\mathbb R$ . Similarly, we define $\\mathbb{P}^{2n+1,\\epsilon}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\epsilon}(\\mathbb{P}^{2n,\\rightarrow})$ such that for any $t\\in[0,1],\\mathbb{P}^{2n+1,\\epsilon}$ is associated with $(\\mathbf{Y}_{1-t})_{t\\in[0,1]}$ where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{Y}_{t}=\\{(\\mathbb{E}_{\\mathbb{P}_{0\\mid t}^{2n,\\star}}[\\mathbf{X}_{0}\\mid\\mathbf{Y}_{t}]-\\mathbf{Y}_{t})/(1-t)+\\varepsilon\\mathbf{Y}_{t}\\}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\mathbf{Y}_{0}\\sim\\pi_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proposition I.2 (Forward-Backward updates): For any $\\textit{n}\\in\\mathbb{N}$ , we have that $\\mathbb{P}_{0,1}^{2n+1,\\ast}\\;=\\;$ ${\\mathcal{N}}(0,\\Sigma^{n+1,\\ast}\\mathrm{Id})$ and $\\mathbb{P}_{0,1}^{2n+1,\\epsilon}=\\mathcal{N}(0,\\Sigma^{n+1,\\epsilon}\\mathrm{Id})$ where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Sigma^{n+1,\\diamond}=\\left(\\!\\!\\begin{array}{c c}{\\mathrm{Id}}&{c_{0,1}^{n+1,\\diamond}\\mathrm{Id}}\\\\ {c_{0,1}^{n+1,\\diamond}\\mathrm{Id}}&{c_{1,1}^{n+1,\\diamond}\\mathrm{Id}}\\end{array}\\!\\!\\right),\\qquad\\Sigma^{n+1,\\bullet}=\\left(\\!\\!\\begin{array}{c c}{c_{0,0}^{n+1,\\bullet}\\mathrm{Id}}&{c_{0,1}^{n+1,\\bullet}\\mathrm{Id}}\\\\ {c_{0,1}^{n+1,\\bullet}\\mathrm{Id}}&{\\mathrm{Id}}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and for any $n\\in\\mathbb N$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{1,1}^{n+1,\\diamond}=f_{\\mathrm{var}}(c_{0,0}^{n,\\bullet},1,c_{0,1}^{n,\\bullet},\\varepsilon),}\\\\ &{c_{0,1}^{n+1,\\diamond}=f_{\\mathrm{cov}}(c_{0,0}^{n,\\bullet},1,c_{0,1}^{n,\\bullet},\\varepsilon),}\\\\ &{c_{0,0}^{n+1,\\leftarrow}=f_{\\mathrm{var}}(1,c_{1,1}^{n,\\bullet},c_{0,1}^{n,\\bullet},\\varepsilon),}\\\\ &{c_{0,1}^{n+1,\\leftarrow}=f_{\\mathrm{cov}}(1,c_{1,1}^{n,\\diamond},c_{0,1}^{n,\\bullet},\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The proof is similar to the one of Proposition I.1. ", "page_idx": 37}, {"type": "text", "text": "Proof. Let $\\mathbb{P}=(\\mathbb{P}_{0,1})\\mathbb{Q}_{|0,1}$ where $\\mathbb{P}_{0,1}$ is a Gaussian random variable with zero mean and covariance matrix $\\Sigma\\in\\mathbb{R}^{2d\\times2d}$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Sigma=\\left(_{c_{0,1}\\mathrm{Id}}^{c_{0,0}\\mathrm{Id}}-_{\\mathrm{Id}}^{c_{0,1}\\mathrm{Id}}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where Id is the $d$ -dimensional identity matrix and $c_{0,1},c_{0,0}>0$ . We denote $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P})$ . We have that $\\mathbb{P}_{1|t}$ is a Gaussian random variable with zero mean. We now compute its covariance matrix. First, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{X}_{t}\\mathbf{X}_{1}^{\\top}]=(1-t)\\mathbb{E}[\\mathbf{X}_{0}\\mathbf{X}_{1}^{\\top}]+t\\mathbb{E}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]=[(1-t)c_{0,1}+t]\\mathrm{Id}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We also have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[{\\mathbf{X}}_{t}{\\mathbf{X}}_{t}^{\\top}]=(1-t)^{2}\\mathbb{E}[{\\mathbf{X}}_{0}{\\mathbf{X}}_{0}^{\\top}]+t(1-t)(\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{0}^{\\top}]+\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{0}^{\\top}])+t^{2}\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{1}^{\\top}]+2t(1-t)\\mathrm{Id}}\\\\ &{~~~~~~~~~~~~~~~=[(1-t)^{2}c_{0,0}+t^{2}+2t(1-t)c_{0,1}+2t(1-t)]\\mathrm{Id}}\\\\ &{~~~~~~~~~~~~~~~=[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}]\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we get that for any $t\\in[0,1]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}_{1\\mid t}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]=([(1-t)c_{0,1}+t]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}])x_{t}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, we have that for any $t\\in[0,1]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{\\mathbb{P}_{1\\mid t}}[\\mathbf{X}_{1}\\mid\\mathbf{X}_{t}=x_{t}]-x_{t}=([(1-t)c_{0,1}+t]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}]-1)x_{t}}\\\\ &{\\qquad=([(1-t)c_{0,1}+t-2t+t^{2}-(1-t)^{2}c_{0,0}-2t(1-t)c_{0,1}]}\\\\ &{\\qquad\\qquad/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}])x_{t}}\\\\ &{\\qquad=([(1-t)(1-2t)c_{0,1}-(1-t)^{2}c_{0,0}-t(1-t)]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}])x_{t}}\\\\ &{\\qquad=(1-t)([(1-2t)c_{0,1}-(1-t)c_{0,0}-t]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}])x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, we have that for any $t\\in[0,1]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(\\mathbb{E}_{\\mathbb{P}_{1}\\mid\\pmb{\\mathscr{K}}_{1}}\\,\\big|\\,\\mathbf{X}_{t}=x_{t}\\big]-x_{t}\\big)/(1-t)}\\\\ &{\\qquad\\qquad=([(1-2t)c_{0,1}-(1-t)c_{0,0}-t]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}])x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that if we set $c_{0,1}=c^{2}$ and $c_{0,0}=1$ , we recover (Shi et al., 2023, Lemma 13) with $\\sigma=2$ Denote $\\mathbb{P}^{\\star}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P})$ . Combining (55) and (53) we get that $\\mathbb{P}^{\\star}$ is associated with $(\\mathbf{X}_{t})_{t\\in[0,1]}$ such that for any $t\\in[0,1]$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vert\\mathbf{X}_{t}=\\{([(1-2t)c_{0,1}-(1-t)c_{0,0}-t]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}])+\\varepsilon\\}\\mathbf{X}_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, we get that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathfrak{C}_{t}=\\exp[\\frac{1}{2}H(t,c_{0,1},c_{0,0},\\varepsilon)]\\mathbf{X}_{0}+(2\\int_{0}^{t}\\exp[-H(s,c_{0,1},c_{0,0},\\varepsilon)]\\mathrm{d}s\\exp[H(t,c_{0,1},c_{0,0},\\varepsilon)])^{1/2}\\mathbf{Z},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\mathbf{Z}\\sim{\\mathcal{N}}(0,\\operatorname{Id})$ is independent from $\\mathbf{X}_{0}$ and for any $t\\in[0,1],c_{0,1},c_{1,1},\\varepsilon>0$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{I}(t,c_{0,1},c_{0,0},\\varepsilon)=2\\int_{0}^{t}[(1-2t)c_{0,1}-(1-t)c_{0,0}-t]/[2t-t^{2}+(1-t)^{2}c_{0,0}+2t(1-t)c_{0,1}]\\mathrm{d}t+2\\varepsilon t.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In addition, we define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{g_{\\mathrm{cov}}(c_{0,1},c_{0,0},\\varepsilon)=\\mathrm{exp}[\\frac{1}{2}H(1,c_{0,1},c_{0,0},\\varepsilon)],}\\\\ {g_{\\mathrm{var}}(c_{0,1},c_{0,0},\\varepsilon)=\\mathrm{exp}[H(1,c_{0,1},c_{0,0},\\varepsilon)]\\Big(1+2\\int_{0}^{1}\\mathrm{exp}[-H(t,c_{0,1},c_{0,0},\\varepsilon)]\\mathrm{d}t\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[{\\mathbf{X}}_{0}{\\mathbf{X}}_{1}^{\\top}]=g_{\\mathrm{cov}}(c_{0,1},c_{0,0},\\varepsilon)\\mathrm{Id},\\qquad\\mathbb{E}[{\\mathbf{X}}_{1}{\\mathbf{X}}_{1}^{\\top}]=g_{\\mathrm{var}}(c_{0,1},c_{0,0},\\varepsilon)\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Remember that $\\mathbb{P}^{0,\\bullet}~=~\\mathbb{P}^{0,\\bullet}~=~(\\pi_{0}~\\otimes~\\pi_{1})\\mathbb{Q}_{|0,1}$ . In addition, for any $\\textit{n}\\in\\mathbb{N}$ , we have $\\mathbb{P}^{2n+2,\\rightarrow}~=~\\mathbb{P}_{0,1}^{2n+1,\\leftarrow}\\mathbb{Q}_{|0,1}$ and $\\mathbb{P}^{2n+2,\\epsilon}~=~\\mathbb{P}_{0,1}^{2n+1,\\diamond}\\mathbb{Q}_{|0,1}$ , i.e. $\\mathbb{P}^{2n+2,\\diamond}~=~\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{2n+1,\\bullet})$ and $\\mathbb{P}^{2n+2,\\epsilon}=\\mathrm{proj}_{\\mathcal{R}}(\\mathbb{P}^{2n+1,\\rightarrow})$ . In addition, we also have $\\mathbb{P}^{2n+1,\\bullet}=\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}(\\mathbb{P}^{2n,\\star})$ and $\\mathbb{P}^{2n+1,\\leftarrow}=$ pr $)\\!\\!\\!\\stackrel{\\varepsilon,\\leftarrow}{\\mathcal{M}}\\!\\!\\!\\binom{\\!\\!\\mathbb{P}^{2n,\\rightarrow}}{\\!\\!\\!\\mathcal{M}}\\!\\!\\!\\binom{\\!\\!\\mathbb{P}^{2n,\\rightarrow}}{\\!\\!\\!\\mathcal{M}}$ . We also define $(c_{0,1}^{n,\\rightarrow},c_{1,1}^{n,\\rightarrow})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb N$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{2n,*}}[\\mathbf{X}_{0}\\mathbf{X}_{1}^{\\top}]=c_{0,1}^{n,\\ast}\\mathrm{Id},\\qquad\\mathbb{E}_{\\mathbb{P}^{2n,*}}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]=c_{1,1}^{n,\\ast}\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, we define $(c_{0,1}^{n,\\epsilon},c_{1,1}^{n,\\epsilon})_{n\\in\\mathbb{N}}$ such that for any $n\\in\\mathbb{N}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{2n,\\epsilon}}[\\mathbf{X}_{0}\\mathbf{X}_{1}^{\\top}]=c_{0,1}^{n,\\epsilon}\\mathrm{Id},\\qquad\\mathbb{E}_{\\mathbb{P}^{2n,\\epsilon}}[\\mathbf{X}_{0}\\mathbf{X}_{0}^{\\top}]=c_{0,0}^{n,\\epsilon}\\mathrm{Id}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using this definition and (56) we get that for any $n\\in\\mathbb N$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{c_{0,1}^{n+1,\\diamond}=g_{\\mathrm{cov}}(c_{0,1}^{n,\\leftarrow},c_{0,0}^{n,\\leftarrow},\\varepsilon),\\quad}&{c_{1,1}^{n+1,\\diamond}=g_{\\mathrm{var}}(c_{0,1}^{n,\\leftarrow},c_{0,0}^{n,\\leftarrow},\\varepsilon),}\\\\ {c_{0,1}^{n+1,\\leftarrow}=g_{\\mathrm{cov}}(c_{0,1}^{n,\\diamond},c_{1,1}^{n,\\diamond},\\varepsilon),\\quad}&{c_{0,0}^{n+1,\\leftarrow}=g_{\\mathrm{var}}(c_{0,1}^{n,\\diamond},c_{1,1}^{n,\\diamond},\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In addition, we have that $c_{0,1}^{n,\\epsilon}=c_{0,1}^{n,\\rightarrow}=0$ and $c_{1,1}^{0,\\bullet}=c_{0,0}^{0,\\bullet}=1$ . This concludes the proof. ", "page_idx": 38}, {"type": "text", "text": "Error accumulation. In Proposition I.1 and Proposition I.2, we derive the sequences corresponding to the evolution of the variance and the covariance throughout the DSBM iterations in forwardforward mode or forward-backward mode. In what follows, we showcase the behavior of these sequences for different values of $\\varepsilon>0$ . We recall that $\\varepsilon$ corresponds to the error made in the Markov projection, i.e. $\\mathrm{proj}_{\\mathcal{M}}$ is replaced by pro $\\mathbf{\\mathcal{E}}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}$ in the forward-forward mode and $\\mathrm{proj}_{\\mathcal{M}}$ is replaced by $\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\rightarrow}$ and $\\mathrm{proj}_{\\mathcal{M}}^{\\varepsilon,\\leftarrow}$ in the forward-backward mode. First, if we consider the perfect scenario, i.e. $\\varepsilon=0$ , then we observe that both the forward-forward mode and the forward-backward mode satisfy that $\\mathbb{E}_{\\mathbb{P}^{2n}}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]=\\mathrm{Id}$ , see Figure 10 and Figure 11. Additionally, we can show that in the perfect scenario, i.e. $\\varepsilon=0$ , then both the forward-forward mode and the forward-backward mode satisfy that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow+\\infty}\\mathbb{E}_{\\mathbb{P}^{2n}}[\\mathbf{X}_{1}\\mathbf{X}_{0}^{\\top}]=(\\sqrt{2}-1)\\mathrm{Id},}\\end{array}$ , see Figure 10 and Figure 11. However, as $\\varepsilon$ increases the behavior between the forward-forward sequence and the forward-backward sequence significantly differs. More precisely, the error explodes as $\\varepsilon$ increases along the DSBM iteration for the forward-forward mode. On the contrary, in the forward-backward mode, the error remains bounded along the DSBM iterations, see Figure 10 and Figure 11. ", "page_idx": 38}, {"type": "image", "img_path": "1F32iCJFfa/tmp/c7d62d2692af0b0242de6415d41e4c2dda9533021f31cd959c6a4e6407a89985.jpg", "img_caption": ["Figure 10: Evolution of $(\\|\\mathbb{E}_{\\mathbb{P}^{2n}}[\\mathbf{X}_{1}\\mathbf{X}_{1}^{\\top}]-\\mathrm{Id}\\|)_{n\\in\\mathbb{N}}$ in log-space along DSBM iterations $\\mathbf{\\dot{X}}$ -axis). Different curves correspond to different values of $\\varepsilon$ , i.e. the larger $\\varepsilon$ the larger the error in the Markovian projection. Left: evolution in the forward-forward mode. Right: evolution in the forwardbackward mode. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "1F32iCJFfa/tmp/b065605a01bddaaaeabdcfbb23534879642ccf4e1913aa677504898ecb04a2a1.jpg", "img_caption": ["Figure 11: Evolution of $(\\|\\mathbb{E}_{\\mathbb{P}^{2n}}[\\mathbf{X}_{1}\\mathbf{X}_{0}^{\\top}]-\\mathrm{Id}\\|)_{n\\in\\mathbb{N}}$ in log-space along DSBM iterations $\\mathbf{\\dot{X}}$ -axis). Different curves correspond to different values of $\\varepsilon$ , i.e. the larger $\\varepsilon$ the larger the error in the Markovian projection. Left: evolution in the forward-forward mode. Right: evolution in the forwardbackward mode. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "J Preconditioning of the loss function ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we provide details on the scaling of the loss function we implement when training our online version of DSBM. We adapt the method of (Karras et al., 2022, Appendix B.2) to the case of bridge matching. We only present our derivations in the case of the forward training of the online version of DSBM, i.e. (9). The preconditioning of the loss described in this setting can be readily extended to the forward-backward loss we consider in practice, i.e. the parametric version of (10). ", "page_idx": 39}, {"type": "text", "text": "We consider the following objective function for any $t\\in[0,1]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{t}=\\lambda_{t}\\mathbb{E}_{\\mathbb{P}}[\\|c_{t}^{o}\\mathrm{nn}_{t}^{\\theta}(c_{t}^{i}\\mathbf X_{t})+c_{t}^{s}\\mathbf X_{t}-\\frac{\\mathbf X_{1}-\\mathbf X_{t}}{1-t}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We also define for any $t\\,\\in\\,[0,1]$ and $\\boldsymbol{x}_{t}\\,\\in\\,\\mathbb{R}^{d}$ , $v_{t}^{\\theta}(x_{t})\\,=\\,c_{t}^{o}\\mathrm{nn}_{t}^{\\theta}(c_{t}^{i}x_{t})+c_{t}^{s}x_{t}$ . Hence, $c_{t}^{i}$ is an input scaling function, $c_{t}^{o}$ is an output scaling function and $c_{t}^{s}$ is a skip-connection function. During the training of the online version of DSBM, $\\mathbb{P}$ will be given by $\\mathbb{P}^{n}$ , where $\\mathbb{P}^{n}\\,=\\,\\mathbb{P}_{v^{\\theta_{n}}}$ , where the sequence $(\\theta_{n})_{n\\in\\mathbb{N}}$ is given by (9). Here, we apply the principles of Karras et al. (2022) to the case where $\\mathbb{P}=\\,(\\pi_{0}\\otimes\\pi_{1})\\mathbb{Q}_{|0,1}$ , i.e. at initialisation of the sequence. In what follows, we assume that $\\mathbb{E}_{\\pi_{0}}[\\|\\mathbf{X}_{0}\\|^{2}]=\\mathbb{E}_{\\pi_{1}}[\\|\\mathbf{X}_{1}\\|^{2}]=d.$ . Note that our considerations can be generalised to $\\mathbb{E}_{\\pi_{0}}[\\|\\mathbf{X}_{0}\\|^{2}]=\\bar{\\sigma}_{0}^{2}d$ and $\\mathbb{E}_{\\pi_{1}}[\\|\\mathbf{X}_{1}\\|^{2}]=\\sigma_{1}^{2}d$ . We also have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf X}_{t}=(1-t){\\bf X}_{0}+t{\\bf X}_{1}+\\sqrt{\\varepsilon t(1-t)}{\\bf Z},\\qquad{\\bf Z}\\sim\\mathcal{N}(0,\\mathrm{Id}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using (58), we have that for any $t\\in[0,1]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{P}_{t}}[\\|\\mathbf{X}_{t}\\|^{2}]=(1-t)^{2}\\mathbb{E}_{\\pi_{0}}[\\|\\mathbf{X}_{0}\\|^{2}]+t^{2}\\mathbb{E}_{\\pi_{1}}[\\|\\mathbf{X}_{1}\\|^{2}]+\\varepsilon t(1-t)d}\\\\ &{\\qquad\\qquad\\qquad=[(1-t)^{2}+t^{2}+\\varepsilon t(1-t)]d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We set $c_{t}^{i}$ so that $\\mathbb{E}[\\|c_{t}^{i}\\mathbf{X}_{t}\\|^{2}]=d$ for every $t\\in[0,1]$ . Hence, we have that for any $t\\in[0,1]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\nc_{t}^{i}=1/\\sqrt{(1-t)^{2}+t^{2}+\\varepsilon t(1-t)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "image", "img_path": "1F32iCJFfa/tmp/ecc1be7f2e31b6998971703db092df80ab6e52321d55059d237650c3e7430445.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Figure 12: From left to right $((c_{t}^{i})^{2})_{t\\in[0,1]}$ , $(c_{t}^{s})_{t\\in[0,1]}$ and $((c_{t}^{o})^{2})_{t\\in[0,1]}$ for different values of $\\varepsilon\\in[0,10]$ . ", "page_idx": 40}, {"type": "text", "text": "Next, we rewrite (57). For any $t\\in[0,1]$ we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{t}=\\lambda_{t}\\mathbb{E}_{\\mathbb{P}}[\\|c_{t}^{o}\\mathrm{nn}_{t}^{\\theta}(c_{t}^{i}\\mathbf{X}_{t})+c_{t}^{s}\\mathbf{X}_{t}-\\frac{\\mathbf{X}_{1}-\\mathbf{X}_{t}}{1-t}\\|^{2}]}\\\\ &{\\quad=(c_{t}^{o})^{2}\\lambda_{t}\\mathbb{E}_{\\mathbb{P}}[\\|\\mathrm{nn}_{t}^{\\theta}(c_{t}^{i}\\mathbf{X}_{t})-[-c_{t}^{s}\\mathbf{X}_{t}+\\frac{\\mathbf{X}_{1}-\\mathbf{X}_{t}}{1-t}]/c_{t}^{o}\\|^{2}]}\\\\ &{\\quad=(c_{t}^{o})^{2}\\lambda_{t}\\mathbb{E}_{\\mathbb{P}}[\\|\\mathrm{nn}_{t}^{\\theta}(c_{t}^{i}\\mathbf{X}_{t})-[\\frac{\\mathbf{X}_{1}-(1+c_{t}^{s}(1-t))\\mathbf{X}_{t}}{1-t}]/c_{t}^{o}\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, we get that for any $\\begin{array}{r}{\\in[0,1],{\\bf T}_{t}=[\\frac{{\\bf X}_{1}-(1+c_{t}^{s}(1-t)){\\bf X}_{t}}{1-t}]/c_{t}^{o}}\\end{array}$ is the target of the network in the regression loss. We are going to fix $c_{t}^{o}$ and $c_{t}^{s}$ such that i) $\\mathbb{E}[\\|\\mathbf{T}_{t}\\|^{2}]=d$ , ii) $c_{t}^{o}$ is as small as possible in order not to minimise the error propagation made by the neural network. Using (58), we have that for any $t\\in[0,1]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{P}_{1,t}}[\\|\\mathbf{X}_{1}-(1+c_{t}^{s}(1-t))\\mathbf{X}_{t}\\|^{2}]=(1+c_{t}^{s}(1-t))^{2}\\mathbb{E}_{\\mathbb{P}_{t}}[\\|\\mathbf{X}_{t}\\|^{2}]+\\mathbb{E}_{\\pi_{1}}[\\|\\mathbf{X}_{1}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad-2(1+c_{t}^{s}(1-t))\\mathbb{E}_{\\mathbb{P}_{1,t}}[\\langle\\mathbf{X}_{t},\\mathbf{X}_{1}\\rangle]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(1+c_{t}^{s}(1-t))^{2}\\mathbb{E}_{\\mathbb{P}_{t}}[\\|\\mathbf{X}_{t}\\|^{2}]+d-2(1+c_{t}^{s}(1-t))t d}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, we get that for any $t\\in[0,1]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(c_{t}^{o})^{2}=((1+c_{t}^{s}(1-t))^{2}\\mathbb{E}_{\\mathbb{P}_{t}}[\\|\\mathbf{X}_{t}\\|^{2}]/d+1-2(1+c_{t}^{s}(1-t))t)/(1-t)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We now minimise $(c_{t}^{o})^{2}$ with respect to $(1+c_{t}^{s}(1-t))$ . We get that ", "page_idx": 40}, {"type": "equation", "text": "$$\n1+c_{t}^{s}(1-t)=t/(\\mathbb{E}_{\\mathbb{P}_{t}}[\\|\\mathbf{X}_{t}\\|^{2}]/d).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, we get that $c_{t}^{s}=t/[(1-t)((1-t)^{2}+t^{2}+\\varepsilon t(1-t))]-1/(1-t)$ . With that choice, we get that for any $t\\in[0,1]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n(c_{t}^{o})^{2}=(1-t^{2}/((1-t)^{2}+t^{2}+\\varepsilon t(1-t)))/(1-t)^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In Karras et al. (2022), the weighting function $\\lambda_{t}$ is set so that the weight in front of the regression loss is equal to one for all times $t\\in[0,1]$ . Hence, Karras et al. (2022) suggests to set $\\lambda_{t}=\\mathbf{\\bar{1}}/(c_{t}^{o})^{2}$ . However, in practice we observe better results by letting $\\lambda_{t}=1$ . This means that the effective weight is given by $\\bar{17}/(c_{t}^{o})^{2}$ . Therefore, for any $t\\in[0,1]$ we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(c_{t}^{i})^{2}=(1+(\\varepsilon-2)t(1-t))^{-1},}\\\\ &{c_{t}^{s}=((\\varepsilon-2)t-1)/(1+(\\varepsilon-2)t(1-t)),}\\\\ &{(c_{t}^{o})^{2}=(1+t+(\\varepsilon-2)t(1-t))/(1-t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "K Experimental details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we delve deeper into the specifics of each experiment, implementation details, and share additional results. ", "page_idx": 40}, {"type": "text", "text": "We consider two ways of parameterising the vector fields: as in DSBM, we can use two separate neural networks to approximate the forward and backward vector fields, or we can use a single neural network that is conditioned on the direction. In the latter case, we do the conditioning in a similar fashion to how DDM\u2019s neural networks, U-Nets or MLPs, are conditioned on time embeddings. After all, if we work with continuous time variables $t\\in[0,1]$ , then the direction signal $s\\in\\{0,\\bar{1}\\}$ can be thought of as a target time. Thus, we perform the same initial transformations on $t$ and $s$ , i.e. computing sinusoidal embeddings followed by a 2-layer MLP, and use the concatenated outputs in adaptive group normalisation layers (Dhariwal and Nichol, 2021; Hudson et al., 2023; Perez et al., 2018). ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "To optimise our networks, we use Adam (Kingma and Ba, 2015) with $\\beta=(0.9,0.999)$ , and we modify the gradients to keep their global norm below 1.0. We re-initialise the optimiser\u2019s state when the finetuning phase starts. ", "page_idx": 41}, {"type": "text", "text": "All image samples in the paper are generated using EMA parameters as it has been known to increase the visual quality of resulting images (Song and Ermon, 2020). Sampling is also the integral part of DSBM\u2019s finetuning stage, both iterative and online. Here, we have two options: sample with EMA or non-EMA parameters. The non-EMA sampling might be easier to implement, while EMA sampling results in a more stable training and slightly better quality, e.g. see AFHQ samples in Figure 23 and Figure 24 for comparison. ", "page_idx": 41}, {"type": "table", "img_path": "1F32iCJFfa/tmp/615c4a261342881855fdaeaebddf0e2f1e57b00468ce490f3450540c4ee1dc69.jpg", "table_caption": ["For every model used in the paper, we provide hyperparameters in Table 3. "], "table_footnote": ["Table 3: Hyper-parameters for each model. Note that for 2-networks models, the architectural hyper-parameters describe only one of the two identical networks. Approximate parameters counts are given for bidirectional networks, except for the Gaussian case, where we only experimented with a 2-networks model. "], "page_idx": 41}, {"type": "text", "text": "K.1 2D Experiments ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In addition to the experiments presented in the main text, we test our models in the simplest 2D data settings used in Tong et al. (2024a) and Shi et al. (2023). Note, that low-dimensional datasets might not be the ideal showcase for $\\alpha$ -DSBM given that one can successfully employ less computationally demanding techniques based on minibatch-OT methods (Tong et al., 2024b). ", "page_idx": 41}, {"type": "text", "text": "The results of our bidirectional model finetuned with online updates are given in Table 4. During finetuning, we generate samples using 100 Euler\u2013Maruyama steps to solve the forward and backward SDEs. At test time, we solve the forward probability flow ODE (PF-ODE) given by: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\frac{1}{2}\\big[v_{\\theta}(1,t,\\mathbf{X}_{t})-v_{\\theta}(0,1-t,\\mathbf{X}_{t})\\big]\\mathrm{d}t,\\qquad\\mathbf{X}_{0}\\sim\\boldsymbol{\\pi}_{0}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "To evaluate model fit, we compute 2-Wasserstein distance between the true and generated samples (generated with 20 Euler steps). Additionally, we estimate path energy as a measure of trajectory simplicity: $\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{{\\mathbf{X}}_{0}\\sim\\pi_{0}}[\\int_{0}^{1}\\|v_{\\theta}(\\bar{t},{\\mathbf{X}}_{t})\\|^{2}\\mathrm{d}t]}\\quad}&{{}}\\end{array}$ where $v_{\\theta}(t,\\mathbf{X}_{t})$ is the drift of PF-ODE in (59), and the integral is approximated using 100 steps. We have made a deliberate effort to closely replicate the experimental setup of Shi et al. (2023) to ensure the comparability of our results. However, as illustrated in Figure 13, 2-Wasserstein distance can be very noisy even with 10K samples in the test set. To mitigate this variance, we averaged the 2-Wasserstein distance across five random sets of 10K samples per run, and then averaged these results across multiple runs. Despite these measures, we recommend a future redesign of these 2D experiments to facilitate more robust comparisons between methods. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "table", "img_path": "1F32iCJFfa/tmp/52e89cb912fa4d1f219fa69dd6fe76af906397c7d3c4d799e3b3bb9985b49a1f.jpg", "table_caption": [], "table_footnote": ["Table 4: 2-Wasserstein distance and path energy for the 2D experiments. We report means $\\pm1$ standard deviations across 5 random seeds. DSBM-IMF\\* and OT-CFM\\* results are copied from Shi et al. (2023). "], "page_idx": 42}, {"type": "image", "img_path": "1F32iCJFfa/tmp/25cc465a32fd70df3ba72d0232c0a7cdddd1c74f9421c091cad75ce8627158ca.jpg", "img_caption": ["Figure 13: A histogram of 2-Wasserstein distances for the \u2018moons $\\rightarrow$ 8gaussians\u2019 task. These distances are calculated between 10K samples from a finetuned $\\alpha$ -DSBM model and 8gaussians distribution, with both sets generated using 100 different random seeds. The wide spread of scores indicates that 2-Wasserstein distance, even computed on 10K samples, may not be an ideal metric for evaluating model fit in this context. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "K.2 Gaussian data ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "To parameterise the forward and backward drifts, we use a 2-layer MLP network with 256 hidden units. To process time variables, we compute sinusoidal time embeddings, followed by a 2-layer MLP with 256 hidden units and 50 output units. The resulting time embeddings are then concatenated with $\\mathbf{X}_{t}$ , so the drift networks receive 100-dimensional input vectors. ", "page_idx": 42}, {"type": "text", "text": "For iterative DSBM finetuning, we perform 40K steps with varying number of outer iterations, i.e. when we switch between training the forward and the backward networks. Alternating every 5K steps, corresponds to 8 outer DSBM iteration. Similarly, changing the direction every 1K steps, leads to 40 outer iterations. ", "page_idx": 42}, {"type": "text", "text": "We do not have a cache dataloader like in the original DSBM implementation2, thus we generate training samples on the fly by sampling either from the forward or the backward network. For this simple task, we also do not use EMA. ", "page_idx": 42}, {"type": "text", "text": "During training and evaluation, we use Euler\u2013Maruyama method with 100 equidistant time steps between 0 and 1. The covariance is evaluated using 10K samples. ", "page_idx": 42}, {"type": "text", "text": "Additional comparison $\\alpha$ -DSBM vs OT bridge-matching. We consider the scalar Gaussian setting. We highlight the dependence of OT bridge matching (256/16/8) on the batch size, as the mini-batch OT coupling can be far from the true OT coupling as the batch size decreases. All experiments are run with similar compute and architecture/training hyperparameters, see Table 5. ", "page_idx": 42}, {"type": "text", "text": "Ablation of the hyperparameter $\\alpha$ . Instead of letting $\\alpha$ be determined by Adam and adaptive for $\\alpha$ -DSBM, we explicitly set it by using Stochastic Gradient Descent (SGD) with learning parameter $\\alpha$ ; see Figure 14. We also ran online $\\alpha$ -DSBM with $\\alpha=10^{-1}$ but the training diverges in this case. ", "page_idx": 42}, {"type": "table", "img_path": "1F32iCJFfa/tmp/2242b3187bbe91dd3fe6128344c2ac7f453e9452517da42a183e0db5319e7ca3.jpg", "table_caption": ["Table 5: Comparison of OT methods "], "table_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "1F32iCJFfa/tmp/3cce63f3fa190a370ce3f71df2ddbaf0b414f0e427032c22eb30e625df76cb50.jpg", "img_caption": ["Figure 14: normFrob between $\\mathrm{C}_{\\star}$ and its estimate for $\\alpha$ -DSBM with different values of $\\alpha$ . "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "K.3 MNIST $\\leftrightarrow$ EMNIST transfer ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We closely follow the setup of Shi et al. (2023) and De Bortoli et al. (2021), and train the models to transfer between 10 EMNIST letters, A-E and a-e, and 10 MNIST digits (CC BY-ND 4.0 license). We use the same U-Net architecture with hyperparameters given in Table 3. ", "page_idx": 43}, {"type": "text", "text": "For DSBM finetuning, we perform 30 outer iterations, i.e. alternating between training the forward and the backward networks, while at each outer iteration a network is trained for 5000 steps. We do not have a cache dataloader and generate training samples on the fly by sampling either from the forward or the backward network with EMA parameters. ", "page_idx": 43}, {"type": "text", "text": "During training and evaluation, we use Euler\u2013Maruyama method with 30 equidistant time steps between 0 and 1. For evaluation, we compute FID based on the whole MNIST training set of 60000 examples and a set of 4000 samples that were initialised from each test image in the EMNIST dataset. MSD is computed between 4000 initial EMNIST test examples and their corresponding MNIST samples. ", "page_idx": 43}, {"type": "text", "text": "In Figures 15\u201318, we provide forward and backward samples, i.e. EMNIST $\\rightarrow$ MNIST and MNIST $\\rightarrow{\\mathrm{EMNIST}}$ , from models that differ in parameterisation, finetuning methods, and sampling strategy. For all the models above, we used $\\varepsilon=1$ . Figure 19 illustrated the behaviour of the samples when we sweep over the $\\varepsilon$ hyperparameter. ", "page_idx": 43}, {"type": "text", "text": "Pretraining a bidirectional model on 4 v3 TPUs takes 1 hour, while the online finetuning stage requires 4 hours on 16 v3 TPUs. The number of pretraining and finetuning steps is chosen to match the experimental setup of Shi et al. (2023). ", "page_idx": 43}, {"type": "text", "text": "K.4 AFHQ: Cat $\\leftrightarrow$ Wild ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We consider the problem of image translation between Cat and Wild domains of AFHQ (Choi et al. (2020); CC BY-NC 4.0 DEED licence) as introduced by Shi et al. (2023). Each domain has approximately 5000 samples in the training set, and around 500 samples in the test set. We resize the original $512\\times512$ images to $64\\!\\times\\!64$ or $256\\!\\times\\!256$ resolutions. ", "page_idx": 43}, {"type": "image", "img_path": "1F32iCJFfa/tmp/25f462c7a35dfd410370cf457d04e7b2cd5a32a3dc57c94895d5d0aaac782fd8.jpg", "img_caption": ["Figure 15: EMNIST to MNIST transfer with a 2-networks model. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "1F32iCJFfa/tmp/0a0424da7fbda888333b2b7cc6ea201a7c525af299ed12d6c465adc08a80f4d5.jpg", "img_caption": ["Figure 16: EMNIST to MNIST transfer with a bidirectional model. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "1F32iCJFfa/tmp/b4dacc990450a79c159d7d130ecfad95056284f5eeeab0bb9db9c94f1877ea49.jpg", "img_caption": ["Figure 17: MNIST to EMNIST transfer with a 2-networks model. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "1F32iCJFfa/tmp/65d83852f34810a397120eb8c890f04cfcd21cfa23b52a1af21de76e8a5598ab.jpg", "img_caption": ["Figure 18: MNIST to EMNIST transfer with a bidirectional model. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Our U-Net (Ronneberger et al., 2015) implementation is based on Ho et al. (2020) with a few improvements suggested in Dhariwal and Nichol (2021); Song et al. (2021b) such as rescaling of skip connections by $^{1}\\!/\\!\\sqrt{2}$ , using residual blocks from BigGAN (Brock et al., 2019), and convolution-based up- and downsampling. Hyperparameters are given in Table 3. Compared to the straightforward parameterisation of the vector fields, we obtained slightly better results using EDM preconditioning Karras et al. (2022), which we derive in Appendix J for the case of bridge matching. During training, we use horizontal flips as a way to augment the data. ", "page_idx": 45}, {"type": "text", "text": "During training and evaluation, we use Euler\u2013Maruyama method with 100 equidistant time steps between 0 and 1. When evaluating the quality of $\\mathrm{Cat}\\to\\mathrm{Wild}$ transfer, we compute FID based on the whole training set of 4576 examples in the Wild domain and a set of 480 samples that were initialised from test images in the Cat domain. LPIPS and MSD are computed between 480 initial Cat images and Wild samples from the model. The same procedure is followed when evaluating in the reverse direction from Wild to Cat. Given that train, and especially the test sets are small, the quantitative results for AFHQ are likely unreliable (Chong and Forsyth, 2020). In Figure 22 we provide samples from the models finetuned either with an iterative or an online method. While their FID scores are different, the samples look similar between the two models. ", "page_idx": 45}, {"type": "text", "text": "As we discussed in the main text, hyperparameter $\\varepsilon$ trades off the visual quality and alignment of the samples in the resulting transfer models. In Figure 20, we provide AFHQ $64\\times64$ samples for pretrained and finetuned models with different values of $\\varepsilon$ . In addition to its relation to EOT, from a DDM perspective, $\\varepsilon$ can be seen as the controlling factor of the noise schedule. As observed by Hoogeboom et al. (2023), noise schedules should be adjusted for different image sizes by shifting the noise schedule of some reference resolution where it is proven to be successful. In our case, if we find a good value of $\\varepsilon$ for $64\\times64$ images, then a shifted $\\varepsilon$ for the $256\\times256$ resolution can be computed as $\\varepsilon_{256}=\\varepsilon_{64}\\left(\\frac{256}{64}\\right)^{2}$ . Thus, if we choose $\\sqrt{\\varepsilon}=0.75$ for AFHQ-64, then for AFHQ-\u221a256, we can expect $\\sqrt{\\varepsilon}=3.0$ to also work well. Samples from an AFHQ-256 model trained with $\\sqrt{\\varepsilon}=3.0$ are given in Figure 27. ", "page_idx": 45}, {"type": "text", "text": "On 16 v3 TPUs, the bidirectional base and finetuned AFHQ-64 models take 4 and 14 hours to train, respectively. For AFHQ-256, the base model trains for 15 hours, and finetuning takes an additional 37 hours. While we did not experiment with varying pretraining and fine-tuning iterations, these training times suggest that a longer pretraining stage followed by fewer fine-tuning steps may be desirable. ", "page_idx": 45}, {"type": "image", "img_path": "1F32iCJFfa/tmp/cf4abc62e986dc6eacd21f2d8ccd21109834ec1af11f4d1e4f6f2929042381ef.jpg", "img_caption": ["(a) Base model "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "1F32iCJFfa/tmp/adbc3bd0a369b8f00bc26c2c2e0465f3b898b12d8e20e43b2fc6afb4f2ae3b7a.jpg", "img_caption": ["Figure 19: MNIST samples transferred from EMNIST letter inputs (top row) using base (pretrained) and fine-tuned models for different values of $\\varepsilon$ . Low noise values result in poor sample quality, particularly in the base model, which finetuning cannot fully rectify. Conversely, excessively high $\\varepsilon$ restricts information passing from the inputs to the outputs, leading to poor alignment. Additionally, high $\\varepsilon$ increases blurriness due to increased noise levels, thus requiring more denoising steps. ", "(b) Finetuned model "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "1F32iCJFfa/tmp/cad37fbbd0a03b331b32d9ef1f6f304a6a91fb0072e9b8ac88b34861a481958a.jpg", "img_caption": ["Figure 20: AFHQ $64\\times64$ Wild $\\rightarrow$ Cat transfer results for different values of $\\sqrt{\\varepsilon}$ in a bidirectional model before and after online finetuning. Low values of $\\varepsilon$ lead to poor sample quality in both base and finetuned models. Excessively high $\\varepsilon$ values impede information passing from the inputs to the outputs, resulting in poor alignment. High values of $\\varepsilon$ also increase blurriness due to noisier SDE trajectories, thus requiring more denoising steps during sampling. "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "1F32iCJFfa/tmp/74bbeef1ba29c08b0e52eddef86761fafd810c553a327d676775d0fd806df62c.jpg", "img_caption": ["Figure 21: AFHQ $64\\times64$ Wild $\\rightarrow$ Cat transfer results for varying number of function evaluations (equivalent to time discretisation steps in the Euler-Maruyama method) in a bidirectional model with $\\bar{\\sqrt{\\varepsilon}}=0.75$ , both before and after online finetuning. Post-finetuning, clearer images are achievable with fewer steps. This observation aligns with findings from Rectified Flows (Liu et al., 2023b). ", "(a) Base model ", "(b) Finetuned model "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "1F32iCJFfa/tmp/a53a49c810505dc43f9aff9df56be85d05978e05ec55836396cd00d1f947b81d.jpg", "img_caption": ["(a) Iterative finetuning: Cat $\\rightarrow$ Wild. FID=27.76, LPIPS=0.503, MSD=0.093 "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "1F32iCJFfa/tmp/7961a82b609f3ff9c2737b7b2576e52969131a4a6dee7ddec935a31cc526842b.jpg", "img_caption": ["(b) Iterative finetuning: Wild $\\rightarrow$ Cat. $\\mathrm{FID}{=}25.24$ , LPIPS=0.483, MSD=0.094 "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "1F32iCJFfa/tmp/09dbf10ce6b078cbaee80f37c86f96e338c8d659693893222b931f405082512a.jpg", "img_caption": ["(c) Online finetuning: Cat $\\rightarrow$ Wild. FID=32.12, LPIPS=0.503, MSD=0.097 "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "1F32iCJFfa/tmp/321d281dc2acd19b8177d15f9b112ba0db6af8c5962d793cfb6eb1f6c6bff26e.jpg", "img_caption": ["(d) Online finetuning: Wild $\\rightarrow$ Cat. FID=27.32, LPIPS=0.485, $\\mathrm{MSD=}0.116$ "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "Figure 22: Samples and metrics from a 2-networks model architecture finetuned with DSBM\u2019s iterative procedure vs online finetuning. Within each two rows, initial and transferred samples are on the top and bottom respectively. ", "page_idx": 49}, {"type": "image", "img_path": "1F32iCJFfa/tmp/21e46629bbb397ee33e0c6c677059200be14b3596a28c1ebe3ab03ce8a8995b6.jpg", "img_caption": ["(b) Backward: Wild $\\rightarrow$ Cat "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "", "img_caption": ["Figure 23: Uncurated samples for AFHQ\u221a $64\\times64$ transfer in a bidirectional model with online finetuning with non-EMA sampling and $\\sqrt{\\varepsilon}=0.75$ . Within each two rows, initial and transferred samples are on the top and bottom respectively. ", "(a) Forward: Cat $\\rightarrow$ Wild "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "1F32iCJFfa/tmp/a4efe6176340eef1425fa0e51efe4d95cae4f3eb4dfe3275ce9bb747614770af.jpg", "img_caption": ["Figure 24: Uncurated samples for AFHQ $64\\times64$ transfer in a bidirectional model with online finetuning and $\\sqrt{\\varepsilon}=0.75$ . Within each two rows, initial and transferred samples are on the top and bottom respectively. ", "(a) Forward: Cat $\\rightarrow$ Wild ", "(b) Backward: Wild $\\rightarrow$ Cat "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "1F32iCJFfa/tmp/e6f8d6214b729493502d827dba69a0de6cd20b9269c8bcb63b4e67de36cc903b.jpg", "img_caption": ["(a) Forward: Cat $\\rightarrow$ Wild with inputs from Wild. "], "img_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "1F32iCJFfa/tmp/0a14f516766867e8a4e3ae7a8f7ff516f911c93cd0e2398d4f37a565a0caa003.jpg", "img_caption": ["(b) Backward: Wild $\\rightarrow$ Cat with inputs from Cat. "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "Figure 25: Samples for AFHQ $64\\times64$ transfer in bidirectional models with online finetuning and different values of $\\varepsilon$ . The models are only trained on Cat and Wild domains, $\\pi_{0}$ and $\\pi_{1}$ , respectively. Thus, in the forward direction the models expect Cat samples as inputs at $t=0$ , and transfer them to the Wild domain at $t=1$ . The reverse transfer holds in the backward direction. Here, we test the models\u2019 behaviour when inputs do not come from the same distribution as during training: we feed Wild samples in the forward direction, and Cat samples in the backward, which is the opposite from what the models expect. Ideally, the model should leave these inputs unchanged, which it does to varying degrees depending on $\\varepsilon$ , variance of the Gaussian noise. As we increase $\\varepsilon$ , less information can pass from the input to the output, thus making them less alike. ", "page_idx": 52}, {"type": "image", "img_path": "1F32iCJFfa/tmp/bfe0050cb14268a094e200da6c9ec53a52178b763f02fcc7dba43e6c13bf32cb.jpg", "img_caption": ["(a) Forward: Cat $\\rightarrow$ Wild with inputs from Dog. ", "Figure 26: Samples for AFHQ $64\\times64$ transfer in a bidirectional model with online finetuning and $\\breve{\\sqrt{\\varepsilon}}=2.0$ . The model is only trained on Cat and Wild domains, $\\pi_{0}$ and $\\pi_{1}$ , respectively. Thus, in the forward direction the model expects Cat samples as inputs at $t=0$ , and transfers them to the Wild domain at $t=1$ . The reverse holds in the backward direction. Notably, the model generalises well to the unseen A\u221aFHQ Dog domain, often producing high-quality translations. T\u221ahese results come from a model with $\\sqrt{\\varepsilon}=2.\\bar{0}$ , which is higher than our chosen default value of $\\sqrt{\\varepsilon}=0.75$ . Higher noise allows the model to better deal with out-of-distribution inputs. "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "1F32iCJFfa/tmp/706bfd75c69266d8f743d2eeda301f292ba184ef69c901ceb1cb2ac16dd89ecd.jpg", "img_caption": ["(b) Backward: Wild $\\rightarrow$ Cat with inputs from Dog. "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "1F32iCJFfa/tmp/efefa91b8fb5907d754fa6d1f3fbf2e0503a1d165d08d3afd1d6929bd222fead.jpg", "img_caption": ["Figure 27: Uncurated samples for AFHQ $256\\times256$ transfer in a bidirectional model with online finetuning and $\\sqrt{\\varepsilon}=3$ . Within each two rows, initial and transferred samples are on the top and bottom respectively. ", "(a) Forward: Cat $\\rightarrow$ Wild ", "(b) Backward: Wild $\\rightarrow$ Cat "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: our main theoretical and experimental contributions are claimed in the abstract and demonstrated in the paper. We summarize our main contributions hereafter. Theoretically, we identify a new family of sequence of path measures related to the IMF algorithm, called $\\alpha$ -IMF. We show that these sequences correspond to non-parametric updates. We then introduce a parametric update that corresponds to an online version of the DSBM algorithm. We show that our procedure retains the favorable properties of DSBM while not requiring the expensive repeated inner minimisation procedure of DSBM. ", "page_idx": 55}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The limitations are addressed in the discussion section. The main limitation of our algorithm is that it is not a sampling free methodology. In future work, we would like to see how to mitigate the fact that our algorithm depends on some self-play. ", "page_idx": 55}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: All theoretical results are proven in the supplementary material, see Appendix D. ", "page_idx": 55}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Full experimental details are provided in Appendix K. ", "page_idx": 55}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [No] ", "page_idx": 55}, {"type": "text", "text": "Justification: Due to IP restrictions, we cannot share the codebase used for this paper. However, we plan to release some notebooks in order to reproduce experiments in a small scale setting. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Full experimental details are provided in Appendix K. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: All metrics are computed using multiple random seeds and error bars are provided. ", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] Justification: Full details on the compute requirements are given in Appendix K. ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: After careful review of the NeurIPS Code of Ethics, we can ensure that the research presented in this paper conforms with the Code of Ethics in every respect. Indeed, we see no immediate safety, security, discrimination, surveillance, deception, harassment, environment, human rights or bias and fairness concerns to our work. In addition, we release details and documentation regarding the datasets and models used. We disclose essential details for reproducibility and have ensured that our work is legally compliant. ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: This paper addresses the problem of unpaired dataset translation and proposes an improvement to the DSBM methodology. As the current paper is mostly theoretical and methodological we do not see immediate societal impact of this work and therefore do not discuss these issues. However, we acknowledge that large scale implementation of our algorithm might suffer from the same societal biases as generative models. We hope to address the limitations of such models when turning to more experimental work. ", "page_idx": 56}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 56}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We have referenced the license of the datasets we use and cite the original papers that produced the code packages and datasets that we use in that paper. ", "page_idx": 56}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 56}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 57}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 57}]