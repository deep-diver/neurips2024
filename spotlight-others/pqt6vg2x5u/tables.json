[{"figure_path": "PQt6Vg2X5u/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, E[L(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table presents a comparison of different PAC-Bayes approaches on MNIST and Fashion MNIST datasets.  It shows the training and testing classification error (0-1 loss) for each method, along with the corresponding generalization bounds. The methods compared are Uninformed priors, Data-informed priors, Data-informed priors with excess loss, and the proposed Recursive PAC-Bayes method with varying recursion depths (T). The results are averaged over 5 repetitions, with standard deviations reported.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_8_2.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, Ep[L(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUninf.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches (Uninformed, Informed, Informed + Excess Loss, and Recursive PAC-Bayes) on MNIST and Fashion MNIST datasets.  For each method and dataset, it shows the training and testing classification error rates (0-1 loss), along with the corresponding generalization bounds. The results are averaged over five independent runs, with standard deviations reported.  The table highlights the improved performance and tighter bounds achieved by the Recursive PAC-Bayes method, particularly as the number of recursive steps increases.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_9_1.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, E<sub>p</sub>[L(h, S<sub>test</sub>)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table presents a comparison of different PAC-Bayes approaches on MNIST and Fashion-MNIST datasets.  It shows the training and test classification error (0-1 loss) for each method, along with the corresponding generalization bounds.  The methods compared include uninformed priors, data-informed priors, data-informed priors with excess loss, and the novel recursive PAC-Bayes approach with varying recursion depths (T). The results are averaged over 5 repetitions, with standard deviations reported.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_12_1.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[\u00ce(h, S)] (Train 0-1), and on the testing data, E,[\u00ce(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches (Uninformed, Informed, Informed + Excess Loss, and Recursive PAC-Bayes) on MNIST and Fashion MNIST datasets.  For each method, it shows the training and testing classification error (0-1 loss), and the corresponding generalization bound.  The results are averaged over five repetitions, with standard deviations reported.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[\u00ce(h, S)] (Train 0-1), and on the testing data, E,[\u00ce(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes methods on MNIST and Fashion MNIST datasets.  The methods are Uninformed Priors, Data-Informed Priors, Data-Informed Priors + Excess Loss, and Recursive PAC-Bayes.  The table shows the training and testing 0-1 loss for each method, along with the corresponding generalization bound.  The Recursive PAC-Bayes method is tested with different recursion depths (T=2,4,6,8).  The results are averaged over 5 repetitions, with standard deviations reported.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_15_2.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, E,[L(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches on MNIST and Fashion MNIST datasets.  It shows the training and testing classification error (0-1 loss) for each method, as well as the corresponding generalization bounds. The methods compared are Uninformed priors, Data-informed priors, Data-informed priors + excess loss, and the novel Recursive PAC-Bayes method with varying recursion depths (T=2,4,6,8). The results demonstrate the improved performance and tighter bounds of the Recursive PAC-Bayes approach, particularly as the recursion depth increases.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_15_3.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, E,[L(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches on MNIST and Fashion MNIST datasets.  The methods compared are: Uninformed priors, Data-informed priors, Data-informed priors + excess loss, and Recursive PAC-Bayes.  The table shows the training and testing error rates (0-1 loss) and the corresponding generalization bounds for each method, averaged over five repetitions, allowing for a comparison of accuracy and the tightness of the bounds produced by each approach.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[\u00ce(h, S)] (Train 0-1), and on the testing data, E,[\u00ce(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches (Uninformed, Informed, Informed+Excess Loss, and Recursive PAC-Bayes) on MNIST and Fashion MNIST datasets.  For each method, it shows the training and testing classification error rates (0-1 loss) and the corresponding generalization bounds.  The Recursive PAC-Bayes results are shown for different recursion depths (T). The table aims to demonstrate the improvement in accuracy and tighter bounds achieved by the Recursive PAC-Bayes method compared to existing approaches.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_16_2.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[\u00ce(h, S)] (Train 0-1), and on the testing data, E,[\u00ce(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches on MNIST and Fashion MNIST datasets.  It shows the training and testing classification error (0-1 loss) for each method, along with the corresponding generalization bounds. The methods compared include the uninformed prior approach, data-informed priors, data-informed priors with excess loss, and the proposed recursive PAC-Bayes approach with varying recursion depths (T=2, 4, 6, 8). The results are averaged over five repetitions and include standard deviations.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_16_3.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, E[L(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different PAC-Bayes approaches on MNIST and Fashion MNIST datasets.  It shows the training and testing classification error rates (0-1 loss) achieved by each method, along with their corresponding generalization bounds. The methods compared are Uninformed priors, Data-informed priors, Data-informed priors + excess loss, and the novel Recursive PAC-Bayes method with varying recursion depths (T). The table presents the mean and standard deviation of these metrics across five repetitions.", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_16_4.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, E<sub>p</sub>[L(h, S<sub>test</sub>)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table compares the performance of four different methods for classification on the MNIST and Fashion MNIST datasets.  It shows the training and testing error rates (0-1 loss) for each method, along with the corresponding PAC-Bayes bounds. The methods compared are Uninformed priors, Data-informed priors, Data-informed priors + excess loss, and the Recursive PAC-Bayes approach with different recursion depths (T=2,4,6,8).", "section": "5 Experiments"}, {"figure_path": "PQt6Vg2X5u/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of the classification loss of the final posterior p on the entire training data, E[L(h, S)] (Train 0-1), and on the testing data, Ep[L(h, Stest)] (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. \u201cUnif.\u201d abbreviates the Uniform approach, \u201cInf.\u201d the Informed, \u201cInf. + Ex.\u201d the Informed + Excess Loss, and \u201cRPB\u201d the Recursive PAC-Bayes.", "description": "This table presents a comparison of different PAC-Bayes approaches (Uninformed, Informed, Informed + Excess Loss, and Recursive PAC-Bayes) on MNIST and Fashion MNIST datasets.  It shows the training and testing classification error rates (0-1 loss) and the corresponding generalization bounds obtained by each method. The results are averaged over 5 repetitions, and standard deviations are provided.  Recursive PAC-Bayes is tested with varying recursion depths (T).", "section": "5 Experiments"}]