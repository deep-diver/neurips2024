[{"type": "text", "text": "Towards training digitally-tied analog blocks via hybrid gradient computation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Timothy Nest\u2217\u2666 timothy.nest@mila.quebec ", "page_idx": 0}, {"type": "text", "text": "Maxence Ernoult\u2020 \u2666 maxence@rain.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Power efficiency is plateauing in the standard digital electronics realm such that new hardware, models, and algorithms are needed to reduce the costs of AI training. The combination of energy-based analog circuits and the Equilibrium Propagation (EP) algorithm constitutes a compelling alternative compute paradigm for gradientbased optimization of neural nets. Existing analog hardware accelerators, however, typically incorporate digital circuitry to sustain auxiliary non-weight-stationary operations, mitigate analog device imperfections, and leverage existing digital platforms. Such heterogeneous hardware lacks a supporting theoretical framework. In this work, we introduce Feedforward-tied Energy-based Models (ff-EBMs), a hybrid model comprised of feedforward and energy-based blocks housed on digital and analog circuits. We derive a novel algorithm to compute gradients end-to-end in ff-EBMs by backpropagating and \u201ceq-propagating\" through feedforward and energy-based parts respectively, enabling EP to be applied flexibly on realistic architectures. We experimentally demonstrate the effectiveness of this approach on ff-EBMs using Deep Hopfield Networks (DHNs) as energy-based blocks, and show that a standard DHN can be arbitrarily split into any uniform size while maintaining or improving performance with increases in simulation speed of up to four times. We then train ff-EBMs on ImageNet32 where we establish a new state-of-the-art performance for the EP literature (46 top- $.1\\ \\%$ ) 3. Our approach offers a principled, scalable, and incremental roadmap for the gradual integration of self-trainable analog computational primitives into existing digital accelerators. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient-based optimization, the cornerstone and most energy greedy component of deep learning, fundamentally relies upon three factors: i) highly parallel digital hardware such as GPUs, ii) feedforward models and iii) backprop (BP). With skyrocketing demands of AI compute, reducing the energy consumption of AI systems has become a matter of great economic, societal and environmental urgency [Strubell et al., 2020], calling for the exploration of novel compute paradigms [Thompson et al., 2020, Scellier, 2021, Stern and Murugan, 2023]. ", "page_idx": 0}, {"type": "text", "text": "One promising path towards this goal is analog in-memory computing [Sebastian et al., 2020]: by mapping weights onto a crossbar of resistive devices, Kirchoff current and voltage laws inherently perform matrix-vector multiplications with constant time complexity [Cosemans et al., 2019]. By stacking multiple such crossbars, an entire neural network can be mapped onto a physical system. An important formalism for such a system is that of energy-based (EB) analog circuits [Kendall et al., 2020, Stern et al., 2023, Dillavou et al., 2023, Scellier, 2024], which are \u201cselflearning\u201d systems that can compute loss gradients through two relaxations to equilibrium (i.e. two \u201cforward passes\u201d). Such a procedure falls under the umbrella of energy-based learning (EBL) algorithms [Scellier et al., 2024]. One such algorithm, Equilibrium Propagation (EP) [Scellier and Bengio, 2017], particularly stands out for its strong theoretical guarantees, relative scalability in the realm of backprop alternatives [Laborieux and Zenke, 2022, 2023] and proven application on small analog systems with 10, $000\\times$ greater energy-efficiency and substantial speedups compared to its GPU-based counterpart [Yi et al., 2023]. This suggests a new alternative compute paradigm for gradient-based optimization consisting of: i) analog hardware, ii) EBMs, and iii) EP. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/a020d72a756b56b49eae9ce16192887c21772d6d4174067642283989979f2f32.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustrating BP-EP backward gradient chaining through feedforward (red) and energy-based (blue) blocks, accounting for digital and analog circuits respectively. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a theoretical framework for extending end-to-end gradient computation to a realistic setting where the system in question may or may not be fully analog. Such a setting is plausible in the near term, for two major reasons. First, analog circuits exhibit non-ideal physical behaviors which affect both the inference pathway [Wang et al., 2023, Ambrogio et al., 2023] and parameter optimization [Nandakumar et al., 2020, Spoon et al., 2021, Lammie et al., 2024], compromising performance. Second, owing to the latency and energy-consumption of resistive devices\u2019 write operations, analog circuits should be fully weight stationary \u2013 weights must be written before the inference procedure begins \u2013 which excludes many operations used conventionally in machine learning including activation functions, normalization, and attention [Spoon et al., 2021, Jain et al., 2022, Liu et al., 2023, Li et al., 2023]. Therefore, analog systems are likely to be used in combination with auxiliary digital circuitry, resulting in hybrid mixed precision systems [Haensch et al., 2018]. While the design of purely inferential engines made up of analog and digital parts is nearing commercial maturity [Ambrogio et al., 2023], in-situ learning of such systems has barely been explored. An important challenge remains in proving EBL algorithms can scale in a manner comparable to backprop, given the requirement of simulating EB systems on GPUs. Because of the necessity of convergence, this amounts in practice in performing lengthy root finding algorithms to simulate physical equilibrium, limiting proof-of-concepts thereof to relatively shallow (5-6 layer) models [Scellier et al., 2024, Scellier, 2024]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our work contends that the best of both worlds can be achieved with the following triad: i) hybrid digital and analog hardware, ii) feedforward and EB models, iii) BP and EP. Namely, by modeling digital and analog parts as feedforward and EB modules respectively, we show how backprop and EP error signals can be chained end-to-end via feedforward and EB blocks respectively in a principled fashion. Rather than opposing digital and analog, or backprop and \u201calternative\u201d learning algorithms, as is often done in the literature, we propose a novel hardware-aware building block which can, in principle, leverage advances from both digital and analog hardware in the near-term. More specifically: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose Feedforward-tied Energy-based Models (ff-EBMs, Section 3.1) as high-level models of mixed precision systems whose inference pathway read as the composition of feedforward and EB modules (Eq. (5), Alg. 1). ", "page_idx": 1}, {"type": "text", "text": "\u2022 We show that gradients in ff-EBMs can be computed in an end-to-end fashion (Section 3.3), backpropagating through feedforward blocks and \u201ceq-propagating\u201d through EB blocks (Theorem 3.1, Alg. 2) and that this procedure is rooted in a deeply-nested optimization problem (Section 3.2). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Finally, we experimentally demonstrate the effectiveness of our algorithm on ff-EBMs where EBM blocks are Deep Hopfield Networks (DHNs) (Section 4).In particular we show that i) final and transient gradient estimates computed by our algorithm (Alg. 2) near perfectly match gradients computed by end-to-end automatic differentiation (Section 4.2), which we also prove mathematically (Theorem 4.1), ii) a standard DHN model can be arbitrarily split into a ff-DHN with the equivalent layers and architectural layers while maintaining or improving performance, remaining on par with automatic differentiation and being up to four times faster to simulate depending on the convergence criterion at use to compute equilibrium (Section 4.3), iii) the proposed approach yields $46\\,\\%$ top-1 $70\\%$ top-5) validation accuracy on ImageNet32 when training a ff-EBM of 15 layers, beating current state-of-the-art for EP by a large margin, without relying on holomorphic transformations inside EBM blocks [Laborieux and Zenke, 2022, 2023] ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Given $A:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{m}$ a differentiable mapping, we denote its total derivative with respect to $s_{j}$ as $d_{s_{j}}A(s):=d A(s)/d s_{j}\\in\\mathbb{R}^{m}$ , its partial derivative with respect to $s_{j}$ as $\\partial_{j}A(s):=$ $\\partial A(s)/\\partial s_{j}\\stackrel{.}{\\in}\\mathbb{R}^{m}$ . When $A$ takes scalar values $\\leftrightharpoons1\\rightleftharpoons1$ ), its gradient with respect to $s_{j}$ is denoted as $\\nabla_{j}A(s):=\\partial_{j}A(s)^{\\top}$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Energy-based models (EBMs) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For a given static input and set of weights, Energy-based models (EBMs) implicitly yield a prediction through the minimization of an energy function. As such they are a particular kind of implicit model. Namely, an EBM is defined by a (scalar) energy function $E:s,\\theta,x\\rightarrow E(s,\\theta,x)\\in\\mathbb{R}$ where $x,s$ , and $\\theta$ respectively denote a static input, hidden and output neurons and model parameters, and each such tuple defines a configuration with an associated scalar energy value. Among all configurations for a given input $x$ and some model parameters $\\theta$ , the model prediction $s_{\\star}$ is implicitly given as an equilibrium state which minimizes the energy function: ", "page_idx": 2}, {"type": "equation", "text": "$$\ns_{\\star}:=\\arg\\operatorname*{min}_{s}E(s,\\theta,x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.2 Standard bilevel optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assuming that $\\nabla_{s}^{2}E(x,s_{\\star},\\theta)$ is invertible, note that the equilibrium state $s_{\\star}$ implicitly depends on $x$ and $\\theta$ by virtue of the implicit function theorem [Dontchev et al., 2009]. Therefore our goal when training an EBM\u2013in a supervised setting, for instance \u2013 is to adjust the model parameters $\\theta$ such that $s_{\\star}(x,\\theta)$ minimizes some cost function $\\ell:s,y\\to\\ell(s,y)\\in\\mathbb{R}$ where $y$ is some ground-truth label associated to $x$ . More formally, this learning objective can be stated with the following bilevel optimization problem [Zucchet and Sacramento, 2022]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{C}(x,\\theta,y):=\\ell(s_{\\star},y)\\quad\\mathrm{s.t.}\\quad s_{\\star}=\\arg\\operatorname*{min}_{s}E(s,\\theta,x)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Solving Eq. (2) in practice amounts to computing the gradient of its outer objective ${\\mathcal{C}}(x,\\theta)$ with respect to $\\theta$ $(d_{\\theta}\\mathcal{C}(x,\\theta))$ and then performing gradient descent over $\\theta$ . ", "page_idx": 2}, {"type": "text", "text": "2.3 Equilibrium Propagation (EP) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An algorithm used to train an EBM model in the sense of Eq. (2) may be called an EBL algorithm [Scellier et al., 2024]. Equilibrium Propagation (EP) [Scellier and Bengio, 2017] is an EBL algorithm which computes an estimate of $d_{\\theta}{\\mathcal{C}}(x,\\theta)$ with at least two phases. During the first phase, the model is allowed to evolve freely to $s_{\\star}=\\arg\\operatorname*{min}_{s}E(s,\\theta,x)$ . Then, the model is slightly nudged towards decreasing values of cost $\\ell$ and settles to a second equilibrium state $s_{\\beta}$ . This amounts to augmenting the energy function $E$ by an additional term $\\beta\\ell(s,y)$ where $\\beta\\in\\mathbb{R}^{\\star}$ is called the nudging factor. Next, the weights are updated to increase the energy of $s_{\\star}$ and decrease that of $s_{\\beta}$ , thereby \u201ccontrasting\u201d these two states. More formally, Scellier and Bengio [2017] show in the seminal EP paper: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{\\beta}:=\\arg\\underset{s}{\\\\operatorname*{min}}\\left[E(s,\\theta,x)+\\beta\\ell(s,y)\\right],\\quad\\Delta\\theta^{\\mathrm{EP}}:=\\frac{\\alpha}{\\beta}\\left(\\nabla_{2}E(s_{\\star},\\theta,x)-\\nabla_{2}E(s_{\\beta},\\theta,x)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha$ denotes some learning rate. EP comes in different flavors depending on the sign of $\\beta$ inside Eq. (3) or on whether two nudged states of opposite nudging strengths $(\\pm\\beta)$ are contrasted, a variant called Centered EP (C-EP) which was shown to work best in practice [Laborieux et al., 2021, Scellier et al., 2024] and reads as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta\\theta^{\\mathrm{C-EP}}:=\\frac{\\alpha}{2\\beta}\\left(\\nabla_{2}E(s_{-\\beta},\\theta,x)-\\nabla_{2}E(s_{\\beta},\\theta,x)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Tying energy-based models with feedforward blocks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the present section we introduce a new model, Feedforward-tied EBMs (ff-EBMs, section 3.1), which read as composition of feedforward and EB transformations (Alg. 1). We show how optimizing ff-EBMs amounts to solving a multi-level optimization problem (Section 3.2) and propose a BP-EP gradient chaining algorithm as a solution(Section 3.3, Theorem 3.1, Alg. 2). We highlight as an edge case the fact that ff-EBMs reduce to standard feedforward nets (Lemma A.1) and the proposed BP-EP gradient chaining algorithm to standard BP (Corollary A.1) when each EB block comprises a single hidden layer. We highlight in red and blue the parts of the model and associated algorithms performed inside feedforward (digital) and EB (analog) blocks respectively. ", "page_idx": 3}, {"type": "text", "text": "3.1 Feedforward-tied Energy-based Models (ff-EBMs) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inference procedure. We define Feedforward-tied Energy-based Models (ff-EBMs) as compositions of feedforward and EB transformations. Namely, an data sample $x$ is fed into the first feedforward transformation $F^{1}$ parametrized by some weights $\\omega^{1}$ , which yields an output $x_{\\star}^{1}$ . Then, $x_{\\star}^{1}$ is fed as a static input into the first EB block $E^{1}$ with parameters $\\theta^{1}$ , which relaxes to an equilibrium state $s_{\\star}^{1}$ . $s_{\\star}^{1}$ is in turn fed into the next feedforward transformation $F^{1}$ with weights $\\omega^{1}$ and the above procedure repeats until reaching the output layer $\\hat{o}$ . More formally, denoting $F^{k}$ and $E^{k}$ the $\\mathrm{k^{th}}$ feedforward and EB blocks parametrized by the weights $\\omega^{k}$ and $\\theta^{k}$ respectively, the inference pathway of a ff-EBM reads as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{s^{0}:=x}\\\\ {x_{\\star}^{k}:=F^{k}(s_{\\star}^{k-1},\\omega^{k}),\\quad s_{\\star}^{k}:=\\arg\\operatorname*{min}_{s}E^{k}(s,\\theta^{k},x_{\\star}^{k})\\quad\\forall k=1\\cdots N-1}\\\\ {\\hat{o}_{\\star}:=F^{N}(s_{\\star}^{N-1},\\omega^{N})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The ff-EBM inference procedure is depicted more compactly inside Fig. 2 (left) and Alg. 1. ", "page_idx": 3}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/3d77ae20300a97a7bef0a1cc641206178a338c9f4e53c6691f9e021235bd056c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "backward (right) pathways through a ff-EBM, with blue and pink blocks denoting EB and feedforward transformations. ", "page_idx": 3}, {"type": "text", "text": "Form of the energy functions. We further specify the form of the energy of the $\\mathrm{k^{th}}$ EB block of a ff-EBM as defined per Eq. (5). The associated energy function $E^{k}$ takes some static input $x^{k}$ from the output of the preceding feedforward transformation, has hidden neurons $s^{k}$ and is parametrized by weights $\\theta^{k}$ . More precisely: ", "page_idx": 3}, {"type": "equation", "text": "$$\nE^{k}(s^{k},\\theta^{k},x^{k}):=G^{k}(s^{k})-s^{k^{\\top}}\\cdot x^{k}+U^{k}(s^{k},\\theta^{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Eq. (6) reveals three different contributions to the energy. The first term determines the non-linearity applied inside the EB block [Zhang and Brand, 2017, H\u00f8ier et al., 2023]: for a given invertible and continuous activation function $\\sigma$ , $G$ is defined such that $\\nabla G=\\sigma^{-1}$ (see Appendix A.1.3). ", "page_idx": 3}, {"type": "text", "text": "The second term inside Eq. (6) accounts for a purely feedforward contribution from the previous feedforward block $F^{k}$ . Finally, the third term accounts for internal interactions within the layers of the EB block. ", "page_idx": 3}, {"type": "text", "text": "Recovering a feedforward net. When taking the gradient of $E^{k}$ as defined in Eq. (6) with respect to $s^{k}$ and zeroing it out, it can be seen that $s_{\\star}^{k}$ is implicitly defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{\\star}^{k}:=\\sigma\\left(x^{k}-\\nabla_{1}U^{k}(s_{\\star}^{k},\\theta^{k})\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A noteworthy edge case highlighted by Eq. (7) is when $U^{k}=0$ for all $k$ \u2019s, i.e. when there are no intra-block layer interactions, or equivalently when the EB block comprises a single layer only. In this case, $s_{\\star}^{k}$ is simply a feedforward mapping $\\dot{x}^{k}$ through $\\sigma$ and in turn the ff-EBM is simply a standard feedforward architecture (see Lemma A.1 inside Appendix A.1.1). ", "page_idx": 4}, {"type": "text", "text": "3.2 Multi-level optimization of ff-EBMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Just as learning EBMs can be naturally cast as a bilevel optimization problem, learning ff-EBMs equates to a multi-level optimization problem where the variables being optimized in the inner subproblems are comprised of EB block variables $s^{1},\\cdots,s^{N-1}$ . To make this clearer, we re-write the energy function of the $\\mathrm{k^{th}}$ block $E^{k}$ from Eq. (6) to highlight the dependence between two consecutive EB block states: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{E}^{k}(s^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}):=E^{k}\\left(s^{k},\\theta^{k},F^{k}\\left(s_{\\star}^{k-1},\\omega^{k-1}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It can be seen from Eq. (8) that the equilibrium state $s_{\\star}^{k}$ obtained by minimizing $E^{k}$ will be dependent upon the equilibrium state $s_{\\star}^{k-1}$ of the previous EB block, which propagates back through prior EB blocks. Denoting $W:=\\hat{\\{}}\\theta^{1},\\cdot\\cdot\\cdot,\\theta^{\\dot{N}-1},\\omega^{1},\\cdot\\cdot\\cdot,\\omega^{N}\\}$ , the learning problem for a ff-EBM can therefore be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{W}{\\operatorname*{min}}\\;\\mathcal{C}(x,W,y):=\\ell(\\hat{\\sigma}_{\\star}=F^{N}(s_{\\star}^{N-1},\\omega^{N}),y)}\\\\ &{\\;\\;\\;\\mathrm{s.t.}\\quad s_{\\star}^{N-1}=\\underset{s}{\\operatorname{arg\\,min}}\\;\\tilde{E}^{N-1}(s,\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1})\\quad\\cdots\\quad\\mathrm{s.t.}\\quad s_{\\star}^{1}=\\underset{s}{\\operatorname{arg\\,min}}\\;\\tilde{E}^{1}(s,\\theta^{1},x,\\omega^{N-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here again and similarly to bilevel optimization, solving Eq. (9) in practice amounts to computing $g_{\\theta^{k}}:=d_{\\theta^{k}}{\\mathcal{C}}$ and $g_{\\omega^{k}}:=d_{\\omega^{k}}\\mathcal{C}$ and performing gradient descent on $\\bar{\\theta^{k}}$ and $\\omega^{k}$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 A BP\u2013EP gradient chaining algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Main result: explicit BP-EP chaining. Based on the multilevel optimization formulation of ffEBMs learning in Eq. (9), we state the main theoretical result of this paper in Theorem 3.1 (see proof in Appendix A.2.1). ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Informal). Assuming a model of the form Eq. (5), we denote $s_{\\star}^{1},x_{\\star}^{1},\\cdot\\cdot\\cdot\\,,s_{\\star}^{N-1},\\hat{o}_{\\star}$ the states computed during the forward pass as depicted in Alg. 1. We define the nudged state of block $k$ , denoted as $s_{\\beta}^{k}$ , implicitly through $\\bar{\\nabla_{1}}\\mathcal{F}^{k}(s_{\\beta}^{k},\\theta^{\\bar{k}},x_{\\star}^{k},\\delta s^{k},\\bar{\\beta})=0$ with: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}^{k}(s^{k},\\theta^{k},x_{\\star}^{k},\\delta s^{k},\\beta):=E^{k}(s^{k},\\theta^{k},x_{\\star}^{k})+{\\beta s^{k}}^{\\top}\\cdot\\delta s^{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denoting $\\delta s^{k}$ and $\\Delta x^{k}$ the error signals computed at the input of the feedforward block $F^{k}$ and of the EB block $E^{k}$ respectively, then the following chain rule applies: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta s^{N-1}:=\\nabla_{s^{N-1}}\\ell(\\hat{\\rho}_{\\star},y),\\quad g_{\\omega^{N}}=\\nabla_{\\omega^{N}}\\ell(\\hat{\\rho}_{\\star},y)}\\\\ &{\\forall k=2\\cdots N-1:}\\\\ &{\\left\\{\\begin{array}{l}{\\Delta x^{k}=d_{\\beta}\\left(\\nabla_{3}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\Big|_{\\beta=0},\\quad g_{\\theta^{k}}=d_{\\beta}\\left(\\nabla_{2}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\Big|_{\\beta=0}}\\\\ {\\delta s^{k-1}=\\partial_{1}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k},\\quad g_{\\omega^{k}}=\\partial_{2}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proposed algorithm: implicit BP-EP chaining. Theorem 3.1 reads intuitively. It prescribes an explicit chaining of EP error signals passing backward through $E^{k}$ ( $\\delta s^{k}\\rightarrow\\Delta x^{k}$ ) and BP error signals passing backward through $\\partial{F^{k^{\\top}}}$ $\\Delta x^{k}\\to\\delta s^{k-1}$ ), which directly mirrors the ff-EBM inference pathway as depicted in Fig. 2. Yet noticing that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l}{\\delta s^{k-1}=\\partial_{1}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k}=d_{\\beta}\\,\\left(\\nabla_{3}\\widetilde{E}^{k}\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\right)\\Big|_{\\beta=0}\\,,}\\\\ {g_{\\omega^{k}}=\\partial_{2}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k}=d_{\\beta}\\,\\left(\\nabla_{4}\\widetilde{E}^{k}\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\right)\\Big|_{\\beta=0}\\,,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the same error signal can by passed through ${\\widetilde{E}}^{k}$ ( $\\delta s^{k}\\,\\rightarrow\\,\\delta s^{k-1},$ ) where BP and EP are implicitly chained inside ${\\widetilde{E}}^{k}$ (see Appendix A.2.1). This insight, along with a centered scheme to estimate derivatives with respect to $\\beta$ around 0 as done for the C-EP algorithm (Eq. (4)), motivates the implicit BP-EP gradient chaining algorithm in Alg. 2 we used for our experiments (see Alg. 5 inside Appendix A.3.1 for its explicit counterpart). Given that the proposed algorithm appears as a a generalization of EP, we refer to Alg. 2as \u201cEP\u201d in the experimental section, for simplicity. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/96611cb83589d64f5dfa8f2c92986931275776f041c3796e83606e0d456426f4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Recovering backprop. When the ff-EBM under consideration is purely feedforward $\\langle U^{k}=0\\rangle$ ), we show that Eqs. (11)\u2013(12) reduce to standard BP through a feedforward net (Corollary A.1, Alg. 6 and Alg. 7 in Appendix A.2.1). Since this case is extremely close to standard BP through feedforward nets, we do not consider this setting in our experiments. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the ff-EBMs used in our experiments (Section 4.1) and carry out static gradient analysis \u2013 computing and analyzing ff-EBM parameter gradients for some $x$ and $y$ (Section 4.2). We extend the observation made by Ernoult et al. [2019] \u2013that transient EP parameter gradients obtained during the second phase match those computed by automatic differentiation through equilibrium and across blocks\u2013 to ff-EBMs (Fig. (3)\u2013(4), Theorem 4.1), showing that gradient estimates of automatic differentiation and EP in our framework, are near perfectly aligned (Fig. 5). We then show on the CIFAR-10 task that performance of ff-EBMs can be maintained or improved across various block splits maintaining the same number of layers, while remaining on par with automatic differentiation(Section 4.3). We show furthermore that blocks of smaller size are up to four times faster to simulate depending on the convergence criterion at use for computing equilibrium inside EB blocks. Finally, we perform further ff-EBM training experiments on CIFAR-100 and ImageNet32 where we establish a new state-of-the-art performance in the EP literature (Section 4.4). ", "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Models. Using the same notations as in Eq. (6), the ff-EBMs at use in this section are defined: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{U_{\\mathrm{FC}}^{k}(s^{k},\\theta^{k}):=-\\frac{1}{2}s^{k^{\\top}}\\cdot\\theta^{k}\\cdot s^{k},}\\\\ {U_{\\mathrm{CONV}}^{k}(s^{k},\\theta^{k}):=-\\frac{1}{2}s^{k}\\bullet\\left(\\theta^{k}\\star s^{k}\\right)}\\end{array}\\right.,\\left\\{\\begin{array}{l l}{F_{\\mathrm{BN}}^{k}(s^{k-1},\\omega^{k}):=\\mathrm{BN}\\left(\\mathcal{P}\\left(\\omega_{\\mathrm{CONV}}^{k}\\star s_{L}^{k-1}\\right);\\omega_{\\alpha}^{k},\\omega_{\\beta}^{k}\\right)}\\\\ {F_{\\mathrm{ID}}^{k}(s^{k-1}):=s_{L}^{k-1}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\operatorname{BN}(\\cdot;\\omega_{\\alpha}^{k},\\omega_{\\beta}^{k})$ , $\\mathcal{P}$ and $\\star$ the batchnorm, pooling and convolution operations, $\\bullet$ the generalized dot product for tensors and $\\boldsymbol{s}^{k}:=\\left(\\boldsymbol{s}_{1}^{k^{\\top}},\\cdots\\boldsymbol{s}_{L}^{k^{\\top}}\\right)^{\\top}$ the state of block $k$ comprising $L$ layers. Such EBM blocks are known as Deep Hopfield Networks (DHNs). DHNs are comprised of fully connected $(U_{\\mathrm{FC}}^{k})$ and convolutional operations $(U_{\\mathrm{CONV}}^{k})$ forming a symmetric weight matrix $\\theta^{k}$ with a sparse, block-wise structure such that each layer $s_{\\ell}^{k}$ is bidirectionally connected to its neighboring layers $s_{\\ell-1}^{k}$ and $s_{\\ell+1}^{k}$ through connections $\\theta_{\\ell-1}^{k}$ and $\\theta_{\\ell}^{k^{\\top}}$ respectively (see Appendix A.1.3). To empirically ensure convergence, the non-linearity $\\sigma$ applied within EB blocks is $\\sigma_{\\alpha}(x):=\\operatorname*{min}\\left(\\operatorname*{max}\\left(\\alpha x,0\\right),1\\right)$ with $\\alpha\\in(0,1)$ . Finally, two design choices were instrumental to the success of ff-EBM gradient computation and subsequent training. First, we initialized the weights of $U_{\\mathrm{FC}}^{k}$ and $U_{\\mathrm{CONV}}^{k}$ using ", "page_idx": 5}, {"type": "text", "text": "Gaussian Orthogonal Ensembles (GOE) [Agarwala and Schoenholz, 2022] to enable faster equilibrium computation (see next paragraph). Second, while the last layer of a given block was simply passed as an input to the next block (i.e. using $F_{\\mathrm{ID}}^{k}$ in Eq. (13)) for small enough models $\\mathcal{L}=6$ iunssiindge $F_{\\mathrm{BN}}^{k}$ eixnp Eerqi. m(1en3t) )d beepcicotmede si ne sSseecnttiioaln  f4o.r3 d),e tehpee ru sme oodfe lbsa.tchnorm layers in between blocks (i.e. ", "page_idx": 6}, {"type": "text", "text": "Equilibrium computation. As depicted in Alg. 2, the steady states $S_{\\pm\\beta}$ may be computed with any loss minimization algorithm. Here, as in past works on EP [Ernoult et al., 2019, Laborieux et al., 2021, Laborieux and Zenke, 2022, Scellier et al., 2024], we employ a fixed-point iteration scheme to compute the EB blocks steady states. Namely, we iterate Eq. (7) until reaching equilibrium (the same scheme is used for ff-EBM inference, Alg. 1, with $\\beta=0.$ ): ", "page_idx": 6}, {"type": "equation", "text": "$$\ns_{\\pm\\beta,t+1}^{k}\\leftarrow\\sigma\\left(x^{k}-\\nabla_{1}U^{k}(s_{\\pm\\beta,t}^{k},\\theta^{k})\\mp\\beta\\delta s^{k}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Important details about how Eq. (14) is executed in practice have to be highlighted. First, we employ a scheme to asynchronously update even $(\\boldsymbol{s}_{2\\ell^{\\prime}}^{k})$ and odd $(s_{2\\ell^{\\prime}+1}^{k})$ layers [Scellier et al., 2024] \u2013 see Appendix A.1.3. Second, Eq. (14) were either executed for a fixed and predetermined number of steps as done in the aforementioned EP literature, or using an $\\epsilon-$ tolerance-based convergence criterion (TOL) which stops executing Eq. (14) when $(s_{t+1}^{i}-s_{t}^{i})/s_{t}^{i}\\leq\\epsilon$ on average \u2013 see Appendix A.5.3 for details. ", "page_idx": 6}, {"type": "text", "text": "Algorithm baseline. As an algorithmic baseline, we simply use automatic differentiation (AD) backward through the fixed-point iteration scheme Eq. (14) with $\\beta\\,=\\,0$ and directly initializing $s_{t=0}^{k}=s_{\\star}$ (Fig. 4). This version of AD, where we backpropagate through equilibrium, is known as \u201cRecurrent Backpropagation\u201d [Almeida, 1987, Pineda, 1987] or Implicit Differentiation (ID). ", "page_idx": 6}, {"type": "text", "text": "4.2 Static comparison of EP and ID on ff-EBMs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In order to study the transient dynamics of ID and EP, we define, with $W^{k}:=\\{\\theta^{k},\\omega^{k}\\}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\widehat{g}_{W^{k}}^{\\mathrm{ID}}(t):=\\sum_{k=0}^{T}d_{W^{k}(T-k)}\\mathcal{C}(x,W^{k},y),}\\\\ {\\widehat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t):=\\frac{1}{2\\beta}\\left(\\nabla_{W^{k}}\\widetilde{E}^{k}(s_{\\beta,t}^{k},W^{k},s_{\\star}^{k-1})-\\nabla_{W^{k}}\\widetilde{E}^{k}(s_{-\\beta,t}^{k},W^{k},s_{\\star}^{k-1})\\right),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where sk\u00b1\u03b2,t is computed from Eq. (14) with the nudging error current $\\delta s^{k}$ computed with Alg. 2, and $T$ is the total number of iterations used for both ID and EP in the gradient computation phase. ", "page_idx": 6}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/130a48c3d27875c984308cf716c3f52c2bc93d4fabc4d84d011071174f99910f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: EP and ID partially computed gradients $((\\widehat{g}_{w}^{\\mathrm{EP}}(t))_{t\\geq0}$ in black dotted curves and $(\\widehat{g}_{w}^{\\mathrm{ID}}(t))_{t\\geq0}$ in plain colored curves) going backward through equilibrium for ID and forward through the nudging phase for EP [Ernoult et al., 2019] for a random sample $x$ and associated label $y$ . The ff-EBM comprises 6 blocks and 15 layers in total, with block sizes of either 2 or 3 layers. Each sub-panel represents a layer (labeled on the y-axis) with each curve corresponding to a randomly selected weight. \u201cBackward\u201d time is indexed from $t=0$ to $T=120$ , starting from block 6 backward to block 1, with 20 fixed-point iteration dynamics (Eq. (14)) being used for both EP and ID within each block. ", "page_idx": 6}, {"type": "text", "text": "For a given block $k$ , $d_{W^{k}(T-k)}\\mathcal{C}(x,W,y)$ is the \u201csensitivity\u201d of the loss $\\mathcal{C}$ to parameter $\\boldsymbol{W}^{k}$ at timestep $T-k$ so that $\\widehat{g}_{W^{k}}^{\\mathrm{ID}}(t)$ is a ID gradient truncated at $T-t$ . Fig. 4 depicts the computational graph that is differentiated through when using $\\mathrm{ID}$ and shows where $\\widehat{g}_{W^{k}}^{\\mathrm{ID}}(t)$ are obtained correspondingly. Similarly, $\\widehat{g}_{W^{k}}^{\\mathrm{EP}}(t)$ is an EP gradient truncated at $t$ steps forward through the nudged phase. When $T$ is sufficiently large, $\\widehat{g}_{W^{k}}^{\\mathrm{ID}}(T)$ and $\\widehat{g}_{W^{k}}^{\\mathrm{EP}}(T)$ converge to $d_{W^{k}}\\mathscr{C}(x,W,y)$ . Fig. 3 displays $(\\widehat{g}_{W^{k}}^{\\mathrm{ID}}(t))_{t\\geq0}$ and $(\\widehat{g}_{W^{k}}^{\\mathrm{EP}}(t))_{t\\geq0}$ on an heterogeneous ff-EBM of 6 blocks and 15 layers (16 if counting the last linear \u201creadout\u201d layer computing the logits) with blocks comprising 2 or 3 layers for a randomly selected sample $x$ and its associated label $y-\\mathrm{see}$ caption for a detailed description. It can be seen EP and ID error weight gradients qualitatively match very well throughout time, across layers and blocks. We also display the cosine similarity between the final EP and ID weight gradient estimate $\\widehat{g}_{W^{k}}^{\\mathrm{ID}}(T)$ and $\\widehat{g}_{W^{k}}^{\\mathrm{EP}}(T)$ for each layer and observe that EP and ID weight gradients are near perfectly   aligned. Theorem 4.1 generalizes the equivalence between EP and ID to ff-EBMs [Ernoult et al., 2019]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall k=1\\cdots N-1,\\,\\forall t=0\\cdots\\tau:\\quad\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t)=\\hat{g}_{W^{k}}^{\\mathrm{ID}}(t)=\\operatorname*{lim}_{\\beta\\to0}\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/1b53323267faa921d23410d8baf8bf15d152f610fda13b63315e68de6ecf1a1c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Light grey: computational graph associated with ff-EBM inference (Alg. 1) when applying fixed-point iteration to compute equilibrium states within each block (Eq. (14)) where the node $s_{t}^{\\bar{k}}$ denotes the state of block $k$ (comprising several layers) at timestep $t$ . Blue arrows: backward automatic differentiation (AD) through the computational graph where $\\hat{g}_{W^{k}}^{\\mathrm{ID}}(t)$ is the partially computed gradient truncated at $T-t$ . Since the states which are differentiated through are taken at equilibrium $\\overline{{(s_{t}^{k}=s_{\\star}^{k}\\,\\forall t=0\\,\\cdot\\,\\cdot\\,\\tau)}}$ this instantiation of AD can be viewed as Implicit Differentiation (ID). ", "page_idx": 7}, {"type": "text", "text": "4.3 Splitting experiment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For a given (standard, single block) EBM with a fixed number of layers, we ask how block splitting of this EBM into a ff-EBM with multiple EB blocks affects training performance and Wall Clock (simulation) Time (WCT). We address this question with two different depths ( $L=6$ and $L=12$ layers in total) and various block sizes (bs), maintaining a fixed total number of layers (e.g. for $L=6$ , 1 block of 6 layers, 2 blocks of 3 layers, etc.). Additionally, to ensure the fairest comparison in terms of WCTs across different splits, we adopt the aforementioned TOL approach to execute the fixed-point dynamics Eq. (14) within each EB block. We display the results obtained on the CIFAR-10 task inside Table 1. We observe that EP performance improves with smaller block sizes ", "page_idx": 7}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/dcf00b051ad9f451bc165dac440a352e780847554a0e222ebc263347b096268f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Cosine similarity between EP and ID weight gradients on a randomly selected sample $x$ and associated label $y$ in the same setting as Fig. 3 using the same color code to label the layers. We observe near-perfect alignment between EP and ID gradients. See Fig. 7 for a precise depiction of the model at use. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Validation accuracy and Wall Clock Time (WCT) obtained on CIFAR-10 by EP (Alg. 2) and ID on models with different number of layers $(L)$ and block sizes (\u201cbs\u201d). 3 seeds are used. ", "page_idx": 7}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/bdcdfe629d29b69553bd4a68277665c1b8684011e6a18e08363e2abc82e47d80.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "(reaching $90.1\\%$ and $92.5\\%$ for $L\\,=\\,6$ and $L\\,=\\,12$ respectively with bs $=~2$ and bs $=3$ ) with overall WCT reduction (up to $\\approx\\times4_{,}$ ) while remaining on par with ID. This significant reduction in ", "page_idx": 7}, {"type": "text", "text": "WCT is due to the fact that inference time for ff-EBMs with DHN blocks by construction scales linearly with the number of blocks rather than supralinearly with the number of layers as has been empirically observed in the EP literature [Ernoult et al., 2019]. When instead using a fixed number of iterations to execute Eq. (14) inside EB blocks (Table 5 in Appendix A.5.3), EP performance is maintained across all splits $(90.1\\%$ and $92.5\\%$ for $L=6$ and $L=12$ resp.) and is still on par with ID. However, there is no advantage in terms of WCTs in this case as the number of iterations is kept the same across all block splits and is much larger than necessary for smaller block sizes. Results for $L=6$ are consistent with the existing literature and those for $L=12$ surpass EP state-of-the-art on CIFAR-10 [Scellier et al., 2024, Laborieux and Zenke, 2022]. Overall these results suggest that: i) ff-EBM performance is agnostic to EB block sizes and are therefore flexible in design, ii) ff-EBMs are much faster to simulate that EBM counterparts of equivalent depth. ", "page_idx": 8}, {"type": "text", "text": "4.4 Scaling experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now consider ff-EBMs of fixed block size 2, and relatively small number of iterations . We train two models of depth $L=12$ and $L=15$ ) on CIFAR-100 and ImageNet32 by EP and ID and show the results obtained in Table 2. Here again we observe that EP matches ID performance on all models and tasks, ff-EBMs benefit from depth, and the performance obtained by training the 15-layer deep ff-EBM by EP exceeds state-of-the-art performance on ImageNet32 by around $10\\%$ top-1 validation accuracy [Laborieux and Zenke, 2022] and by around $5\\%$ the best performance reported on this benchmark among all backprop alternatives [H\u00f8ier et al., 2023]. ", "page_idx": 8}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/b257cab3875896ce077462fb1ec7fa8ffab9d8d4bdadd6e702b4c8190d98ad9c.jpg", "table_caption": ["Table 2: Validation accuracy and Wall Clock Time (WCT) obtained on CIFAR100 and ImageNet32 by EP and Autodiff on models with different number of layers $(L)$ and a block size of 2 $\\scriptstyle({\\mathrm{bs}}=2)$ . 3 seeds are used. We compare our results against best published results on ImageNet32 by EP [Laborieux and Zenke, 2022] and against all backprop alternatives [H\u00f8ier et al., 2023]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "EP literature. Ever since fixed-point iteration schemes were first proposed to facilitate EP experiments [Ernoult et al., 2019, Laborieux et al., 2021], there has been a growing body of work assessing scalability of EP and its algorithmic extensions on standard vision tasks. Most notably, Laborieux and Zenke [2022] introduced a holomorphic version of EP where loss gradients are computed with adiabatic oscillations of the model by nudging in the complex plane, which was very recently extended to more general implicit models [Laborieux and Zenke, 2023]. Moving further towards physical implementations of EP, Scellier et al. [2022] proposed a fully black-box version of EP where details about the system may not be known. All these advances could be readily applied inside our EP-BP chaining algorithm to EB blocks. The work closest to ours, albeit with a purely theoretical motivation and without clear algorithmic prescriptions, is that of Zach [2021] where feedforward model learning is cast as a deeply nested optimization in which consecutive layers are tied by elemental pair-wise energy functions. This work more recently inspired the Dual Propagation algorithm [H\u00f8ier et al., 2023]. Such a setting can be construed as a particular case of ff-EBM learning by EP where each EB block comprises a single layer $\\smash{U^{k}=0}$ ) inside Eq. (6)\u2013which, as we have shown, is tantamount to standard BP(see last paragraph of Section 3.3). ", "page_idx": 8}, {"type": "text", "text": "Forward-only learning beyond EP. Given that zeroth-order (ZO) optimization and \u201cforwardforward\u201d (FF) algorithms [Dellaferrera and Kreiman, 2022, Hinton, 2022] can be applied to any model, and\u2013like EP\u2013 compute a learning rule through multiple inference steps, one may wonder why it is important that our models should be energy-based. While mechanistically appealing for analog hardware [Oguz et al., 2023, Momeni et al., 2023, 2024, Xue et al., 2024], these forward-only approaches do not match the performance of automatic differentiation on equivalent models, even if they are roughly the same size as those studied in our work. On the one hand, weight perturbation [Fiete et al., 2007] (WP or \u201cSPSA\u201d [Spall, 1998]), yields unbiased yet noisy gradient estimates with variance scaling cubically with the model dimensionality [Ren et al., 2022], resulting in a significant gap in model performance compared to backprop, that can only be partially mitigated when using heuristics [Silver et al., 2021, Ren et al., 2022, Fournier et al., 2023, Chen et al., 2023]. On the other hand FF algorithms, as learning heuristics, suffer from a lack of theoretical guarantees which may impact the resulting model performance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. Since our recipe advocates EP\u2013BP chaining by construction, it is fair to say that ff-EBM learning partially inherits the pitfalls of BP. Fortunately, nothing prevents feedforward modules inside ff-EBMs from being trained by any BP alternative to mitigate specific issues. For instance: BP can be parameterized by feedback weights to obviate weight transport from the inference circuit to the gradient computation circuit [Akrout et al., 2019]; BP gradients can be approximated as finite differences of feedback operators [Ernoult et al., 2022]; or computed via implicit forward-mode differentiation [Hiratani et al., 2022, Fournier et al., 2023, Malladi et al., 2023]; local layer-wise self-supervised or supervised loss functions can be used to prevent \u201cbackward locking\u201d [Belilovsky et al., 2019, Ren et al., 2022, Hinton, 2022]. This insight may help exploring many variants of ff-EBM training. ", "page_idx": 9}, {"type": "text", "text": "Pursuing the core motivation of this work, one natural extension of this study is to incorporate more hardware realism into ffEBMs. Beyond Deep Hopfield networks, Deep Resistive Nets (DRNs) \u2013 developed by Scellier [2024] and strongly inspired by Kendall et al. [2020] \u2013 are exact models of idealized analog circuits, trainable by EP, promising fast simulation times. As such, using DRNs as EB blocks inside ff-EBMs is an exciting research direction \u2013 see Fig. 6. Still, further work in this direction presents new challenges especially given device non-idealities which may affect the inference pathway, such as analog-todigital and digital-to-analog noise [Rasch et al., 2023, Lammie et al., 2024]. Finally, considerable work is needed to prove ff-EBM further at scale on more difficult tasks (e.g. standard ImageNet), considerably deeper architectures, and moving beyond vision tasks. One other exciting research direction would be the design of $\\scriptstyle f$ -EBM based transformers, with attention layers being chained with energy-based fully connected layers inside attention blocks. ", "page_idx": 9}, {"type": "text", "text": "Concluding remarks and broader impact. We show that ff-EBMs constitute a novel framework for deep-learning in heterogeneous hardware settings. We hope that the proposed algorithm can help to overcome the typical division between digital versus analog or BP versus BP-free algorithms and that the greater energy-efficiency afforded by this framework provides a pragmatic, near-term blueprint to mitigating the dramatic carbon footprint of AI training [Strubell et al., 2020]. While we are still a long way from fully analog training accelerators at commercial maturity, we believe this work offers an incremental and sustainable roadmap to gradually integrate analog, energy-based computational primitives as they are developed into existing digital accelerators. ", "page_idx": 9}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/dee8cc5178574c85b14eddade7cba9b9a44ca53d6d5b2af69ae9dd09fcebc1c5.jpg", "img_caption": ["Figure 6: ff-EBMs as hierarchical systems implementing EP at chip scale (adapted from [Yi et al., 2023]) using energy-based analog processors made up of resistors (green edges), diodes (in blue), voltage sources (in purple), ADCs and DACs (adapted from [Scellier, 2024]), digital processors, memory buffers, all of these being connected by digital buses (red lines). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgements and disclosure of funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors warmly thank Irina Rish, Jack Kendall and Suhas Kumar for their support of the project idea from the very start, Gregory Kollmer and Mohammed Fouda for helpful feedback on the manuscript as well as Benjamin Scellier for useful discussions last year which led to an alternative derivation of our main result (Appendix A.2.2). TN acknowledges the support from the Canada Excellence Research Chairs Program, as well as CIFAR and Union Neurosciences et Intelligence Artificielle Quebec (UNIQUE). This research was enabled by the computational resources provided by the Summit supercomputer, awarded through the Frontier DD allocation and INCITE 2023 program for the project \"Scalable Foundation Models for Transferable Generalist AI\" and SummitPlus allocation in 2024. These resources were supplied by the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, with support from the Office of Science of the U.S. Department of Energy. ME acknowledges funding from Rain AI which commercializes technologies based on brain-inspired learning algorithms, as well as Constance Castres Saint-Martin for her unwavering support at the maternity hospital where most of this manuscript was written. ", "page_idx": 10}, {"type": "text", "text": "Author contributions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "TN was responsible for implementation, architecture design, coding all algorithmic details and running training experiments, as well as discovery of criteria for stable convergence. TN also participated in writing relevant portions of this manuscript. ME designed the study, derived all theoretical results, debugged and refactored the initial codebase and wrote most of the manuscript. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A. Agarwala and S. S. Schoenholz. Deep equilibrium networks are sensitive to initialization statistics. In International Conference on Machine Learning, pages 136\u2013160. PMLR, 2022.   \nM. Akrout, C. Wilson, P. Humphreys, T. Lillicrap, and D. B. Tweed. Deep learning without weight transport. Advances in neural information processing systems, 32, 2019.   \nL. B. Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In Proceedings, 1st First International Conference on Neural Networks, volume 2, pages 609\u2013618. IEEE, 1987.   \nS. Ambrogio, P. Narayanan, A. Okazaki, A. Fasoli, C. Mackin, K. Hosokawa, A. Nomura, T. Yasuda, A. Chen, A. Friz, et al. An analog-ai chip for energy-efficient speech recognition and transcription. Nature, 620(7975):768\u2013775, 2023.   \nS. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. Advances in neural information processing systems, 32, 2019.   \nE. Belilovsky, M. Eickenberg, and E. Oyallon. Greedy layerwise learning can scale to imagenet. In International conference on machine learning, pages 583\u2013593. PMLR, 2019.   \nM. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-L\u00f3pez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. Advances in neural information processing systems, 35:5230\u20135242, 2022.   \nA. Chen, Y. Zhang, J. Jia, J. Diffenderfer, J. Liu, K. Parasyris, Y. Zhang, Z. Zhang, B. Kailkhura, and S. Liu. Deepzero: Scaling up zeroth-order optimization for deep model training. arXiv preprint arXiv:2310.02025, 2023.   \nP. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.   \nS. Cosemans, B. Verhoef, J. Doevenspeck, I. Papistas, F. Catthoor, P. Debacker, A. Mallik, and D. Verkest. Towards 10000tops/w dnn inference with analog in-memory computing\u2013a circuit blueprint, device options and requirements. In 2019 IEEE International Electron Devices Meeting (IEDM), pages 22\u20132. IEEE, 2019.   \nG. Dellaferrera and G. Kreiman. Error-driven input modulation: Solving the credit assignment problem without a backward pass. In International Conference on Machine Learning, pages 4937\u20134955. PMLR, 2022.   \nS. Dillavou, B. Beyer, M. Stern, M. Miskin, A. Liu, and D. Durian. Transistor-based self-learning networks. In APS March Meeting Abstracts, volume 2023, pages D07\u2013006, 2023.   \nA. L. Dontchev, R. T. Rockafellar, and R. T. Rockafellar. Implicit functions and solution mappings: A view from variational analysis, volume 616. Springer, 2009.   \nM. Ernoult, J. Grollier, D. Querlioz, Y. Bengio, and B. Scellier. Updates of equilibrium prop match gradients of backprop through time in an rnn with static input. Advances in neural information processing systems, 32, 2019.   \nM. M. Ernoult, F. Normandin, A. Moudgil, S. Spinney, E. Belilovsky, I. Rish, B. Richards, and Y. Bengio. Towards scaling difference target propagation by learning backprop targets. In International Conference on Machine Learning, pages 5968\u20135987. PMLR, 2022.   \nI. R. Fiete, M. S. Fee, and H. S. Seung. Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances. Journal of neurophysiology, 98(4):2038\u20132057, 2007.   \nL. Fournier, S. Rivaud, E. Belilovsky, M. Eickenberg, and E. Oyallon. Can forward gradient match backpropagation? In International Conference on Machine Learning, pages 10249\u201310264. PMLR, 2023.   \nW. Haensch, T. Gokmen, and R. Puri. The next generation of deep learning hardware: Analog computing. Proceedings of the IEEE, 107(1):108\u2013122, 2018.   \nG. Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv preprint arXiv:2212.13345, 2022.   \nN. Hiratani, Y. Mehta, T. Lillicrap, and P. E. Latham. On the stability and scalability of node perturbation learning. Advances in Neural Information Processing Systems, 35:31929\u201331941, 2022.   \nR. H\u00f8ier, D. Staudt, and C. Zach. Dual propagation: Accelerating contrastive hebbian learning with dyadic neurons. In International Conference on Machine Learning, 2023. URL https: //icml.cc/virtual/2023/poster/23795.   \nS. Jain, H. Tsai, C.-T. Chen, R. Muralidhar, I. Boybat, M. M. Frank, S. Woz\u00b4niak, M. Stanisavljevic, P. Adusumilli, P. Narayanan, et al. A heterogeneous and programmable compute-in-memory accelerator architecture for analog-ai using dense 2-d mesh. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(1):114\u2013127, 2022.   \nJ. Kendall, R. Pantone, K. Manickavasagam, Y. Bengio, and B. Scellier. Training end-to-end analog neural networks with equilibrium propagation. arXiv preprint arXiv:2006.01981, 2020.   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nA. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.   \nA. Laborieux and F. Zenke. Holomorphic equilibrium propagation computes exact gradients through finite size oscillations. Advances in Neural Information Processing Systems, 35:12950\u201312963, 2022.   \nA. Laborieux and F. Zenke. Improving equilibrium propagation without weight symmetry through jacobian homeostasis. arXiv preprint arXiv:2309.02214, 2023.   \nA. Laborieux, M. Ernoult, B. Scellier, Y. Bengio, J. Grollier, and D. Querlioz. Scaling equilibrium propagation to deep convnets by drastically reducing its gradient estimator bias. Frontiers in neuroscience, 15:633674, 2021.   \nC. Lammie, F. Ponzina, Y. Wang, J. Klein, M. Zapater, I. Boybat, A. Sebastian, G. Ansaloni, and D. Atienza. Lionheart: A layer-based mapping framework for heterogeneous systems with analog in-memory computing tiles. arXiv preprint arXiv:2401.09420, 2024.   \nW. Li, M. Manley, J. Read, A. Kaul, M. S. Bakir, and S. Yu. H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2023.   \nR. Liao, Y. Xiong, E. Fetaya, L. Zhang, K. Yoon, X. Pitkow, R. Urtasun, and R. Zemel. Reviving and improving recurrent back-propagation. In International Conference on Machine Learning, pages 3082\u20133091. PMLR, 2018.   \nS. Liu, C. Mu, H. Jiang, Y. Wang, J. Zhang, F. Lin, K. Zhou, Q. Liu, and C. Chen. Hardsea: Hybrid analog-reram clustering and digital-sram in-memory computing accelerator for dynamic sparse self-attention in transformer. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2023.   \nI. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.   \nS. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora. Fine-tuning language models with just forward passes. Advances in Neural Information Processing Systems, 36:53038\u2013 53075, 2023.   \nA. Momeni, B. Rahmani, M. Mall\u00e9jac, P. Del Hougne, and R. Fleury. Backpropagation-free training of deep physical neural networks. Science, 382(6676):1297\u20131303, 2023.   \nA. Momeni, B. Rahmani, B. Scellier, L. G. Wright, P. L. McMahon, C. C. Wanjura, Y. Li, A. Skalli, N. G. Berloff, T. Onodera, et al. Training of physical neural networks. arXiv preprint arXiv:2406.03372, 2024.   \nS. Nandakumar, M. Le Gallo, C. Piveteau, V. Joshi, G. Mariani, I. Boybat, G. Karunaratne, R. Khaddam-Aljameh, U. Egger, A. Petropoulos, et al. Mixed-precision deep learning based on computational memory. Frontiers in neuroscience, 14:406, 2020.   \nI. Oguz, J. Ke, Q. Weng, F. Yang, M. Yildirim, N. U. Dinc, J.-L. Hsieh, C. Moser, and D. Psaltis. Forward\u2013forward training of an optical neural network. Optics Letters, 48(20):5249\u20135252, 2023.   \nF. J. Pineda. Generalization of back-propagation to recurrent neural networks. Physical review letters, 59(19):2229, 1987.   \nM. J. Rasch, C. Mackin, M. Le Gallo, A. Chen, A. Fasoli, F. Odermatt, N. Li, S. Nandakumar, P. Narayanan, H. Tsai, et al. Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators. Nature communications, 14 (1):5282, 2023.   \nM. Ren, S. Kornblith, R. Liao, and G. Hinton. Scaling forward gradient with local losses. arXiv preprint arXiv:2210.03310, 2022.   \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \nB. Scellier. A deep learning theory for neural networks grounded in physics. arXiv preprint arXiv:2103.09985, 2021.   \nB. Scellier. A fast algorithm to simulate nonlinear resistive networks. arXiv preprint arXiv:2402.11674, 2024.   \nB. Scellier and Y. Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.   \nB. Scellier and Y. Bengio. Equivalence of equilibrium propagation and recurrent backpropagation. Neural computation, 31(2):312\u2013329, 2019.   \nB. Scellier, S. Mishra, Y. Bengio, and Y. Ollivier. Agnostic physics-driven deep learning. arXiv preprint arXiv:2205.15021, 2022.   \nB. Scellier, M. Ernoult, J. Kendall, and S. Kumar. Energy-based learning algorithms for analog computing: a comparative study. Advances in Neural Information Processing Systems, 36, 2024.   \nA. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou. Memory devices and applications for in-memory computing. Nature nanotechnology, 15(7):529\u2013544, 2020.   \nD. Silver, A. Goyal, I. Danihelka, M. Hessel, and H. van Hasselt. Learning by directional gradient descent. In International Conference on Learning Representations, 2021.   \nJ. C. Spall. Implementation of the simultaneous perturbation algorithm for stochastic optimization. IEEE Transactions on aerospace and electronic systems, 34(3):817\u2013823, 1998.   \nK. Spoon, H. Tsai, A. Chen, M. J. Rasch, S. Ambrogio, C. Mackin, A. Fasoli, A. M. Friz, P. Narayanan, M. Stanisavljevic, et al. Toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. Frontiers in Computational Neuroscience, 15:675741, 2021.   \nM. Stern and A. Murugan. Learning without neurons in physical systems. Annual Review of Condensed Matter Physics, 14:417\u2013441, 2023.   \nM. Stern, S. Dillavou, D. Jayaraman, D. J. Durian, and A. J. Liu. Physical learning of power-efficient solutions. arXiv preprint arXiv:2310.10437, 2023.   \nE. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13693\u201313696, 2020.   \nN. C. Thompson, K. Greenewald, K. Lee, and G. F. Manso. The computational limits of deep learning. arXiv preprint arXiv:2007.05558, 2020.   \nZ. Wang, P. S. Nalla, G. Krishnan, R. V. Joshi, N. C. Cady, D. Fan, J.-s. Seo, and Y. Cao. Digitalassisted analog in-memory computing with rram devices. In 2023 International VLSI Symposium on Technology, Systems and Applications (VLSI-TSA/VLSI-DAT), pages 1\u20134. IEEE, 2023.   \nZ. Xue, T. Zhou, Z. Xu, S. Yu, Q. Dai, and L. Fang. Fully forward mode training for optical neural networks. Nature, 632(8024):280\u2013286, 2024.   \nS.-i. Yi, J. D. Kendall, R. S. Williams, and S. Kumar. Activity-difference training of deep neural networks using memristor crossbars. Nature Electronics, 6(1):45\u201351, 2023.   \nC. Zach. Bilevel programs meet deep learning: A unifying view on inference learning methods. arXiv preprint arXiv:2105.07231, 2021.   \nZ. Zhang and M. Brand. Convergent block coordinate descent for training tikhonov regularized deep neural networks. Advances in Neural Information Processing Systems, 30, 2017.   \nN. Zucchet and J. Sacramento. Beyond backpropagation: implicit gradients for bilevel optimization. arXiv preprint arXiv:2205.03076, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Model details 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1.1 Feedforward-tied EBMs (ff-EBMs) 16   \nA.1.2 Feedforward nets as a special case . . . 16   \nA.1.3 Equilibrium computation . . 16 ", "page_idx": 14}, {"type": "text", "text": "A.2 Main theoretical result 19 ", "page_idx": 14}, {"type": "text", "text": "A.2.1 Proof of Theorem 3.1 19   \nA.2.2 An alternative proof of Theorem 3.1 23 ", "page_idx": 14}, {"type": "text", "text": "A.3 Resulting algorithms 26 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.3.1 Explicit BP-EP chaining 26   \nA.3.2 Recovering backprop through feedforward nets as a special case . 26   \nA.3.3 Detailed implementation of the implicit BP-EP chaining algorithm (Alg. 2) 27 ", "page_idx": 14}, {"type": "text", "text": "A.4 Static gradient analysis 29 ", "page_idx": 14}, {"type": "text", "text": "A.4.1 Algorithmic baselines 29   \nA.4.2 Proof of Theorem 4.1 . 31   \nA.4.3 Details about Fig. 3 . . 33 ", "page_idx": 14}, {"type": "text", "text": "A.5 Experimental Details 34 ", "page_idx": 14}, {"type": "text", "text": "A.5.1 Datasets . 34   \nA.5.2 Data preprocessing 34   \nA.5.3 Simulation details 34 ", "page_idx": 14}, {"type": "text", "text": "A.1 Model details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1.1 Feedforward-tied EBMs (ff-EBMs) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first formally define Feedforward-tied Energy-based Models (ff-EBMs) with precise assumptions on the energy-based and feedforward blocks. ", "page_idx": 15}, {"type": "text", "text": "Definition A.1 (ff-EBMs). A Feedforward-tied Energy-based Model $(f\\!\\!\\!/$ -EBM) of size $N$ comprises $N$ twice differentiable feedforward mapping $F^{1},\\cdots,F^{N}$ and $N-1$ twice differentiable energy functions $\\check{E}^{1},\\cdots,E^{N^{-1}}$ with respect to all their variables. For a given $x$ , the inference procedure reads as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{s^{0}:=x}\\\\ {x_{\\star}^{k}:=F^{k}(s_{\\star}^{k-1},\\omega^{k}),\\quad s_{\\star}^{k}:=\\arg\\operatorname*{min}_{s}E^{k}(s,\\theta^{k},x_{\\star}^{k})\\quad\\forall k=1\\cdots N-1}\\\\ {\\hat{o}_{\\star}:=F^{N}(s_{\\star}^{N-1},\\omega^{N})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we assume that $\\forall k=1\\cdot\\cdot\\cdot N-1,$ , $\\nabla_{1}^{2}E^{k}(s_{\\star}^{k},\\theta^{k},\\omega^{k})$ is invertible. ", "page_idx": 15}, {"type": "text", "text": "A.1.2 Feedforward nets as a special case ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We show that when energy-based blocks comprise a single layer only, the ff-EBM becomes purely feedforward. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1. We consider ff-EBM per Def. (A.1) where the energy functions $E^{k}$ have the form: ", "page_idx": 15}, {"type": "equation", "text": "$$\nE^{k}(s^{k},\\theta^{k},x^{k}):=G^{k}(s^{k})-s^{k^{\\top}}\\cdot x^{k}+U^{k}(s^{k},\\theta^{k}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We assume that $U^{k}=0\\,f o r\\,k=1\\cdots N-1,\\,s\\to\\nabla$ $s\\rightarrow\\nabla G(s)$ is invertible and we denote $\\sigma:=\\nabla G^{-1}$ . Then, the resulting model is $a$ feedforward model described by the following recursive equations: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{s_{\\star}^{0}=x}\\\\ {x_{\\star}^{k}=F^{k}(s_{\\star}^{k-1},\\omega^{k}),\\quad s_{\\star}^{k}=\\sigma(x_{\\star}^{k})\\quad\\forall k=1\\cdot\\cdot\\cdot N-1}\\\\ {\\hat{o}_{\\star}:=F^{N}(s_{\\star}^{N-1},\\omega^{N})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.1. Let $k\\in[1,N-1]$ . By definition of $s_{\\star}^{k}$ and $x_{\\star}^{k}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{1}E^{k}(s_{\\star}^{k},\\theta^{k},x_{\\star}^{k})=0}\\\\ {\\Leftrightarrow}&{\\nabla G^{k}(s_{\\star}^{k})-x_{\\star}^{k}+\\nabla_{1}U^{k}(s_{\\star}^{k},\\theta^{k})=0}\\\\ {\\Leftrightarrow}&{s_{\\star}^{k}=\\sigma\\left(x_{\\star}^{k}-\\nabla_{1}U^{k}(s_{\\star}^{k},\\theta^{k})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore Eq. (19) is immediately obtained from Eq. (20) with $U^{k}=0$ . ", "page_idx": 15}, {"type": "text", "text": "A.1.3 Equilibrium computation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a single block. As mentioned in Section 3.1, the energy function of the $\\mathrm{k^{th}}$ EB block has the form: ", "page_idx": 15}, {"type": "equation", "text": "$$\nE^{k}(s^{k},\\theta^{k},x^{k}):=G^{k}(s^{k})-s^{k^{\\top}}\\cdot x^{k}+U^{k}(s^{k},\\theta^{k}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x^{k}$ is the output of the preceding feedforward block. For a given choice of a continuously invertible activation function, $\\dot{G}_{\\sigma}^{k}$ is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nG_{\\sigma}^{k}(s^{k}):=\\sum_{i=1}^{\\dim(s^{k})}\\int^{s_{i}}\\sigma_{i}^{-1}(u_{i})d u_{i}\\quad\\mathrm{such~that}\\quad\\nabla G_{\\sigma}^{k}(s^{k})_{i}=\\sigma_{i}^{-1}(s_{i}^{k})\\quad\\forall i=1\\cdot\\cdot\\dim(s^{k}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To be more explicit and as we did previously, we re-write the augmented energy-function which encompasses both the $\\mathrm{k^{th}}$ EB block and the feedforward module that precedes it: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widetilde{E}^{k}(s^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}):=E^{k}\\left(s^{k},\\theta^{k},F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Deep Hopfield Nets (DHNs) as EB blocks. In our experiments, we used weight matrices of the form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta^{k}=\\left[\\begin{array}{c c c c c c}{0}&{\\theta_{1}^{k^{\\top}}}&{0}&&&\\\\ {\\theta_{1}^{k}}&{0}&{\\theta_{2}^{k^{\\top}}}&&&\\\\ {0}&{\\theta_{2}^{k}}&{\\ddots}&{\\ddots}&\\\\ &&{\\ddots}&{0}&{\\theta_{L}^{k^{\\top}}}\\\\ &&&&{\\theta_{L}^{k}}&{0}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "whereby each layer $\\ell$ is only connected to its adjacent neighbors. Therefore, fully connected and convolutional DHNs with $L$ layers have an energy function of the form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{U_{\\mathrm{FC}}^{k}(s^{k},\\theta^{k}):=-\\displaystyle\\frac{1}{2}s^{k\\top}\\cdot\\theta^{k}\\cdot s^{k}=-\\displaystyle\\frac{1}{2}\\sum_{\\ell}s_{\\ell+1}^{k^{\\top}}\\cdot\\theta_{\\ell}^{k}\\cdot s_{\\ell}^{k}}}\\\\ {{U_{\\mathrm{CONV}}^{k}(s^{k},\\theta^{k}):=-\\displaystyle\\frac{1}{2}s^{k}\\bullet\\big(\\theta^{k}\\star s^{k}\\big)=-\\displaystyle\\frac{1}{2}\\sum_{\\ell}s_{\\ell+1}^{k}\\bullet\\big(\\theta_{\\ell}^{k}\\star s_{\\ell}^{k}\\big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Synchronous fixed-point iteration. We showed that when $G$ is chosen such that $\\nabla G=\\sigma^{-1}$ for some activation function $\\sigma$ , then the steady state of the $\\mathrm{k^{th}}$ block reads: ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{\\star}^{k}:=\\sigma\\left(x^{k}-\\nabla_{1}U^{k}(s_{\\star}^{k},\\theta^{k})\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which justifies the following fixed-point iteration scheme, when the block is influenced by some error signal $\\delta s$ with nudging strength $\\beta$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{\\pm\\beta,t+1}^{k}\\leftarrow\\sigma\\left(x^{k}-\\nabla_{1}U^{k}(s_{\\pm\\beta,t}^{k},\\theta^{k})\\mp\\beta\\delta s^{k}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The dynamics prescribed by Eq. 28 are also used for the inference phase with $\\beta=0$ . To further refine Eq. (28), let us re-write Eq. (28) with a layer index $\\ell$ where $\\ell\\in[\\bar{1},L_{k}]$ with $L_{k}$ being the number of layers in the $\\mathrm{k^{th}}$ block, and replacing $x^{k}$ by its explicit expression: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall\\ell=1\\cdots L_{k}:\\ s_{\\ell,\\pm\\beta,t+1}^{k}\\gets\\sigma\\left(F^{k}\\left(s_{\\star}^{k-1},\\omega^{k-1}\\right)-\\nabla_{s_{\\ell}^{k}}U^{k}\\big(s_{\\pm\\beta,t}^{k},\\theta^{k}\\big)\\mp\\beta\\delta s^{k}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As done in past EP works [Ernoult et al., 2019, Laborieux et al., 2021, Laborieux and Zenke, 2022, 2023, Scellier et al., 2024] and for notational convenience, we introduce the primitive function of the $\\mathrm{k^{th}}$ block as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi^{k}\\left(s^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right):=s^{k^{\\top}}\\cdot F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)-U^{k}(s^{k},\\theta^{k})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that Eq. (29) re-writes: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall\\ell=1\\cdot\\cdot\\cdot L_{k}:s_{\\ell,\\pm\\beta,t+1}^{k}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\pm\\beta,t}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\mp\\beta\\delta s^{k}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Eq. (31) depicts a synchronous scheme where all layers are simultaneously updated at each timestep. ", "page_idx": 16}, {"type": "text", "text": "Asynchronous fixed-point iteration. Another possible scheme, employed by Scellier et al. [2024], instead prescribes to asynchronously update odd and even layers and was shown to speed up convergence in practice: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\forall\\,\\mathrm{odd}\\,\\ell\\in\\{1,\\cdots,L_{k}\\}:\\quad s_{\\ell,\\pm\\beta,t+\\frac{1}{2}}^{k}\\leftarrow\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\pm\\beta,t}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\mp\\beta\\delta s^{k}\\right),}\\\\ {\\forall\\,\\mathrm{even}\\,\\ell\\in\\{1,\\cdots,L_{k}\\}:\\quad s_{\\ell,\\pm\\beta,t+1}^{k}\\leftarrow\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\pm\\beta,t+\\frac{1}{2}}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\mp\\beta\\delta s^{k}\\right).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We formally depict this procedure as the subroutine Asynchronous inside Alg. 3. In practice, we observe that it was more practical to use a fixed number of iterations rather than using a convergence criterion with a fixed threshold. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 3 Asynchronous (for all blocks until penultimate) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: $T,\\theta^{k},\\omega^{k},s_{\\star}^{k-1},\\beta,\\delta s^{k}$ Output: $s_{\\beta}^{k}$ 1: $s^{k}\\gets0$ 2: for $t=1\\cdot\\cdot\\cdot T$ do $\\begin{array}{r l}&{3:\\quad\\forall\\,\\mathrm{odd}\\,\\ell\\in\\left\\{1,\\cdots,L_{k}\\right\\}:\\,s_{\\ell,\\beta}^{k}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)-\\beta\\delta s^{k}\\right)}\\\\ &{4:\\quad\\forall\\,\\mathrm{even}\\,\\ell\\in\\left\\{1,\\cdots,L_{k}\\right\\}:\\,s_{\\ell,\\beta}^{k}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)-\\beta\\delta s^{k}\\right)}\\end{array}$ 5: end for ", "page_idx": 17}, {"type": "text", "text": "Resulting ff-EBM inference algorithm. With the aforementioned details in hand, we re-write the inference algorithm Alg. 1 presented in the main as a Forward subroutine. ", "page_idx": 17}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/b7d2e447a71e79dc2052386de08d42325342d43c907d61e5683a0ad9f519e467.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.2 Main theoretical result ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.2.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proof of Theorem 3.1 is structured as follows: ", "page_idx": 18}, {"type": "text", "text": "\u2022 We directly solve the multilevel problem optimization defined inside Eq. (9) using a Lagrangian-based approach (Lemma A.2), yielding optimal Lagrangian multipliers, block states and loss gradients.   \n\u2022 We show that by properly nudging the blocks, EP implicitly estimates the previously derived Lagrangian multipliers (Lemma A.3).   \n\u2022 We demonstrate Theorem 3.1 by combining Lemma A.2 and Lemma A.3.   \n\u2022 Finally, we highlight that when a ff-EBM is a feedforward net (Lemma A.1), then the proposed algorithm reduces to BP (Corollary A.1). ", "page_idx": 18}, {"type": "text", "text": "Lemma A.2 (Lagrangian-based approach). Assuming a ff-EBM (Def. A.1), we denote $s_{\\star}^{1},x_{\\star}^{1},\\cdot\\cdot\\cdot\\,,s_{\\star}^{N-1},$ $\\hat{O}_{\\star}$ the states computed during the forward pass as prescribed by Eq. (17). Then, the gradients of the objective function $\\mathcal{C}:=\\ell(\\hat{o}(s_{\\star}^{N-1}),y)$ as defined in the multilevel optimization problem (Eq. (9)), where it is assumed that $\\ell$ is differentiable, read: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{d_{\\omega^{N}}\\mathcal{C}=\\partial_{2}F^{N}(s_{\\star}^{N-1},\\omega^{N})^{\\top}\\cdot\\partial_{1}\\ell(\\hat{o}_{\\star},y),}\\\\ {d_{\\theta^{k}}\\mathcal{C}=\\nabla_{1,2}^{2}\\tilde{E}^{k}\\big(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\cdot\\lambda_{\\star}^{k}\\quad\\forall k=1\\cdot\\cdot\\cdot N-1,}\\\\ {d_{\\omega^{k}}\\mathcal{C}=\\nabla_{1,4}^{2}\\tilde{E}^{k}\\big(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\cdot\\lambda_{\\star}^{k}\\quad\\forall k=1\\cdot\\cdot\\cdot N-1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lambda_{\\star}^{1},\\cdot\\cdot\\cdot\\,,\\lambda_{\\star}^{N-1}$ satisfy the following conditions: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\nabla_{s^{N-1}}\\ell(\\hat{o}(s_{\\star}^{N-1}),y)+\\nabla_{1}^{2}\\widetilde{E}^{N-1}(s_{\\star}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1})\\cdot\\lambda_{\\star}^{N-1}=0}\\\\ {\\forall k=N-2,\\cdots,1:}\\\\ {\\nabla_{1,3}^{2}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\cdot\\lambda_{\\star}^{k+1}+\\nabla_{1}^{2}\\widetilde{E}^{k}\\left(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\cdot\\lambda_{\\star}^{k}=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.2. Denoting $s\\;:=\\;(s^{1},\\cdot\\cdot\\cdot\\;,s^{N-1})^{\\top}$ the state variables of the energy-based blocks, $\\lambda:=(\\lambda^{1},\\cdot\\cdot\\cdot\\,,\\lambda^{N-1})^{\\bar{\\top}}$ the Lagrangian multipliers associated with each of these variables, $W:=\\{\\theta_{1},\\omega_{1},\\cdot\\cdot\\cdot\\,,\\theta_{N-1},\\omega_{N-1}\\}$ the energy-based and feedforward parameters and $\\hat{o}(s^{N-1}):=$ $F^{N}\\left(s^{N-1},\\omega^{N-1}\\right)$ the logits, the Lagrangian of the multilevel optimization problem as defined in Eq. (9) reads: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(s,\\lambda,W):=\\ell\\left(\\hat{o}(s^{N-1}),y\\right)+\\sum_{k=1}^{N-1}\\lambda^{k^{\\top}}\\cdot\\nabla_{1}\\widetilde{E}^{k}(s^{k},\\theta^{k},s^{k-1},\\omega^{k}),\\quad s^{0}:=x\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Writing the associated Karush-Kuhn-Tucker (KKT) conditions $\\partial_{1,2}{\\mathcal{L}}(s_{\\star},\\lambda_{\\star},W):=0$ satisfied by optimal states and Lagrangian multipliers $s_{\\star}$ and $\\lambda_{\\star}$ , we get : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{T}_{1}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})=0\\quad\\forall k=1,\\cdots,N-1}&{(36)}\\\\ &{\\mathcal{T}_{s^{N-1}}\\ell(\\hat{o}(s_{\\star}^{N-1}),y)+\\nabla_{1}^{2}\\widetilde{E}^{N-1}(s_{\\star}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1})\\cdot\\lambda_{\\star}^{N-1}=0}&{(37)}\\\\ &{\\mathcal{T}_{1,3}^{2}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\cdot\\lambda_{\\star}^{k+1}+\\nabla_{1}^{2}\\widetilde{E}^{k}\\left(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\cdot\\lambda_{\\star}^{k}=0}&{\\forall k=N-2,\\cdots,1}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Eq. (36) governs the bottom-up block-wise relaxation procedure (as depicted in Alg. 1), while Eq. (37) and Eq. (38) governs error propagation in the last block and previous blocks respectively. Given $s_{\\star}$ and $\\lambda_{\\star}$ by Eq. (36) \u2013 Eq. (38), the total derivative of the loss function with respect to the model parameters read: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{W}\\ell(\\hat{o}_{\\star},y)=d_{W}\\left(\\ell\\left(\\hat{o}_{\\star},y\\right)+\\displaystyle\\sum_{k=1}^{N-1}\\lambda_{\\star}^{k^{\\top}}\\cdot\\underbrace{\\nabla_{1}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})}_{=0\\quad\\mathrm{(Eq.~(36))}}\\right)}\\\\ &{\\qquad\\qquad=d_{W}\\mathcal{L}(s_{\\star},\\lambda_{\\star},W)}\\\\ &{\\qquad\\qquad=d_{W}s_{\\star}^{\\top}\\cdot\\underbrace{\\partial_{1}\\mathcal{L}(s_{\\star},\\lambda_{\\star},W)}_{=0\\quad\\mathrm{(Eq.~(36))}}+d_{W}\\lambda_{\\star}^{\\top}\\cdot\\underbrace{\\partial_{2}\\mathcal{L}\\big(s_{\\star},\\lambda_{\\star},W\\big)}_{=0\\quad\\mathrm{(Eq.~(37)-(38))}}+\\partial_{3}\\mathcal{L}(s_{\\star},\\lambda_{\\star},W)}\\\\ &{\\qquad\\qquad=\\partial_{3}\\mathcal{L}(s_{\\star},\\lambda_{\\star},W)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "More precisely, applying Eq. (39) to the feedforward and energy-based block parameters yields: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\omega^{N}}\\ell(\\hat{o}_{\\star},y)=\\partial_{2}F^{N}(s_{\\star}^{N-1},\\omega^{N})^{\\top}\\cdot\\nabla_{1}\\ell(\\hat{o}_{\\star},y),}\\\\ &{\\;d_{\\theta^{k}}\\ell(\\hat{o}_{\\star},y)=\\nabla_{1,2}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot\\lambda_{\\star}^{k}\\quad\\forall k=1\\cdots N-1}\\\\ &{\\;d_{\\omega^{k}}\\ell(\\hat{o}_{\\star},y)=\\nabla_{1,4}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot\\lambda_{\\star}^{k}\\quad\\forall k=1\\cdots N-1}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.3 (Computing Lagrangian multipliers by EP). Under the same hypothesis as Lemma A.2, we define the nudged state of block $k$ , denoted as $s_{\\beta}^{k}$ , implicitly through $\\nabla_{1}\\mathcal{F}^{\\bar{k}}\\big(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k},\\delta s^{k},\\beta\\big)=0$ with: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}^{k}(s^{k},\\theta^{k},x_{\\star}^{k},\\delta s^{k},\\beta):=E^{k}(s^{k},\\theta^{k},x_{\\star}^{k})+\\beta s^{k^{\\top}}\\cdot\\delta s^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Defining $(\\delta s^{k})_{k=1\\cdots N-1}$ recursively as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s^{N-1}:=\\nabla_{s^{N-1}}\\ell(\\hat{\\sigma}_{\\star},y),\\quad\\delta s^{k}:=d_{\\beta}\\,\\left.\\left(\\nabla_{3}\\widetilde{E}^{k+1}\\left(s_{\\beta}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\right)\\right|_{\\beta=0}\\,\\forall k=1\\cdot\\cdot\\cdot N-2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\star}^{k}=d_{\\beta}\\left(s_{\\beta}^{k}\\right)|_{\\beta=0}\\quad\\forall k=1\\cdot\\cdot\\cdot N-1,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(\\lambda_{k})_{k=1}...N{-}1$ are the Lagrangian multipliers associated to the multilevel optimization problem defined in Eq. (9). ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma A.3. We prove this result by backward induction on $k$ . ", "page_idx": 19}, {"type": "text", "text": "Initialization (k = N \u22121). By definition, s\u03b2N $s_{\\beta}^{N-1}$ satisfies : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\nabla_{s^{N-1}}\\ell\\left(\\hat{o}_{\\star},y\\right)+\\nabla_{1}\\widetilde{E}^{N-1}\\left(s_{\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1}\\right)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Differentiating Eq. (43) with respect to $\\beta$ and evaluating the resulting expression at $\\beta=0$ , we obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{s^{N-1}}\\ell\\left(\\hat{o}_{\\star},y\\right)+\\nabla_{1}^{2}\\widetilde{E}^{N-1}\\left(s_{\\star}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1}\\right)\\cdot d_{\\beta}s_{\\beta}^{N-1}|_{\\beta=0}=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Substracting out Eq. (37) defining the Lagrangian multiplier $\\lambda_{\\star}^{N-1}$ and Eq. (44), we obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{1}^{2}\\widetilde{E}^{N-1}\\left(s_{\\star}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1}\\right)\\cdot\\left(d_{\\beta}s_{\\beta}^{N-1}|_{\\beta=0}-\\lambda_{\\star}^{N-1}\\right)=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By invertibility of $\\nabla_{1}^{2}\\widetilde{E}^{N-1}\\left(s_{\\star}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1}\\right)$ , s\u22c6N\u22122, \u03c9N\u22121 , we therefore have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\star}^{N-1}=d_{\\beta}s_{\\beta}^{N-1}|_{\\beta=0}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Backward induction step $(k+1\\rightarrow k)$ ). Let us assume that $\\lambda_{\\star}^{k+1}=d_{\\beta}s_{\\beta}^{k+1}|_{\\beta=0}$ . We want to prove that $\\lambda_{\\star}^{k}=d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}$ . Again, $s_{\\beta}^{k+1}$ satisfies by definition: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\delta\\boldsymbol{s}^{k}+\\nabla_{1}\\widetilde{E}^{k}\\left(s_{\\beta}^{k},\\boldsymbol{\\theta}^{k},s_{\\star}^{k-1},\\omega^{k}\\right)=0,\\quad\\delta\\boldsymbol{s}^{k}:=d_{\\beta}\\,\\left(\\nabla_{3}\\widetilde{E}^{k+1}\\left(s_{\\beta}^{k+1},\\boldsymbol{\\theta}^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\right)\\Big|_{\\beta=0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the one hand, proceeding as for the initialization step, differentiating Eq. (47) with respect to $\\beta$ and taking $\\beta=0$ yields: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta s^{k}+\\nabla_{1}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the other hand, note that $\\delta s^{k}$ rewrites : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta s^{k}=d_{\\beta}\\,\\left(\\nabla_{3}\\widetilde{E}^{k+1}\\left(s_{\\beta}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\right)\\Big|_{\\beta=0}}\\\\ &{\\quad=\\nabla_{1,3}^{2}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\cdot d s_{\\beta}^{k+1}\\Big|_{\\beta=0}}\\\\ &{\\quad=\\nabla_{1,3}^{2}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\cdot\\lambda_{\\star}^{k+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used at the last step the recursion hypothesis. Therefore combining Eq. (48) and Eq. (49), we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{1,3}^{2}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},\\theta^{k+1},s_{\\star}^{k},\\omega^{k+1}\\right)\\cdot\\lambda_{\\star}^{k+1}+\\nabla_{1}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we substract out Eq. (38) and Eq. (50) to obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{1}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot(d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}-\\lambda_{\\star}^{k})=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We conclude again by invertibility of $\\nabla_{1}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})$ that $\\lambda_{\\star}^{k}=d_{\\beta}s_{\\beta}^{k}\\vert_{\\beta=0}$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem A.1 (Formal). Assuming a model of the form Eq. (5), we denote $s_{\\star}^{1},x_{\\star}^{1},\\cdot\\cdot\\cdot\\,,s_{\\star}^{N-1},\\hat{o}_{\\star}$ N\u22121, o\u02c6\u22c6the states computed during the forward pass as prescribed by Alg. 1. We define the nudged state of block $k$ , denoted as $s_{\\beta}^{k}$ , implicitly through $\\nabla_{1}\\bar{\\mathcal{F}}^{k}(\\dot{s_{\\beta}^{k}},\\theta^{k},x_{\\star}^{k},\\dot{\\delta s^{k}},\\breve{\\beta})=0$ with: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}^{k}(s^{k},\\theta^{k},x_{\\star}^{k},\\delta s^{k},\\beta):=E^{k}(s^{k},\\theta^{k},x_{\\star}^{k})+\\beta s^{k^{\\top}}\\cdot\\delta s^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denoting $\\delta s^{k}$ and $\\Delta x^{k}$ the error signals computed at the input of the feedforward block $F^{k}$ and of the energy-based block $E^{k}$ respectively, g\u03b8k and $g_{\\omega^{k}}$ the gradients of the loss function: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall k=1,\\cdots,N-1:\\,g_{\\theta^{k}}:=d_{\\theta^{k}}{\\mathcal{C}},\\qquad\\forall k=1\\cdot\\cdot\\cdot N:\\,g_{\\omega^{k}}:=d_{\\omega^{k}}{\\mathcal{C}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then the following chain rule applies: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta s^{N-1}:=\\nabla_{s^{N-1}}\\ell(\\hat{o}_{\\star},y),\\quad g_{\\omega^{N}}=\\partial_{2}F^{N}\\left(s_{\\star}^{N-1},\\omega^{N}\\right)^{\\top}\\cdot\\nabla_{1}\\ell(\\hat{o}_{\\star},y)}\\\\ &{\\forall k=1\\cdots N-1:}\\\\ &{\\left\\{\\begin{array}{l}{\\Delta x^{k}=d_{\\beta}\\left(\\nabla_{3}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\Big|_{\\beta=0},\\quad g_{\\theta^{k}}=d_{\\beta}\\left(\\nabla_{2}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\Big|_{\\beta=0}}\\\\ {\\delta s^{k-1}=\\partial_{1}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k},\\quad g_{\\omega^{k}}=\\partial_{2}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem A.1. Combining Lemma A.2 and Lemma A.3, the following chain rule computes loss gradients correctly: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta s^{N-1}:=\\nabla_{s^{N-1}}\\ell(\\hat{\\rho}_{\\star},y),\\quad g_{\\omega^{N}}=\\partial_{2}F^{N}\\left(s_{\\star}^{N-1},\\omega^{N}\\right)^{\\top}\\cdot\\nabla_{1}\\ell(\\hat{\\rho}_{\\star},y)}\\\\ &{\\forall k=1\\cdot\\cdots N-1:}\\\\ &{\\left\\{\\begin{array}{l}{\\Delta s^{k-1}=d_{\\beta}\\,\\left(\\nabla_{3}\\tilde{E}^{k}\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\right)\\Big|_{\\beta=0},\\quad g_{\\theta^{k}}=\\nabla_{1,2}^{2}\\tilde{E}^{k}\\big(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\cdot d_{\\beta}s_{\\beta}^{k}\\big|_{\\beta=0}}\\\\ {g_{\\omega^{k}}=\\nabla_{1,4}^{2}\\tilde{E}^{k}\\big(s_{\\star}^{k},\\theta^{k},s^{k-1},\\omega^{k}\\big)\\cdot d_{\\beta}s_{\\beta}^{k}\\big|_{\\beta=0}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore to conclude the proof, we need to show that $\\forall k=1,\\cdots\\,,N-1$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\beta}\\left(\\nabla_{3}\\widetilde{E}^{k}\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\right)\\Big|_{\\beta=0}=\\partial_{1}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot d_{\\beta}\\left(\\nabla_{3}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\big|_{\\beta=0}}\\\\ &{\\nabla_{1,2}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}=d_{\\beta}\\left(\\nabla_{2}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\big|_{\\beta=0}}\\\\ &{\\nabla_{1,4}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k})\\cdot d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}=\\partial_{2}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot d_{\\beta}\\left(\\nabla_{3}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\big|_{\\beta=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $k\\in[1,N-1]$ . We prove Eq. (58) as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\beta}\\,\\left(\\nabla_{3}\\widetilde{E}^{k}\\left(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\right)\\Big|_{\\beta=0}=d_{\\beta}\\,\\left(\\nabla_{s^{k-1}}E^{k}\\left(s_{\\beta}^{k},\\theta^{k},F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)\\right)\\right)\\big|_{\\beta=0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\partial_{1}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot d_{\\beta}\\left(\\nabla_{3}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\big|_{\\beta=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Eq. (59) can be obtained as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{1,2}^{2}\\widetilde{E}^{k}\\big(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\cdot d_{\\beta}s_{\\beta}^{k}|_{\\beta=0}=d_{\\beta}\\left(\\nabla_{2}\\widetilde{E}^{k}\\big(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\right)\\Big|_{\\beta=0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=d_{\\beta}\\left(\\nabla_{2}E^{k}\\big(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k}\\big)\\right)\\big|_{\\beta=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally and similarly, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{1,4}^{2}\\widetilde{E}^{k}\\big(s_{\\star}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\cdot d_{\\beta}s_{\\beta}^{k}\\vert_{\\beta=0}=d_{\\beta}\\left.\\big(\\nabla_{4}\\widetilde{E}^{k}\\big(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\big)\\right\\vert_{\\beta=0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=d_{\\beta}\\left(\\nabla_{\\omega^{k}}E^{k}\\big(s_{\\beta}^{k},\\theta^{k},F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)\\big)\\right)\\big\\vert_{\\beta=0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=d_{\\beta}\\left(\\partial_{2}F\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\nabla_{3}E^{k}\\big(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k}\\big)\\right)\\Big\\vert_{\\beta=0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\partial_{2}F\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot d_{\\beta}\\left(\\nabla_{3}E^{k}\\big(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k}\\big)\\right)\\big\\vert_{\\beta=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "A.2.2 An alternative proof of Theorem 3.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "An energy function for ff-EBMs? While it is clear that the energy function of a ff-EBM is not $\\begin{array}{r}{E=\\sum_{k=1}^{\\bar{N}-1}\\widetilde{E}^{k}}\\end{array}$ kN=\u221211E k (which would correspond in this case to the \u201csingle block\u201d standard case), one may wonder if: ", "page_idx": 22}, {"type": "text", "text": "\u2022 ff-EBM inference (Alg. 1) can still be described as the minimization of some energy function? \u2022 Therefore, if Theorem 3.1 can be derived by directly applying EP to this energy function? ", "page_idx": 22}, {"type": "text", "text": "We show below that this is indeed the case. We follow Zach [2021], denoting $s:=$ $({s^{1}}^{\\top},\\cdots,{s^{N-1}}^{\\top})^{\\top}$ and $W=\\{W_{1},\\cdots\\,,W_{N-1}\\}$ , by picking the following energy function: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{F}(s,W,x,\\beta):=\\sum_{k=1}^{N-1}\\left\\{\\widetilde{E}^{k}\\left(s^{k},W^{k},s_{\\star}^{k-1}\\right)\\right.}}\\\\ &{}&{\\quad\\left.+\\left[\\nabla_{3}\\widetilde{E}^{k+1}\\left(s^{k+1},W^{k+1},s_{\\star}^{k}\\right)-\\nabla_{3}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},W^{k+1},s_{\\star}^{k}\\right)\\right]^{\\top}\\cdot\\left(s^{k}-s_{\\star}^{k}\\right)\\right\\}}\\\\ &{}&{\\quad+\\left.\\widetilde{E}^{N-1}\\left(s^{N-1},W^{N-1},s_{\\star}^{N-2}\\right)+\\beta\\widetilde{\\ell}(s^{N-1},y,W^{N}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we locally redefine $x$ as the concatenation of all block inputs, i.e. $x\\leftarrow$ $(\\boldsymbol{x}^{\\top},\\boldsymbol{s}_{\\star}^{1^{\\top}},\\cdot\\cdot\\cdot\\ ,\\boldsymbol{s}_{\\star}^{N-2^{\\top}})^{\\top}$ , and with $\\begin{array}{r l r}{s_{\\star}}&{{}:=}&{(s_{\\star}^{1^{\\top}},\\cdot\\cdot\\cdot\\cdot,s_{\\star}^{N-1^{\\top}})}\\end{array}$ implicitly defined through $\\nabla_{1}\\mathcal{F}(s_{\\star},W,x,\\beta\\;=\\;0)\\;=\\;0$ . In Lemma A.4, we show that the free steady state of the above energy function ( $s_{\\star}$ obtained with $\\beta=0$ inside Eq. (61)) indeed corresponds to the states computed by the ff-EBM inference scheme (Alg. 1). ", "page_idx": 22}, {"type": "text", "text": "Lemma A.4. Let $\\widetilde{E}^{1},\\cdots,\\widetilde{E}^{N-1}$ be the block-wise energy functions of a ff-EBM defined per Def. A.1. Assume $s_{\\star}$ implicitly defined through $\\nabla_{1}\\mathcal{F}(s_{\\star},W,\\beta=0)=0$ where $\\mathcal{F}$ is defined by Eq. (61). Then: ", "page_idx": 22}, {"type": "equation", "text": "$$\ns_{\\star}^{0}:=x,\\quad\\forall k=1,\\cdot\\cdot\\cdot N-1:\\quad\\nabla_{1}\\widetilde{E}^{k}(s_{\\star}^{k},W^{k},s_{\\star}^{k-1})=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma A.4. For $k=N-1$ , the stationarity condition $\\nabla_{s^{N-1}}\\mathcal{F}(s_{\\star},W,x,\\beta)$ yields: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{1}\\widetilde{E}^{N-1}\\left(s_{\\star}^{N-1},W^{N-1},s_{\\star}^{N-2}\\right)+0=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, for any $1\\leq k<N-1,\\nabla_{s^{k}}\\mathcal{F}(s_{\\star},W,x,\\beta)=0$ yields: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{1}\\widetilde{E}^{k}(s_{\\star}^{k},W^{k},s_{\\star}^{k-1})+\\underbrace{\\left[\\nabla_{3}\\widetilde{E}^{k+1}\\left(s^{k+1},W^{k+1},s_{\\star}^{k}\\right)-\\nabla_{3}\\widetilde{E}^{k+1}\\left(s_{\\star}^{k+1},W^{k+1},s_{\\star}^{k}\\right)\\right]}_{=0}=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Eq. (63) and Eq. (63) indeed correspond to ff-EBM inference as depicted inside Alg. 1. ", "page_idx": 22}, {"type": "text", "text": "The EP fundamental Lemma. For self-containedness of this paper, we restate the fundamental EP result below inside Lemma A.5. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.5 ([Scellier, 2021]). Let $\\mathcal{F}(s,W,x,\\beta)$ be a twice differentiable function of the three variables $s$ , $W$ and $\\beta$ . For fixed $W$ , $x$ and $\\beta$ , let $s_{\\beta}$ be a point that satisfies the stationarity condition: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{1}\\mathcal{F}(s_{\\beta},W,x,\\beta)=0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and suppose that $\\nabla_{1}^{2}\\mathcal{F}(s_{\\beta},W,x,\\beta)$ is invertible. Then, in the neighborhood of this point, we can define a continuously differentiable function $(x,W,\\beta)\\to s_{\\beta}$ such that Eq. (65) holds for any $(x,W,\\beta)$ in this neighborhood. Furthermore, we have the following identity: ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{W}\\left(\\nabla_{\\beta}\\mathcal{F}(s_{\\beta},W,x,\\beta)\\right)=d_{\\beta}\\left(\\nabla_{2}\\mathcal{F}(s_{\\beta},W,x,\\beta)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In particular, Eq. (66) may be evaluated with $\\mathcal{F}=E+\\beta\\ell$ at $\\beta=0$ to yield the EP learning rule, denoting $\\mathcal{C}:=\\ell(s_{\\star},y)$ [Scellier and Bengio, 2017]: ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{W}\\mathscr{C}=d_{\\beta}\\left(\\nabla_{2}\\mathcal{F}(s_{\\beta},W,x,\\beta)\\right)\\vert_{\\beta=0}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Theorem 3.1 as a direct application of EP. Now we know Eq. (61) defines a valid energy function for ff-EBMs and with Lemma A.5 in hand, we are ready to apply EP directly to this energy function. We rewrite below the block-wise free energy functions at use inside Theorem 3.1 and used in practice inside Alg. 2 to nudge a block of energy ${\\widetilde{E}}^{k}$ given some top-down error signal $\\delta^{k}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\widetilde{\\mathcal{F}}^{k}(s^{k},W^{k},s_{\\star}^{k-1},\\delta s^{k},\\beta):=\\widetilde{E}^{k}(s^{k},W^{k},s_{\\star}^{k-1})+\\beta s^{k^{\\top}}\\cdot\\delta s^{k},}\\\\ {\\delta s^{k}:=\\,d_{\\beta}\\left(\\nabla_{3}\\widetilde{E}^{k+1}\\left(s_{\\beta}^{k},W^{k+1},s_{\\star}^{k}\\right)\\right)\\Big|_{\\beta=0}\\,\\,\\,\\mathrm{if}\\,\\,k<N-1\\mathrm{~else~}\\nabla_{1}\\widetilde{\\ell}(s_{\\star}^{N-1},y,W^{N})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In Theorem A.2, we show that the direct application of Lemma A.5 to $\\mathcal{F}$ as defined inside Eq. (61) yields the same gradient formula for each parameter $\\boldsymbol{W}^{k}$ and the same nudged block states as those prescribed by Theorem 3.1 for sufficiently small $\\beta$ . ", "page_idx": 23}, {"type": "text", "text": "Theorem A.2 (Informal). Let $\\mathcal{F}$ be defined as in Eq. $(6l)$ ) satisfying the same assumptions as in Lemma A.5. For fixed $W$ , $x$ and $\\beta$ , let $s_{\\beta}$ satisfy the stationarity condition: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{1}\\mathcal{F}(s_{\\beta},W,x,\\beta)=0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nd_{W^{k}}\\mathscr{C}=\\left.d_{\\beta}\\left(\\nabla_{2}\\widetilde{E}^{k}\\big(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1}\\big)\\right)\\right|_{\\beta=0},\\quad\\nabla_{1}\\widetilde{\\mathcal{F}}^{k}\\big(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1},\\delta s^{k},\\beta\\big)=\\mathscr{O}(\\beta^{2})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem A.2. Lemma A.5 yielding: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{d_{W^{k}}\\mathcal{C}=\\left.d_{\\beta}\\left(\\nabla_{W^{k}}\\mathcal{F}(s_{\\beta},W,x,\\beta)\\right)\\right|_{\\beta=0},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "proving Eq. (69) amounts to show that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\beta}\\left(\\nabla_{W^{k}}\\mathcal{F}(s_{\\beta},W,x,\\beta))\\vert_{\\beta=0}=d_{\\beta}\\left(\\nabla_{2}\\widetilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})\\right)\\Big\\vert_{\\beta=0}\\,,}\\\\ &{\\nabla_{1}\\widetilde{\\mathcal{F}}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1},\\delta s^{k},\\beta)=\\mathcal{O}(\\beta^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the one hand, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{W^{k}}\\mathcal{F}(s_{\\beta},W,x,\\beta)=\\nabla_{2}\\widetilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\nabla_{2,3}^{2}\\widetilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})-\\nabla_{2,3}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},W^{k},s_{\\star}^{k-1})\\right)\\cdot\\left(s_{\\beta}^{k-1}-s_{\\star}^{k-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For notational convenience, we define $A(\\beta):=\\Big(\\nabla_{2,3}^{2}\\widetilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})-\\nabla_{2,3}^{2}\\widetilde{E}^{k}(s_{\\star}^{k},W^{k},s_{\\star}^{k-1})\\Big).$ Note that $A(\\beta=0)=0$ . Differentiating Eq. (72) with respect to $\\beta$ and taking $\\beta=0$ yields: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\beta}\\left(\\nabla_{W^{k}}\\mathcal{F}(s_{\\beta},W,x,\\beta)\\right)|_{\\beta=0}=d_{\\beta}\\left(\\nabla_{2}\\widetilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})\\right)|_{\\beta=0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;d_{\\beta}A(\\beta)|_{\\beta=0}\\cdot\\underbrace{\\left(s_{\\beta=0}^{k-1}-s_{\\star}^{k-1}\\right)}_{=0}+\\underbrace{A(0)}_{=0}\\cdot\\left(s_{\\beta}^{k-1}-s_{\\star}^{k-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=d_{\\beta}\\left(\\nabla_{2}\\widetilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})\\right)|_{\\beta=0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which proves Eq. (70). ", "page_idx": 23}, {"type": "text", "text": "On the other hand, the stationary condition $\\nabla_{s^{k}}\\mathcal{F}(s_{\\beta},W,x,\\beta)$ on the last block $\\!\\;k=N-1)$ ) yields: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{1}\\widetilde{E}^{N-1}(s_{\\beta}^{N-1},W^{N-1},s_{\\star}^{N-2})+\\beta\\nabla_{1}\\widetilde{\\ell}(s_{\\beta}^{N-1},y,W^{N})=0}\\\\ &{\\Rightarrow\\nabla_{1}\\widetilde{E}^{N-1}(s_{\\beta}^{N-1},W^{N-1},s_{\\star}^{N-2})+\\beta\\nabla_{1}\\widetilde{\\ell}(s_{\\star}^{N-1},y,W^{N})=\\mathcal{O}(\\beta^{2})}\\\\ &{\\Leftrightarrow\\nabla_{1}\\widetilde{\\mathcal{F}}^{N-1}(s_{\\beta}^{N-1},W^{N-1},s_{\\star}^{N-2},\\delta s^{N},\\beta)=\\mathcal{O}(\\beta^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For previous blocks, i.e. $k<N-1$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{s^{k}}\\mathcal{F}(s_{\\beta},W,x,\\beta)=0}\\\\ &{\\Leftrightarrow\\nabla_{1}\\tilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})+\\nabla_{3}\\tilde{E}^{k+1}\\left(s_{\\beta}^{k+1},W^{k+1},s_{\\star}^{k}\\right)-\\nabla_{3}\\tilde{E}^{k+1}\\left(s_{\\star}^{k+1},W^{k+1},s_{\\star}^{k}\\right)=0}\\\\ &{\\Rightarrow\\nabla_{1}\\tilde{E}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1})+\\,d_{\\beta}\\left(\\nabla_{3}\\tilde{E}^{k+1}\\left(s_{\\beta}^{k+1},W^{k+1},s_{\\star}^{k}\\right)\\right)\\Big|_{\\beta=0}=\\mathcal{O}(\\beta^{2})}\\\\ &{\\Leftrightarrow\\nabla_{1}\\tilde{\\mathcal{F}}^{k}(s_{\\beta}^{k},W^{k},s_{\\star}^{k-1},\\delta s^{k},\\beta)=\\mathcal{O}(\\beta^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Altogether, Eq. (73) and Eq. (74) finishes to prove Eq. (71). ", "page_idx": 24}, {"type": "text", "text": "A.3 Resulting algorithms ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "A.3.1 Explicit BP-EP chaining ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We presented in Alg. 2 a \u201cpure\u201d EP algorithm where the BP-EP gradient chaining is implicit. We show below, inside Alg. 5, an alternative implementation (equivalent in the limit $\\beta\\rightarrow0,$ ) where the use of BP through feedforward modules is explicit and which is the direct implementation of Theorem A.1. We also show the resulting algorithm when the ff-EBM reduces to a feedforward net (Lemma A.1) inside Alg. 7, highlight in blue the statements which differ from the general case presented inside Alg. 5. ", "page_idx": 25}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/58cf29d8cfdb8cebd4a2937d49bb26a8f8bcced8a896fd7ae65a5165b4df4c0f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "A.3.2 Recovering backprop through feedforward nets as a special case ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Corollary A.1. Under the same hypothesis as Theorem A.1 and Lemma A.1, then the following chain rule applies to compute error signals backward from the output layer: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l}{\\delta_{s}^{N-1}:=\\nabla_{s^{N-1}}\\ell(\\hat{o}_{\\star},y),\\quad g_{\\omega^{N}}=\\nabla_{\\omega^{N}}\\ell(\\hat{o}_{\\star},y)}\\\\ {\\Delta x^{k}=\\sigma^{\\prime}(x_{\\star}^{k})\\odot\\delta s^{k}}\\\\ {\\delta s^{k-1}=\\partial_{1}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k},\\quad g_{\\omega^{k}}=\\partial_{2}F^{k}\\left(s_{\\star}^{k-1},\\omega^{k}\\right)^{\\top}\\cdot\\Delta x^{k}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Corollary A.1. Let $k\\in[1,N-1]$ . As we can directly apply Theorem A.1 here, proving the result simply boils down to showing that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta x^{k}=\\sigma^{\\prime}(x_{\\star}^{k})\\odot\\delta s^{k}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "First, we notice that when $E^{k}$ is of the form of Eq. (18), then $\\Delta x^{k}$ reads as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta x^{k}=\\,d_{\\beta}\\left.\\left(\\nabla_{3}E^{k}(s_{\\beta}^{k},\\theta^{k},x_{\\star}^{k})\\right)\\right\\vert_{\\beta=0}=-\\left.d_{\\beta}\\left(s_{\\beta}^{k}\\right)\\right\\vert_{\\beta=0}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$s_{\\beta}^{k}$ satisfies, by definition and when $U^{k}=0$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{-1}(s_{\\beta}^{k})-x_{\\star}^{k}+\\beta\\delta s^{k}=0}\\\\ {\\Leftrightarrow}&{s_{\\beta}^{k}=\\sigma\\left(x_{\\star}^{k}-\\beta\\delta s^{k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining Eq. (77) and Eq. (78) yields Eq. (76), and therefore, along with Theorem A.1, the chain-rule Eq. (75). \u53e3 ", "page_idx": 25}, {"type": "text", "text": "We showcase in Alg. 6 and Alg. 7 the resulting algorithms implicit and explicit BP-EP chaining respectively, with lines in blue highlighting differences with the general algorithm Alg. 2. ", "page_idx": 25}, {"type": "text", "text": "Algorithm 6 Implicit BP-EP gradient chaining with $U^{k}=0$ ", "page_idx": 26}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/63bf265375364ee2e200818d737196aed92310d18f5a6ed1d18811c542e69c71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Algorithm 7 Explicit BP-EP gradient chaining with $U^{k}=0$ ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/3b9e9358a02746c07c7f41896891c67fcdf2851807e71997feca10f07974f853.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "A.3.3 Detailed implementation of the implicit BP-EP chaining algorithm (Alg. 2) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Nudging the last block. From looking at the procedure prescribed by Theorem 3.1 and algorithms thereof (Alg. 2, Alg. 5), all the error signals used to nudge the EB blocks are stationary, including the top-most block where the loss error signal is fed in. Namely, the augmented energy function of the last block reads as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathscr{F}^{N-1}(s^{N-1},\\theta^{N-1},x_{\\star}^{N-1},\\beta):=E^{N-1}(s^{N-1},\\theta^{N-1},x_{\\star}^{N-1})+\\beta s^{N-1^{\\top}}\\cdot\\nabla_{s^{N-1}}\\ell(\\hat{o}_{\\star},y),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\hat{o}_{\\star}:=F^{N}\\left(s_{\\star}^{N-1},\\omega^{N}\\right)$ is constant. Up to a constant, Eq. (80) uses the cost function linearized around $s_{\\star}^{N-1}$ instead of the cost function itself. This is, however, in contrast with most EP implementations where the nudging force acting upon the EB block is usually elastic, i.e. the nudging depends on the current state of the EB block. More precisely, instead of using Eq. (79), we instead use: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathscr{F}^{N-1}(s^{N-1},\\theta^{N-1},x_{\\star}^{N-1},\\beta):=E^{N-1}(s^{N-1},\\theta^{N-1},x_{\\star}^{N-1})+\\beta\\ell(F^{N}(s^{N-1},\\omega^{N}),y),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This results in the following asynchronous fixed-point dynamics for the last block: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\forall\\,\\mathrm{odd}\\,\\ell\\in\\{1,\\cdots,L_{k}\\}:\\quad s_{\\ell,\\pm\\beta,t+\\frac{1}{2}}^{k}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\pm\\beta,t}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\mp\\beta\\nabla_{s^{k}}\\ell(s_{\\pm\\beta,t}^{k},y)\\right),}\\\\ {\\forall\\,\\mathrm{even}\\,\\ell\\in\\{1,\\cdots,L_{k}\\}:\\quad s_{\\ell,\\pm\\beta,t+1}^{k}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{k}}\\Phi\\left(s_{\\pm\\beta,t+\\frac{1}{2}}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\right)\\mp\\beta\\nabla_{s^{k}}\\ell(s_{\\pm\\beta,t}^{k},y)\\right)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The resulting Asynchronous subroutine, applying for the last block, is depicted inside Alg. 8. ", "page_idx": 26}, {"type": "text", "text": "Readout. Laborieux et al. [2021] introduced the idea of the \u201creadout\u201d whereby the last linear layer computing the loss logits is not part of the EB free block dynamics but simply \u201creads out\u201d the state of the penultimate block. In all our experiments we use such a readout in combination with the cross entropy loss function. Using our formalism, our readout is simply the last feedforward transformation used inside $\\ell$ , namely $F^{N}\\check{(}\\cdot,\\omega^{N})$ . ", "page_idx": 26}, {"type": "text", "text": "Detailed implicit EP-BP chaining algorithm. We provide a detailed implementation of our algorithm presented in the main (Alg. 2) in Alg. 11. As usually done for EP experiments, we always perform a \u201cfree phase\u201d to initialize the block states (Forward subroutine, Alg. 4). Then, two ", "page_idx": 26}, {"type": "text", "text": "Algorithm 8 Asynchronous (for last block) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: T, $\\theta^{N-1}$ , $\\omega^{N-1}$ , $\\omega^{N}$ , $s_{\\star}^{k-1}$ , \u03b2, \u2113(cost function), y Output: s\u03b2N $s_{\\beta}^{N-1}$ ", "page_idx": 27}, {"type": "text", "text": "1: $s^{N-1}\\leftarrow0$   \n2: for $t=1\\cdot\\cdot\\cdot T$ do   \n3: \u2200odd $\\ell\\in\\{1,\\cdots\\,,L_{N}\\}$ :   \n4: $\\begin{array}{r l}&{\\mathsf{s u m}_{s_{\\ell,\\beta}}^{\\mathbf{N}-1}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{N-1}}\\Phi\\left(s_{\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1}\\right)-\\beta\\nabla_{s_{\\ell}^{N-1}}\\ell(F^{N}(s^{N-1},\\omega^{N}),y)\\right)}\\\\ &{\\mathsf{s v e n}\\,\\ell\\in\\{1,\\cdots,L_{N}\\};}\\\\ &{s_{\\ell,\\beta}^{N-1}\\gets\\sigma\\left(\\nabla_{s_{\\ell}^{N-1}}\\Phi\\left(s_{\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1}\\right)-\\beta\\nabla_{s_{\\ell}^{N-1}}\\ell(F^{N}(s^{N-1},\\omega^{N}),y)\\right)}\\end{array}$   \n5: \u2200   \n6:   \n7: end for ", "page_idx": 27}, {"type": "text", "text": "nudged phases are applied to the last block and parameter gradients subsequently computed, as done classically (BlockGradient subroutine for the last block, Alg. 9), with an extra computation to compute the error current to be applied to the penultimate block $(\\delta s^{N-2})$ . Then, the same procedure is recursed backward through blocks (Alg. 10), until reaching first block. ", "page_idx": 27}, {"type": "text", "text": "Algorithm 9 BlockGradient (for last block) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: $T,s_{\\star-2}^{N-2},\\theta^{N-1},\\omega^{N-1},\\omega^{N},\\beta,\\ell,y$   \nOutput:\u03b4sN\u22122   \n1: s \u03b2N\u22121\u2190Asynchronous $\\left(T,\\theta^{N-1},\\omega^{N-1},\\omega^{N},\\beta,\\ell,y\\right)$ \u25b7Alg. 8   \n2: s\u2212\u03b2 $s_{-\\beta}^{N-1}\\leftarrow$ Asynchronous $(T,\\theta^{N-1},\\omega^{N-1},\\omega^{N},-\\beta,\\ell,y)$   \n3: $\\begin{array}{r}{g_{\\omega^{N}}\\gets\\frac{1}{2}\\left(\\nabla_{s^{N-1}}\\ell\\big(F^{N}\\left(s_{\\beta}^{N-1},\\omega^{N}\\right)\\big)+\\nabla_{s^{N-1}}\\ell\\big(F^{N}\\left(s_{-\\beta}^{N-1},\\omega^{N}\\right)\\big)\\right)}\\end{array}$   \n5: $\\begin{array}{c}{{g_{\\theta^{N-1}}\\leftarrow\\frac{\\mathrm{i}}{2\\beta}\\left(\\nabla_{2}\\widetilde{E}^{N-1}(s_{\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1})-\\dot{\\nabla}_{2}\\widetilde{E}^{N-1}(s_{-\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-2}-\\omega^{N})\\right)}}\\\\ {{g_{\\omega^{N-1}}\\leftarrow\\frac{1}{2\\beta}\\left(\\nabla_{4}\\widetilde{E}^{N-1}(s_{\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1})-\\nabla_{4}\\widetilde{E}^{N-1}(s_{-\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-2}-\\omega^{N})\\right)}}\\\\ {{\\cdots\\qquad\\cdots\\qquad\\cdots\\qquad\\cdots\\qquad\\cdots\\qquad\\cdots\\qquad\\cdots\\qquad\\cdots\\qquad\\cdots}}\\end{array}$   \n6: $\\begin{array}{r}{\\delta s^{N-2}\\gets\\frac{1}{2\\beta}\\left(\\nabla_{3}\\widetilde{E}^{N-1}(s_{\\beta}^{N-1},\\theta^{N-1},s_{\\star}^{N-2},\\omega^{N-1})-\\nabla_{3}\\widetilde{E}^{N-1}(s_{-\\beta}^{N-1},\\theta^{N-1},\\omega^{N-1})\\right).}\\end{array}$ s\u22c6N\u22122, \u03c9N\u22122) ", "page_idx": 27}, {"type": "text", "text": "Algorithm 10 BlockGradient (for all blocks until penultimate) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: $T,s_{\\star}^{k-1},\\theta^{k},\\omega^{k},\\beta,\\delta s$   \nOutput:\u03b4sk\u22121   \n1: $s_{\\beta}^{k}\\gets$ Asynchronous $\\left(T,\\theta^{k},\\omega^{k},\\beta,\\delta s\\right)$ \u25b7Alg. 3   \n2: $s_{-\\beta}^{k}\\leftarrow$ Asynchronous $\\left(T,\\theta^{k},\\omega^{k},-\\beta,\\delta s\\right)$   \n3: $\\begin{array}{r l}&{g_{\\theta^{k}}\\leftarrow\\frac{1}{2\\beta}\\left(\\nabla_{2}\\widetilde{E}^{k}\\big(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)-\\nabla_{2}\\widetilde{E}^{k}\\big(s_{-\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\right)}\\\\ &{g_{\\omega^{k}}\\leftarrow\\frac{1}{2\\beta}\\left(\\nabla_{4}\\widetilde{E}^{k}\\big(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)-\\nabla_{4}\\widetilde{E}^{k}\\big(s_{-\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\right)}\\\\ &{\\delta s^{k-1}\\leftarrow\\frac{1}{2\\beta}\\left(\\nabla_{3}\\widetilde{E}^{k}\\big(s_{\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)-\\nabla_{3}\\widetilde{E}^{k}\\big(s_{-\\beta}^{k},\\theta^{k},s_{\\star}^{k-1},\\omega^{k}\\big)\\right)}\\end{array}$   \n5: ", "page_idx": 27}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/f9a99d6714061d19b6f236532e76baceda578fde393b7fb7793869ac1316788f.jpg", "table_caption": ["Algorithm 11 Detailed implicit BP-EP gradient chaining "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "A.4 Static gradient analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Important foreword. The whole subsection is dedicated to an important tool when developing code for EP research. While EP is agnostic to how the steady states are obtained \u2013 the EP theory only prescribes they are energy minimizers \u2013 they can be obtained in practice (i.e. in simulations) through fixed-point iteration schemes (see Appendix A.1.1). The below formally defines the computational graph spanned by these schemes and abstract them away into a transition function $K$ and defines three different techniques to compute gradients on this graph: Automatic Differentiation (AD, Prop. A.1), Implicit Differentiation (ID, Def. A.3) or Equilibrium Propagation (EP, Def. A.4). After defining each of these algorithms formally, we will state and demonstrate an equivalence between EP and ID (Theorem A.3) which we test numerically and relied upon for the development of our codebase. ", "page_idx": 28}, {"type": "text", "text": "A.4.1 Algorithmic baselines ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Definition of the computational graph being optimized. We abstract fixed-point iteration dynamics away into a kernel function $K$ which, given some block state $s_{t}^{k}$ yields $s_{t+1}^{k}$ . ", "page_idx": 28}, {"type": "text", "text": "Definition A.2 (Form of the computational graph through equilibrium). ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k=1,\\cdots,N-1,\\,\\forall t=1,\\cdots,\\tau:}\\\\ &{\\quad x^{0}=x,\\quad s_{t}^{k}=K(s_{t-1}^{k},W_{t-1}^{k}=W^{k},x^{k}=s_{\\tau}^{k-1}),\\quad\\mathscr{C}=\\ell(F^{N}(s_{\\tau}^{N-1},\\omega^{N}),y):=\\tilde{\\ell}(s^{N-1},y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that we emphasize, through the $W_{t-1}^{k}=W^{k}$ notation, that the parameters $\\boldsymbol{W}^{k}$ are shared across all timesteps $t=1,\\cdot\\cdot\\cdot,\\tau$ . This will help us define loss gradient with respect to $W_{t-1}^{k}$ further below, i.e. how much $\\boldsymbol{W}^{k}$ contributes at time $t-1$ to changing the loss $\\mathcal{C}$ . The total contribution of $\\boldsymbol{W}^{k}$ reads as the sum of the elemental contributions of all $\\bar{W}_{t}^{k}$ . This intuition is more precisely illustrated further below. Given the computational graph defined in Def. A.2, we can now formally define the Automatic Differentiation (AD) baseline. ", "page_idx": 28}, {"type": "text", "text": "Automatic Differentiation (AD). Our goal is to compute: ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\ng_{W^{k}}^{\\mathrm{AD}}:=\\hat{g}_{W^{k}}^{\\mathrm{AD}}(\\tau)\\quad\\mathrm{with:}\\quad\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t):=\\sum_{k=1}^{t}\\partial_{W_{\\tau-k}^{k}}\\mathcal{C}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In plain words, $\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t)$ denotes the loss gradient for parameter $\\boldsymbol{W}^{k}$ truncated at the $t^{\\mathrm{th}}$ step moving backward in time. We formally define below Automatic Differentiation (AD). ", "page_idx": 28}, {"type": "text", "text": "Proposition A.1 (Automatic Differentiation (AD)). The gradients $\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t)$ can be computed using the following recursive equations: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{'k=N-1\\cdot\\cdots1:}\\\\ &{~~\\delta s_{0}^{k}=\\delta x_{\\tau}^{k+1}~i f k<N-1~e l s e~\\nabla_{1}\\tilde{\\ell}(s_{\\tau}^{N-1},y)}\\\\ &{~~\\delta x_{0}^{k}=0,~~~\\hat{g}_{W^{k}}^{\\mathrm{AD}}(0)=0}\\\\ &{~\\forall t=1,\\cdots\\,\\tau:}\\\\ &{~~\\left\\{\\begin{array}{l l}{\\delta s_{t}^{k}=\\partial_{1}K(s_{\\tau-t}^{k},W^{k},x^{k}=s_{\\tau}^{k-1})^{\\top}\\cdot\\delta s_{t-1}^{k-1}}\\\\ {\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t)=\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t-1)+\\partial_{2}K(s_{\\tau-t}^{k},W^{k},x^{k}=s_{\\tau}^{k-1})^{\\top}\\cdot\\delta s_{t-1}^{k}}\\\\ {\\delta x_{t}^{k}=\\delta x_{t-1}^{k}+\\partial_{3}K(s_{\\tau-t}^{k},W^{k},x^{k}=s_{\\tau}^{k-1})^{\\top}\\cdot\\delta s_{t-1}^{k}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Prop. A.1. This is a straightforward application of the chain rule applied to Eq. (81). ", "page_idx": 28}, {"type": "text", "text": "Implicit Differentiation $(\\mathbf{D})$ . We define the steady state of block $k$ , which we denote $s_{\\star}^{k}$ , as the fixed point of Eq. (81). With this notation in hand, we can define Implicit Differentiation (ID) in this setting. ", "page_idx": 28}, {"type": "text", "text": "Definition A.3 (Implicit Differentiation (ID)). Denoting $s_{\\star}^{k}$ the fixed point of Eq. (81) inside block k, we define Implicit Differentiation $(I D)$ through the following recursive equations: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k=N-1\\cdots1:}\\\\ &{\\begin{array}{r l}&{~~6s_{0}^{k}=\\delta x_{\\tau}^{k+1}~i f k<N-1~e l s e\\;\\nabla_{1}\\tilde{\\ell}(s_{\\tau}^{N-1},y)}\\\\ &{~~\\delta x_{0}^{k}=0,\\quad\\;\\tilde{g}_{W^{k}}^{\\mathrm{ID}}(0)=0}\\end{array}}\\\\ &{\\begin{array}{r l}&{\\forall t=1,\\cdots,\\tau:}\\\\ &{\\forall t=1,\\cdots,\\tau\\,\\langle}\\\\ &{\\left\\{\\begin{array}{r l}&{\\delta s_{t}^{k}=\\partial_{1}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\delta s_{t-1}^{k-1}}\\\\ &{\\hat{g}_{W^{k}}^{\\mathrm{ID}}(t)=\\hat{g}_{W^{k}}^{\\mathrm{ID}}(t-1)+\\partial_{2}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\delta s_{t-1}^{k}}\\\\ &{\\delta x_{t}^{k}=\\delta x_{t-1}^{k}+\\partial_{3}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\delta s_{t-1}^{k}}\\end{array}\\right.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We are now ready to state a simple algorithmic equivalence between ID and AD, which we built upon for our implementation of Alg. 12. ", "page_idx": 29}, {"type": "text", "text": "Corollary A.2 (Equivalence of ID and AD). Assuming that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall k=1,\\cdots,N-1,\\,\\forall t=1,\\cdots\\,,\\tau:\\quad s_{t}^{k}=s_{\\star}^{k},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $s_{\\star}^{k}$ denotes the fixed-point of Eq. 81, then automatic differentiation (Prop. A.1) and implicit differentiation (Def. A.3) are equivalent, namely: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall k=1,\\cdots,N-1,\\,\\forall t=1,\\cdots\\,,\\tau:\\quad\\hat{g}_{W^{k}}^{\\mathrm{ID}}(t)=\\hat{g}_{W^{k}}^{\\mathrm{AD}}(t)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Corollary A.2. This is a straightforward application of the definition of AD (Prop. A.1 along with the hypothesis made inside Corollary A.2. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Resulting implementation of ID. We describe our implementation of ID inside Alg. 12. First, we relax all blocks sequentially to equilibrium following Alg. 4 and we do not track gradients throughout this first phase, using $T_{\\mathrm{free}}$ fixed-point iteration steps per block. Then, initializing the block states with those computed at the previous step, we re-execute the same procedure (still with Alg. 4), this time tracking gradients and using $T_{\\mathrm{nudge}}$ steps fixed-point iteration steps for each block. Then, we use automatic differentiation to backpropagate through the last $T_{\\mathrm{nudge}}$ steps for each block, namely backpropagating, backward in time, through equilibrium. ", "page_idx": 29}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/abdb8a051a406bde91baca1fe819b780f140209adf1ba7e368261b633a1ca945.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "An important note about this implementation of ID. Note that this is not a standard implementation of ID and it may be surprising at first glance to implement ID as AD, thereby loosing the constant $\\mathcal{O}(1)$ memory cost of ID with respect to the length of the computational graph. Instead, the memory cost of Alg. 12 is $\\mathcal{O}((N-1)\\tau)$ 4. However, our goal is not so much to optimize for memory usage (as in the context of Deep Equilibrium Models [Bai et al., 2019]) but to code an algorithmic baseline which we know to be equivalent to EP. Lastly, note that this implementation of ID is also known as Recurrent Backprop (RBP, [Almeida, 1987, Pineda, 1987]) or Von-Neumann RBP [Liao et al., 2018], and that ID generally comes in many more algorithmic flavors [Blondel et al., 2022]. ", "page_idx": 29}, {"type": "text", "text": "A.4.2 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In order to state a formal equivalence between EP and ID, we first need to formally define EP in the context of the aforementioned computational graph defined in Def. A.2. ", "page_idx": 30}, {"type": "text", "text": "Definition A.4 (Equilibrium Propagation (EP)). Denoting $s_{\\star}^{k}$ the fixed point of Eq. (81) inside block $k$ and assuming that the transition kernel $K$ has the form $K(s,W^{k},x^{k})=\\nabla_{1}\\Phi(s,W^{k},x^{k})$ , we define Equilibrium Propagation $(E P)$ through the following recursive equations: ", "page_idx": 30}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\mathfrak{s}}=N-1\\cdot\\mathord{\\lambda}:}\\\\ &{\\delta\\mathfrak{s}^{k}=\\Delta\\mathfrak{x}_{\\tau}^{k+1}\\,{i f k}<N-1\\;e l s e\\;\\nabla_{1}\\tilde{\\ell}(\\mathfrak{s}_{\\tau}^{N-1},y)}\\\\ &{\\Delta\\mathfrak{x}_{0}^{k}=0,\\quad\\frac{\\hat{y}^{\\mathrm{EP}}}{\\tilde{g}W^{k}}(0)=0,\\quad\\mathfrak{s}_{\\beta,t=0}^{k}=s_{\\star}^{k}}\\\\ &{\\forall t=1,\\cdots,\\tau:}\\\\ &{\\left\\{\\begin{array}{l l}{s_{\\beta,t+1}^{k}=\\nabla_{1}\\Phi(s_{\\beta,t}^{k},W^{k},x^{k}=s_{\\star}^{k-1})-\\beta\\delta\\mathfrak{s}^{k}}\\\\ {\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t)=-\\frac{1}{2\\beta}\\left(\\nabla_{2}\\Phi(s_{\\beta,t+1}^{k},W^{k},x^{k}=s_{\\star}^{k-1})-\\nabla_{2}\\Phi(s_{-\\beta,t+1}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\right)}\\\\ {\\Delta\\mathfrak{x}_{\\beta,t}^{k}=-\\frac{1}{2\\beta}\\left(\\nabla_{3}\\Phi(s_{\\beta,t+1}^{k},W^{k},x^{k}=s_{\\star}^{k-1})-\\nabla_{3}\\Phi(s_{-\\beta,t+1}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\right)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now that we have properly defined ID and EP, we are ready to state the main result of this section about the algorithmic equivalence between ID and EP which our coding work significantly built upon. Theorem A.3 (Extension of [Ernoult et al., 2019] to ff-EBMs). Assuming that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall k=1,\\cdots,N-1,\\,\\forall t=1,\\cdots\\,,\\tau:\\quad s_{t}^{k}=s_{\\star}^{k},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $s_{\\star}^{k}$ denotes the fixed-point of Eq. (81) and that the transition kernel $K$ has the form $K(s,W^{k},x^{k})\\,=\\,\\nabla_{1}\\Phi(s,W^{k},x^{k})$ , then implicit differentiation (Def. A.3) and equilibrium propagation (Def. A.4) are equivalent in the limit $\\beta\\rightarrow0$ , namely: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall k=1,\\cdots\\,,N-1,\\,\\forall t=1,\\cdots\\,,\\tau:\\quad\\operatorname*{lim}_{\\beta\\to0}\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t)=\\hat{g}_{W^{k}}^{\\mathrm{ID}}(t)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Theorem A.3. This proof follows the exact same methodology as that of Ernoult et al. [2019]. For self-containedness though and because of some subtle differences, we carry out here the derivation. We first define: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Delta s_{t}^{k}:=d_{\\beta}s_{t+1}^{k}\\vert_{\\beta=0}-d_{\\beta}s_{t}^{k}\\vert_{\\beta=0}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that since $s_{\\beta,t=0}^{k}=s_{\\star}$ , $d_{\\beta}s_{t}^{k}\\vert_{\\beta=0}=0$ since $s_{\\star}$ does not depend on $\\theta$ . Furthermore, note that by differentiating the equation satisfied by $s_{\\beta,t+1}^{k}$ with respect to $\\beta$ and evaluating the resulting expressions at $\\beta=0$ yields: ", "page_idx": 30}, {"type": "equation", "text": "$$\nd_{\\beta}s_{\\beta,t+1}^{k}|_{\\beta=0}=\\partial_{1}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\cdot d_{\\beta}s_{\\beta,t}^{k}|_{\\beta=0}-\\delta s^{k}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular, evaluating Eq. (90) for $t=0$ yields: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta s_{0}^{k}=d_{\\beta}s_{\\beta,1}^{k}|_{\\beta=0}-\\underbrace{d_{\\beta}s_{\\beta,0}^{k}|_{\\beta=0}}_{=0}=-\\delta s^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, substracting Eq. (90) across two timesteps yields altogether: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta s_{t}^{k}=\\partial_{1}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\cdot\\Delta s_{t-1}^{k}}\\\\ &{\\qquad=\\nabla_{1}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\cdot\\Delta s_{t-1}^{k}}\\\\ &{\\qquad=\\nabla_{1}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\Delta s_{t-1}^{k}}\\\\ &{\\qquad=\\partial_{1}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\Delta s_{t-1}^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that $\\hat{g}_{W^{k}}^{\\mathrm{EP}}(t)$ rewrites: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{W^{k}}^{\\mathrm{EP}}(\\beta,t)=-d_{\\beta}\\left(\\nabla_{2}\\Phi(s_{\\beta,t+1}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\right)+\\mathcal{O}(\\beta^{2})}\\\\ &{\\quad\\quad\\quad\\quad=-\\nabla_{1,2}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\cdot d_{\\beta}s_{\\beta,t+1}^{k}|_{\\beta=0}+\\mathcal{O}(\\beta^{2})}\\\\ &{\\quad\\quad\\quad=-\\nabla_{1,2}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\cdot\\Delta s_{t}^{k}-\\nabla_{1,2}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})\\cdot d_{\\beta}s_{\\beta,t}^{k}|_{\\beta=0}+\\mathcal{O}(\\beta^{2})}\\\\ &{\\quad\\quad\\quad=-\\underbrace{\\nabla_{1,2}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})}_{=\\nabla_{2,1}^{2}\\Phi(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}}\\cdot\\Delta s_{t}^{k}+\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t-1)+\\mathcal{O}(\\beta^{2})}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\partial_{2}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\left(-\\Delta s_{t}^{k}\\right)+\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t-1)+\\mathcal{O}(\\beta^{2})}\\\\ &{\\quad\\quad\\quad\\quad=\\partial_{2}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\left(-\\Delta s_{t}^{k}\\right)+\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t-1)+\\mathcal{O}(\\beta^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Likewise, we can show that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Delta x_{\\beta,t}^{k}=\\partial_{3}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\left(-\\Delta s_{t}^{k}\\right)+\\Delta x_{\\beta,t-1}^{k}+\\mathcal{O}(\\beta^{2})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Altogether, Eq. (91), Eq. (92) Eq. (93) and Eq. (94) yield, denoting $\\begin{array}{r}{\\hat{g}_{W^{k}}^{\\mathrm{EP}}(t):=\\operatorname*{lim}_{\\beta\\to0}\\hat{g}_{W^{k}}^{\\mathrm{EP}}(\\beta,t)}\\end{array}$ and \u2206xtk := lim\u03b2\u21920 \u2206xk\u03b2,t: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall k=N-1,\\cdot\\cdot\\cdot,1:\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-\\Delta s_{t}^{k}}&{=\\partial_{1}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot(-\\Delta s_{t-1}^{k})}\\\\ {\\hat{g}_{W^{k}}^{\\mathrm{EP}}(t)}&{=\\hat{g}_{W^{k}}^{\\mathrm{EP}}(t-1)+\\partial_{2}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\left(-\\Delta s_{t}^{k}\\right)}\\\\ {\\Delta x_{t}^{k}}&{=\\partial_{3}K(s_{\\star}^{k},W^{k},x^{k}=s_{\\star}^{k-1})^{\\top}\\cdot\\left(-\\Delta s_{t}^{k}\\right)+\\Delta x_{t-1}^{k}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Starting from $\\textit{k}=\\textit{N}-\\textit{1}$ , $(-\\Delta s_{t}^{N-1})_{t\\in[1,\\tau]}$ and $(\\delta s_{t}^{N-1})_{t\\in[\\![1,\\tau]\\!]}$ , $(\\Delta x_{t}^{N-1})_{t\\in[1,\\tau]}$ and $(\\delta x^{N-1})_{t\\in[\\![1,\\tau]\\!]}$ , $(\\hat{g}_{W^{N-1}}^{\\mathrm{EP}}(t))_{t\\in[\\![1,\\tau]\\!]}$ and $(\\hat{g}_{W^{N-1}}^{\\mathrm{ID}}(t))_{t\\in[\\![1,\\tau]\\!]}$ satisfy the  sa me initial condi tion s and recursive eq uat ions, therefore th ere  are all (pair-wise) eq ual  for $t=1,\\cdot\\cdot\\cdot,\\tau$ . Therefore in particular, $\\Delta x_{\\tau}^{N-1}\\,=\\,\\delta x_{\\tau}^{N-1}$ such that $(-\\Delta s_{t}^{N-2})_{t\\in[\\![1,\\tau]\\!]}$ and $(\\delta s_{t}^{N-2})_{t\\in[\\![1,\\tau]\\!]}$ from the previous $(N-2)^{\\mathrm{th}}$ block satisfy the same initial conditions, suc h th at the reasonning  app lying to $k=N-1$ recurses for $k<N-1$ , which yields Eq. (88). ", "page_idx": 31}, {"type": "text", "text": "A.4.3 Details about Fig. 3 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Precise hyperparameters to reproduce Fig. 5 and Fig. 3 can be found inside our repository.   \nFig. 7 precisely depict the architecture at use for these experiments. ", "page_idx": 32}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/c9e29102ca491c36794ef8a634e51b373d2daaf9eed1cc5a89addcebd22fe397.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 7: Architecture used for the static gradient analysis. The color code used to label layers matches that of Fig. 3 and Fig. 5. In the context of the static gradient analysis, \u201cblock\u201d $k$ is defined as all layers participating in ${\\tilde{E}}^{k}$ , which therefore includes $F^{k}$ and $E^{k}$ modules (rather than one of these taken alone). ", "page_idx": 32}, {"type": "text", "text": "A.5 Experimental Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "A.5.1 Datasets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Simulations were run on CIFAR-10, CIFAR-100 and Imagenet32 datasets, all consisting of color images of size $32\\times32$ pixels. CIFAR-10 [Krizhevsky, 2009] includes 60,000 color images of objects and animals. Images are split into 10 classes, with 6,000 images per class. Training data and test data include 50,000 images, and 10,000 images respectively. CIFAR-100 [Krizhevsky, 2009] likewise comprises 60,000 and features a diverse set of objects and animals split into 100 distinct classes. Each class contains 600 images. Like CIFAR-10, the dataset is divided into a training set with 50,000 images and a test set containing the remaining 10,000 images. The ImageNet32 dataset [Chrabaszcz et al., 2017] is a downsampled version of the original ImageNet dataset Russakovsky et al. [2015] containing 1,000 classes with 1,281,167 training images, 50,000 validation images, 100,000 test images and 1000 classes. ", "page_idx": 33}, {"type": "text", "text": "A.5.2 Data preprocessing ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "All data were normalized according to statistics shown in 3 and augmented with $50\\%$ random horizontal filps. Images were randomly cropped and padded with the last value along the edge of the image. ", "page_idx": 33}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/bd3bad62174242e1c0fd9cdd8d7665af450101d5bcd7d598141c3bd890c0dc67.jpg", "table_caption": ["Table 3: Data Normalization. Input images were normalized by conventional mean $(\\mu)$ and standard deviation $(\\sigma)$ values for each dataset. All images used are color (three channels). "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "A.5.3 Simulation details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Weight initialization. EP, similar to other machine learning paradigms reliant on fixed-point iteration [Bai et al., 2019], is highly sensitive to initialization statistics [Agarwala and Schoenholz, 2022], hence conventionally difficult to tune, and requiring many iterations for the three relaxation phases. Initialization of weights as Gaussian Orthogonal Ensembles (GOE) ensures better stability (reduced variance) and, combined with other stabilizing measures discussed below, empirically yields faster convergence. ", "page_idx": 33}, {"type": "text", "text": "According to GOE, weights are intialized as: ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{i j}\\sim\\left\\{{\\mathcal{N}}(0,{\\frac{V}{N}}),\\quad{\\mathrm{if~}}i\\neq j\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathcal{N}(\\mu,\\sigma^{2})$ denotes a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^{2}$ . $N$ was manually tuned for each architecture. ", "page_idx": 33}, {"type": "text", "text": "State initialization. All layers are initialized as zero matrices. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Activation functions. An important detail for faithful reproduction of these experiments is the choice and placement of activation functions applied during the iterative fixed-point procedure. In the literature, activations (i.e. \u201cclamping\u201d) is conventionally applied at each layer, with the exception of the final layer, where it is sometimes included e.g. Scellier et al. [2024], and sometimes omitted Laborieux et al. [2021], depending on the loss function at use. For these experiments we used both the standard hard activation employed by Ernoult et al. [2019] and Scellier et al. [2024], and the more conservative one given in [Laborieux et al., 2021]. For the tolerance based and splitting experiments, we generalize the approach of Laborieux et al. [2021], by scaling values by a variable factor $\\alpha$ instead of a fixed value 0.5 . Details are given in Table 4. ", "page_idx": 33}, {"type": "text", "text": "In practice, we find that the smaller scaling factors corresponding with the \u201claborieux\u201d activation, in conjunction with GOE, and the omission of clamping at the output of each block significantly enhances gradient stability and speeds convergence in deep multi-block settings. In the interest of multi-scale uniformity and consistency with previous literature [Laborieux et al., 2021] Ernoult et al. [2019], we apply clamping activations on all layers in our 6-layer architecture. ", "page_idx": 33}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/d33fe209f913777f79ecaf3e29a8a9e6abc68ee01d364b2de981f64d9d30954f.jpg", "table_caption": ["Table 4: Activation functions "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "For the scaling experiments, we apply the \u201claborieux\u201d activation at every layer except the output of each block. For the 12-layer splitting experiment, we do the same, omitting clamping from the output of the final layer of each block in the block-size $=4$ and block-size $_{-3}$ experiments. However, in the block-size $^{=2}$ case we clamp the output of the second and fourth blocks to preserve dynamics of the block-size $^{=4}$ split. Such consistency is not possible for the block-size $=3$ experiment, constituting a possible discrepancy in dynamics. ", "page_idx": 34}, {"type": "text", "text": "Cross-entropy loss and softmax readout. Following [Laborieux et al., 2021], all models were implemented such that the output $y$ is removed from the system (e.g. not included in the relaxation dynamics) but is instead the function of a weight matrix: $W_{\\mathrm{{out}}}$ of size $\\dim(y)\\times\\dim(s)$ , where $s$ is the state of the final layer. For each time-step $t$ , $\\hat{y}_{t}=\\operatorname{softmax}(W_{\\operatorname{out}}s_{t})$ . ", "page_idx": 34}, {"type": "text", "text": "The cross-entropy cost function associated with the softmax readout is then: ", "page_idx": 34}, {"type": "equation", "text": "$$\nl(s,y,W_{\\mathrm{out}})=-\\sum_{c=1}^{C}y_{c}\\log(\\mathrm{softmax}_{c}(W_{\\mathrm{out}}\\cdot s)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Convention to count layers. It is important to note that by convention we refer to architectures throughout this text to the exclusion of the softmax readout, which is technically an additional layer, though not involved in the relaxation process. ", "page_idx": 34}, {"type": "text", "text": "Architecture. All convolutional layers used in experiments are of kernel size 3 and stride and padding 1. Max-pooling was applied with a window of $2\\times2$ and stride of 2. For the 6-layer model used in Table 1 , batchnorm was applied after the first layer convolution and pooling operation. All other models in both experiments use batch-normalization on the first layer of each block after convolution and pooling (where applied). These details exclude the linear softmax readout of size $n$ classes. ", "page_idx": 34}, {"type": "text", "text": "Hyperparameters. Detailed hyperparameters for to reproduce Table 1 and Table 2 are given inside the configuration files of our repository. Note that all architectural details for the 12-layer models are identical across splitting and scaling experiments. Additionally, identical hyperparameters were used for CIFAR100 and Imagenet experiments of Table 2. Unlike previous literature, the use of GOE intialization eliminates the need for separate layerwise learning rates and initialization parameters. One noteworthy detail is that only 100 epochs were used for the larger model for Table 2 compared with 200 epochs for the smaller 12-layer model. This was due to prohibitively long run-time of training the larger model. Noteably, performance still significantly improves with decreased overall runtime. ", "page_idx": 34}, {"type": "text", "text": "Root-finding algorithms. While in principle any root-finding algorithm may be used for the two relaxation phases of our EP implementation (for inference and gradient computation), our implementation utilized a simple fixed-point iteration procedure, in which neuron states are initialized as zero vectors with values updated asynchronously on each iteration to that of the gradient of the total system energy with respect to current state. An approximate illustration of this procedure is found in Alg. 3. As indicated in Section 4.3, two variants of the convergence procedure were employed, one in which the average value of current state is compared to that of the previous state for each layer, and relaxation is truncated when values for all layers have a difference of less than 1e-4. This was known as the tolerance-based (TOL) procedure. Notably, tolerance-based convergence criteria were applied on the free phase only, with nudging computed with a fixed value of iterations. This was to ensure consistency between ID and EP, though in practice a tolerance can be applied equally to the nudging phase. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "image", "img_path": "bMTn8KKrbq/tmp/452083a24300d12d4c5db7e3c575c41f5679b963eaf7aa593323ca5b9206d303.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Supplementary results with a fixed number of iterations. In addition to the TOL-based procedure, we obtained results for 4.3 using the more conventional approach of [Scellier and Bengio, 2019][Laborieux et al., 2021][Ernoult et al., 2019], applying fixed number of iterations on the first and second relaxation phases (see 1). This approach was also the default used for our scaling experiments in 4.4. Importantly, with the TOL procedure described above Alg 3 becomes Alg 13. Results using a fixed iteration root-finding scheme are shown in 5 ", "page_idx": 35}, {"type": "text", "text": "Table 5: Validation accuracy and Wall Clock Time (WCT) obtained on CIFAR-10 by EP (Alg. 2) and ID on models with different number of layers $(L)$ and block sizes (\u201cbs\u201d). 3 seeds are used. ", "page_idx": 35}, {"type": "table", "img_path": "bMTn8KKrbq/tmp/e34c9f02a076291ecb82939c49bd80b2c50372dcf14359a0e3474d876a1034b0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Other details. All experiments were run using Adam optimizer [Kingma and Ba, 2014]and Cosine Annealing scheduler[Loshchilov and Hutter, 2017], specifying some minimum learning rates and setting maximum T equal to epochs (i.e. no warm restarts). Code was implemented in Pytorch 2.0 and all simulations were run on NVIDIA A100 SXM4 40GB GPUs. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: we have a dedicated paragraph in the \u201cDiscussion\u201d section of the paper which explicitly mentions limitations and future work. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All proofs are included in the appendix. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: detailed information to reproduce experiments along with configuration files are provided on our github repository. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The URL to our github repository is provided in the main. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: all these details are provided in the appendix. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: we perform each of our training simulations on 3 different seeds and reported mean and standard deviation of the resulting performance. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: this information is also inside our appendix. ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: we wrote a dedicated paragraph inside our \u201cDiscussion\u201d section. ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work poses no such risk at present as it only provides proof-of-concepts for systems which do not yet exist. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: this work does not use existing assets. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: this paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}]