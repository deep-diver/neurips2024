{"references": [{"fullname_first_author": "Samuel J Gershman", "paper_title": "Learning latent structure: carving nature at its joints", "publication_date": "2010-04-01", "reason": "This paper is foundational to the study of how animals and humans leverage change to discover latent structure, a key concept in the current work."}, {"fullname_first_author": "Andrew M. Saxe", "paper_title": "Exact solutions to the nonlinear dynamics of learning in deep linear networks", "publication_date": "2013-12-01", "reason": "This paper provides the theoretical foundation for understanding the learning dynamics of linear networks, which is essential to the analytical approach in the current study."}, {"fullname_first_author": "Andrew M. Saxe", "paper_title": "A mathematical theory of semantic development in deep neural networks", "publication_date": "2019-06-18", "reason": "This study provides a theoretical framework for understanding how abstract representations emerge in neural networks, directly related to the investigation of task abstraction in this work."}, {"fullname_first_author": "Timo Flesch", "paper_title": "Comparing continual task learning in minds and machines", "publication_date": "2018-10-23", "reason": "This paper highlights the challenges of continual learning for artificial neural networks, motivating the need for task abstraction mechanisms explored in the current study."}, {"fullname_first_author": "Michael McCloskey", "paper_title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem", "publication_date": "1989-01-01", "reason": "This seminal paper introduces the concept of catastrophic forgetting in neural networks, a major issue addressed by the task abstraction method proposed in this study."}]}