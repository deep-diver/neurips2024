[{"figure_path": "qvdc0oCX2n/tables/tables_7_1.jpg", "caption": "Table 2: Results on DataComp-medium from methods that use only OpenAI's CLIP-L/14 model, i.e., all methods are from the D1 category. The \"dataset size\" represents the size of the subset obtained from different approaches. NormSim(IN-1k) denotes using the training data of ImageNet-1k as the target while NormSim(Target) represents using that of all 24 available downstream tasks. NormSim-D refers to the methods that use an iteratively selected subset from the training set as the target proxy.", "description": "This table presents the results of data selection methods that exclusively utilize OpenAI's CLIP-L/14 model for filtering, categorized under the D1 category in the paper.  The table compares various filtering strategies using metrics such as CLIPScore, negCLIPLoss, and NormSim, showcasing their impact on downstream task performance across different datasets (ImageNet-1k, IN Dist. Shift, VTAB, Retrieval) with varying subset sizes.  NormSim is further broken down by its target data source (ImageNet-1k training data or all 24 downstream tasks), and a dynamic version (NormSim-D) using an iteratively refined subset of training data is also evaluated.", "section": "4.3 Main Results and Discussions"}, {"figure_path": "qvdc0oCX2n/tables/tables_7_2.jpg", "caption": "Table 1: Results on DataComp-medium from the top methods that use only OpenAI's CLIP-B/32 model or public version of DFN (DFN-P). \"NormSimB/32\" represents using OAI CLIP-B/32 to calculate NormSim\u221e.", "description": "This table presents the results on the DataComp-medium benchmark for various data selection methods.  It focuses on comparing the performance of different strategies using only OpenAI's CLIP-B/32 model or the publicly available DFN-P model.  The table shows the dataset size used, ImageNet-1k accuracy, VTAB average score, and the overall average across all 38 downstream tasks in the benchmark.  The results highlight the impact of different data selection techniques on model performance in the context of multimodal contrastive learning.", "section": "4.3 Main Results and Discussions"}, {"figure_path": "qvdc0oCX2n/tables/tables_8_1.jpg", "caption": "Table 3: Results of all D1&D2&D3 top methods on DataComp-medium. The results of MLM [42] are from their paper, while all other baselines are reproduced on our downloaded dataset using their official UIDs. \"Ours (20%)\" refers to use \u201cnegCLIPLoss (30%) \u2229 NormSim\u221e(Target)\" to get 20% of original data, while \u201cOurs (10%)\u201d denotes applying \"negCLIPLoss (20%) \u2229 NormSim\u221e(Target)\" to get 10%. And we use \"\" to indicate the case where we choose the intersection of the data selected by using OAI CLIP-B/32 and OAI CLIP-L/14 separately, which results in about 15M data for \"Ours (20%)*\" and 7.4M data for \"Ours (10%)*\"", "description": "This table presents a comparison of the top-performing data selection methods from categories D1, D2, and D3 on the DataComp-medium benchmark.  It shows the performance (ImageNet-1k, IN Dist. Shift, VTAB, Retrieval, and average across 38 downstream tasks) achieved by each method.  The methods are categorized based on the resources used: D1 uses only the original OpenAI CLIP model, D2 trains a new CLIP model using external data, and D3 uses external non-CLIP models to aid in data selection.  The table also shows that the proposed methods (Ours) outperform most of the D3 methods and can boost the performance of the SOTA method by combining them.", "section": "4.3 Main Results and Discussions"}, {"figure_path": "qvdc0oCX2n/tables/tables_9_1.jpg", "caption": "Table 4: Results of the top methods on the full DataComp-medium dataset (128M data).", "description": "This table presents the ImageNet-1k and average performance across 38 downstream tasks for various data selection methods on the full DataComp-medium dataset (128 million image-text pairs).  It compares the performance of different methods, including those that only use OpenAI's CLIP model, those that train new CLIP models, and those that use external models or datasets. Notably, it shows that combining the proposed negCLIPLoss and NormSim methods with existing state-of-the-art methods (DFN and HYPE) leads to further improvements in performance.", "section": "4 Experimental Results"}, {"figure_path": "qvdc0oCX2n/tables/tables_22_1.jpg", "caption": "Table 5: Comparison of preprocessing time and external resources needed between our method and other D3 category methods. We skip DFN since it's orthogonal to our negCLIPLoss method and we can directly improve it as mentioned in Table 1. Here since all the baselines below except MLM use a pretrained CLIP model, we only count the time that doesn't contain that for inferring CLIP image/text embeddings (about 50 L40 hours for OAI CLIP-B/32), which is also adopted in DataComp benchmark [1]. The external dataset corresponds to the external multimodal dataset used for training or finetuning the external model. Notably, the preprocessing time for the following methods are all approximately linearly proportional to the amount of unfiltered pretrained dataset.", "description": "This table compares the preprocessing time and external resources required by different data selection methods. It highlights that the proposed method (negCLIPLoss) significantly reduces the preprocessing time compared to other methods that utilize external models or datasets.  The table emphasizes the computational efficiency of the proposed approach.", "section": "Details of Experiments"}, {"figure_path": "qvdc0oCX2n/tables/tables_23_1.jpg", "caption": "Table 7: Results on DataComp-medium from the top methods that use only OpenAI's CLIP-B/32 model or public version of DFN (DFN-P).", "description": "This table compares different data selection methods using two different CLIP models (OAI CLIP-B/32 and DFN-P). It shows the performance on ImageNet-1k and several downstream tasks (IN Dist. Shift, VTAB, Retrieval) using different filtering strategies, including CLIPScore and negCLIPLoss, both alone and in combination with NormSim(Target).  It highlights the impact of negCLIPLoss in improving the performance compared to the baseline CLIPScore across various evaluation metrics and dataset sizes. The table also demonstrates the compatibility of negCLIPLoss with other advanced models (DFN-P).", "section": "4.3 Main Results and Discussions"}, {"figure_path": "qvdc0oCX2n/tables/tables_27_1.jpg", "caption": "Table 2: Results on DataComp-medium from methods that use only OpenAI's CLIP-L/14 model, i.e., all methods are from the D1 category. The \"dataset size\" represents the size of the subset obtained from different approaches. NormSim(IN-1k) denotes using the training data of ImageNet-1k as the target while NormSim(Target) represents using that of all 24 available downstream tasks. NormSim-D refers to the methods that use an iteratively selected subset from the training set as the target proxy.", "description": "This table presents the results of data selection methods that exclusively utilize OpenAI's CLIP-L/14 model, categorized under the D1 category. It compares different strategies using various metrics such as CLIPScore, negCLIPLoss, and NormSim, combined with different target datasets and subset selection techniques. The table shows the performance across multiple downstream tasks, including ImageNet-1k, IN Dist. Shift, VTAB, and Retrieval, with the average performance across 38 tasks.", "section": "4.3 Main Results and Discussions"}, {"figure_path": "qvdc0oCX2n/tables/tables_28_1.jpg", "caption": "Table 8: Comparison between NormSim and nearest neighbor selection. We use OAI CLIP-L/14 as the teacher model and assume both methods have been intersected with negCLIPLoss (30%). The size of the selected subset is 22M.", "description": "This table compares the performance of NormSim and nearest neighbor selection methods on the downstream tasks (IN-1k and VTAB). Both methods were used in conjunction with negCLIPLoss (30%).  The selected subset size is 22M for both approaches. The table shows that NormSim\u221e (Target) outperforms both negCLIPLoss(30%) and Nearest Neighbor Selection on both ImageNet-1k and VTAB tasks, and achieves the highest average performance.", "section": "4.3 Main Results and Discussions"}, {"figure_path": "qvdc0oCX2n/tables/tables_28_2.jpg", "caption": "Table 9: Ablation Study on the NormSim and its variants on DataComp-small (11M). All experiments first select 45% data based on the CLIP score, then use corresponding approaches to obtain 3.3M data. \u201cimage\u201d or \u201ctext\u201d means using the variance of image or text embeddings to represent \u03a3target, and \u201cimage \u00d7 text\u201d means representing \u03a3target with the cross-covariance of image and text embeddings.", "description": "This table presents the ablation study of NormSim and its variants on the DataComp-small dataset.  It compares the performance of using image-only, text-only, and image-text embeddings to represent the target data distribution when calculating NormSim.  The results show the impact of using different embedding types on downstream task performance. The experiment setup involves initial data selection based on CLIP score (45%) and further filtering based on different NormSim variants to a final subset of 3.3M data points.", "section": "D.3 NormSim\u221e is Better than Nearest Neighbor Selection"}]