[{"type": "text", "text": "CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiping Wang\u2217 Yifang Chen\u2217 Wendan Yan   \nUniversity of Washington University of Washington University of Washington Alex Fang Wenjing Zhou Kevin Jamieson   \nUniversity of Washington University of Michigan University of Washington ", "page_idx": 0}, {"type": "text", "text": "Simon Shaolei Du University of Washington ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data selection has emerged as a core issue for large-scale visual-language model pretraining (e.g., CLIP), particularly with noisy web-curated datasets. Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric). While the first two approaches have been extensively studied, the third remains under-explored. In this paper, we advance the third approach by proposing two new methods. Firstly, instead of classical CLIP scores that only consider the alignment between two modalities from a single sample, we introduce negCLIPLoss, a method inspired by CLIP training loss that adds the alignment between one sample and its contrastive pairs as an extra normalization term to CLIPScore for better quality measurement. Secondly, when downstream tasks are known, we propose a new norm-based metric, NormSim, to measure the similarity between pretraining data and target data. We test our methods on the data selection benchmark, DataComp [1]. Compared to the best baseline using only OpenAI\u2019s CLIP-L/14, our methods achieve a $5.3\\%$ improvement on ImageNet-1k and a $2.8\\%$ improvement on 38 downstream evaluation tasks. Moreover, both negCLIPLoss and NormSim are compatible with existing techniques. By combining our methods with the current best methods DFN [2] and HYPE [3], we can boost average performance on downstream tasks by $0.9\\%$ , achieving a new state-of-the-art on the DataComp-medium benchmark2. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Curating large-scale visual-language datasets from web-sourced data has become common for pretraining multi-modal models. However, the quality of these web-curated data pairs remains a critical bottleneck. Research has shown that the choice of dataset significantly impacts model performance, irrespective of the models and training techniques employed [4\u201311], and this motivates the development of various data selection strategies. This paper focuses on optimizing subset selection from a fixed data pool to train a CLIP model [4] that achieves superior performance on zero-shot downstream tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Classical methods rely solely on OpenAI\u2019s (OAI) pretrained CLIP model (i.e., a teacher model) and focus on better utilizing the embeddings. The most commonly used one is calculating CLIPScore, which measures the cosine similarity between the visual and language embeddings of the CLIP model for the same sample, to eliminate low-quality data with mismatches between text and image. Other works also leverage heuristic distribution alignment techniques to select samples relevant to downstream tasks, such as image-based filtering [1]. These approaches are generally viewed as providing only limited enhancements. However, we argue that the potential of those embeddings has been heavily under-explored. This work seeks a universal method to better employ any given embeddings, not only from OAI CLIP, but also from other CLIP-style models. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, recent leading data flitering methods, instead of focusing on improving embedding utilization stategy itself, mainly follow the other two directions, both employing external resources. They either (1) use external non-CLIP models that aid in data selection, (2) or use external highquality multi-modal data to train a better CLIP-style embedding model than the original OAI CLIP to filter out low-quality data. Specifically, in the first line of works, HYPE [3] leverages embeddings from hyperbolic models instead of the classical Euclidean-based CLIP to measure how each data point has semantically overlaps with other data points and fliters out data with low specificity. T-MARS [12] removes images where the text is the only feature correlated with the caption using FAST [13], an off-the-shelf OCR text detection model. Devil [14] applies fasttext [15] to remove non-English texts and use BLIP-2 [16] model for digit recognition to keep useful images with digits. The second direction, represented by Data Filtering Network (DFN) [2], involves training a new CLIP-style teacher model that uses high-quality datasets like HQITP-350M. Although the embeddings extracted from this model perform worse than the OAI CLIP in downstream tasks, it is particularly good at flitering out low-quality data. Notably, some of these methods can be combined and indeed, merging the selected data from DFN and HYPE achieves current state-of-art as shown in HYPE [3]. ", "page_idx": 1}, {"type": "text", "text": "Previous works mainly focus on improving the CLIP embedding quality or utilizing an external model to do flitering but employ the CLIP embedding in a suboptimal way by only using classical methods like CLIPScore. In contrast, in this work, we focus on improving the filtering methods themselves for any given CLIP embedding. We show that there are universal and more effective strategies for utilizing any CLIP teacher model, regardless of its architecture (e.g., B/32 or L/14) or the dataset it was trained on (e.g., OpenAI-WIT-400M or DFN\u2019s high-quality dataset). These strategies should always be orthogonal to the use of any newly trained CLIP-style models like DFN and might also be compatible with methods using external models like FAST and BLIP-2. ", "page_idx": 1}, {"type": "text", "text": "Our Contributions. We propose an alternative to CLIPScores that we call negCLIPLoss that more accurately characterizes data quality. We also introduce a new distribution metric we call the p-Norm Similarity Score (NormSim) when knowledge about downstream tasks is available. Two major observations directly inform our proposals: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Firstly, we observe that classical methods measure the quality of a multi-modal sample by computing the cosine similarity between its visual and language embeddings, believing that lower similarity indicates that the text does not match its image part well. However, we find that some less informative samples may have a systematic bias, which leads to higher CLIPScores. For example, the language part containing the word \"image\" can result in higher similarity with any visual part, even when the text does not accurately describe its image content. Our proposed method negCLIPLoss, inspired by the standard CLIPLoss, normalizes the original CLIPScore by the similarity between a sample and its contrastive pairs. For example, the high score caused by the word \"image\" is typically consistent across its contrastive pairs, so our adjustment reduces this bias. As we have highlighted, such replacement can be universally applied across different embedding models. See Fig. 2 for illustrations. \u2022 Secondly, if one has access to examples drawn from the same distribution as the target task, it is natural to assume that this extra knowledge could be leveraged to inform the data filtering process. We propose the NormSim metric to measure the vision similarity between a training sample $x$ and the target task dataset $X_{\\mathrm{target}}^{v}\\ \\in\\ \\mathbb{R}^{n\\times D}$ defined as $\\|f_{v}(X_{\\mathrm{target}}^{v^{\\cdot}})f_{v}(x^{v})\\|_{p}$ , where $f_{v}:\\mathbb{R}^{D}\\to\\mathbb{R}^{d}$ is the vision encoder of teacher model so that $f_{v}(X_{\\mathrm{target}}^{v})\\in\\mathbb{R}^{n\\times d}$ , $f_{v}(x^{v})\\in\\mathbb R^{d}$ , and $f_{v}(X_{\\mathrm{target}}^{v})f_{v}(x^{v})\\in\\mathbb{R}^{n}$ , and $\\|\\cdot\\|_{p}$ is the $p$ norm; effective choices are $p=2$ or $\\infty$ . Notably, unlike previous ImagetNet-based filtering [1], which tries to keep the training set as diverse as downstream tasks by clustering the training set and finding the nearest neighbor group for every target sample, our method does not explicitly consider the diversity but select examples as long as it is close to any target sample (i.e. select high NormSim score). Notably, negCLIPLoss and NormSim enjoy complementary effect in data selection. See Fig. 3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To illustrate the effectiveness of our methods, we use a widely used benchmark DataComp [1] as our primary method of evaluating the datasets created by our data filtering methods. We show that, by simply replacing the CLIPScores with negCLIPLoss and utilizing NormSim we are able to exceed the best OAI-CLIP(L/14)-based baseline by $5.3\\%$ on ImageNet-1k and $2.8\\%$ on average across 38 downstream tasks, which is similar or even better than the performance achieved by many external-resources-based methods. Notably, even if the target downstream tasks are not available, using NormSim on a proxy downstream task constructed from the training set, called $\\mathbf{NormSim_{\\mathrm{2}}-I}$ D, combined with negCLIPLoss, can also gain a $1.9\\%$ improvement on 38 downstream evaluation. ", "page_idx": 2}, {"type": "text", "text": "Moreover, the improvements achieved by our methods are not limited to OAI CLIP-based methods but can also be obtained by combining our methods with advanced models that require external resources. By merging the subset selected by negCLIPLoss and NormSim with the subset selected by current state-of-the-art method ${}^{\\prime}\\!H Y P E\\cup D F N^{\\prime}$ , we can further improve it by $O.9\\%$ on both ImageNet-1k and on average 38 downstream tasks. Besides, we can also achieve a $\\it{0.8\\%}$ improvement on average 38 tasks over $^{\\prime\\prime}H Y P E\\cup D F N^{\\prime\\prime}$ using only the data selected by DFN and our strategies. More importantly, we demonstrate that negCLIPLoss, as a replacement for CLIPScore, can be applied to any other embedding models like OAI-L/14, OAI-B/32, and DFN-B/32, universally boosting performance from $0.4\\%$ to $3.0\\%$ on an average of 38 tasks. This result is not only technically insightful for understanding the information available in embeddings but also practically significant. Compared to existing methods, our approach saves a significant amount of computational time on both reprocessing and new embedding retraining as shown in Table 5. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data Filtering on Multimodal Dataset. We are given a training dataset $D_{\\mathrm{train}}=\\{x^{v},x^{l}\\}$ , where $(x^{v},x^{l})\\ \\in\\ \\mathbb R^{D}$ is the image-text (vision-language) training pair. For convenience, we will let superscript $v l$ denote either modality so that, for example, $x^{v l}\\in x^{v},x^{l}$ . Our goal is to identify a subset $S\\subset D_{\\mathrm{train}}$ that maximizes the zero-shot accuracy of the CLIP model on some downstream tasks when $S$ is used to train the CLIP model. ", "page_idx": 2}, {"type": "text", "text": "CLIP score and embedding. Recent efforts, such as LAION [5] and DataComp [1], use OpenAI\u2019s CLIP ViT-L/14 model [4] as a teacher model to obtain quality score. Here we denote this vanilla CLIP model as $\\bar{f_{v l}}$ . For any pair $x^{v l}$ , the model outputs a normalized unit-vector ${\\bar{f}}_{v l}(x^{v l})$ . If $X^{v l}~:=~\\{x_{1}^{v l},\\bar{\\dots},x_{m}^{v l}\\}$ denotes a dataset containing $m$ samples, then we define $\\bar{f}_{v l}(X^{v l})\\,=\\,[\\bar{f}_{v l}(x_{1}^{v l}),\\ldots,\\bar{f}_{v l}(x_{m}^{v l})]^{\\top}\\,\\in\\,\\mathbb{R}^{m\\times d}$ as the embedding matrix. The popular filtering metric \u201cCLIPScore\u201d is defined as $\\langle\\bar{f}_{v}(x^{v}),\\bar{f}_{l}(x^{l})\\rangle\\in[-1,1]$ . ", "page_idx": 2}, {"type": "text", "text": "Dataset and model. Here we follow the pipeline of Datacomp [1] to standardize the training and evaluation process. This is a testbed for dataset experiments aiming to open-source and further improve the vanilla CLIP model and is widely adopted in previous data selection papers [17, 18, 12, 2, 19, 7]. We will give more details in Sec. 4. ", "page_idx": 2}, {"type": "text", "text": "3 Data Filtering Strategy ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 negCLIPLoss: A Better Metric than CLIPScore ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce a better and statistically interpretable quality metric called negCLIPLoss, which directly replaces the common metric CLIPScore. Fig. 1 illustrates how negCLIPLoss works. This new metric only requires negligible extra computational costs and no additional external data collection costs. As the name suggested, this metric is inspired by the standard CLIP loss used in the actual training process of the teacher CLIP model, which is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell_{B^{*}}(x_{i}^{v l})=-\\frac{1}{2}\\left[\\log\\frac{\\exp(\\bar{f}_{v}(x_{i}^{v})^{\\top}\\bar{f}_{l}(x_{i}^{l})/\\tau)}{\\sum_{j\\in B^{*}}\\exp(\\bar{f}_{v}(x_{i}^{v})^{\\top}\\bar{f}_{l}(x_{j}^{l})/\\tau)}+\\log\\frac{\\exp(\\bar{f}_{v}(x_{i}^{v})^{\\top}\\bar{f}_{l}(x_{i}^{l}))/\\tau}{\\sum_{j\\in B^{*}}\\exp(\\bar{f}_{v}(x_{j}^{v})^{\\top}\\bar{f}_{l}(x_{i}^{l})/\\tau)}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here $B^{*}$ is the random batch where $i$ -th sample belongs during a particular training step, and $\\tau$ is the learnable temperate parameter. Notably, the teacher loss differs from CLIPScore primarily by a ", "page_idx": 2}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/9b0e072ee093eac218124bdb514f6f878f6380aff9cecea51cd34b52dbc904e5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term $\\mathcal{R}$ . negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, \u201cTop $\\mathrm{{X}\\%}'$ denotes that the score represents the top $X\\%$ high values within the entire dataset (i.e., the $(100{-}\\mathrm{X})\\%$ percentile among all the values). For example, $\\mathbf{\\mathcal{R}}:\\mathrm{Top~100\\%}^{,}$ means this data has almost the smallest $\\mathcal{R}$ among the whole dataset, which represents that it contains highly specific elements in both images and texts. ", "page_idx": 3}, {"type": "text", "text": "normalization term $\\mathcal{R}^{*}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\tau\\cdot\\ell_{B^{*}}(x_{i}^{v l})=\\underbrace{\\bar{f}_{v}(x_{i}^{v})^{\\top}\\bar{f}_{l}(x_{i}^{l})}_{\\mathrm{CLIPScore}}-\\underbrace{\\frac{\\tau}{2}\\left[\\log\\sum_{j\\in B^{*}}\\exp(\\frac{\\bar{f}_{v}(x_{i}^{v})^{\\top}\\bar{f}_{l}(x_{j}^{l})}{\\tau})+\\log\\sum_{j\\in B^{*}}\\exp(\\frac{\\bar{f}_{v}(x_{j}^{v})^{\\top}\\bar{f}_{l}(x_{i}^{l})}{\\tau})\\right]}_{\\mathrm{CLIPScore}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In practice, since the training dataset of teacher CLIP models, like OAI-WIT400M [4], and the actual batch divisions $B^{*}$ is inaccessible, we randomly select $K$ batches from the student model\u2019s training data and use the averaged results from $\\{B_{k}\\}_{i=1}^{K}$ to estimate the normalization term $\\mathcal{R}^{*}$ on $B^{*}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{negCLIPLoss}(x_{i}^{v l}):=-\\frac{\\tau}{K}\\sum_{k=1}^{K}\\ell_{B_{k}}(x_{i}^{v l})\\;\\approx\\;\\mathrm{CLIPScore}(x_{i}^{v l})-\\mathcal{R}^{*}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\{B_{k}\\}_{i=1}^{K}$ are some batches randomly selected from the student model\u2019s training data and $x_{i}\\in B_{k},\\forall k$ . We choose $K=10$ in our experiments, but any sample size larger than 5 is sufficiently stable for estimating the original CLIPLoss (Details in Appendix D.1). Besides, in Sec. 4.3.3 we also show that the computational cost introduced by $\\mathcal{R}$ remains negligible compared to other baselines. The temperature $\\tau$ and batch size $|B^{*}|$ can be directly obtained from the parameters of the pretrained teacher model. More details of negCLIPLoss are in Appendix, including the concentration analysis of $\\mathcal{R}$ (Appendix A.1), pseudocode (Algorithm 1), and the ablation study of $\\tau$ and $|B|$ (Appendix C.2). ", "page_idx": 3}, {"type": "text", "text": "Motivation behind negCLIPLoss. Other existing works also use loss-guided data selection, such as LESS [20] in NLP, CoDis [21] in CV, and RHO [22] in general data scheduling scenarios. However, it is still unclear whether selecting based on teacher loss is suitable for multi-modal contrastive learning. Here we give an affirmative answer as shown in Fig. 2, where we can see negCLIPLoss performs better than or on par with CLIPScore consistently. ", "page_idx": 3}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/b973c107f53fd5135875f3993b35e20d44652c8f546b42a53259b93075947f54.jpg", "img_caption": ["Figure 2: Comparison of negCLIPLoss and CLIPScore across different downsampling ratios on DataComp-medium. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "To illustrate how teacher loss helps our selection, we demonstrate that the normalization term provided by negCLIPLoss is crucial for correcting the overestimation or underestimation inherent in CLIPScore. A high normalization term implies that either the image embedding, text embedding, or both can easily match multiple contrastive pairs beyond their ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/2dfcc6a2c3db0ce251f63fbaed1cd36cf886d5547a69d03708616fbf9af479dc.jpg", "img_caption": ["(a) Data Type for Different negCLIPLoss and \ud835\udc0d\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc12\ud835\udc22\ud835\udc26\ud835\udfd0 "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Illustration of NormSim on DataComp. $X_{\\mathrm{target}}$ is the target prior data. \u201cTop $\\mathrm{{X}\\%}'$ \u201d denotes that the score represents the top $X\\%$ high values within the entire dataset. (a) Visualization of data with different NormSim and negCLIPLoss. Here we use $\\mathrm{{NormSim}_{2}}$ (ImageNet-1k) as an example. Although both Type 2 and Type 4 data have high negCLIPLoss and thus high quality, data with low $\\mathrm{{NormSim}_{2}}$ (Type 4) are more irrelevant to downstream tasks like ImageNet, VTAB, and MSCOCO. For example, they contain many images dominated by OCR content and make little contribution to improving downstream performance. (b) Illustration of a rough comparison of sampling data for different filtering methods. Using \u201cnegCLIPLoss $\\cap{\\mathrm{{NormSim}^{*}}}$ filtering can balance the quality and relevance to downstream tasks, thus increasing the proportion of Type 2 data. (Refer to Appendix E for more visualization.) ", "page_idx": 4}, {"type": "text", "text": "corresponding counterparts. For example, in the bottom right of Fig. 1, the text containing \u201cImage\u201d or \u201cPhoto\u201d can be easily matched with any visual content. Similarly, the image of \u201cverloopring\u201d only contains very simple features and can be matched with many words like \u201cwhite\u201d, \u201cempty\u201d or \u201ccircle\u201d, etc. Consequently, despite a high absolute CLIPScore, the relative negCLIPLoss within its batch can be lower. In contrast, the bottom left features highly specific elements in both text and images, such as \"Islands Harbor,\" \"American football\", and \"sheep at green\". These elements are specific and less likely to match with contrastive pairs, resulting in a higher relative negCLIPLoss. ", "page_idx": 4}, {"type": "text", "text": "3.2 NormSim: A New Training-Target Similarity Metric ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our proposed negCLIPLoss is a universal approach to improve filtering performance by estimating quality better, and it does not rely on any downstream task. Now, if we can access some knowledge of the downstream tasks, we could further improve the performance by using a vision-only $p$ -norm similarity to target data metric to measure the relationship between each training sample and the downstream target data. We will discuss the reason to use vision-only embedding later in this section. ", "page_idx": 4}, {"type": "text", "text": "Specifically, we assume access to the target set of downstream tasks and denote them as $X_{\\mathrm{target}}=$ $\\{x_{\\mathrm{target},(1)},\\ldots,x_{\\mathrm{target},(m)}\\}$ , where each $x_{\\mathrm{target},(i)}\\in\\mathbb{R}^{d}$ is i.i.d.-sampled from the target downstream distribution $\\ensuremath{\\mathcal{P}}_{\\mathrm{target}}{}^{3}$ , but without overlapping with the test set. Then, for each training sample $x^{v l}$ and the corresponding target set $X_{\\mathrm{target}}$ , the NormSim is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{NormSim}_{p}(X_{\\mathrm{target}},x):=\\|\\bar{f}_{v}(X_{\\mathrm{target}}^{v})\\bar{f}_{v}(x^{v})\\|_{p}=\\left(\\sum_{x_{t}\\in X_{\\mathrm{target}}}|\\langle\\bar{f}_{v}(x_{t}^{v}),\\bar{f}_{v}(x^{v})\\rangle|^{p}\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We select the subset $S$ by choosing the samples with top- $\\mathcal{N}$ highest NormSim scores. The choice of the norm type $p$ can be based on the data distribution and training process. In this paper, we consider two instantiations of $p$ : ", "page_idx": 4}, {"type": "text", "text": "When $p=2$ , our data selection method can be regarded as the following equation. It\u2019s equivalent to selecting a subset that aligns with the principal components of the target set variance (Appendix C.6.1). ", "page_idx": 4}, {"type": "equation", "text": "$$\nS=\\arg\\operatorname*{max}_{|S|=N}\\sum_{i\\in S}\\mathrm{NormSim}_{2}(x_{t},x_{i}),\\quad\\mathrm{NormSim}_{2}(x_{t},x_{i})=\\left(\\sum_{x_{t}\\in X_{\\mathrm{taget}}}\\left|\\bar{f}_{v}(x_{t}^{v})^{\\top}\\bar{f}_{v}(x^{v})\\right|^{2}\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When $p=\\infty$ , the distance metric can be regarded as an even more optimistic measure, such that a training sample will be selected if it has high similarity to any target sample. Note that this is different from nearest-neighbor-based method used in image-based flitering [1], where they are trying to find the nearest training sample of every target sample. In this case, it can be regarded as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS=\\arg\\operatorname*{max}_{|S|=N}\\sum_{i\\in S}\\mathrm{NormSim}_{\\infty}(x_{t},x_{i}),\\qquad\\mathrm{NormSim}_{\\infty}(x_{t},x_{i})=\\operatorname*{max}_{x_{t}\\in X_{\\mathrm{target}}}\\bar{f}_{v}(x_{t}^{v})^{\\top}\\bar{f}_{v}(x_{i}^{v})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix D.3, we also show that our $\\mathrm{NormSim}_{\\infty}$ can outperform the nearest neighbor selection on the downstream target tasks. Here, we show an example selected via the $\\mathrm{NormSim_{2}}$ (ImageNet-1k) in Fig. 3, showing that this vision-target-aware method is complementary to the quality-based one. ", "page_idx": 5}, {"type": "text", "text": "Choice of Target Data. In the experiment parts, we try two kinds of target data: training data from ImageNet-1k (1.3M) or training data from all 24 accessible downstream tasks $(2.1\\mathsf{M})^{4}$ . We denote them as $\\mathbf{NormSim}_{p}(\\mathbf{IN-1k})$ and $\\mathbf{NormSim}_{p}$ (Target), respectively. ", "page_idx": 5}, {"type": "text", "text": "Necessity of using vision-only information We use only the visual information $x^{v}$ instead of multimodal information $x^{v l}$ for measuring similarity. This is because common crawled text often has brief captions, making the OAI CLIP language embedding weaker than its visual embedding model [1, 23\u201325]. Consequently, the language part cannot characterize the pre-training and downstream task distribution as well as the visual part. This phenomenon is also observed in Gadre et al. [1], where image-based filtering (select data whose image embeddings are similar to that from ImageNet-1k) outperforms text-based filtering (select data whose captions contain words from ImageNet-21k). More ablation studies are provided in Appendix D.4. ", "page_idx": 5}, {"type": "text", "text": "Generality of NormSim in choosing teacher model. Notably, since we just use image embeddings in the NormSim metric, we believe it unnecessary to use CLIP model to obtain NormSim. NormSim can be a general metric for selecting target-related image/image-text data if any good image representations are given, like the representations obtained from pretrained ResNet-50. ", "page_idx": 5}, {"type": "text", "text": "Theoretical justification. Unlike many existing methods that force diversity by selecting training samples around each $\\mathbf{\\mathcal{x}}_{\\mathrm{target}}$ , our strategy maximizes similarity without directly considering data diversity. For the $p\\,=\\,2$ case, we demonstrate that maximizing $\\mathrm{NormSim_{2}}$ is optimal under a linear model $\\bar{f}_{v}$ , as shown in Appendix A.2. Our theorem also provides error guarantees for noisy embeddings and explains when vision-only embeddings outperform combined vision and language embeddings. Recent work by Joshi et al. [26] provides a similar analysis but focuses on high-quality data and cross-variance between images and texts. This approach is less effective than image-only methods for filtering noisy datasets, as discussed above. ", "page_idx": 5}, {"type": "text", "text": "Using proxy when downstream $X_{\\mathbf{target}}$ is inaccessible. Surprisingly, we show that the 2-norm can also be used when only the pre-training set is available. In this case, we construct a proxy \u201ctarget\u201d set from the pre-training set itself. Specifically, let $S_{i}$ be the selected subset at step $i$ , then we treat the current $S_{i}$ as the proxy \u201ctarget\u201d set. To construct the next smaller set, we select the next data batch Si+1 satisfying arg maxSi+1\u2282Si x\u2208S $\\begin{array}{r}{\\operatorname*{max}_{S_{i+1}\\subset S_{i}}\\sum_{x\\in S}\\mathrm{{NormSim}}_{2}(S_{i},x)}\\end{array}$ , until reaching an N size subset. We call this approach $\\mathbf{NormSim_{2}}$ -D (Dynamic) and will specify the algorithm details in Appendix C.3. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the performance of negCLIPLoss and NormSim, aiming to address the following questions: Q1: Given a fixed CLIP teacher model, can our methods more effectively utilize CLIP embeddings for data flitering? Q2: Are our methods applicable to diverse CLIP teacher models with varying architectures or different pretrained datasets? Q3: How does our method compare to other leading approaches that utilize external models or multimodal datasets? Additionally, could our method be compatible with these methods and enhance their effectiveness? ", "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We adhere to the standardized training and evaluation protocols of the DataComp benchmark [1]. Training configuration. We employ the medium-scale training configuration of DataComp (DataComp-medium). It provides a substantial dataset comprising 128 million low-quality, webcurated image-text pairs to be flitered. Once the data subset is obtained by some data flitering strategy, it will be used to train a fixed CLIP-B/32 model in a fixed training budget that allows the model to pass 128 million data points an epoch. Therefore, smaller subsets will be repeated more frequently, ensuring a fair comparison. We note that the size of the DataComp dataset becomes smaller over time since some URLs of images become invalid5, and we only successfully downloaded about 110M data. Therefore, the results of baselines on the leaderboard do not apply to our datasets, and we reproduce all the top baselines on the leaderboard with their public UIDs of the selected data. ", "page_idx": 6}, {"type": "text", "text": "Evaluation. We measured the model performance on 38 downstream datasets including image classification and retrieval tasks followed by DataComp. The image classification tasks contain ImageNet-1k [27], ImageNet distribution shifts [28\u201331], 11 datasets from the Visual Task Adaptation Benchmark (VTAB) [32] and 3 datasets from WILDS [33, 34]. Retrieval datasets contain Flickr30k [35], MSCOCO [36] and WinoGAViL [37]. ", "page_idx": 6}, {"type": "text", "text": "Teacher model architecture. Our experiments utilize two architectures for OpenAI\u2019s CLIP teacher models: ViT-L/14 and ViT-B/32. Additionally, we use the public version of DFN (DFN-P) proposed by Fang et al. [2] as a teacher model, and its architecture is also ViT-B/32. ", "page_idx": 6}, {"type": "text", "text": "4.2 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We restate the three current research directions mentioned before based on how much external resources are employed: (D1) using OAI CLIP alone while optimizing embedding employment strategies, (D2) training and using a more advanced CLIP embedding model based on external data, and (D3) utilizing non-CLIP external models to aid data selection. It is important to note that D2 and D3 may also incorporate strategies from D1. For example, CLIPScore (D1) has been used in almost all the top methods. Therefore, we categorize baselines by the largest possible category they encompass. According to the above categorization, we summarize the baselines we used in our experiments as follows. Please refer to Fig. 4 and Appendix C.4 for more details. ", "page_idx": 6}, {"type": "text", "text": "D1: OAI CLIP embedding only. The learner can only access the pretraining dataset (like DataCompmedium), the original OAI CLIP teacher model that is used to extract embeddings, and some target data of the downstream tasks which is much smaller than the pretraining dataset (like ImageNet-1k). In this category, we don\u2019t use any existing external non-CLIP models or any newly trained CLIP model based on external multi-modal dataset. In detail, This category includes (1) CLIPScore [38], which only uses CLIPScore for filtering as we mentioned before. (2) Image-based filtering [1], which uses ImageNet-1K training data as the downstream target data for data filtering. It applies $\\mathbf{k}\\cdot$ -means clustering to the image embeddings of training data and selects clusters closest to the ImageNet-1K embeddings. Gadre et al. [1] also try to combine image-based flitering and CLIPScore together. (3) $\\mathbb{D}^{2}$ Pruning [18], which represents the dataset as an undirected graph and selects the data by combining difficulty and diversity. They use the CLIP score to initialize their graph. ", "page_idx": 6}, {"type": "text", "text": "D2, D3: Accessible external model and multi-modal data. All the current top baselines enable the learner to utilize external resources, either to train a better CLIP teacher model or to help filtering using existing models\u2019 properies. In detail, (1) DFN [2] trains another CLIP data filtering network via external high-quality data. Their currently public model (DFN-P) is trained on CC12M $[39]+$ CC3M $[40]+\\mathrm{SS}15\\mathrm{M}$ [41], while the best DFN is trained on nonpublic HQITP-350M [2], which is even larger than DataComp-medium. (2) HYPE [3] leverages hyperbolic embeddings (different from CLIP embedding) and the concept of entailment cones to filter out samples with meaningless or underspecified semantics, enhancing the specificity of each sample. (3) HYPE \u222aDFN proposed by [3] samples subset separately for each method and then merge them. This is the state-of-the-art method on the DataComp benchmark for medium size. (4) Other methods including T-MARS [12], Devils [14], MLM [42], which leverage external models such as text detection model FAST [13], BLIP-2 [16] and LLaVA-1.5 [43, 44] to heuristically select data. See details in Appendix C.4. ", "page_idx": 6}, {"type": "text", "text": "Cross-setting comparison. We make these separations for fair comparison. Intuitively, performance should be ranked as D2, $\\mathbf{D3}>\\mathbf{D1}$ . However, our results show that cross-setting comparisons are possible and our D1 methods can perform similar or even better than most of D3 methods. ", "page_idx": 6}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/9cd4e3f45bfda46c2349e41b3ccf93e664a6eeab866189b49c4eaf3a7d1426be.jpg", "table_caption": ["Table 2: Results on DataComp-medium from methods that use only OpenAI\u2019s CLIP-L/14 model, i.e., all methods are from the D1 category. The \u201cdataset size\u201d represents the size of the subset obtained from different approaches. NormSim(IN-1k) denotes using the training data of ImageNet-1k as the target while NormSim(Target) represents using that of all 24 available downstream tasks. NormSim-D refers to the methods that use an iteratively selected subset from the training set as the target proxy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Main Results and Discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Comparision on D1 Category (Q1) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Table 2, we compare the D1 methods where only the OAI CLIP model is allowed to be used. ", "page_idx": 7}, {"type": "text", "text": "Our Methods leverage OAI CLIP-L/14 better. First, negCLIPLoss outperforms CLIPScore on all metrics, regardless of whether it is used alone or combined with other methods. These results support our claim that negCLIPLoss can more accurately estimate the data quality. ", "page_idx": 7}, {"type": "text", "text": "Second, even when target knowledge is unavailable, use $\\mathrm{{NormSim}_{2}.}$ -D together with negCLIPLoss can still improve the flitering performance by $1.9\\%$ on average 38 downstream tasks. Third, when target knowledge is available, $\\mathrm{NormSim_{2}}$ and $\\mathrm{NormSim}_{\\infty}$ can improve flitering more significantly compared with $\\mathrm{NormSim_{2^{-}}}$ D, and in general, Norm $S i m_{\\infty}$ is the best choice. Especially, compared with the best baseline \u2018CLIPScore $(30\\%)^{\\circ}$ , our best combination \u2018negCLIPLoss $\\cap{\\mathrm{\\:NormSim}}_{\\infty}$ (Target)\u2019 improves $\\bar{\\mathsf{5.3}}\\%$ on ImageNet-1k and $2.8\\%$ on average 38 downstream tasks, respectively. Later in Table 3 we will see that this result outperform all the D3 baselines except DFN \u222aHYPE. On the other hand, when using ImageNet-1k as the target data, the choice of norm has very little influence. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Results on DataComp-medium from the top methods that use only OpenAI\u2019s CLIPB/32 model or public version of DFN (DFN-P). $\\cdot\\mathrm{\\DeltaNormSim}_{\\infty}^{\\mathrm{B}/32},$ represents using OAI CLIP-B/32 to calculate $\\mathrm{NormSim}_{\\infty}$ . ", "page_idx": 7}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/7d7793d2e9cc6bfed150c1ed6f6328bee093f4111850d4c677b0ce6307c7c879.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Results of all D1&D2&D3 top methods on DataComp-medium. The results of MLM [42] are from their paper, while all other baselines are reproduced on our downloaded dataset using their official UIDs. \u201cOurs $(20\\%)^{\\circ}$ refers to use \u201cnegCLIPLoss $(30\\%)\\cap\\mathrm{NormSim}_{\\infty}($ (Target)\u201d to get $20\\%$ of original data, while \u201cOurs $(10\\%)^{\\circ}$ \u201d denotes applying \u201cnegCLIPLoss $(20\\%)\\cap\\mathrm{NormSim}_{\\infty}($ (Target)\u201d to get $10\\%$ . And we use \u201c\\*\u201d to indicate the case where we choose the intersection of the data selected by using OAI CLIP-B/32 and OAI CLIP-L/14 separately, which results in about 15M data for \u201cOurs $(20\\%)^{*}{}^{*}{}^{*}$ and 7.4M data for \u201cOurs $(10\\%)^{*}$ . ", "page_idx": 8}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/19f2c1e87366ca242e374a5bc4a9977aa15504587d4ba624020035e50e3b3965.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.2 Try Other Teacher Models (Q2) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate whether our method applies to other CLIP teacher models, we replaced OAI CLIP-L/14 with OAI CLIP-B/32 and DFN-P as embedding models. We compare the best baseline \u201cCLIPScore\u201d with our \u201cnegCLIPLoss\u201d and best strategy \u201cnegCLIPLoss $\\cap{\\mathrm{NormSim}}_{\\infty}$ (Target)\u201d as shown in Table 1 and Appendix D.2. Note that the original DFN paper selects a subset comprising 19.2M data points, which accounts for approximately $\\bar{1}7.5\\%$ of our dataset and $15\\%$ of their dataset, we incorporate these sampling ratios into our comparison. ", "page_idx": 8}, {"type": "text", "text": "negCLIPLoss can be applied to different CLIP embedding models. Our proposed negCLIPLoss, as a replacement of CLIPScore, not only leads to better performance compared to all the other baselines using OAI CLIP-L/14 as shown in Table 2, but also achieves universal improvement on the other two CLIP embedding models, OAI CLIP-B/32 and DFN-P as shown in Table 1. Our methods can consistently outperform all downstream tasks for different filtering ratios and models, like a $0.5\\%{-}5.4\\%$ increase on ImageNet-1k. ", "page_idx": 8}, {"type": "text", "text": "Embedding required by NormSim should have good downstream performance. When combining negCLIPLoss with $\\mathrm{NormSim}_{\\infty}$ , OAI CLIP-B/32 and DFN-P exhibit completely different behaviors. The former obtains results even better than those in Table 2, which uses OAI CLIP-L/14 as the teacher model, while DFN-P achieves results even worse than using negCLIPLoss alone6. The reason is that, unlike OAI CLIP-B/32, DFN-P is specially designed for data flitering at the expense of downstream task performance, as claimed by its authors. For example, the ImageNet-1k accuracy for DFN-P, OAI CLIP-B/32, and OAI CLIP-L/14 are $45\\%$ , $63\\%$ , and $75\\%$ , respectively. This indicates that the embeddings obtained from DFN on target data might be highly unreliable, leading to inaccurate similarity calculations between training and target data. To support this, if we use DFN-P to evaluate negCLIPLoss but utilize OAI CLIP-B/32 for calculating NormSim, as shown in \"negCLIPLoss $(17.5\\bar{\\%})\\cap\\mathrm{NormSim}_{\\infty}^{\\mathrm{B/32}}(\\mathrm{Target})^{\\ast}$ , we can further improve the results compared to using negCLIPLoss alone. Its average performance on 38 tasks is even higher than utilizing the best DFN (trained on HQITP-350M) with CLIPScore, as shown in Table 3. ", "page_idx": 8}, {"type": "text", "text": "4.3.3 Comparison with D2 & D3 Categories (Q3) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this part, we compare all the D2 & D3 baselines mentioned in Sec. 4.2 together with our best strategy in Table 3. Here we reproduce all the baselines if their official UIDs are available. For \u201cA \u222a B\u201d mentioned in Table 3, we follow the way of \u201cHYPE \u222aDFN\u201d in Kim et al. [3] to merge the data, which generates the sampling subset separately for each method and then merge them. This will result in oversampling the shared data, which is intuitively more important.7 We also show the best result we obtain by combining our method with DFN [2] and HYPE [3] on the full DataComp-medium dataset in Table 4, where the baselines are from DataComp benchmark. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Our methods can outperform most of the D3 methods. In Table 3, we show that without using any external models or data, our best combination, i.e., using OAI CLIP-B/32 for \u201cnegCLIPLoss $(30\\%)$ $\\mathrm{\\uparrowNormSim_{\\infty}}$ (Target)\u201d (Ours $(20\\%)$ ), still outperforms all methods except DFN and \u201cDFN \u222aHYPE\u201d. This answers the first part of Q3 and further indicates that some external models may be redundant since CLIP embeddings already contain necessary information. ", "page_idx": 9}, {"type": "text", "text": "We can further improve the SOTA method. In Table 3, we show that our model can further boost the performance of the current SOTA method \u201cHYPE $\\cup$ DFN\u201d by $0.9\\%$ on both ImageNet-1k and on average 38 downstream tasks, and close results can be achieved even without combining HYPE which utilizes the external embedding model MERU [45]. And we update the SOTA performance of the DataComp-medium (full dataset) benchmark as shown in Table 4. Here we use the data selected by both OAI CLIP-B/32 and L/14, which we found is more robust than using one of them alone. Our better results answer the second part of Q3, that is, our methods can be compatible with other D2&D3 methods. ", "page_idx": 9}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/c8bb7874bb5fea1f02e54f18d8f4a72dad19cedce7ecb9e677a69faa5631c04c.jpg", "table_caption": ["Table 4: Results of the top methods on the full DataComp-medium dataset (128M data). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce two metrics, negCLIPLoss and NormSim, to enhance data selection in multimodal contrastive learning without relying on external resources. negCLIPLoss provides a more accurate quality metric compared to the commonly used CLIPScore, while NormSim measures the similarity between pretraining data and target data for known downstream tasks. Experiments show that our methods achieve results that are competitive with or even better to approaches using external models or datasets. Additionally, negCLIPLoss and NormSim are compatible with existing top techniques, allowing us to achieve a new state-of-the-art by combining them. ", "page_idx": 9}, {"type": "text", "text": "A notable limitation of our study is the exclusion of larger pretraining datasets, such as the large and xlarge scales of DataComp. However, DataComp-medium is the most commonly used benchmark for data selection in CLIP pretraining, and our method has demonstrated both effectiveness (Table 2-3) and efficiency (Table 5) on it. Future directions include exploring better ways to merge data selected by different methods and incorporating our methods into data scheduling scenarios. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Tong Chen, Pang Wei Koh, Xiaochuang Han, Rui Xin, Luyao Ma, Lei Chen, and other members in the UW ML Group for many insightful discussions and helpful feedback. The research of Kevin Jamieson and Yifang Chen are partially supported by the NSF through the University of Washington Materials Research Science and Engineering Center, DMR-2308979, and awards CCF 2007036. SSD acknowledges the support of NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, NSF CCF 2019844, and NSF IIS 2229881. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.   \n[2] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023.   \n[3] Wonjae Kim, Sanghyuk Chun, Taekyung Kim, Dongyoon Han, and Sangdoo Yun. Hype: Hyperbolic entailment flitering for underspecified images and texts. arXiv preprint arXiv:2404.17507, 2024.   \n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[5] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829, 2023.   \n[7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \n[8] Huy V Vo, Vasil Khalidov, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, et al. Automatic data curation for self-supervised learning: A clustering-based approach. arXiv preprint arXiv:2405.15613, 2024. [9] Tzu-Heng Huang, Changho Shin, Sui Jiet Tay, Dyah Adila, and Frederic Sala. Multimodal data curation via object detection and filter ensembles. arXiv preprint arXiv:2401.12225, 2024.   \n[10] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:34201\u201334227, 2023.   \n[11] Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, and Ari S Morcos. Effective pruning of web-scale datasets based on complexity of concept clusters. arXiv preprint arXiv:2401.04578, 2024.   \n[12] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. Tmars: Improving visual representations by circumventing text feature learning. arXiv preprint arXiv:2307.03132, 2023.   \n[13] Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, and Tong Lu. Fast: Faster arbitrarily-shaped text detector with minimalist kernel representation, 2021.   \n[14] Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. The devil is in the details: A deep dive into the rabbit hole of data filtering. arXiv preprint arXiv:2309.15954, 2023.   \n[15] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016.   \n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[17] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. arXiv preprint arXiv:2307.10350, 2023.   \n[18] Adyasha Maharana, Prateek Yadav, and Mohit Bansal. D2 pruning: Message passing for balancing diversity and difficulty in data pruning. arXiv preprint arXiv:2310.07931, 2023.   \n[19] Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, and Ari Morcos. Sieve: Multimodal dataset pruning using image captioning models. arXiv preprint arXiv:2310.02110, 2023.   \n[20] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024.   \n[21] Xiaobo Xia, Bo Han, Yibing Zhan, Jun Yu, Mingming Gong, Chen Gong, and Tongliang Liu. Combating noisy labels with sample selection by mining high-discrepancy examples. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1833\u20131843, October 2023.   \n[22] S\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 15630\u201315649. PMLR, 17\u201323 Jul 2022.   \n[23] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.   \n[24] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. In International Conference on Machine Learning, pages 25994\u2013 26009. PMLR, 2022.   \n[25] Yutaro Yamada, Yingtian Tang, and Ilker Yildirim. When are lemons purple? the concept association bias of clip. arXiv preprint arXiv:2212.12043, 2022.   \n[26] Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman. Data-efficient contrastive language-image pretraining: Prioritizing data quality over quantity. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 1000\u20131008. PMLR, 02\u201304 May 2024.   \n[27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[28] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5389\u20135400. PMLR, 09\u201315 Jun 2019.   \n[30] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021.   \n[31] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.   \n[32] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.   \n[33] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.   \n[34] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wilds benchmark for unsupervised adaptation. arXiv preprint arXiv:2112.05090, 2021.   \n[35] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.   \n[36] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.   \n[37] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549\u2013 26564, 2022.   \n[38] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.   \n[39] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.   \n[40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, Melbourne, Australia, July 2018. Association for Computational Linguistics.   \n[41] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. Advances in Neural Information Processing Systems, 35:21455\u201321469, 2022.   \n[42] Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, and Heng Wang. Finetuned multimodal language models are high-quality image-text data fliters. arXiv preprint arXiv:2403.02677, 2024.   \n[43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[44] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.   \n[45] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakrishna Vedantam. Hyperbolic image-text representations. In International Conference on Machine Learning, pages 7694\u20137731. PMLR, 2023.   \n[46] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang. Understanding multimodal contrastive learning and incorporating unpaired data. In International Conference on Artificial Intelligence and Statistics, pages 4348\u20134380. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[47] Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013. ", "page_idx": 13}, {"type": "text", "text": "[48] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[49] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.   \n[50] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter. Scaling laws for data filtering\u2013data curation cannot be compute agnostic. arXiv preprint arXiv:2404.07177, 2024. ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Interpretation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Concentration of Normalization Term in negCLIPLoss ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we construct a theorem using the concentration inequality to show that when the batch size is sufficiently large, the normalization term $R^{B_{k}}$ obtained from actual batch $B_{k}$ can approximate $R^{B^{*}}$ calculated using ground truth batch $B^{*}$ quite well. The details are as follows: ", "page_idx": 14}, {"type": "text", "text": "We assume that the pretraining dataset $\\mathcal{D}$ is ndependent and identically distributed (i.i.d.) sampled from some distribution $\\mathcal{P}$ . Besides, to use pretraining data batch to approximate the ground truth batch, one necessary condition is that their distribution is similar. Here for simplicity, we assume that they are also i.i.d.. ", "page_idx": 14}, {"type": "text", "text": "Assumption A.1. We assume that the ground-truth batch of data $B^{*}$ used by the teacher model is i.i.d. to the pretraining dataset $\\mathcal{D}$ which is required to be filtered. ", "page_idx": 14}, {"type": "text", "text": "For simplicity, we denote $s_{i j}=\\bar{f}_{v}(x_{i}^{v})^{\\top}\\bar{f}_{l}(x_{j}^{l}),i,j\\in B$ to be the cross-image-text similarities in the batch $B$ . Then the normalization term can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}_{i}^{B}=\\frac{\\tau}{2}\\left[\\log(\\sum_{j\\in B}\\exp(s_{i j}/\\tau))+\\log(\\sum_{j\\in B}\\exp(s_{j i}/\\tau))\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here $s_{i j}\\in[-1,1]$ . We will show that $\\mathcal{R}_{i}^{B}=\\left(1+o(1)\\right)\\cdot\\mathcal{R}_{i}^{B^{\\ast}}$ for all $i$ when $|B|$ is sufficiently large, which means that we can use the random batch to approximate the ground-truth batch. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. If Assumption A.1 holds and the batch size satisfies $|B|\\,=\\,|B^{*}|,$ , then we have $\\mathcal{R}_{i}^{B}=\\Theta(\\log(|B|))$ while $\\begin{array}{r}{|\\mathcal{R}_{i}^{B}-\\mathcal{R}_{i}^{B^{*}}|=O(\\frac{1}{\\sqrt{|B|}})}\\end{array}$ for any $i\\in B\\cap B^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Since $s_{i j}~\\in~[-1,1]$ , It\u2019s obvious that $\\mathcal{R}_{i}^{B}\\ =\\ \\Theta(\\log(|B|))$ . Let $\\alpha_{i j}\\;:=\\;\\exp(s_{i j}/\\tau)\\:-\\:$ $\\mathbb{E}_{j}[\\exp(s_{i j}/\\tau)]$ , then $\\alpha_{i j}$ is zero-mean. Note that since the data is i.i.d., so does $\\alpha_{i j}$ , and we denote $\\gamma:=\\mathbb{E}_{j}[\\alpha_{i j}^{2}]$ . Note that $|\\alpha_{i j}|\\le e^{1/\\tau}=:M$ , from Bernstein inequality we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\sum_{j\\in B}\\alpha_{i j}|\\geq t)\\leq2\\exp(-\\frac{\\frac{1}{2}t^{2}}{|B|\\gamma+\\frac{1}{3}M t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A similar conclusion holds for $B^{*}$ . These result that with probability at least $1-\\eta$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\sum_{j\\in B}\\alpha_{i j}|\\leq\\operatorname*{max}\\{2\\sqrt{|B|\\gamma\\ln(\\frac{2}{\\eta})},\\frac{4}{3}M\\ln(\\frac{2}{\\eta})\\}=:t(|B|,\\gamma,\\eta,M)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we have $\\begin{array}{r}{|\\sum_{j\\in B}\\exp(\\frac{s_{i j}}{\\tau})\\!-\\!\\sum_{j\\in B^{*}}\\exp(\\frac{s_{i j}}{\\tau})|\\leq2t(|B|,\\gamma,\\eta)}\\end{array}$ . Furthermore, for any $x_{1},x_{2}>1$ , it\u2019s easy to prove that | log(x1)\u2212log(x2)| \u2264 m|ixn1(x\u22121x,2x|2) . Therefore, we have $\\begin{array}{r}{|\\log(\\sum_{j\\in B}\\exp(\\frac{s_{i j}}{\\tau}))-}\\end{array}$ $\\begin{array}{r}{\\log(\\sum_{j\\in B^{*}}\\exp(\\frac{s_{i j}}{\\tau}))|\\lesssim O(\\frac{1}{\\sqrt{|B|}})}\\end{array}$ . Similar claims hold for $\\lvert\\mathcal{R}_{i}^{B}-\\mathcal{R}_{i}^{B^{*}}\\rvert$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Optimality of $\\mathbf{NormSim_{\\mathrm{2}}}$ Under Linear Assumption ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we give a theoretical justification on the NormSim metric when $p=2$ under the linear model assumptions when low quality image and mismatched text has already been removed. In other words, we mainly focus on the following strategy. ", "page_idx": 14}, {"type": "equation", "text": "$$\nS=\\arg\\operatorname*{max}_{|S|=N}\\sum_{i\\in S}\\bar{f}_{v}(x_{i}^{v})^{\\top}\\left(\\frac{1}{|X_{\\mathrm{target}}|}\\sum_{x_{t}\\in X_{\\mathrm{target}}}\\bar{f}_{v}(x_{t}^{v})\\bar{f}_{v}(x_{t}^{v})^{\\top}\\right)\\bar{f}_{v}(x_{i}^{v})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2.1 Theoretical Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Training data. For any $\\mathbf{\\boldsymbol{x}}^{v},\\mathbf{\\boldsymbol{x}}^{l}\\in\\mathbb{R}^{d}$ observable image and text training pairs, we define $z^{v},z^{l}$ to be the corresponding latent vectors which contain all semantically pertinent information about our tasks of interest. Similar to previous theoretical work [46], we assume each i.i.d pair $z^{v l}$ follows zero-mean sub-gaussian distribution whose cross-covariance satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Cov}(z^{v},z^{l})=\\Sigma_{\\mathrm{train}}=\\mathrm{diag}(\\sigma_{1},\\sigma_{2},\\ldots),\\qquad\\qquad\\qquad\\|z^{v l}\\|=1\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and each $x^{v l}$ is generated based on a linear model such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}^{v l}=G_{v l}^{*}z^{v l}+\\pmb{\\xi}^{v l}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here $G_{v l}^{*}\\in O_{d\\times r}$ is the othonormal ground truth representation mapping from the latent vector space to the input space, and $\\xi^{v l}\\sim\\mathcal{N}(0,I_{d})$ are i.i.d. random noise. ", "page_idx": 15}, {"type": "text", "text": "Also we denote the cross covariance of any finite dataset $S^{\\prime}$ (e.g. the given train set $D_{\\mathrm{train.}}$ ) as $\\Sigma_{S^{\\prime}}$ . ", "page_idx": 15}, {"type": "text", "text": "Test data. For any zero-shot downstream task, we assume it shares almost same data generation process as the training set, except its the cross-covariance $\\Sigma_{\\mathrm{target}}$ does not necessarily equal $\\Sigma_{\\mathrm{train}}$ , which necessitate the choice of \u03a3\u00aftarget_proxy. ", "page_idx": 15}, {"type": "text", "text": "CLIP embedding model as teacher. Under the linear model assumption, we have a teacher model $\\bar{f}_{v l}=\\bar{G}_{v l}$ , whose generated clip embedding can partially recover the ground truth hidden vector $z^{v l}$ with error. ", "page_idx": 15}, {"type": "text", "text": "Formally, we say teacher has $\\epsilon_{v}^{n}$ error if for all possible $n$ budget subsets $S\\subset D_{\\mathrm{train}}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{|S|}\\left\\|\\sum_{\\pmb{x}^{v l}\\in S}\\bar{G}_{v}^{\\top}\\pmb{x}^{v}(\\pmb{x}^{v})^{\\top}\\bar{G}_{v}-\\sum_{\\pmb{x}^{v l}\\in S}z^{v}(z^{v})^{\\top}\\right\\|_{\\pmb{\\mathscr{r}}}\\leq\\epsilon_{v}^{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the same notation applies for the language modal. By the orthonormal assumption on the ground truth matrix $G_{v l}^{*}$ , we see that $\\bar{G}_{v}^{\\top}$ is aiming to inverting the map. In addition, we say the teacher has $\\epsilon_{v*l}^{n}$ cross modal error ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{|S|}\\|\\sum_{\\pmb{x}^{v l}\\in S}\\bar{G}_{v}^{\\top}\\pmb{x}^{v}(\\pmb{x}^{l})^{\\top}\\bar{G}_{l}-\\sum_{\\pmb{x}^{v l}\\in S}z^{v}(z^{l})^{\\top}\\|_{*}\\leq\\epsilon_{v*l}^{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When all $\\epsilon_{v}^{n},\\epsilon_{l}^{n},\\epsilon_{v*l}^{n}\\rightarrow0$ as $n\\to\\infty$ , then we say the teacher is strong for both modalities. But it might also be possible that only one modal, for example, visual is strong. That is $\\epsilon_{v}^{n}\\to0,\\epsilon_{l}^{n},\\epsilon_{v*l}^{n}\\gg$ $\\epsilon_{v}^{n}$ . ", "page_idx": 15}, {"type": "text", "text": "Model and training. According to Lemma 4.1 in [46], using the CLIP loss to optimize the linear model has approximately the same training dynamics as using the regularized linear loss. Therefore, here we assume that we are learning $G_{v},G_{l}$ by maximizing the clip score gap between the contrastive pairs, plus a regularizer, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{G_{v},G_{l}}\\mathcal{L}_{S}^{\\rho}(G_{v},G_{l}):=\\operatorname*{min}_{G_{v},G_{l}}\\frac{\\sum_{i\\in S}\\sum_{j\\in S}(s_{i j}-s_{i i})}{|S|(|S|-1)}+\\frac{\\rho}{2}\\frac{|S|}{|S|-1}\\|G_{v}G_{l}^{\\top}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $s_{i j}:=\\langle{G_{v}^{\\top}}{\\pmb x}_{i}^{v},{G_{l}^{\\top}}{\\pmb x}_{j}^{l}\\rangle$ and $\\rho>0$ is some regularizer-related constant. Note that this objective maximizes self-similarity and minimizes similarity between disparate pairs. Note that this \u201closs\u201d can be negative, avoiding the trivial null solution of all zeros. We denote this training process from any given $S$ as $G_{v l}=A^{\\rho}(S)$ . ", "page_idx": 15}, {"type": "text", "text": "Goal and metric. Under the same principle as our training loss function, we measure the performance of any learnt $G_{v},G_{l}$ on some downstream task with distribution $\\mathcal{D}_{\\mathrm{target}}$ as test loss ${\\mathcal{L}}_{\\mathrm{target}}(G_{v},G_{l}):=$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pmb{x}^{v l}\\sim\\mathcal{D}_{\\mathrm{target}}}(\\langle G_{v}^{\\top}\\pmb{x}^{v},G_{l}^{\\top}\\pmb{x}_{2}^{l}\\rangle-\\langle G_{v}^{\\top}\\pmb{x}^{v},G_{l}^{\\top}\\pmb{x}^{l}\\rangle)}\\\\ &{\\quad\\pmb{x}_{2}^{v l}\\sim\\mathcal{D}_{\\mathrm{target}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is inspired by the following classification accuracy. Assume that the test data including $C$ class, and the class distribution is $\\mathcal{C}$ . For every class $c$ , the training data $\\pmb{x}=(\\pmb{x}^{v},\\pmb{x}^{l})$ satisfies distribution $\\mathcal{P}_{c}$ . We further assume the corresponding classification templates are $\\{x_{c}\\}_{c=1}^{C}$ . Thus we define classification accuracy as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{AC}(G_{v},G_{l})=\\mathbb{E}_{c,c^{\\prime}\\sim{\\mathcal{C}}\\times{\\mathcal{C}}}\\left[\\mathbb{E}_{{\\pmb{x}}_{i}\\sim{\\mathcal{P}}_{c}}\\mathbf{1}[s_{i c}>s_{i c^{\\prime}}]\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore our goal is to minimize its gap between the best hind-side subset, for any $\\rho$ , without budget constraints, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta^{\\rho}(S)=\\mathcal{L}_{\\mathrm{target}}(\\hat{G}_{v l})-\\operatorname*{min}_{S^{\\prime}\\in D_{\\mathrm{train}}}\\mathcal{L}_{\\mathrm{target}}(\\mathcal{A}^{\\rho}(S^{\\prime})),\\hat{G}_{v l}=\\mathcal{A}^{\\rho}(S)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2.2 Generalization Guarantees ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now provide theoretical guarantees and postpone our proof into Appendix A.2.3. Firstly, we are going to prove the intuition behind NormSim2score. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.1 (Intuition behind $\\mathrm{{NormSim}_{2}}$ ). With high probability at least $\\textstyle1-{\\frac{1}{|S|d}}$ , suppose the hind-side best subset has at least $\\underline{n}$ number of samples, then we have ", "page_idx": 16}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/f50ce296179bce431d1c844b391e79c151660187db928f866c401934258c6ed5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Proof sketch. $\\pmb{\\mathrm{\\Sigma}}$ Under the assumption that both $z^{v l},\\xi_{v l}$ is zero-mean, maximizing the clip score gap is equivalent to maximizing the clip score of the same sample. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{target}}(\\hat{G}_{v},\\hat{G}_{l}):=-\\mathbb{E}_{{\\pmb x}^{v l}\\sim{\\pmb\\mathcal{D}}_{\\mathrm{target}}}\\langle\\hat{G}_{v}^{\\top}{\\pmb x}^{v},\\hat{G}_{l}^{\\top}{\\pmb x}^{l}\\rangle\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\pmb{\\varphi}$ By minimizing the regularized training loss $\\mathcal{L}_{S}^{\\rho}(G_{v},G_{l})$ using Eckart-Young-Mirsky Theorem, we get a closed form solution of G\u02c6 as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{G}_{v}\\hat{G}_{l}^{\\top}\\approx\\frac{1}{\\rho}G_{v}^{*}\\Sigma_{S}\\cdot(G_{l}^{*})^{\\top}+\\mathrm{noise\\;depend\\;on}\\;S\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\pmb{\\otimes}$ Combining the result in $\\pmb{\\varphi}$ and $\\pmb{\\mathrm{\\Sigma}}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{target}}(\\hat{G}_{v l})\\approx-\\frac{1}{\\rho}\\operatorname{Tr}\\left(\\Sigma_{\\mathrm{target}}\\Sigma_{S}\\right)-\\mathrm{noise\\;depend\\;on}\\;S\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The same analysis can be applied on $\\mathrm{min}_{S^{\\prime}\\in D_{\\mathrm{train}}}\\,\\mathcal{L}_{\\mathrm{target}}(A(S^{\\prime}))$ as well. Rearranging these two equations gives us the final result. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "This lemma shows the the $\\Delta(S)$ is depend on the $\\mathrm{{NormSim}_{2}}$ -related term and the noise term which comes from $\\xi$ . When $\\underline{n}$ and $|S|$ is large enough, then the $\\mathrm{{NormSim}_{2}}$ -related term will become dominant. This aligns with our practice experience that the final performance is less sensitive to the small variation in the number of select data as long as that is sufficient. Moreover, in some special cases where test distribution has identity cross-variance, then sampling by choosing CLIP score might be enough. ", "page_idx": 16}, {"type": "text", "text": "Now we are ready to give a proof on the choice of $\\bar{\\Sigma}_{\\mathbf{target}}$ and visual-only information. Specifically, the strategy error mainly comes from (1). The unknown test distribution shift from training. (2). The unobservable ground truth $\\Sigma_{S}$ . To tackle error (1), we assume some prior knowledge on test by using the proxy test variance $\\bar{\\Sigma}_{\\mathrm{target}}$ . To tackle the error (2), there are two possible solutions as shown below. Based on the theoretical interpretation, we should choose different strategy based on the property of the teacher embedding model. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{\\mathrm{vision+language}}=\\underset{S}{\\arg\\operatorname*{max}}\\operatorname{Tr}\\left(\\bar{\\Sigma}_{\\mathrm{target}}\\big(\\displaystyle\\sum_{\\mathbf{x}^{v l}\\in S}\\bar{G}_{v}^{\\top}\\mathbf{x}^{v}(\\mathbf{x}^{l})^{\\top}\\bar{G}_{l}\\big)\\right)}\\\\ &{S_{\\mathrm{vision~only}}=\\underset{S}{\\arg\\operatorname*{max}}\\operatorname{Tr}\\left(\\bar{\\Sigma}_{\\mathrm{target}}\\big(\\displaystyle\\sum_{\\mathbf{x}^{v l}\\in S}\\bar{G}_{v}^{\\top}\\mathbf{x}^{v}(\\mathbf{x}^{v})^{\\top}\\bar{G}_{v}\\big)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem A.2 (Main). Under the assumption of Lemma A.1, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\Delta^{\\rho}(S)\\leq n o i s e+\\displaystyle\\frac{1}{\\rho}\\|\\bar{\\Sigma}_{t a r g e t}-\\Sigma_{t a r g e t}\\||\\Sigma_{S}-\\Sigma_{b e s t}\\|_{*}}\\\\ {+\\displaystyle\\frac{1}{\\rho}\\left\\{\\epsilon_{v*l}^{S}\\quad\\middle(\\nu i s i o n\\!+\\!l a n g u a g e\\right)\\right.}\\\\ {\\left.\\epsilon_{v}^{S}+\\sqrt{1-\\frac{1}{|S|}\\sum_{i\\in[S]}\\langle z^{v},z^{l}\\rangle)}\\quad\\middle(\\nu i s i o n\\;o n l y)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Firstly, it is evident that the greater the difference between $\\bar{\\Sigma}_{\\mathrm{target}}$ and $\\Sigma_{\\mathrm{target}}$ , the less improvement we can expect. Moreover, in scenarios where $\\epsilon_{l}$ is large (indicating lower accuracy in the language part) while $\\epsilon_{v}$ is small (indicating higher accuracy in the vision part), it may be advisable to opt for visiononly embeddings. However, the learner should also consider the term $\\begin{array}{r}{\\sqrt{1-\\frac{1}{|S|}\\sum_{i\\in S}\\langle z^{v},z^{l}\\rangle}}\\end{array}$ , which represents the alignment between the ground truth visual and language latent vectors, essentially reflecting the intrinsic quality of the data. If this term is already significant, relying solely on vision information as a proxy for language information could lead to suboptimal results. ", "page_idx": 17}, {"type": "text", "text": "A.2.3 Detailed proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma A.2. Let ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{G}_{v},\\hat{G}_{l}=\\arg\\operatorname*{min}_{G_{v},G_{l}\\in\\mathbb{R}^{d\\times r}}\\mathcal{L}(G_{v},G_{l})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{G}_{v}\\hat{G}_{l}^{\\top}=\\frac{1}{\\rho}G_{v}^{*}\\Sigma_{S}(G_{l}^{*})^{\\top}+P_{1}+P_{2}+P_{3}+P_{4}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where noise terms $P_{i}$ are defined in (12) , (13), (14) and (15). ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that $s_{i j}=(\\pmb{x}_{j}^{l})^{\\top}G_{l}G_{v}^{\\top}\\pmb{x}_{i}^{v}=\\mathrm{Tr}(G_{v}^{\\top}\\pmb{x}_{i}^{v}(\\pmb{x}_{j}^{l})^{\\top}G_{l})$ , like the proof of Corollary B.1. in [46], we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\Sigma(G_{v},G_{l})}&{=}&{\\displaystyle\\frac{\\sum_{i\\in S}\\sum_{j\\in S}\\left(s_{i j}-s_{i i}\\right)}{|S|(|S|-1)}+\\displaystyle\\frac{\\rho}{2}\\frac{|S|}{|S|-1}\\|G_{v}G_{l}^{\\top}\\|_{F}^{2}}\\\\ &{=}&{\\displaystyle\\frac{\\sum_{i\\in S}\\sum_{j\\in S}s_{i j}-|S|\\sum_{i\\in S}s_{i i}}{|S|(|S|-1)}+\\displaystyle\\frac{\\rho}{2}\\frac{|S|}{|S|-1}\\|G_{v}G_{l}^{\\top}\\|_{F}^{2}}\\\\ &{=}&{\\displaystyle-\\operatorname{Tr}\\left(G_{v}^{\\top}\\left[\\frac{1}{|S|-1}\\sum_{i\\in S}\\mathbf{x}_{i}^{v}(\\mathbf{x}_{i}^{l})^{\\top}-\\frac{|S|}{|S|-1}\\bar{\\mathbf{x}}^{v}(\\bar{\\mathbf{x}}^{l})^{\\top}\\right]G_{l}\\right)+\\displaystyle\\frac{\\rho}{2}\\frac{|S|}{|S|-1}\\|G_{v}G_{l}^{\\top}\\|_{F}^{2}}\\\\ &{=:}&{\\displaystyle-\\operatorname{Tr}(G_{v}^{\\top}\\Gamma G_{l})+\\displaystyle\\frac{\\rho}{2}\\frac{|S|}{|S|-1}\\|G_{v}G_{l}^{\\top}\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\pmb{x}}^{v l}:=(\\sum_{i\\in S}{\\pmb{x}}_{i}^{v l})/|S|}\\end{array}$ . Then by the Eckart-Young-Mirsky Theorem (For example, Theorem 2.4.8 in Golub et al. [47]), we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\arg_{G_{v}\\in\\mathbb{R}^{d\\times r},G_{l}\\in\\mathbb{R}^{d\\times r}}\\mathcal{L}(G_{v},G_{l})}\\\\ {=}&{\\arg_{G_{v}\\in\\mathbb{R}^{d\\times r},G_{l}\\in\\mathbb{R}^{d\\times r}}\\mathrm{Tr}(G_{v}^{\\top}\\Gamma G_{l})-\\frac{\\rho}{2}\\frac{|S|}{|S|-1}\\|G_{v}G_{l}^{\\top}\\|_{F}^{2}}\\\\ {=}&{\\{(G_{v},G_{l})\\in\\mathbb{R}^{d\\times r}\\times\\mathbb{R}^{d\\times r}:G_{v}G_{l}^{\\top}=\\frac{1}{\\rho}\\frac{|S|-1}{|S|}\\mathrm{SVD}_{r}(\\Gamma)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(Eckart-Young-Mirsky Theorem) ", "page_idx": 17}, {"type": "text", "text": "where the notation $\\operatorname{SVD}_{r}(\\Gamma)$ means choosing the first $r$ components of the matrix $\\Gamma$ . Further note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\Gamma}&{=}&{\\displaystyle\\frac{1}{|S|-1}\\sum_{i\\in S}\\pmb{x}_{i}^{v}(\\pmb{x}_{i}^{l})^{\\top}-\\frac{|S|}{|S|-1}\\bar{\\pmb{x}}^{v}(\\bar{\\pmb{x}}^{l})^{\\top}}\\\\ &{=:}&{P_{0}+P_{1}+P_{2}+P_{3}+P_{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here note that $\\begin{array}{r}{\\Sigma_{S}=\\frac{1}{|S|}\\sum_{i\\in S}z_{i}^{v}(z_{i}^{l})^{\\top}}\\end{array}$ , we have $P_{i}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{P_{0}}}&{{:=}}&{{{\\displaystyle\\frac{|S|}{|S|-1}G_{\\nu}^{*}\\cdot\\Sigma_{S}\\cdot(G_{l}^{*})^{\\top}}}}\\\\ {{P_{1}}}&{{:=}}&{{{\\displaystyle\\frac{1}{|S|-1}G_{\\nu}^{*}\\sum_{i\\in S}z_{i}^{\\nu}(\\xi_{i}^{l})^{\\top}}}}\\\\ {{P_{2}}}&{{:=}}&{{{\\displaystyle\\frac{1}{|S|-1}\\sum_{i\\in S}\\xi_{i}^{\\nu}(z_{i}^{l})^{\\top}(G_{l}^{*})^{\\top}}}}\\\\ {{P_{3}}}&{{:=}}&{{{\\displaystyle\\frac{1}{|S|-1}\\sum_{i\\in S}\\xi_{i}^{(1)}(\\xi_{i}^{(2)})^{\\top}}}}\\\\ {{P_{4}}}&{{:=}}&{{-{\\displaystyle\\frac{|S|}{|S|-1}\\frac{\\bar{x}^{\\nu}}{\\bar{x}^{\\nu}}(\\bar{x}^{l})^{\\top}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It\u2019s clear that the rank of the matrix $P_{0}$ is no more than $r$ , so $\\mathrm{SVD}_{r}(P_{0})=P_{0}$ . And for $i\\in\\{1,2,3,4\\}$ , $P_{i}$ are noise terms with $\\mathbb{E}[P_{i}]=O$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma A.3. For any fixed $S,\\,w.h.p\\,1-\\delta$ the noise term can be upper bounded by $\\sqrt{\\frac{d\\log(1/\\delta)}{|S|}}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. To upper bound the P1 and P2, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\displaystyle\\sum_{i}z_{i}^{v l}(\\xi_{i}^{v l})^{\\top}\\Vert_{*}^{2}=\\mathrm{Tr}\\left(\\displaystyle\\sum_{i,j}\\xi_{i}^{v l}(z_{i}^{v l})^{\\top}z_{j}^{v l}\\xi_{j}^{v l}\\right)=\\displaystyle\\sum_{i,j}(z_{i}^{v l})^{\\top}z_{j}^{v l}(\\xi_{j}^{v l})^{\\top}\\xi_{i}^{v l}}\\\\ &{\\mathbb{E}\\Vert\\displaystyle\\sum_{i}z_{i}^{v l}(\\xi_{i}^{v l})^{\\top}\\Vert_{*}^{2}=\\mathbb{E}\\left[\\displaystyle\\sum_{i}(z_{i}^{v l})^{\\top}z_{i}^{v l}(\\xi_{i}^{v l})^{\\top}\\xi_{i}^{v l}\\right]=|S|d}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Regarding each $(z_{i}^{v l})^{\\top}z_{j}^{v l}(\\xi_{j}^{v l})^{\\top}\\xi_{i}^{v l}$ as weakly dependent variable, then by using Bernstein inequality, we have, with high probability $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\sum_{i}z_{i}^{v l}(\\xi_{i}^{v l})^{\\top}\\|_{*}^{2}\\le|S|d+\\sqrt{d|S|^{2}\\sigma_{\\xi}^{2}\\log(1/\\delta)}\\le|S|d\\sqrt{\\log(1/\\delta)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So $\\begin{array}{r}{\\frac{1}{|S|}\\|\\sum_{i}z_{i}^{v l}(\\xi_{i}^{v l})^{\\top}\\|_{*}\\,\\leq\\,\\sqrt{\\frac{d\\log(1/\\delta)}{|S|}}}\\end{array}$ d lo|gS(|1/\u03b4). Note that \u2225x\u00afvl\u2225 \u2272 $\\|\\bar{\\mathbf{x}}^{v l}\\|\\,\\lesssim\\,\\sqrt{\\frac{\\log(|S|d)}{|S|}}$ (like Proposition 2.5 in Wainwright et al. [48]), it is easy to see that P3 ad P4 are the low order terms if $\\begin{array}{r}{\\delta\\lesssim\\frac{1}{|S|d}}\\end{array}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma A.4 (Intuition behind VAS). With high probability $1-\\delta$ , suppose the hind-side best subset has at least $\\underline{n}$ number of samples, then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta(S)=\\frac{1}{\\rho}\\operatorname*{max}_{S^{\\prime}\\in D_{t r a n}}\\left(\\mathrm{Tr}\\left(\\Sigma_{t a r g e t}(\\Sigma_{S^{\\prime}}-\\Sigma_{S})\\right)\\right)+\\sqrt{\\frac{d\\log(1/\\delta)}{\\underline{{n}}}}+\\sqrt{\\frac{d\\log(1/\\delta)}{|S|}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For any learnt $G_{v},G_{l}$ based on dataset $S$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{test}}(G_{v},G_{l})=\\mathrm{Tr}(G_{v}^{\\top}\\mathbb{E}_{x_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]}G_{l})}\\\\ &{\\qquad\\qquad=\\mathrm{Tr}(\\mathbb{E}_{{\\pmb x}_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]G_{l}G_{v}^{\\top}})}\\\\ &{\\qquad\\qquad=\\frac{1}{\\rho}\\,\\mathrm{Tr}\\left(\\mathbb{E}_{{\\pmb x}_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]G_{l}^{*}\\Sigma_{S}(G_{v}^{*})^{\\top}}\\right)-\\mathrm{Tr}\\left(\\mathbb{E}_{{\\pmb x}_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]{\\bf n o i s e}_{S}}\\right)}\\\\ &{\\qquad\\qquad=\\frac{1}{\\rho}\\,\\mathrm{Tr}\\left((G_{v}^{*})^{\\top}\\mathbb{E}_{{\\pmb x}_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]G_{l}^{*}\\Sigma_{S}}\\right)-\\mathrm{Tr}\\left(\\mathbb{E}_{{\\pmb x}_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]{\\bf n o i s e}_{S}}\\right)}\\\\ &{\\qquad\\qquad=-\\frac{1}{\\rho}\\,\\mathrm{Tr}\\left(\\Sigma_{\\mathrm{tage}}\\Sigma_{S}\\right)-\\mathrm{Tr}\\left(\\mathbb{E}_{{\\pmb x}_{v l}\\sim\\mathcal{D}_{\\mathrm{tage}}[{\\pmb x}^{v}({\\pmb x}^{l})^{\\top}]{\\bf n o i s e}_{S}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here the first equation comes from Theorem A.4 and the third equation comes from Lemma A.2. Consequently, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\underset{S^{\\prime}\\in D_{\\mathrm{toin}}}{\\operatorname*{min}}\\mathcal{L}_{\\mathrm{test}}(A(S^{\\prime}))=\\underset{S^{\\prime}\\in D_{\\mathrm{tran}}}{\\operatorname*{max}}\\left(\\frac{1}{\\rho}\\operatorname{Tr}\\left(\\Sigma_{\\mathrm{target}}\\Sigma_{S^{\\prime}}\\right)+\\operatorname{Tr}\\left(\\mathbb{E}_{\\mathbf{x}_{v}l}{\\sim}\\mathcal{D}_{\\mathrm{urge}}[x^{v}(x^{l})^{\\top}]{\\mathrm{noise}}_{S^{\\prime}}\\right)\\right)}\\\\ &{\\ \\ \\leq\\frac{1}{\\rho}\\underset{S^{\\prime}\\in D_{\\mathrm{tran}}}{\\operatorname*{max}}\\left(\\operatorname{Tr}\\left(\\Sigma_{\\mathrm{target}}\\Sigma_{S^{\\prime}}\\right)\\right)+\\|\\mathbb{E}_{\\mathbf{x}_{v}l\\sim\\mathcal{D}_{\\mathrm{targe}}}[x^{v}(x^{l})^{\\top}]\\|\\|\\mathrm{noise}_{S^{\\prime}}\\|_{*}}\\\\ &{\\ \\ \\leq\\frac{1}{\\rho}\\underset{S^{\\prime}\\in D_{\\mathrm{tran}}}{\\operatorname*{max}}\\left(\\operatorname{Tr}\\left(\\Sigma_{\\mathrm{target}}\\Sigma_{S^{\\prime}}\\right)\\right)+\\mathcal{O}\\left(\\sqrt{\\frac{d\\log(1/\\delta)}{\\underline{{n}}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we have the final result as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(S)=\\mathcal{L}_{\\mathrm{test}}(\\hat{G}_{v l})-\\underset{S^{\\prime}\\in\\mathcal{D}_{\\mathrm{train}}}{\\operatorname*{min}}\\mathcal{L}_{\\mathrm{test}}(A(S^{\\prime}))}\\\\ &{\\qquad=\\frac{1}{\\rho}\\underset{S^{\\prime}\\in\\mathcal{D}_{\\mathrm{train}}}{\\operatorname*{max}}\\left(\\mathrm{Tr}\\left(\\Sigma_{\\mathrm{target}}(\\Sigma_{S^{\\prime}}-\\Sigma_{S})\\right)\\right)+\\mathcal{O}\\left(\\sqrt{\\frac{d\\log(1/\\delta)}{\\underline{{n}}}}+\\sqrt{\\frac{d\\log(1/\\delta)}{|S|}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem A.3 (Main). Under the assumption of Lemma A.1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(S)\\leq n o i s e+\\|\\bar{\\Sigma}_{t a r g e t}-\\Sigma_{t a r g e t}\\|\\|\\Sigma_{S}-\\Sigma_{b e s t}\\|_{*}}\\\\ &{\\qquad+\\left\\{\\epsilon_{v*l}^{S}\\quad(\\nu i s i o n\\!+\\!l a n g u a g e)\\right.}\\\\ &{\\qquad\\left.\\left(\\epsilon_{v}^{S}+\\sqrt{1-\\frac{1}{|S|}\\sum_{i\\in[S]}\\langle z^{v},z^{l}\\rangle)}\\right)\\quad(\\nu i s i o n\\;o n l y)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Based on Lemma A.1, we will focus on the error cause from selecting subset $S$ , that is, $\\mathrm{Tr}\\,\\Sigma_{\\mathrm{target}}\\Sigma_{S}$ . Since the exact $\\Sigma_{\\mathrm{target}}$ is unknown, we assume the access to some proxy $\\bar{\\Sigma}_{\\mathrm{target}}$ instead. ", "page_idx": 19}, {"type": "text", "text": "Recall that, for any $S$ , we have ground-truth $\\Sigma_{S}=\\mathbb{E}_{z_{v l}\\in S}z^{v}(z^{l})^{\\top}$ . Unfortunately, this is not directly observable by the learner. Instead, the learner is able to observe some proxy $\\bar{\\Sigma}_{S}$ based on the teacher model $\\bar{G}_{v l}$ and therefore solving ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underset{S}{\\arg\\operatorname*{max}}\\,\\mathrm{{Tr}}\\left(\\bar{\\Sigma}_{\\mathrm{target}}\\bar{\\Sigma}_{S}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and therefore, denote $\\Sigma_{\\mathrm{best}}=\\arg\\operatorname*{max}_{S^{\\prime}\\in D_{\\mathrm{train}}}\\mathrm{Tr}\\left(\\Sigma_{\\mathrm{target}}\\Sigma_{S^{\\prime}}\\right)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\Gamma}\\big(\\Sigma_{\\mathrm{target}}\\big(\\Sigma_{\\mathrm{best}}-\\Sigma_{S}\\big)\\big)=\\mathrm{Tr}\\left(\\bar{\\Sigma}_{\\mathrm{target}}\\big(\\Sigma_{\\mathrm{best}}-\\bar{\\Sigma}_{S}\\big)\\right)+\\mathrm{Tr}\\left(\\bar{\\Sigma}_{\\mathrm{target}}\\big(\\bar{\\Sigma}_{S}-\\Sigma_{S}\\big)\\right)+\\mathrm{Tr}\\left(\\big(\\Sigma_{\\mathrm{target}}-\\bar{\\Sigma}_{\\mathrm{target}}\\big)\\big(\\Sigma_{\\mathrm{best}}-\\Sigma_{S}\\big)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{Tr}\\left(\\bar{\\Sigma}_{\\mathrm{target}}\\big(\\bar{\\Sigma}_{S}-\\Sigma_{S}\\big)\\right)+\\mathrm{Tr}\\left(\\big(\\Sigma_{\\mathrm{target}}-\\bar{\\Sigma}_{\\mathrm{target}}\\big)\\big(\\Sigma_{\\mathrm{best}}-\\Sigma_{S}\\big)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\Sigma_{\\mathrm{target}}\\|\\|\\bar{\\Sigma}_{S}-\\Sigma_{S}\\|_{*}+\\|\\bar{\\Sigma}_{\\mathrm{target}}-\\Sigma_{\\mathrm{target}}\\|\\|\\Sigma_{S}-\\Sigma_{\\mathrm{best}}\\|_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality is by the definition of $\\bar{\\Sigma}_{S}$ and the second inequality comes from holder\u2019s inequality. Now the key is to upper bound $\\lVert\\bar{\\boldsymbol{\\Sigma}}_{S}-\\boldsymbol{\\Sigma}_{S}\\rVert_{*}$ based on our chosen strategy. ", "page_idx": 19}, {"type": "text", "text": "In option 1, we use the clip embedding from both visual and language modal. That is, choose $\\begin{array}{r}{\\bar{\\Sigma}_{S}\\stackrel{.}{=}\\sum_{\\pmb{x}_{v l}\\in S}(\\bar{G}_{v})^{\\top}\\pmb{x}^{v}(\\pmb{x}^{l})^{\\top}\\bar{G}_{l}}\\end{array}$ . Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\bar{\\Sigma}_{S}-\\Sigma_{S}\\|_{*}\\leq\\frac{1}{|S|}\\|\\sum_{\\substack{{\\boldsymbol x}_{v l}\\in S}}(\\bar{G}_{v})^{\\top}{\\boldsymbol x}^{v}({\\boldsymbol x}^{l})^{\\top}\\bar{G}_{l}-\\sum_{{\\boldsymbol x}_{v l}\\in S}z^{v}(z^{l})^{\\top}\\|_{*}\\leq\\epsilon_{v\\ast l}^{S}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In option 2, we use the clip embedding from language model only. That is choose $\\bar{\\Sigma}_{S}\\ =$ $\\begin{array}{r}{\\sum_{\\pmb{x}_{v l}\\in\\cal S}\\bar{G}_{v}^{\\top}\\pmb{x}^{v}(\\pmb{x}^{v})^{\\top}\\bar{G}_{v}}\\end{array}$ . Then, by definition of $\\epsilon_{S}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\bar{\\Sigma}_{S}-\\Sigma_{S}\\|_{*}\\leq\\displaystyle\\frac{1}{|S|}\\|\\displaystyle\\sum_{x_{v l}\\in S}\\bar{G}_{v}^{\\top}\\boldsymbol{x}^{v}(\\boldsymbol{x}^{v})^{\\top}\\bar{G}_{v}-\\displaystyle\\sum_{\\boldsymbol{x}_{v l}\\in S}z^{v}(\\boldsymbol{z}^{v})^{\\top}\\|_{*}+\\displaystyle\\frac{1}{|S|}\\|\\displaystyle\\sum_{\\boldsymbol{x}_{v l}\\in S}z^{v}(\\boldsymbol{z}^{v})^{\\top}-\\Sigma_{S}\\|_{*}}\\\\ &{\\qquad\\qquad\\leq\\epsilon_{v}^{S}+\\displaystyle\\frac{1}{|S|}\\|\\displaystyle\\sum_{x_{v l}\\in S}z^{v}(\\boldsymbol{z}^{v})^{\\top}-\\Sigma_{S}\\|_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now to further bound the second term, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{|S|}\\|\\displaystyle\\sum_{\\alpha_{v}\\in S}z^{v}(z^{v})^{\\top}-\\Sigma_{S}\\|_{*}\\leq\\displaystyle\\frac{1}{|S|}\\|Z_{v}^{\\top}\\|_{*}\\|Z_{v}-Z_{l}\\|_{*}}&{}\\\\ {=\\displaystyle\\frac{1}{|S|}\\sqrt{\\mathrm{Tr}\\,Z_{v}Z_{v}^{\\top}}\\sqrt{\\mathrm{Tr}(Z_{v}-Z_{l})^{\\top}(Z_{v}-Z_{l})}}&{}\\\\ {=\\displaystyle\\frac{1}{|S|}\\sqrt{\\mathrm{Tr}(I_{n x n})}\\sqrt{2\\,\\mathrm{Tr}\\left(I_{n x n}-Z_{v}Z_{l}^{\\top}\\right)}}&{}\\\\ {=\\displaystyle\\frac{1}{|S|}\\sqrt{2|S|(|S|-\\displaystyle\\sum_{i\\in[S]}z^{v},z^{I}))}}&{}\\\\ {=\\sqrt{1-\\frac{1}{|S|}\\displaystyle\\sum_{i\\in[S]}\\left\\langle z^{v},z^{I}\\right\\rangle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we finish the proof. ", "page_idx": 20}, {"type": "text", "text": "Theorem A.4 (A simplified version of test loss). Under the assumption that both $z_{v l},\\xi_{v l}$ is zero-mean, maximizing the clip score gap is equivalent to maximize the clip score of the same sample. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t a r g e t}(G_{v},G_{l}):=-\\mathbb{E}_{{\\mathbf{x}}_{v l}\\sim\\mathcal{D}_{t a r g e t}}\\langle G_{v}^{\\top}{\\mathbf{x}}_{v},G_{l}^{\\top}{\\mathbf{x}}_{l}\\rangle\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. For any ${\\bf\\Delta}x_{v l}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pmb{x}_{v l}^{\\prime}\\sim\\mathcal{D}_{\\mathrm{taget}}}(\\langle G_{v}^{\\top}\\pmb{x}_{v},G_{l}^{\\top}\\pmb{x}_{l}^{\\prime}\\rangle-\\langle G_{v}^{\\top}\\pmb{x}_{v},G_{l}^{\\top}\\pmb{x}_{l}\\rangle)}\\\\ &{\\ =\\langle G_{v}^{\\top}\\pmb{x}_{v},G_{l}^{\\top}\\mathbb{E}_{\\pmb{x}_{v}^{\\prime}\\sim\\mathcal{D}_{\\mathrm{taget}}}(\\pmb{x}_{l}^{\\prime}-\\pmb{x}_{l})\\rangle}\\\\ &{\\ =-\\langle G_{v}^{\\top}\\pmb{x}_{v},G_{l}^{\\top}\\pmb{x}_{l}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/655c3929273d87859eebaf9b79fccae8bb436a1d9393326792b762f5557ae9b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 4: Illustration of different directions for data selection methods for multimodal contrastive learning. Here we use four colors to denote the four main resources we can obtain: CLIP teacher model, downstream target data (which is much smaller than the external multimodal dataset or pretraining dataset), the external image-text dataset, and the external non-CLIP model. Direction 1 denotes the methods that only use the original OAI CLIP teacher model and the downstream target data. Direction 2 represents the methods that use external datasets to train a new CLIP teacher model for improving filtering, like DFN [2]. Direction 3 denotes the methods that use external non-CLIP model to select the data that may be heuristically helpful for downstream tasks, like image without too much text or be more special. In general, $D I$ method using only CLIP embedding, like negCLIPLoss, is orthogonal to $D2$ . And both D1 and $D2$ can be combined with D3 to explore better flitering results. In the experiments part of the main paper (Sec. 4), we further show that our proposed D1 methods: NormSim and negCLIPLoss, can outperform all the D3 baselines except the best method \u201cHYPE \u222a DFN\u201d. And we can achieve the new state-of-the-art by combining our methods with that method. ", "page_idx": 21}, {"type": "text", "text": "B Illstration of Different Directions for Data Selection in Multimodal Contrastive Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We summarize our main idea of categorizing the current top data selection methods in Figure 4. ", "page_idx": 21}, {"type": "text", "text": "C Details of Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Computation Cost ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our algorithm can significantly reduce the computational cost compared to many existing works as shown in Table 5. For example, when the CLIP embeddings are obtained (cost about 50 hours for CLIP-B/32), both T-MARS [12] and MLM [42] still require more than 900 hours data pre-processing time to extract the required information from 110M size dataset of DataComp-medium, while we only need about 5 hours. On the other hand, DFN, although has a similar forward speed (i.e. preprocessing time), requires retraining a new CLIP teacher model on the HQITP-350M, which is larger than DataComp-medium. ", "page_idx": 21}, {"type": "text", "text": "We give some details in estimating the preprocessing time of other methods: ", "page_idx": 21}, {"type": "text", "text": "\u2022 For T-MARS and $\\mathbb{D}^{2}$ pruning, we run their official code on DataComp-small (11M) data, and simply scale the preprocessing time by 10 for DataComp-medium, given that the preprocessing time for T-MARS is proportional to the size of the pretraining dataset, while $\\bar{\\mathbb{D}}^{2}$ pruning is no faster than linear. ", "page_idx": 21}, {"type": "text", "text": "Table 5: Comparison of preprocessing time and external resources needed between our method and other D3 category methods. We skip DFN since it\u2019s orthogonal to our negCLIPLoss method and we can directly improve it as mentioned in Table 1. Here since all the baselines below except MLM use a pretrained CLIP model, we only count the time that doesn\u2019t contain that for inferring CLIP image/text embeddings (about $50\\,{\\mathrm{L}}40$ hours for OAI CLIP-B/32), which is also adopted in DataComp benchmark [1]. The external dataset corresponds to the external multimodal dataset used for training or finetuning the external model. Notably, the preprocessing time for the following methods are all approximately linearly proportional to the amount of unfiltered pretrained dataset. ", "page_idx": 22}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/2870d0fb0b29c8c294fbcfb4565f49f4eb01331134e6af4e440faf0a12e3b1f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "\u2022 For MLM, we get the estimated time from their paper. They mention that they need 6.1 minutes to process 10k samples on A100, which results in 1120 A100 hours for our dataset (110M). We need to mention that their estimation time of calculating CLIP embedding is inaccurate and we can do it much faster than their claim using the DataComp pipeline. ", "page_idx": 22}, {"type": "text", "text": "\u2022 For Devil, it needs to run the $\\mathbf{k}$ -means clustering algorithm from the faiss library on the embedding space, which is estimated to cost $120\\,{\\mathrm{L}}40$ hours on DataComp-medium. Using BLIP-2 [16] to scan the whole dataset will need about 470 A100 hours from the experimental details in [17]. From https://lambdalabs.com/gpu-benchmarks, we roughly assume that 120 L40 hours are at least comparable to 40 A100 hours for K-means clustering. ", "page_idx": 22}, {"type": "text", "text": "\u2022 For HYPE, they claim that MERU is as efficient as CLIP, but they still need at least 120 L40 hours for processing 110M data for their final score, since it uses the image embedding clusters on DataComp-medium obtained from running $\\mathrm{k}$ -means clustering algorithm. ", "page_idx": 22}, {"type": "text", "text": "C.2 Details of negCLIPLoss ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We give the pseudocode of calculating negCLIPLoss in Algorithm 1, which is specially designed for pytorch-style parallel matrix calculation. It can be fully accelerated and the computation cost introduced by the normalization term is negligible compared with the training time or preprocessing time of other top baselines as detailed in Table C.1. ", "page_idx": 22}, {"type": "text", "text": "In negCLIPLoss, we need to get the batch size $|B|$ and the value of the learnable temperature parameter $\\tau$ at the final step of the teacher model pretraining stage. For OAI CLIP-L/14 and OAI CLIP-B/32, these values are $\\tau=0.01$ and $|B|=3\\bar{2}768$ . ", "page_idx": 22}, {"type": "text", "text": "We also have an ablation study about the temperature parameter and batch size chosen for CLIP teacher models as shown in Table 6. We will see that in general, a larger batch size will result in better performance, and $\\tau=0.01,b=32768$ is the best choice for both OAI CLIP-B/32 and DFN-P. The reason for such a batch size is that a larger batch can contain more contrastive data pairs, which is also supported by the concentration result of the normalization term proved in Appendix A.1, and thus it can check the image-text matching between more different data. Therefore, we always consider the largest batch size 32768 which can fti into a single 24G GPU in the CLIP forward pass, which is also the OAI CLIP training batch size. ", "page_idx": 22}, {"type": "text", "text": "C.3 Details of NormSim $^2$ -D ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we illustrate the details of our NormSim2-D algorithm. The top- $N$ selection method is aiming to achieve the object: ", "page_idx": 22}, {"type": "equation", "text": "$$\nS=\\arg\\operatorname*{max}_{|S|=N}\\sum_{i\\in S}\\bar{f}_{v}(x_{i}^{v})^{\\top}\\left(\\frac{1}{|X_{\\mathrm{target}}|}\\sum_{x_{t}\\in X_{\\mathrm{target}}}\\bar{f}_{v}(x_{t}^{v})\\bar{f}_{v}(x_{t}^{v})^{\\top}\\right)\\bar{f}_{v}(x_{i}^{v})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Table 6: Ablation study about the temperature parameters $\\tau$ and batch size $b$ for CLIP teacher model. The values obtained from the last training step of the teacher models are $\\tau=0.01$ , $b=32768$ for OAI CLIP-B/32, OAI CLIP-L/14, and $b=16384$ , $\\tau=0.07$ for DFN-P. In the main paper, we use $b=32768$ , $\\tau=0.01$ for all three kinds of teacher models. ", "page_idx": 23}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/8af1ccc80ca4c80400848713930f7fab2da1ae6cf6f8b51a6963df0f04d5cef5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "when the actual $X_{\\mathrm{target}}$ is unknown. In practice, removing one data at a time is too slow. Therefore, we remove a batch of data for every step. In detail, if the number of steps is $\\tau$ , and let $\\bar{\\Sigma}_{\\mathrm{test},i}\\,=$ $\\begin{array}{r}{\\frac{1}{|S_{i}|}\\sum_{j\\in S_{i}}\\bar{f}_{v}(\\pmb{x}_{j}^{v})\\bar{f}_{v}(\\pmb{x}_{j}^{v})^{\\top}}\\end{array}$ where $S_{i}$ is the selected subset at step $i$ , then we will remove the data satisfies the following equation step-by-step until reaching the final subset size: ", "page_idx": 23}, {"type": "equation", "text": "$$\nS_{i}\\setminus S_{i+1}=\\arg\\operatorname*{min}_{x_{l}\\in S_{i}}\\left[\\bar{f}_{v}(x_{l}^{v})^{T}\\cdot\\left(\\frac{1}{\\left|S_{i}\\right|}\\sum_{x_{t}\\in S_{i}}\\bar{f}_{v}(x_{t}^{v})\\bar{f}_{v}(x_{t}^{v})^{\\top}\\right)\\cdot\\bar{f}_{v}(x_{l}^{v})\\right],\\quad i\\in\\{0,\\ldots,\\tau-1\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we can detail the algorithm process of $\\mathrm{{NormSim}_{2}.}$ -D in Algorithm 2. In general, the smaller the step size, the better the results. But in experiments, we find that it\u2019s already enough to get good results when $\\tau=500$ . ", "page_idx": 23}, {"type": "text", "text": "C.4 Details of Related Works ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We add some details about the baselines used in our paper as follows. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Text-based filtering. [1] proposes a text-based filtering that tries to select the data that contains caption overlapping with the class name from ImageNet-21K or ImageNet-1K. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Image-based filtering. [1] also proposes a heuristic way to sample the visual content overlaps with ImageNet-1K classes. They first apply flitering by language (only choose English caption by fasttext [15]) and caption length (over two words and 5 characters). Then they cluster the image embeddings from training data to 100K groups using Faiss [49], and keep the groups whose cluster center is the nearest neighbor to at least one image embedding of ImageNet-1K image. ", "page_idx": 23}, {"type": "text", "text": "Inputs: image/text embeddings of the pretraining data $F^{v l}=[\\{\\bar{f}_{v l}(x_{1}^{v l})\\},\\ldots,\\{\\bar{f}_{v l}(x_{N}^{v l})\\}]^{\\top}\\in$   \n$\\mathbb{R}^{N\\times d}$ , batch size $b$ , temperature parameter $\\tau$ , the number of times negCLIPLoss is random   \n$K(=10)$ .   \nInitialize negCLIPLoss array $\\pmb{r}=[0,\\dots,0]\\in\\mathbb{R}^{N}$   \nfor $k=1$ to $K$ do Get a random batch division $S_{k}=\\{B_{1},\\ldots,B_{s}\\}$ such that $s=\\lceil N/b\\rceil$ . Every $B_{i}\\in S_{k}$ is the index of a batch of data. for $j=1$ to s do Get batch of embeddings in batch $j$ : $F_{j}^{v l}=F^{v l}[B_{j}]\\in\\mathbb{R}^{b\\times d}$ Get the similarity matrix: $E_{j}=F_{j}^{v}(F_{j}^{l})^{\\top}\\in\\mathbb{R}^{b\\times b}$ Get the CLIPScores: $\\pmb{c}_{j}=\\mathrm{diag}(E_{j})\\in\\mathbb{R}^{b}$ Define $G_{j}=\\exp(E_{j}/\\bar{\\tau})$ Define $\\pmb{g}_{j}^{v}\\in\\mathbb{R}^{b}$ be the vector containing the sum of each row vector in $G_{j}$ (i.e., over image). Define $\\pmb{g}_{j}^{l}\\in\\mathbb{R}^{b}$ be the vector containing the sum of each column vector in $G_{j}$ (i.e., over text). Get the negCLIPLoss: $r[B_{j}]=\\pmb{c}_{j}-0.5\\tau\\cdot(\\log(\\pmb{g}_{j}^{v})+\\log(\\pmb{g}_{j}^{v}))$ , here we use element-wise operation. end for   \nend for   \nTake the mean of each random division as output: $\\mathrm{negCLIPLoss}=r/K$ ", "page_idx": 24}, {"type": "text", "text": "Algorithm 2 NormSim-D strategy ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Inputs: image embeddings of the data after CLIP score filtering $\\{\\bar{f}_{v}(x_{i}^{v})\\}_{i\\in S}$ , target size $N$ ,   \nnumber of steps $\\tau$   \nInitialize $S_{0}=S,N_{0}=|S|$   \nfor $t=1$ to $\\tau$ do Size at step $\\begin{array}{r}{t:N_{t}=N_{0}-\\frac{t}{\\tau}(N_{0}-N)}\\end{array}$ . Prior matrix: $\\begin{array}{r}{\\bar{\\Sigma}_{\\mathrm{test},t-1}=\\sum_{j\\in S_{t-1}}\\bar{f}_{v}(x_{j}^{v})\\bar{f}_{v}(x_{j}^{v})^{\\top}}\\end{array}$ Updated $\\mathrm{{NormSim}_{2}.}$ -D for each sample $i$ in $S_{t-1}$ : $\\begin{array}{r}{\\mathrm{NormSim_{2}-D}(x_{i})=\\bar{f}_{v}(x_{i}^{v})^{\\top}\\cdot\\bar{\\Sigma}_{\\mathrm{test},t-1}\\cdot\\bar{f}_{v}(x_{i}^{v})}\\end{array}$ Construct $S_{t}$ such that it contains the data with highest $\\mathrm{NormSim_{2}{\\cdot}D}$ in $S_{t-1}$ and satisfies $|S_{t}|=N_{t}$ .   \nend for ", "page_idx": 24}, {"type": "text", "text": "\u2022 $\\mathbb{D}^{2}$ Pruning. [18] tries to represent the dataset as an undirected graph for coreset selection. They assign the difficulty for each example and use message passing to update the difficulty score incorporating the difficulty of its neighboring examples, and finally try to keep both diverse and difficult subsets. For our experiments, we adhere to the default hyperparameters of $\\mathbb{D}^{2}$ on DataComp as specified in their official codebase. ", "page_idx": 24}, {"type": "text", "text": "\u2022 T-MARS [12] uses a text detection model like FAST [13] to fliter out the data that only contain the texts of caption in the image and don\u2019t have other useful image features. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Devils [14] combines many ways for data filtering. At the very first it filter data based on heuristic rules like text length, frequency of texts, and image size, and it also use CLIPScore for cross-modality matchment. Then it adopts target distribution alignment methods similar to image-based filtering, but instead of using ImageNet-1k only, it uses 22 downstream tasks as the target set. Further, it adopts external models fasttext [15] to remove non-English captions and image-captioning model BLIP-2 [50] to select images with MNIST-style digits. ", "page_idx": 24}, {"type": "text", "text": "\u2022 MLM [42] prompts GPT-4V to construct instruction data including the image-text data, and use it to fine-tune a smaller vision-language model like LLaVA-1.5 [43, 44] into a filtering network. Nevertheless, the number of parameters of LLaVA-1.5 is still much larger than CLIP, and thus LLaVA-1.5 has a much longer preprocessing time as mentioned in Table C.1. ", "page_idx": 24}, {"type": "text", "text": "C.5 How to Choose Hyperparameters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The main hyper-parameters of our negCLIPLoss and NormSim are the target numbers for filtering (refer to Appendix C.2 for the setting of temperature and batch size), which is also the main concerns for all the top baselines like DFN, MLM, and T-MARS. In the case of DataComp settings, noting that all the top baselines in DataComp-medium benchmark keep the downsampling ratios ranging from $15\\%$ $30\\%$ to achieve the best results, we can set the sampling ratio as some previous baselines. Our method with OAI CLIP teacher model first selects the data with the top $30\\%$ negCLIPLoss, and then selects the top $66.7\\%$ NormSim scores to keep $20\\%$ of the original pool. We don\u2019t tune the target size carefully here for fair comparison. ", "page_idx": 25}, {"type": "text", "text": "In more general cases, we can recommend some training-dataset-independent thresholds for NormSim, since the scores only depends on the norm $p$ and target data rather than other data in the pool. We recommend to set the threshold as 0.7 for $\\mathrm{NormSim}_{\\infty}$ (Target) and 0.15 for NormSim2(IN-1k) in general. On the other hand for negCLIPLoss, note that like NormSim, CLIPScore is also trainingdataset-independent, we recommend to first find the percentile of the data with CLIPScore $_{=0.21}$ , and then downsample the dataset using CLIPLoss until that particular percentile. ", "page_idx": 25}, {"type": "text", "text": "Overall, finding optimal filtering ratio for data selection algorithm is always difficult and out of the scope of this paper. From the paper about the scaling law for data filtering [51], downsampling size even depends on the computation budget. When you have more budget, you should sample more data for learning. And thus another possible solution is to use their ftiting formula to get some recommended downsampling ratios. ", "page_idx": 25}, {"type": "text", "text": "At last, we also note that in data selection problem, visualization is a simple but effective way for tuning parameters or finding downsampling ratios. People can first randomly select a small subset (like 1000 data) on some pretraining data subset, and then calculate the target scores (CLIPScore, negCLIPLoss, NormSim or any other metrics) on them, and fianlly visualize the data corresponding to scores at different percentiles, like top $10\\%$ , $30\\%$ , $50\\%$ and $70\\%$ of the negCLIPLoss. In this way, we can determine the threshold of flitering directly by observating the data. We also give some visualization examples of our methods in Appendix E, We believe this is an effective way to give some guidance on how to roughly select the initial downsampling ratios. ", "page_idx": 25}, {"type": "text", "text": "C.6 Discussion of NormSim ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "C.6.1 How NormSim $^{\\cdot2}$ Connects to Selecting the Data in Principal Components. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For convenience, we let $f(x_{t})$ denote the image embedding of the target data $x_{t}\\in X_{T}$ , and $f(x_{s})$ denotes the image embeddings of training data $x_{s}\\in X_{S}$ . Then the definition of NormSim on a data $x_{s}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{NormSim}_{p}(X_{T},x_{s})=\\left(\\sum_{x_{t}\\in X_{T}}[f(x_{t})^{\\top}f(x_{s})]^{p}\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then when $p=2$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{NormSim}_{2}(X_{T},x_{s})=}&{}&{\\displaystyle\\left(\\sum_{x_{t}\\in X_{T}}[f(x_{s})^{\\top}f(x_{t})]\\cdot[f(x_{t})^{\\top}f(x_{s})]\\right)^{1/2}}\\\\ {=}&{}&{\\displaystyle\\left(f(x_{s})^{\\top}\\cdot\\sum_{x_{t}\\in X_{T}}[f(x_{t})f(x_{t})^{\\top}]\\cdot f(x_{s})\\right)^{1/2}}\\\\ {\\propto}&{}&{\\displaystyle\\left[f(x_{s})^{\\top}\\left(\\frac{1}{|X_{T}|}\\sum_{x_{t}\\in X_{T}}f(x_{t})f(x_{t})^{\\top}\\right)f(x_{s})\\right]^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\Lambda=\\frac{1}{|X_{T}|}\\sum_{x_{t}\\in X_{T}}f(x_{t})f(x_{t})^{\\top}}\\end{array}$ is the variance matrix of the target image embeddings. Then using $\\mathrm{{NormSim}_{2}}$ for filtering, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle{S=\\arg\\operatorname*{max}_{|S|=N}\\sum_{x_{s}\\in X_{S}}\\mathrm{NormSim}_{2}(X_{T},x_{s})}}\\\\ &{\\mathrm{NormSim}_{2}(X_{T},x_{s})=f(x_{s})^{\\top}\\cdot\\Lambda\\cdot f(x_{s})}\\\\ &{\\displaystyle{=f(x_{s})^{\\top}U\\cdot S\\cdot U^{\\top}f(x_{s})}}\\\\ &{\\displaystyle{=\\sum_{j=1}^{r}s_{j}\\cdot[f(x_{s})^{\\top}u_{j}]^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here $\\Lambda=U S U^{\\top}$ is the eigen decoposition of $\\Lambda$ , where $S=\\mathrm{diag}{(s_{1},\\ldots,s_{r})}$ with $s_{1}>...>s_{r}$ are the matrix of eigenvalues, and $U\\,=\\,[u_{1},\\dots,u_{r}]\\,\\in\\,\\mathbb{R}^{d\\times r}$ are the corresponding eigenvectors (i.e., the principal component directions). Note that the column vectors of $U$ and $f(x_{s})$ are all unit vectors, (24) shows that $\\mathrm{{NormSim}_{2}}$ select the data that match with the principal components, i.e., eigen directions $u_{j}$ with large eigen values $s_{j}$ . ", "page_idx": 26}, {"type": "text", "text": "C.6.2 Why NormSim works well without explictly considering data diversity. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We answer this question by the following reasons: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Many top baselines, such as DFN and T-MARS, also don\u2019t explicitly consider diversity, yet they still provide good performance. Devil even shows that valuable data is worth sampling multiple times, which they call \u201cquality duplication\u201d. Therefore, one important reason why NormSim works well without explicitly considering diversity may be that when the computing budget is limited, as in the DataComp benchmark, the model first needs to learn the most useful and representative data, which should be similar to some target data.   \n\u2022 Moreover, we chose validation data from 24 downstream tasks ranging from ImageNet to EuroSet, which may have covered a sufficiently diverse range of target examples for NormSim to calculate similarity. The diversity of the target data will consequently result in the diversity of the selected subset. And this also implies the importance of selecting a good target dataset.   \n\u2022 An additional reason may be that our proposed negCLIPLoss already implicitly selects more diverse data, as shown in Figure 1 of the main paper. If some training data are diverse, they will match less with other data and thus have a lower normalization term. This results in a larger negCLIPLoss and a higher probability of being sampled. ", "page_idx": 26}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Stability Analysis of Batch Sampling Numbers in negCLIPLoss ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We show that negCLIPLoss is not sensitive to the number of random select batches $K$ in Figure 5. ", "page_idx": 26}, {"type": "text", "text": "D.2 Universality of negCLIPLoss over Different Teacher Models ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We show the complete results of applying our methods to different teacher models like OAI CLIP-B/32 and DFN-P in Table 7. Detail descriptions are in Sec. 4. ", "page_idx": 26}, {"type": "text", "text": "D.3 $\\mathbf{NormSim}_{\\infty}$ is Better than Nearest Neighbor Selection ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We also try to use near-neighbor selection for aligning downstream distribution. Here, we calculate the ranks of pretraining data for each target (the higher the rank, the higher the similarity), and then for each pre-train data, we keep its highest rank. Finally, we select the data with the highest ranks as the nearest neighbor selected subset. ", "page_idx": 26}, {"type": "text", "text": "In Table 8, we show that given the training data of 22 downstream tasks, our $\\mathrm{NormSim}_{\\infty}$ can outperform near neighbor selection under the same downsampling ratio. The reason may be that the distribution between the target and pretraining set is not well aligned, so if you force the algorithm to find the nearest train data for each target, that train data may be sometimes random and not helpful. On the other hand, $\\mathrm{NormSim}_{\\infty}$ will not select this kind of data. It will select the data whose best similarity score exceeds some general threshold, rather than just consider ranks. ", "page_idx": 26}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/dad21655689927eca6d548c773035cfd4804ef85bd9d80d16fc3c92ddb98acb6.jpg", "img_caption": ["Comparison of Methods Across Different Metrics ", "Figure 5: Results of negCLIPLoss with a different number of batch samples (denoted as $K$ ) on DataComp-medium. Solid lines denote negCLIPLoss, while dashed lines denote CLIPScore. Here, we use OAI CLIP-L/14 as the pretrained model. We can see that once $K\\,\\geq\\,5$ , negCLIPLoss consistently outperforms CLIPScore across all subtask metrics. In the main paper, we set $K=10$ . "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/72486ead1253a91e4e411827eae1c03e4cda228dc966ef50144771a1734cac9c.jpg", "table_caption": ["Table 7: Results on DataComp-medium from the top methods that use only OpenAI\u2019s CLIP-B/32 model or public version of DFN (DFN-P). "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "D.4 Vision-Only NormSim is Better than Using Both Vision and Language ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In DataComp [1], they show that image-based flitering is better than text-based flitering. In our paper, we also do an ablation study to support this. Due to the restriction of computation resources, we run our $\\mathrm{NormSim_{2}(I N{-}1k)}$ and $\\mathrm{{NormSim}_{2}}$ -D on DataComp-small as an example. Since ImageNet-1k ", "page_idx": 27}, {"type": "text", "text": "Table 8: Comparison between $\\mathrm{NormSim}_{\\infty}$ and nearest neighbor selection. We use OAI CLIP-L/14 as the teacher model and assume both methods have been intersected with negCLIPLoss $(30\\%)$ . The size of the selected subset is 22M. ", "page_idx": 28}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/39d5fb76b15d6162e3c404084617c71275b639261d12346e5778106fd51a24f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "only has labels rather than long texts for describing images, we need to generate the caption before calculating $\\mathrm{NormSim_{2}(I N}$ -1k). We select 80 templates as the original CLIP paper [4], generate prompts for each class, and take the mean of their embeddings as the representative text embedding for images within that class. ", "page_idx": 28}, {"type": "text", "text": "The results are in Table 9. We can see that for both metrics, we have \u201cimage only\u201d $>$ \u201cimage $\\times$ text\u201d $>$ \u201ctext only\u201d. We believe the reason for $\\mathrm{{NormSim}_{2}(}$ (IN-1k) is that the images themselves can convey significantly more features than the text prompts generated by labels. For NormSim2-D, it should be related to the large amounts of low-quality captions in the web-curated dataset. And \u201cimage $\\times$ text\u201d will also be influenced by the informativeness and the quality of captions. In short, for NormSim, using vision-only embeddings is a best choice. ", "page_idx": 28}, {"type": "text", "text": "Table 9: Ablation Study on the NormSim and its variants on DataComp-small (11M). All experiments first select $45\\%$ data based on the CLIP score, then use corresponding approaches to obtain $3.3\\mathbf{M}$ data.\u201cimage\u201d or \u201ctext\u201d means using the variance of image or text embeddings to represent $\\bar{\\Sigma}_{\\mathrm{target}}$ , and \u201cimage $\\times$ text\u201d means representing $\\bar{\\Sigma}_{\\mathrm{target}}$ with the cross-covariance of image and text embeddings. ", "page_idx": 28}, {"type": "table", "img_path": "qvdc0oCX2n/tmp/6928df77c18af951a83379bce2ed65c435a282e2dd7e506744cfb8ca4195fd9e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "E Additional Visualization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We further visualize8 more data with different negCLIPLoss in Figure 6, 7 and 8. And similar for NormSim $\\infty$ (Target) in Figure 9, 10 and 11. ", "page_idx": 28}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/962c497a067dcce7d036dfc6549fc909f99cc966e3c0290e117d0f18990835f2.jpg", "img_caption": ["Figure 6: Visualization of a small subset whose negCLIPLoss rank top $100\\%$ high in DataCompmedium. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/20fbb2e1297ce05153fcb1792d7624d34599cdfb7c5c9e420a6124118e8be567.jpg", "img_caption": ["Figure 7: Visualization of a small subset whose negCLIPLoss rank top $50\\%$ high in DataCompmedium. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/18c91acc6750374302a1fb9186d49099b4f846826a33b43c1d232d09532b3c52.jpg", "img_caption": ["Figure 8: Visualization of a small subset whose negCLIPLoss rank top $10\\%$ high in DataCompmedium. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/3d9d8c609af29f9a159ada4457dd1fc4f96339f1930b406bc2fd208c2c54f0b4.jpg", "img_caption": ["Figure 9: Visualization of the images from a small subset whose NormSim $\\infty$ (Target) rank top $100\\%$ high in DataComp-medium. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/9bc55b311f1a4c48d623b3eacba14f2085822bd25c659e7c59d6317fd5a8c902.jpg", "img_caption": ["Figure 10: Visualization of the images from a small subset whose $\\mathrm{NormSim}_{\\infty}$ (Target) rank top $50\\%$ high in DataComp-medium. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "qvdc0oCX2n/tmp/e782e1a83a95a3cc7b8583ed967ce81657b1027e7c9c637ef0be181a69b137f1.jpg", "img_caption": ["Figure 11: Visualization of the images from a small subset whose NormSim $\\infty$ (Target) rank top $10\\%$ high in DataComp-medium. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes we clearly define 1. the benchmark we are using; 2.the methods with its key insights 3.the empirical improvement. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We discuss this briefly in the last section. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The full version of theory of NormSim results are in Appendix. A and we provide all the assumptions and proofs. We briefly mentioned this in Sec. 3.2. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The main results are in the Sec. 4. We also provide experiment details in Appendix C. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The code will be provided according to Neurips code submission guidance.   \nAfter got accepted, we will open source that. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The main results are in the Sec.4. We also provide experiment details in Appendix C ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: Almost all existing works, like DFN, HYPE, and MLM, only run the training once on DataComp-medium. Training on a 128M size dataset is very costly and relatively stable, so it is commonly believed that there is no need to rerun experiments with different training seeds. In the experiments, we fix all the training seeds to be 0 for fair comparison. For our algorithm, most of them are deterministic. The only one involving randomness is negCLIPLoss, which requires resampling $K{=}10$ times. For it we provide a sensitivity analysis in Fig. 5. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We discuss the computing cost estimation and comparison in Appendix C.1. We didn\u2019t explicitly calculate the memories since it is quite standard under the DataComp benchmark. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This research focuses on the methodology part of data selection. All experiments are performed under the existing standard dataset. So as long as those datasets itself maybe harmless, our research will not make any negative impact. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper will only provide UID\u2019s of selected data from existing datasets (DataComp-medium [1]). This paper will not release any model or new dataset. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We cite the DataComp [1] which introduces the URL for the dataset and the code/models used to implement the benchmark. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects.   \nAll metrics are fixed evaluations. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]