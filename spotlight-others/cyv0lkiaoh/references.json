{"references": [{"fullname_first_author": "J. Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a significant technical report on a large language model, providing context for the field of generative models and their capabilities."}, {"fullname_first_author": "S. Alemohammad", "paper_title": "Self-consuming generative models go MAD", "publication_date": "2024", "reason": "This paper is highly relevant to the topic of self-consuming generative models and their potential issues which is the central theme of the current research."}, {"fullname_first_author": "Q. Bertrand", "paper_title": "On the stability of iterative retraining of generative models on their own data", "publication_date": "2024", "reason": "This paper directly addresses the stability issues of self-consuming generative models providing a foundation for understanding the current work's contributions on stability."}, {"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020", "reason": "As a foundational paper on diffusion models, it provides the theoretical background for many of the generative models used in the experiments."}, {"fullname_first_author": "A. Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021", "reason": "This is an important contribution to the field of text-to-image generation, setting a benchmark for many of the generative models used in the experiments and providing background for data curation strategies."}]}