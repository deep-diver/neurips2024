[{"type": "text", "text": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuxin Du1,2, Fan Bai1,2,3, Tiejun Huang2,4, Bo Zhao1,2\u2020 ", "page_idx": 0}, {"type": "text", "text": "Shanghai Jiao Tong University 2Beijing Academy of Artificial Intelligence 3The Chinese University of Hong Kong 4Peking University \u2020Corresponding author: Bo Zhao <bo.zhao@sjtu.edu.cn> ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that $\\mathrm{SegVol}$ outperforms the competitors in 19 tasks, with improvements up to $37.24\\%$ compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at: https://github.com/BAAI-DCAI/SegVol. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Volumetric medical segmentation, involving extracting 3D regions of interest, such as organs, lesions, and tissues, plays a pivotal role in medical image analysis by accurately modeling the 3D structural information of the human body from volumetric medical images such as CT or MRI. The accurate segmentation can benefti numerous clinical applications including tumors monitoring[1, 2], surgical planning[3, 4], disease diagnosis[5], therapy optimization[6, 7], etc. ", "page_idx": 0}, {"type": "text", "text": "Compared to 2D medical image segmentation[8, 9, 10, 11, 12, 13, 14, 15, 16, 17], volumetric image segmentation is notably more challenging due to the labor-intensive annotation and resourceconsuming computation. Recently, the research of volumetric medical image segmentation has garnered substantial attention, leading to a series of advancements[18, 19, 20, 21, 22, 23]. However, existing volumetric medical segmentation methods have several key limitations which prevent their application in challenging tasks, e.g., liver tumor or colon cancer segmentation[24, 25, 26, 27], and real-world tasks, e.g., human-interactive segmentation[28, 29, 30, 31, 32]. ", "page_idx": 0}, {"type": "text", "text": "Firstly, the publicly available volumetric medical image datasets usually consist of a small number of mask annotations from a few varying categories. Due to the different label spaces, the traditional task-specific segmentation models trained on one dataset have difficulty in generalizing to others. For example, the CT-ORG dataset[33, 34, 24, 35] contains the \u2018lungs\u2019 category, while this category is split into two sub-classes and named \u2018left lung\u2019 and \u2018right lung\u2019 in the LUNA16 dataset[36]. Hence, a universal segmentation model has to understand the semantics of anatomical categories. Secondly, traditional segmentation models have inferior performance when segmenting complex structures, such as tumors and cysts[37]. This is because these models are trained on insufficient data and are also not able to leverage the spatial information through user interaction. Thirdly, previous solutions are computationally expensive in the inference process. They typically employ a sliding window to infer the whole volumetric input. This strategy is not only time-consuming but also short-sighted, as the sliding window contains only local information. Recently, there have been some works[29, 38, 39] that introduce spatial-prompt into medical image segmentation, shown in Table 1. However, most of them lack the ability to process the 3D input directly and naturally, and none of them is able to understand the semantics of anatomical categories. ", "page_idx": 0}, {"type": "image", "img_path": "105ZuvpdyW/tmp/49c166c01e5c4e662654fb71cfbf8f3486bc2b80df0e456694e155f2f2cac1ff.jpg", "img_caption": ["Figure 1: Overview of $\\mathrm{SegVol}$ model architecture. $\\mathrm{SegVol}$ produces precise segmentation of 3D anatomical structures from volumetric inputs with easy user interactions, including point, bounding box, and text prompts. Zoom-out-zoom-in mechanism: SegVol initially produces a rough prediction mask with zoom-out inference, then refines it with zoom-in inference on the identified ROI. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose the first foundation model for volumetric medical image segmentation \u2013 SegVol. The proposed model enables universal and interactive 3D segmentation of more than 200 anatomical categories, supporting both spatial and semantic prompts. SegVol can also be driven by the combination of multi-prompt, like \u2018bounding box+text\u2019 or \u2018point+text\u2019 prompts, achieving highprecision segmentation and semantic disambiguation. To enable efficient and precise segmentation of volumetric images, we develop a zoom-out-zoom-in mechanism that enables the model to be efficient and precise. We evaluate the proposed SegVol on 22 volumetric medical image segmentation tasks and the results demonstrate our method surpasses other SAM-like interactive segmentation methods[28, 38, 39, 29] by a large margin. Extensive case studies and ablation experiments are also carried out to prove the advantages of SegVol and the effectiveness of the zoom-out-zoom-in mechanism and multi-prompt combination. ", "page_idx": 1}, {"type": "text", "text": "We summarize our key contributions as follows: ", "page_idx": 1}, {"type": "text", "text": ". Collect and process 25 public volumetric medical segmentation datasets, encompassing over 200 anatomical categories. The pseudo label is introduced to relieve the spurious correlation in the training data. ", "page_idx": 1}, {"type": "table", "img_path": "105ZuvpdyW/tmp/583ed571ec01414c8c42c79569bfecbb85e524938f798eb4a7e2e5378118073c.jpg", "table_caption": ["Table 1: The different settings and functions of SAM-like interactive segmentation methods "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2. Implement massive 3D pre-training on 96K CT volumes and supervised fine-tuning on the 6k labeled datasets.   \n3. Support spatial-prompt, semantic-prompt, and combined-prompt segmentation, achieving high-precision segmentation and semantic disambiguation.   \n4. Design a zoom-out-zoom-in mechanism that significantly reduces the computational cost, meanwhile preserving precise segmentation. ", "page_idx": 2}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Dataset Construction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One of the main challenges of training a universal volumetric medical segmentation model is the absence of large-scale publicly available volumetric medical data, especially CTs with segmentation annotations. Doing our utmost, we collected 25 open-source segmentation CT datasets, including CHAOS[40, 41, 42], HaN-Seg[43], AMOS22[44], AbdomenCT-1k[45], KiTS23[46], KiPA22[47, 48, 49, 50], KiTS19[51], BTCV[52], Pancreas-CT[53, 54, 35], 3D-IRCADB[55], FLARE22[56, 57], TotalSegmentator[58], CT-ORG[33, 34, 24, 35], VerSe19, VerSe20[59, 60, 61], SLIVER07[62], QUBIQ[63], six MSD datasets[56], LUNA16[36], and WORD[64]. Their detailed information and availability are shown in the Section A. These CTs originate from various medical institutions, captured by different machines with varying parameter settings and scanning regions. To standardize these datasets, we use the mean voxel value of each volume to fliter the background and then perform normalization on the foreground voxels. ", "page_idx": 2}, {"type": "text", "text": "Volumetric segmentation datasets suffer from the notorious problem of partial labels. Most of these datasets have annotations of only a few segmentation targets, e.g., several organs. Therefore, the deep models may learn the spurious correlation between datasets and segmentation targets, and thus produce inferior results during the inference phase. To relieve this problem, we introduce the pseudo labels by utilizing the Felzenswalb-Huttenlocher (FH)[65] algorithm to generate pseudo masks for each CT scan. Pseudo masks can supplement unlabeled categories in a dataset, therefore relieving the spurious correlation problem. To restrain the noise and numerous tiny masks in pseudo labels, we employ the following strategies: 1) The pseudo masks are replaced with ground truth masks when applicable. 2) We fliter out tiny structures smaller than $1\\%o$ of the whole volume size. 3) Each mask is refined by dilation and erosion operations. ", "page_idx": 2}, {"type": "text", "text": "2.2 Model Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Motivated by the recent advance in 2D nature image segmentation, Segment Anything (SAM)[28], we design a novel model for interactive and universal volumetric medical image segmentation, named, $\\mathrm{SegVol}$ . The model is illustrated in Figure 1. SegVol supports three types of prompts for interactive segmentation: \u2018bounding box(bbox)\u2019 prompt, including the coordinates of two diagonal vertices; \u2018point\u2019 prompt, composed of a set of positive and negative points; and \u2018text\u2019 prompt, such as \u2018liver\u2019 or \u2018cervical spine C2\u2019. The model consists of four modules: image encoder, text encoder, prompt encoder, and mask decoder. ", "page_idx": 2}, {"type": "text", "text": "We employ 3D ViT (Vision Transformer)[66, 67] as the image encoder, which exhibits remarkable advantages over convolutional models[68] when pre-trained on large-scale datasets. The 3D ViT structure is designed as follows: patch size=(4, 16, 16), layers number $=12$ , heads numbe ${\\tt=}12$ , hidden size $\\scriptstyle=768$ . We first pre-train 3D ViT using SimMIM algorithm[69] on the collected 96K CTs, and then conduct further supervised fine-tuning on the 6K CTs with 150K labeled segmentation masks. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "One of the main limitations of traditional segmentation models is that the models learn dataset-specific labels encoded as integers which cannot generalized to unseen datasets or tasks, preventing their real-world applications. We enable universal segmentation across datasets by leveraging the text encoder from CLIP model[70] to encode the input text prompt, as CLIP[70] has been trained to align image and text embeddings on web-scale image-text pairs. Given a word or phrase as the text prompt, we complete it using the template $^{\\star}A$ computerized tomography of a [text prompt]\u2019[71] and then encode it into text embedding. The off-the-shelf text encoder is frozen during training due to the limited text data in CT datasets. Following SAM[28], we obtain the spatial-prompt embedding using positional encoding[72] on point and bbox prompt. ", "page_idx": 3}, {"type": "text", "text": "After obtaining the image embedding and prompt embedding, we input them into the mask decoder and predict the mask. We use self-attention and cross-attention in two directions to fuse the image embedding and prompt embedding, and then employ the transposed convolutions and interpolation operations to generate masks. Since text embedding is the key to universal segmentation and it is also challenging to learn the correlation between text and volumetric regions, we enhance the text information by introducing a parallel text input branch beside the joint prompt embedding. ", "page_idx": 3}, {"type": "text", "text": "2.3 Prompt Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "$\\mathrm{SegVol}$ accepts multiple types of prompts, including individual point, bbox, and text prompts, and also their combinations. To make full use of the segmentation training data, we generate kinds of prompts for each datum and construct kinds of prompt-mask data pairs for training. ", "page_idx": 3}, {"type": "text", "text": "The point prompt is built from ground truth or pseudo masks, consisting of three kinds of points, namely, positive point, negative point, and ignored point. Positive point means that it is within the target mask region, while negative points are those outside. Ignored points are utilized to ensure a uniform length of the point prompts for input completion. Notably, these ignored points are not considered by the model. ", "page_idx": 3}, {"type": "text", "text": "The bbox prompt is generated based on the ground truth or pseudo masks, integrated with random jitter to enhance the model\u2019s robustness. When generating the bbox prompt for some pseudo mask, the bbox may also cover other masks due to the irregular 3D shapes. To address this problem, we compute the Intersection over Union (IoU) between the generated bbox and the included pseudo masks. Any mask with an IoU greater than 0.9 will also be integrated and considered as part of the target mask corresponding to this bbox prompt. ", "page_idx": 3}, {"type": "text", "text": "The text prompts are constructed based on their category names. As pseudo masks produced by the unsupervised FH algorithm[65] do not have the semantic information, we only use point and bbox prompts for training on masks of pseudo labels. ", "page_idx": 3}, {"type": "text", "text": "2.4 Zoom-out-zoom-in Mechanism ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "SAM-like interaction with large-volume images is laborious for users, especially in the scene where the sliding window has to be used due to the limited view of those 3D models. To provide users with an easy SAM-like interface, we design a zoom-out-zoom-in mechanism, which is efficient and precise, consisting of zoom-out-zoom-in inference and multi-size training. As demonstrated in Figure 1, the zoom-out process involves resizing a volumetric image, which is input into the model with user prompts to generate a coarse segmentation mask. Then, the Region of Interest (ROI) from the original image is cropped for zoom-in analysis. In the zoom-in process, a sliding window is used to perform precise inference driven by prompts generated from the coarse segmentation mask. After that, the ROI prediction mask will be back-fliled to the coarse segmentation mask to finish the final prediction. Besides, multi-size training involves augmenting the input data by resizing CTs for the zoom-out view and cropping them into cubes for the zoom-in view. The zoom-out-zoom-in mechanism realizes the computational cost reduction meanwhile producing precise segmentation of the ROI. ", "page_idx": 3}, {"type": "text", "text": "2.5 Loss Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We apply SimMIM algorithm[69] to pre-train the image encoder of SegVol with the masked image modeling loss $\\mathcal{L}_{\\mathrm{pre-training}}(\\theta_{\\mathrm{IE}};D_{1})$ . The loss function is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{pre-training}}(\\pmb{\\theta}_{\\mathrm{IE}};\\mathcal{D}_{1})=\\frac{1}{\\Omega(\\pmb{a}_{\\mathrm{M}})}||\\pmb{b}_{\\mathrm{M}}-\\pmb{a}_{\\mathrm{M}}||_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{\\mathrm{IE}}$ is the parameter set of SegVol\u2019s image encoder. $\\mathbf{a},b\\in\\mathbb{R}^{D\\times H\\times W}$ are the input voxel values and predicted values, respectively. M denotes the set of masked voxels, $\\Omega(\\cdot)$ is the number of elements, and $\\mathcal{D}_{1}$ is the pre-training dataset. ", "page_idx": 4}, {"type": "text", "text": "We combine the Binary Cross-Entropy (BCE) loss and Dice loss as the supervised fine-tuning loss function $\\mathcal{L}_{\\mathrm{fine-tuning}}(\\theta;\\mathcal{D}_{2})$ to train the model with trainable parameters $\\pmb{\\theta}$ (text encoder frozen). $\\mathcal{D}_{2}$ is the supervised fine-tuning dataset and $\\mathbf{x},y\\in\\mathbb{R}^{D\\times H\\times W}$ are the predicted mask and ground-truth mask, respectively. $\\mathcal{F}(\\cdot,\\bar{\\pmb\\theta})$ is the forward function of SegVol. The loss function is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{BCE}}(\\pmb{\\theta};\\mathcal{D}_{2})=-\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}_{2}}[\\langle\\pmb{y},\\log(\\mathcal{F}(\\pmb{x},\\pmb{\\theta}))\\rangle+\\langle1-\\pmb{y},\\log(1-\\mathcal{F}(\\pmb{x},\\pmb{\\theta}))\\rangle]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Dice}}(\\pmb{\\theta};\\mathcal{D}_{2})=1-\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}_{2}}[\\frac{2\\cdot\\langle\\pmb{y},\\mathcal{F}(\\pmb{x},\\pmb{\\theta})\\rangle}{\\|\\pmb{y}\\|_{1}+\\|\\mathcal{F}(\\pmb{x},\\pmb{\\theta})\\|_{1}}]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{fine-tuning}}(\\pmb{\\theta};\\mathcal{D}_{2})=\\mathcal{L}_{\\mathrm{BCE}}(\\pmb{\\theta};\\mathcal{D}_{2})+\\mathcal{L}_{\\mathrm{Dice}}(\\pmb{\\theta};\\mathcal{D}_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The detailed fine-tuning algorithm of SegVol is presented in Section B. ", "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we conduct extensive experiments on 22 volumetric medical image segmentation tasks to compare $\\mathrm{SegVol}$ with other SAM-like medical image segmentation methods[28, 38, 39, 29]. Ablation studies are also carried out to prove the effectiveness of the zoom-out-zoom-in mechanism and provide more insights about dataset scale and multi-prompt combination. Detailed case studies are conducted to discuss the disambiguation ability of semantic-prompt and the capability of identifying the segmentation results with spatial-prompt. ", "page_idx": 4}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "During the pre-training, we follow SimMIM algorithm[69] to train the 3D ViT encoder of $\\mathrm{SegVol}$ on the collected 96K CTs for 2000 epochs. In the supervised fine-tuning stage, we train $\\mathrm{SegVol}$ (with the text encoder frozen) on the labeled 25 volumetric medical image segmentation datasets for 270 epochs with batch size 32 and input size (32, 256, 256), using AdamW optimizer[73]. SimMIM pre-training takes about $20\\times8$ GPU hours, while fine-tuning takes about $300\\times8$ GPU hours. All the above training process is implemented on 8 NVIDIA A100-SXM4-40GB. Three external datasets[44, 74, 75] and $20\\%$ testing data preserved from 25 collected datasets are used in the following experiments. ", "page_idx": 4}, {"type": "text", "text": "3.2 Compared with SAM-like Interactive Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Several efforts have been made to construct a SAM-like interactive medical image segmentation model. However, some of these works, such as MedSAM[29] and SAM-MED2D[38], focus on 2D tasks and cannot process 3D input directly. The other 3D-based methods, such as SAM-MED3D[39], only support small cropped input and do not support semantic-prompt segmentation, which are still far from building a comprehensive foundation model for volumetric medical image analysis. ", "page_idx": 4}, {"type": "text", "text": "Competitors and configures. In this experiment, MedSAM[29] and SAM(bounding box)[28] use bounding box prompts. SAM(5 clicks)[28], SAM-MED2D[38] and SAM-MED3D[39] use point prompts and a five-step correction procedure, which means that the point prompt in each step will be given according to the previous-step output and ground truth, rather than giving all at once. In this experiment, $\\mathrm{SegVol}$ uses bounding box and text prompt which performs better than other kinds of prompt combinations. Detailed ablation study on prompt combination is demonstrated in Figure 3 (b). In addition, we compare SegVol with traditional task-specific segmentation models, e.g., 3DUX-NET[23], SwinUNETR[20], and nnU-Net[22], in Section C, though the direct comparison is unsuitable due to the different settings and objectives. ", "page_idx": 4}, {"type": "table", "img_path": "105ZuvpdyW/tmp/600dbfb45eec3c6934df32f01a2306f61ae769c23681946b85377355819e4f72.jpg", "table_caption": ["Table 2: Quantitative comparative experiment results for $\\mathrm{SegVol}$ and other 5 SAM-like interactive segmentation methods settings in terms of the median value of Dice score. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "105ZuvpdyW/tmp/eb8cbea9b1d80af5d7816ec2b98fcf81f7c13e11a9503b63d5d1e96b866b8438.jpg", "img_caption": ["Figure 2: Violin plots for quantitative comparison experiment results of $\\mathrm{SegVol}$ and SAM-like interactive methods[28, 38, 39, 29]. The vertical axis represents the Dice score. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Testing data. To compare with these SAM-like interactive segmentation models, we evaluate the models on 1,778 cases from the validation set of AMOS22[44], the whole novel annotated set of Universal Lesion Segmentation Challenge 23(ULS23)[74], and the released labeled set of SegTHOR[75]. The validation set of AMOS22 contains 120 cases annotated with 15 major organs. The novel annotated ULS23 dataset is composed of three subsets, namely, DeepLesion3D, Radboudumc Bone, and Radboudumc Pancreas. The DeepLesion3D subset contains 200 abdominal lesions, 100 bone lesions, 50 kidney lesions, 50 liver lesions, 100 lung lesions, 100 mediastinal lesions, and 150 assorted lesions cases. There are 744 bone lesion cases in the Radboudumc Bone subset and 124 pancreas lesion cases in the Radboudumc Pancreas subset. The 40 cases from SegTHOR, which are contoured manually by an experienced radiotherapist, focus on the heart, trachea, aorta, and esophagus that surround the tumor and must be preserved from irradiations during radiotherapy. ", "page_idx": 5}, {"type": "table", "img_path": "105ZuvpdyW/tmp/a5e695edc5d31b8e253421c15c0f358b82bc74bdc959fe88fc6b3d38f5dc195a.jpg", "table_caption": ["Table 3: Ablation experiment on the zoom-out-zoom-in mechanism. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "105ZuvpdyW/tmp/d81c267a8a0cdfb606b9e6d062ae1e0152375d68680a9e95861e85e5094a09b4.jpg", "img_caption": ["Figure 3: (a) The performance of $\\mathrm{SegVol}$ improves as the training data scales up. (b) The quantitative experimental results on 19 anatomical segmentation tasks of split $20\\%$ test data demonstrate that using the combination of semantic and spatial prompts can achieve better performances. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Quantitative results. The quantitative results of comparative experiments are shown in Table 2, which verify our method is the best in most of the tasks including both lesions and organs, compared to other SAM-like interactive models[28, 38, 39, 29]. Specifically, our method outperforms the second-ranked SAM-MED2D on the AMOS22 dataset by a significant improvement of $19.25\\%$ (average Dice score). On the SegTHOR dataset, our method surpasses the runner-up \u2013 SAM-MED3D by an average Dice score improvement of $4.28\\%$ . The ULS23 dataset, characterized by small patchlike masks, presents a unique challenge. In this scenario, $\\mathrm{SegVol}$ still exhibits good performance, comparable to MedSAM, which excels in using bbox prompts for segmenting small objects. We visualize the Dice score distributions of all methods in all the tasks as violin plots, depicted in Figure 2. More detailed results and visualization are present in Section C. ", "page_idx": 6}, {"type": "text", "text": "3.3 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Zoom-out-zoom-in mechanism. One of the key designs of $\\mathrm{SegVol}$ is the zoom-out-zoom-in mechanism. We compare it with the intuitive resize strategy and the popular sliding window algorithm on the split $20\\%$ test data, 48 cases covering 15 major organs with a variety of sizes, belonging to the AMOS22[44] dataset. Two evaluation dimensions, i.e., performance (Dice score) and inference time cost (per case), are compared, as shown in Table 3. The zoom-out-zoom-in mechanism achieves the best average Dice score and a very competitive inference speed compared to the simple resize strategy. The reason for computational cost reduction is that the traditional sliding window method requires scanning the entire 3D CT and processing thousands of windows. In contrast, the proposed zoom-out-zoom-in mechanism only requires one global inference of 3D CT and then scanning the ROI with dozens of windows. Detailed experiment results are shown in Section C. ", "page_idx": 6}, {"type": "text", "text": "Scaling up training data. The success of scaling up training data has been witnessed in multiple computer vision tasks [28, 70]. We conduct an ablation study to investigate the importance of scaling up training images and masks. The split $20\\%$ test data of BTCV dataset[52], which includes 13 main organs, is set as an anchor to evaluate the model trained separately on 1, 2, and 8 datasets for 500 epochs, as well as the final model trained on 25 datasets. The detailed results are shown in Figure 3 (a). As a lightweight model, the performance of $\\mathrm{SegVol}$ is weak when only one dataset is used. However, with the increase of training data, the Dice score increases rapidly, especially in the text prompt setting. The results indicate that our method is scalable and better performance can be achieved if more training data is available. ", "page_idx": 7}, {"type": "text", "text": "Multi-prompt combination. As a universal model, our approach achieves precise segmentation for over 200 organs, tissues, and lesions using both spatial and semantic prompts. In Figure 3 (b), we quantitatively analyze the mutually supportive relationship between semantic-prompt and spatial-prompt in 19 segmentation tasks of the $20\\%$ split test data. On the one hand, spatial-prompt allows the model to locate the specific part in the 3D space. According to Figure 3 (b), the average Dice score of the \u2018bbox+text\u2019 prompt is boosted by $5.85\\%$ compared to the \u2018text\u2019 prompt on average. On the other hand, semantic-prompt clarifies the reference to the anatomical structure, eliminating the ambiguity of spatial-prompt and the plausible masks of multiple categories. This is reflected in Figure 3 (b) as the average Dice score of \u2018point+text\u2019 prompts is $4.62\\%$ higher than using \u2018point\u2019 prompts alone. Spatial and semantic prompts mutually support each other, ultimately endowing the model with powerful segmentation capabilities. ", "page_idx": 7}, {"type": "text", "text": "3.4 Case Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Disambiguation via semantic-prompt. It is a notorious problem in interactive segmentation that one spatial-prompt may correspond to multiple plausible outputs [28]. As illustrated in the images on the top left in Figure 4, three of them correspond to three anatomical concepts, namely, kidney tumor, left kidney, and the whole kidneys, while they are all plausible to the same point prompt. Similarly, in the bottom left three images, the bounding box selects the region of the liver. However, liver tumors, hepatic vessels, and the liver itself are also plausible target structures. In these cases, SAM chooses to return multiple masks to match different levels of plausible results. Unlike SAM\u2019s solution, we use semantic-prompt to clarify the targets. As shown in Figure 4, the captions below the images are the text prompts, and the masks in the images are the predictions of $\\mathrm{SegVol}$ , which show that semantic-prompt can effectively disambiguate the spatial-prompt. ", "page_idx": 7}, {"type": "text", "text": "Identifying the spatial-prompt segmentation. Furthermore, we study the capability of SegVol to identify the semantic category of the spatial-prompt results. Figure 5 reveals that $\\mathrm{SegVol}$ can give accurate semantic categories based on the spatial-prompt results. In the top left image in Figure 5, the spatial-prompt on the liver results in a 0.997 prediction score for the liver. The top right image in the sub-figure shows if the spatial-prompt is the point on the liver tumor, SegVol will output a 0.619 prediction score for the tumor category and a 0.339 prediction score for the liver based on the spatial relationship of liver tumor and liver. We implement this identification experiment by decoding the semantic prompts from a category set. The softmax function is applied to the decoding results to get the prediction probabilities of different categories. The probabilities on the initial predicted mask, driven by the spatial-prompt, are used to calculate the final classification result. ", "page_idx": 7}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Scalability. The scaling law of foundation models has been verified in multiple CV and NLP tasks. Since SegVol uses a transformer-based architecture and self-supervised pre-training algorithm, it has strong data and architecture scalability. In this work, we achieve the success of scaling law in 3D medical segmentation by the design of universal prompts and pseudo masks for joint learning on datasets with inconsistent annotations. The ablation study of scaling up training data shows that 1) the performance improves significantly with more training data in the 3D segmentation task, 2) SegVol has not yet reached its ceiling if more training data is provided. We believe the performance of SegVol can be continuously improved when more data and computational resources are used. ", "page_idx": 7}, {"type": "image", "img_path": "105ZuvpdyW/tmp/bdddea4f17f14b3a572b33b0e61db07e4bc1c2d27f2b10e2b7af1a38090d5864.jpg", "img_caption": ["Figure 4: The four cases demonstrate that semantic-prompt can clarify the ambiguity of spatialprompt and avoid multi-plausible outputs. Each image shows the segmentation result of $\\mathrm{SegVol}$ using the spatial-prompt, i.e. point or bounding box, and semantic-prompt, i.e. the caption below the image. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "105ZuvpdyW/tmp/7e54a0032006400747e041347b98d040db5db70278c0a1d175cabec26b84084c.jpg", "img_caption": ["Figure 5: We identify the semantic categories of the spatial-prompt segmentation results. Each image shows the spatial-prompt and the mask prediction. The bar charts rank the top 8 semantic categories with the highest classification probabilities. The results show that SegVol is capable of identifying the anatomical category of the segmentation mask using spatial prompts. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Generalizability to unseen modality. Although we develop $\\mathrm{SegVol}$ on Computed Tomography (CT) data due to its advantages of easy acquisition, wide usage, and high resolution, we find that SegVol can generalize to other medical image modality, like MRI. Namely, the $\\mathrm{SegVol}$ model trained only on CT data can be used to segment MRI with semantic and spatial prompts. This emerging ability demonstrates that our foundation model understands the anatomical structure of human body. We provide detailed experiments and analysis of this generalizability in Section C. The impressive generalizability makes SegVol a versatile tool in medical image analysis. We leave the joint training of $\\mathrm{SegVol}$ on multi-modality data as the future work. ", "page_idx": 8}, {"type": "text", "text": "Limitations. Although SegVol shows remarkable semantic-prompt segmentation performance, there still remains gap between it and the referring volumetric segmentation. A promising solution is to construct the referring segmentation data with diverse semantic and spatial prompts, and then train SegVol on it. We leave it as the future work. More discussions can be found in Section E. ", "page_idx": 8}, {"type": "text", "text": "Broader impact. We contribute a foundation model for universal and interactive volumetric medical image segmentation, which can benefti numerous clinical study and applications. As a foundational research work, we do not see any obvious negative societal impact of the proposed method and model. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose SegVol, a universal and interactive volumetric medical image segmentation model, supporting both spatial-prompt and semantic-prompt segmentation of more than 200 anatomical categories. We construct a large-scale dataset, which consists of 90K unlabeled CTs and 25 open-source medical datasets, to train the foundation model. We design the zoom-out-zoom-in mechanism to facilitate efficient and precise inference in the region of interest. Extensive experiments on 22 segmentation tasks demonstrate the outstanding performance of our method. Detailed ablation studies are also carried out to prove the effectiveness of the zoom-out-zoom-in mechanism, dataset scale, and multi-prompt combination strategy. As a foundation model, we believe that SegVol will advance the volumetric medical segmentation and benefit numerous downstream tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. This work is funded by NSFC-62306046. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sidra Sajid, Saddam Hussain, and Amna Sarwar. Brain tumor detection and segmentation in mr images using deep learning. Arabian Journal for Science and Engineering, 44, 2019.   \n[2] Stefano Trebeschi, Zuhir Bodalal, Thierry N Boellaard, Teresa M Tareco Bucho, Silvia G Drago, Ieva Kurilova, Adriana M Calin-Vainak, Andrea Delli Pizzi, Mirte Muller, Karlijn Hummelink, et al. Prognostic value of deep learning-mediated treatment monitoring in lung cancer patients receiving immunotherapy. Frontiers in Oncology, 11:609054, 2021.   \n[3] Jordi Minnema, Anne Ernst, Maureen van Eijnatten, Ruben Pauwels, Tymour Forouzanfar, Kees Joost Batenburg, and Jan Wolff. A review on the application of deep learning for ct reconstruction, bone segmentation and surgical planning in oral and maxillofacial surgery. Dentomaxillofacial Radiology, 51(7):20210437, 2022.   \n[4] Vincenzo Ferrari, Marina Carbone, Carla Cappelli, Luigi Boni, Franca Melf,i Mauro Ferrari, Franco Mosca, and Andrea Pietrabissa. Value of multidetector computed tomography image segmentation for preoperative planning in general surgery. Surgical endoscopy, 26, 2012.   \n[5] Chen Chen, Chen Qin, Huaqi Qiu, Giacomo Tarroni, Jinming Duan, Wenjia Bai, and Daniel Rueckert. Deep learning for cardiac image segmentation: a review. Frontiers in Cardiovascular Medicine, 7:25, 2020. [6] Gihan Samarasinghe, Michael Jameson, Shalini Vinod, Matthew Field, Jason Dowling, Arcot Sowmya, and Lois Holloway. Deep learning for segmentation in radiation therapy planning: a review. Journal of Medical Imaging and Radiation Oncology, 65(5):578\u2013595, 2021. [7] Habib Zaidi and Issam El Naqa. Pet-guided delineation of radiation therapy treatment volumes: a survey of image segmentation techniques. European journal of nuclear medicine and molecular imaging, 37:2165\u20132187, 2010.   \n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.   \n[9] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: A nested u-net architecture for medical image segmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, pages 3\u201311. Springer, 2018.   \n[10] Nahian Siddique, Sidike Paheding, Colin P Elkin, and Vijay Devabhaktuni. U-net and its variants for medical image segmentation: A review of theory and applications. IEEE Access, 9:82031\u201382057, 2021.   \n[11] Xiao-Xia Yin, Le Sun, Yuhan Fu, Ruiliang Lu, Yanchun Zhang, et al. U-net-based medical image segmentation. Journal of Healthcare Engineering, 2022, 2022.   \n[12] Jiawei Zhang, Yuzhen Jin, Jilan Xu, Xiaowei Xu, and Yanchun Zhang. Mdu-net: Multi-scale densely connected u-net for biomedical image segmentation. arXiv preprint arXiv:1812.00352, 2018.   \n[13] Mehreen Mubashar, Hazrat Ali, Christer Gr\u00f6nlund, and Shoaib Azmat. $\\mathbf{R}2\\mathbf{u}++$ : a multiscale recurrent residual u-net with dense skip connections for medical image segmentation. Neural Computing and Applications, 34(20):17723\u201317739, 2022.   \n[14] Debesh Jha, Michael A Riegler, Dag Johansen, P\u00e5l Halvorsen, and H\u00e5vard D Johansen. Doubleunet: A deep convolutional neural network for medical image segmentation. In 2020 IEEE 33rd International symposium on computer-based medical systems (CBMS). IEEE, 2020.   \n[15] Ziang Zhang, Chengdong Wu, Sonya Coleman, and Dermot Kerr. Dense-inception u-net for medical image segmentation. Computer methods and programs in biomedicine, 192:105395, 2020.   \n[16] Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huaying Hao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, and Jiang Liu. Ce-net: Context encoder network for 2d medical image segmentation. IEEE transactions on medical imaging, 38(10):2281\u20132292, 2019.   \n[17] Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, and Rynson W. H. Lau. Boosting weakly-supervised referring image segmentation via progressive comprehension, 2024.   \n[18] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574\u2013584, 2022.   \n[19] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation, 2022.   \n[20] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images, 2022.   \n[21] Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20730\u201320740, 2022.   \n[22] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203\u2013211, 2021.   \n[23] Ho Hin Lee, Shunxing Bao, Yuankai Huo, and Bennett A. Landman. 3d ux-net: A large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation, 2023.   \n[24] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). Medical Image Analysis, 84:102680, 2023.   \n[25] Patrick Ferdinand Christ, Florian Ettlinger, Felix Gr\u00fcn, Mohamed Ezzeldin A Elshaera, Jana Lipkova, Sebastian Schlecht, Freba Ahmaddy, Sunil Tatavarty, Marc Bickel, Patrick Bilic, et al. Automatic liver and tumor segmentation of ct and mri volumes using cascaded fully convolutional neural networks. arXiv preprint arXiv:1702.05970, 2017.   \n[26] Ishak Pacal, Dervis Karaboga, Alper Basturk, Bahriye Akay, and Ufuk Nalbantoglu. A comprehensive review of deep learning in colon cancer. Computers in Biology and Medicine, 126:104003, 2020.   \n[27] A Ben Hamida, Maxime Devanne, Jonathan Weber, Caroline Truntzer, Valentin Derang\u00e8re, Fran\u00e7ois Ghiringhelli, Germain Forestier, and C\u00e9dric Wemmert. Deep learning for colon cancer histopathological images analysis. Computers in Biology and Medicine, 136:104730, 2021.   \n[28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[29] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images, 2023.   \n[30] Hiba Ramadan, Chaymae Lachqar, and Hamid Tairi. A survey of recent interactive image segmentation methods. Computational visual media, 6:355\u2013384, 2020.   \n[31] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16, pages 59\u201375. Springer, 2020.   \n[32] Fan Bai, Yuxin Du, Tiejun Huang, Max Q. H. Meng, and Bo Zhao. M3d: Advancing 3d medical image analysis with multi-modal large language models, 2024.   \n[33] Blaine Rister, Kaushik Shivakumar, Tomomi Nobashi, and Daniel L Rubin. Ct-org: Ct volumes with multiple organ segmentations [dataset]. The Cancer Imaging Archive, 2019.   \n[34] Blaine Rister, Darvin Yi, Kaushik Shivakumar, Tomomi Nobashi, and Daniel L Rubin. Ct organ segmentation using gpu data augmentation, unsupervised labels and iou loss. arXiv preprint arXiv:1811.11226, 2018.   \n[35] Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen Moore, Stanley Phillips, David Maffitt, Michael Pringle, et al. The cancer imaging archive (tcia): maintaining and operating a public information repository. Journal of digital imaging, 26:1045\u20131057, 2013.   \n[36] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis, 42:1\u201313, 2017.   \n[37] Huiyan Jiang, Zhaoshuo Diao, and Yu-Dong Yao. Deep learning techniques for tumor segmentation: a review. The Journal of Supercomputing, 78(2):1807\u20131851, 2022.   \n[38] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sam-med2d. arXiv preprint arXiv:2308.16184, 2023.   \n[39] Haoyu Wang, Sizheng Guo, Jin Ye, Zhongying Deng, Junlong Cheng, Tianbin Li, Jianpin Chen, Yanzhou Su, Ziyan Huang, Yiqing Shen, Bin Fu, Shaoting Zhang, Junjun He, and Yu Qiao. Sam-med3d, 2023.   \n[40] A. Emre Kavur, N. Sinem Gezer, Mustafa Bar\u0131\u00b8s, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, et al. Chaos challenge - combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis, 69:101950, April 2021.   \n[41] Ali Emre Kavur, M. Alper Selver, O\u02d8guz Dicle, Mustafa Bar\u0131\u00b8s, and N. Sinem Gezer. Chaos - combined (ct-mr) healthy abdominal organ segmentation challenge data. April 2019.   \n[42] A. Emre Kavur, Naciye Sinem Gezer, Mustafa Bar\u0131\u00b8s, Yusuf S\u00b8ahin, Sava\u00b8s \u00d6zkan, Bora Baydar, Ula\u00b8s Y\u00fcksel, \u00c7ag\u02d8lar K\u0131l\u0131k\u00e7\u0131er, S\u00b8ahin Olut, G\u00f6zde Bozdag\u02d8\u0131 Akar, G\u00f6zde \u00dcnal, Og\u02d8uz Dicle, and M. Alper Selver. Comparison of semi-automatic and deep learning based automatic methods for liver segmentation in living liver transplant donors. Diagnostic and Interventional Radiology, 26:11\u201321, January 2020.   \n[43] Ga\u0161per Podobnik, Primo\u017e Strojan, Primo\u017e Peterlin, Bulat Ibragimov, and Toma\u017e Vrtovec. Han-seg: The head and neck organ-at-risk ct and mr segmentation dataset. Medical physics, 50(3):1917\u20131927, 2023.   \n[44] Yuanfeng Ji, Haotian Bai, Jie Yang, Chongjian Ge, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, et al. Amos: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation. arXiv preprint arXiv:2206.08023, 2022.   \n[45] Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, Shucheng Cao, Qi Zhang, Shangqing Liu, Yunpeng Wang, Yuhui Li, Jian He, and Xiaoping Yang. Abdomenct-1k: Is abdominal organ segmentation a solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10), 2022.   \n[46] Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, et al. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct, 2023.   \n[47] Yuting He, Guanyu Yang, Jian Yang, Rongjun Ge, Youyong Kong, Xiaomei Zhu, Shaobo Zhang, Pengfei Shao, Huazhong Shu, Jean-Louis Dillenseger, et al. Meta grayscale adaptive network for 3d integrated renal structures segmentation. Medical image analysis, 71:102055, 2021.   \n[48] Yuting He, Guanyu Yang, Jian Yang, Yang Chen, Youyong Kong, Jiasong Wu, Lijun Tang, Xiaomei Zhu, Jean-Louis Dillenseger, Pengfei Shao, et al. Dense biased networks with deep priori anatomy and hard region adaptation: Semi-supervised learning for fine renal artery segmentation. Medical image analysis, 63:101722, 2020.   \n[49] Pengfei Shao, Chao Qin, Changjun Yin, Xiaoxin Meng, Xiaobing Ju, Jie Li, Qiang Lv, Wei Zhang, and Zhengquan Xu. Laparoscopic partial nephrectomy with segmental renal artery clamping: technique and clinical outcomes. European urology, 59(5):849\u2013855, 2011.   \n[50] Pengfei Shao, Lijun Tang, Pu Li, Yi Xu, Chao Qin, Qiang Cao, Xiaobing Ju, Xiaoxin Meng, Qiang Lv, Jie Li, et al. Precise segmental renal artery clamping under the guidance of dualsource computed tomography angiography during laparoscopic partial nephrectomy. European urology, 62(6):1001\u20131008, 2012.   \n[51] Nicholas Heller, Fabian Isensee, Klaus H Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin, Miofei Han, et al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct imaging: Results of the kits19 challenge. Medical Image Analysis, page 101821, 2020.   \n[52] Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner, T Langerak, and Arno Klein. Miccai multi-atlas labeling beyond the cranial vault\u2013workshop and challenge. In Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault\u2014Workshop Challenge, volume 5, page 12, 2015.   \n[53] Holger R Roth, Amal Farag, E Turkbey, Le Lu, Jiamin Liu, and Ronald M Summers. Data from pancreas-ct. the cancer imaging archive. IEEE Transactions on Image Processing, 2016.   \n[54] Holger R Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim B Turkbey, and Ronald M Summers. Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part I 18, pages 556\u2013564. Springer, 2015.   \n[55] Luc Soler, Alexandre Hostettler, Vincent Agnus, Arnaud Charnoz, Jean-Baptiste Fasquel, Johan Moreau, Anne-Blandine Osswald, Mourad Bouhadjar, and Jacques Marescaux. 3d image reconstruction for comparison of algorithm database. URL: https://www. ircad. fr/research/datasets/liver-segmentation-3d-ircadb-01, 2010.   \n[56] Amber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv preprint arXiv:1902.09063, 2019.   \n[57] Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, et al. Abdomenct-1k: Is abdominal organ segmentation a solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6695\u20136714, 2021.   \n[58] J Wasserthal, M Meyer, HC Breit, J Cyriac, S Yang, and M Segeroth. Totalsegmentator: Robust segmentation of 104 anatomical structures in ct images 2022. arXiv, 2022.   \n[59] Anjany Sekuboyina, Malek E Husseini, Amirhossein Bayat, Maximilian L\u00f6ffler, Hans Liebl, Hongwei Li, Giles Tetteh, Jan Kukac\u02c7ka, Christian Payer, Darko \u0160tern, et al. Verse: a vertebrae labelling and segmentation benchmark for multi-detector ct images. Medical image analysis, 73:102166, 2021.   \n[60] Maximilian T L\u00f6ffler, Anjany Sekuboyina, Alina Jacob, Anna-Lena Grau, Andreas Scharr, Malek El Husseini, Mareike Kallweit, Claus Zimmer, Thomas Baum, and Jan S Kirschke. A vertebral segmentation dataset with fracture grading. Radiology: Artificial Intelligence, 2(4):e190138, 2020.   \n[61] Hans Liebl, David Schinz, Anjany Sekuboyina, Luca Malagutti, Maximilian T L\u00f6ffler, Amirhossein Bayat, Malek El Husseini, Giles Tetteh, Katharina Grau, Eva Niederreiter, et al. A computed tomography vertebral segmentation dataset with anatomical variations and multi-vendor scanner data. Scientific data, 8(1):284, 2021.   \n[62] Tobias Heimann, Bram Van Ginneken, Martin A Styner, Yulia Arzhaeva, Volker Aurich, Christian Bauer, Andreas Beck, Christoph Becker, Reinhard Beichel, Gy\u00f6rgy Bekes, et al. Comparison and evaluation of methods for liver segmentation from ct datasets. IEEE transactions on medical imaging, 28(8):1251\u20131265, 2009.   \n[63] Quantification of uncertainties in biomedical image quantification challenge 2021. https: //qubiq21.grand-challenge.org/. Accessed: 18 Aug 2023.   \n[64] Xiangde Luo, Wenjun Liao, Jianghong Xiao, Jieneng Chen, Tao Song, Xiaofan Zhang, Kang Li, Dimitris N. Metaxas, Guotai Wang, and Shaoting Zhang. WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image. Medical Image Analysis, 82:102642, 2022.   \n[65] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. International journal of computer vision, 59:167\u2013181, 2004.   \n[66] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.   \n[67] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation, 2021.   \n[68] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[69] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling, 2022.   \n[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[71] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A. Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection, 2023.   \n[72] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren $\\mathrm{Ng}$ . Fourier features let networks learn high frequency functions in low dimensional domains, 2020.   \n[73] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.   \n[74] Max de Grauw. Universal lesion segmentation challenge 23. https://uls23.grand-challenge.org/.   \n[75] Z. Lambert, C. Petitjean, B. Dubray, and S. Ruan. Segthor: Segmentation of thoracic organs at risk in ct images, 2019.   \n[76] brgfx. Image by brgfx on freepik. https://www.freepik.com/free-vector/anatomical-structurehuman-body 27539420.htm. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Dataset Details and Availability ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this work, we collect 25 open-source datasets for supervised fine-tuning $\\mathrm{SegVol}$ and some external open-source datasets specifically for comparative experiments. The detailed information on anatomical categories and dataset scales of these open-source datasets is shown in Table 4. The availability of these datasets is demonstrated in Table 5. Additionally, to avoid privacy concerns, we collect 90K unlabeled CTs from publicly accessible professional medical websites: https://radiopaedia.org/. ", "page_idx": 14}, {"type": "text", "text": "The collected segmentation datasets include major regions of the human body, i.e., the head, neck, thorax, abdomen, and pelvis, comprising over 200 categories of organs and tissues, and 28 lesion tasks from different benchmarks. The detailed categories information are shown in Figure 6 and some representative samples are shown in Figure 7. ", "page_idx": 14}, {"type": "table", "img_path": "105ZuvpdyW/tmp/f4a1dc459b6e5236762d3ac3275188d6dee0b7b3d47cfcdd193289f320f34063.jpg", "table_caption": ["Table 4: Information of datasets involved in supervised fine-tuning and experiments. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Training Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Due to the complexity of the training steps, which include decoder reuse, the combination of different datasets, and cooperative training of ground-truth and pseudo labels, we abstract the core training code as Algorithm 1 and Figure 8 to clarify the training process of SegVol. As shown in Figure 8, each case (training sample) consists of an Image $\\textbf{\\em x}$ , a Ground Truth(GT) Mask Set $\\mathbf{\\deltaY}$ , and a Pseudo Mask Set $Z$ . The training loss of each sample consists of the ground-truth loss and the pseudo loss. The ground-truth loss is computed by inputting the image, the ground-truth mask (label), and the sampled prompt into the model, while the pseudo loss is computed by inputting the image, the pseudo label, and the pre-designed prompt into the model. Finally, the model is optimized by minimizing the weighted sum of the two losses. ", "page_idx": 14}, {"type": "image", "img_path": "105ZuvpdyW/tmp/5df7c3733d355a63d360e3b14dfac46ea318fb3e0746509acd1d1960032d51ce.jpg", "img_caption": ["Figure 6: Overview of the collected datasets for supervised fine-tuning. The joint dataset comprises 47 important regions, with each region containing one or multiple significant anatomical structures within that spatial area. Image of the human body by brgfx on Freepik[76]. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "105ZuvpdyW/tmp/7d386116a1993cd4447d8e2af1fb1aabae9a6750eeae7be2cbcded9778f5c90e.jpg", "img_caption": ["Figure 7: The joint dataset encompasses various anatomical structures in major regions of the human body. Several volume examples are demonstrated as 2D slices and 3D shapes in the images respectively. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "105ZuvpdyW/tmp/2454b3bcf534f76af2b1c71c7d6d37deb2b2cbb6d8cdb3e53e6e18efdc3f3a17.jpg", "table_caption": ["Table 5: Availability of datasets involved in supervised fine-tuning and experiments. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "105ZuvpdyW/tmp/098e097f9014475fb7913cd339ec0f6cd787c33c1f375be4e7c981df3a3f2b40.jpg", "table_caption": ["Table 6: Complexity comparison of popular methods. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Besides, we add a reinforcement branch for semantic-prompt in the mask decoder. We further compute a similarity matrix between the up-scaled embedding from the transposed convolution output and the text embedding. The element-wise multiplication of the similarity matrix with the mask prediction is applied before interpolation, after which the model outputs the masks. ", "page_idx": 16}, {"type": "text", "text": "Input: SegVol model, training image $\\textbf{\\em x}$ , ground truth mask set $Y_{x}=\\{y_{i}\\}_{i=1}^{n}$ , pseudo mask set   \n$\\mathbf{Z}_{x}=\\overline{{\\{z_{i}\\}}}_{i=1}^{m}$   \nOutput: SegVol model parameters   \n1: $n\\Leftarrow6$ # Number of combinations of 3 prompt types: text, point, and bbox.   \n2: $\\alpha\\Leftarrow0.1$ # Pseudo loss weight.   \n3: # Loop for each category of this case.   \n4: for $i\\Leftarrow1$ to $n$ do   \n5: $f_{\\mathrm{img}}\\Leftarrow$ model.ImageEncoder ${\\bf\\Pi}({\\bf\\boldsymbol{x}})$   \n6: ptspatial, ptsemantic, $\\Leftarrow$ prompt_generate $\\left({{\\pmb y}_{i}}\\right)$   \n7: $l_{\\mathrm{gt}}\\stackrel{\\cdot}{\\leftarrow}0$   \n8: # Loop for possible prompt combination types of ground truth mask.   \n9: for $p\\Leftarrow1$ to $n$ do   \n10: # Choose prompt combination type.   \n11: $\\begin{array}{r l}&{p t_{\\mathrm{spatial}}^{\\prime},p\\dot{t}_{\\mathrm{semantic}}^{\\prime}\\Leftarrow\\mathrm{PromptStrate}^{\\prime}\\mathrm{sp}(p t_{\\mathrm{spatial}},p t_{\\mathrm{semantic}})}\\\\ &{f_{\\mathrm{text}}\\Leftarrow\\mathrm{model.TextEncoder}(p t_{\\mathrm{semantic}}^{\\prime})}\\\\ &{f_{\\mathrm{prompt}}\\Leftarrow\\mathrm{model.PromptEncoder}(p t_{\\mathrm{spatial}}^{\\prime},f_{\\mathrm{text}})}\\\\ &{p r e d_{\\mathrm{gt}}\\Leftarrow\\mathrm{model.Decoder}(f_{\\mathrm{ing}},f_{\\mathrm{prompt}},f_{\\mathrm{text}})}\\\\ &{l_{\\mathrm{gt}}\\Leftarrow l_{\\mathrm{gt}}+\\mathrm{DiceLoss}(p r e d_{\\mathrm{gt}},y_{i})+\\mathrm{BCELoss}(p r e d_{\\mathrm{gt}},y_{i})}\\end{array}$   \n12:   \n13:   \n14:   \n15:   \n16: end for   \n17: $l_{\\mathrm{pseudo}}\\Leftarrow0$   \n18: # Loop for several pseudo masks.   \n19: for $p\\Leftarrow1$ to $n$ do   \n20: # Random select a pseudo mask of this case for training.   \n21: $\\begin{array}{r l}&{z_{p}\\gets\\mathrm{RandomSelect}(Z_{x},\\left[1,m\\right])}\\\\ &{p t_{\\mathrm{spatial}}\\gets\\mathrm{prompt\\mathrm{.generate}}(z_{p})}\\\\ &{f_{\\mathrm{prompt}}\\gets\\mathrm{model.PromptEncoder}(p t_{\\mathrm{spatial}})}\\\\ &{p r e d_{\\mathrm{pseualo}}\\gets\\mathrm{model.Decoder}(f_{\\mathrm{img}},f_{\\mathrm{prompt}})}\\\\ &{\\underset{a}{l_{\\mathrm{pseudo}}}\\gets l_{\\mathrm{pseudo}}+\\mathrm{DiceLoss}(p r e d_{\\mathrm{pseudo}},z_{p})+\\mathrm{BCELoss}(p r e d_{\\mathrm{pseudo}},z_{p})}\\end{array}$   \n22:   \n23:   \n24:   \n25:   \n26: end for   \n27: $l\\Leftarrow l_{\\mathrm{gt}}+\\alpha\\times l_{\\mathrm{pseudo}}$   \n28: update(model, l)   \n29: end for   \n30: return model ", "page_idx": 17}, {"type": "text", "text": "C Additional Experimental Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Comparative experiments to compare with task-specific segmentation models. Task-specific segmentation models mainly fall into two architectures, CNN-based models and Transformer-based models. We conduct comparative experiments with representative CNN-based models i.e. 3DUXNet[23] and nnU-Net[22], and representative Transformer-based models i.e. SwinUNETR[20]. We conduct additional comparative experiments on the split $20\\%$ test set of the datasets. 10 segmentation tasks are selected from BTCV[52] and MSD-spleen[56] datasets, which focus on organ segmentation, and from MSD-lung, MSD-colon, and MSD-liver datasets, which focus on lesion segmentation. We train task-specific segmentation models on each dataset individually for each method. ", "page_idx": 17}, {"type": "text", "text": "The quantitative experimental results are summarized in Figure 9. Generally speaking, SegVol, jointly trained on 25 datasets, outperforms traditional task-specific segmentation models trained on a single dataset. Compared to these strong baselines, SegVol exhibits a narrower distribution of Dice scores across the eight tasks, indicating its robustness and good generalization ability. This mainly owes to the massive knowledge learned from diverse samples of the same categories but different datasets. SegVol depicts excellent performance on lesion tasks which are more challenging in semantic understanding and spatial locating. We present a detailed comparison to nnU-Net[22] on lesion tasks. As shown in Table 7, the average Dice score of $\\mathrm{SegVol}$ is $14.76\\%$ higher than that of nnU-Net for lesion tasks. We visualize the prediction results of the two methods in Figure 10, which intuitively show that SegVol performs more precise segmentation of the tumors than nnU-Net. The detailed scores and visualization results are presented in Table 9 and Figure 11 12, and 13. ", "page_idx": 17}, {"type": "image", "img_path": "105ZuvpdyW/tmp/6f0f6d48d382f69b3e14bea684b5ff75d5390d38cb108d09b3d39f3023fe85c6.jpg", "img_caption": ["Figure 8: The demonstration of the training algorithm. Specifically, each case (training sample) consists of an Image $x.$ , a Ground Truth(GT) Mask Set Y, and a Pseudo Mask Set Z. The training loss of each sample consists of the ground-truth loss and the pseudo loss. The ground-truth loss is computed by inputting the image, the ground-truth mask (label), and the sampled prompt into the model, while the pseudo loss is computed by inputting the image, the pseudo label, and the fixed prompt into the model. Finally, the model is updated by minimizing the weighted sum of the two losses. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "105ZuvpdyW/tmp/c75749d5ef67c1db57df62faf969ee8bfd0ef5d53544e1fa647b7fe59e0e354b.jpg", "img_caption": ["Figure 9: Violin plots for comparing experiment results of SegVol and task-specific methods. The vertical axis is the Dice score. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "We analyze that there are mainly three factors that make $\\mathrm{SegVol}$ more powerful than traditional taskspecific models: 1) Massive generative pre-training on unlabeled data endows SegVol with a complete understanding of the volumetric structures and the discriminative feature representations, which is much superior to learning from a small number of samples. 2) Learning from joint datasets with semantic-prompt makes SegVol generalize better to unseen data and categories. For instance, SegVol can learn from both the \u2018left kidney\u2019 and \u2018kidney\u2019 categories based on their semantic correlation, while traditional task-specific models treat the two categories independently. 3) SegVol can be prompted with (spatial) points/bboxes, which provide a precise spatial reference, and (semantic) texts, which disambiguate the overlap of multiple categories in the same space. In contrast, traditional methods are not able to understand semantics. This ability enables SegVol to perform better than traditional methods in challenging tasks, e.g., segmenting lesions. ", "page_idx": 18}, {"type": "text", "text": "Table 7: The comparison of the average Dice score of $\\mathrm{SegVol}$ and nnU-Net[22] across 3 lesion segmentation tasks. ", "page_idx": 19}, {"type": "table", "img_path": "105ZuvpdyW/tmp/a49c7e9c2980635aa931eecbd09df75c01eae205feb488f6ce7fb1e854467280.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "105ZuvpdyW/tmp/866c178e3a415675d194e7794a54e7dee3dcb05be4fed16c386eb1dfab44673d.jpg", "img_caption": ["Figure 10: Visualization results of SegVol and nnU-Net across 3 lesion segmentation tasks. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Supplement results for comparative experiments on SAM-like interactive segmentation methods. In this work, we compare SegVol with 5 SAM-like interactive segmentation methods on AMOS22[44], ULS23[74], and SegTHOR[75] datasets. The detailed records of Dice score are demonstrated in Table 10. The visualization results are shown in Figure 14, Figure 15, Figure 16. In this experiment, SegVol is driven by \u2018bbox+text\u2019 prompt. We demonstrate the consistency results among different prompt settings of $\\mathrm{SegVol}$ in Figure 17, which is also conducted on AMOS22[44] and ULS23[74]. Relatively poor text prompt results in ULS23 are due to the unclear category of the dataset. ", "page_idx": 19}, {"type": "text", "text": "We also compare the Total Parameters, the average Multiply-Accumulates(MACs), and the average Time required to process a case of the different SAM-like methods, as shown in Table6. The comparison indicates that our method takes less computational cost while achieving much better performance. Note that when calculating MACs Per Case and Time Per Case, the slice-by-slice calculation of the 2D method and the scanning process of the 3D method are accumulated respectively. SAM-MED3D[39] only processes volume with a size of $128\\times128\\times128$ . The experiments are implemented on the validation set of AMOS22[44], and the setting is the same as that in Sec. 3.2 ", "page_idx": 19}, {"type": "text", "text": "Supplement results for ablation studies on zoom-out-zoom-in mechanism. We conduct ablation study on zoom-out-zoom-in mechanism on the split $20\\%$ test data of AMOS22[44] dataset. As shown in Table 11, the zoom-out-zoom-in mechanism achieves higher Dice scores compared to the resize and sliding window strategies in 15 organ categories. In the aspect of inference efficiency, the zoom-out-zoom-in mechanism is also very competitive and quite close to the simple resize method. ", "page_idx": 19}, {"type": "table", "img_path": "105ZuvpdyW/tmp/4bccc6619d8d0d6c95809408d100a6297eedc84e90cb52cf8652e96ed4765b87.jpg", "table_caption": ["Table 8: Few-shot fine-tuning experiment on FLARE22[56, 57] and MSD-spleen[56]. SegVol\\* represents the model fine-tuned on all datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "105ZuvpdyW/tmp/c4244ea874bacc9b3543428fd03817126cf59fcc7fc405200c4cfc1648334616.jpg", "img_caption": ["Figure 11: Visualized aorta and left kidney prediction results of 3DUX-NET[23], SwinUNETR[20], nnU-Net[22] and $\\mathrm{SegVol}$ on 4 cases from the split test set. For the integrality of aorta and left kidney structure modeling, SegVol significantly outperforms 3DUX-NET and SwinUNETR and is comparable to nnU-Net. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "105ZuvpdyW/tmp/0aa01bdeb7df3ee4181f02506b2b97c0695a3715f293d4f9d51ac3e1cc1cf503.jpg", "img_caption": ["Figure 12: Visualized liver and pancreas prediction results of 3DUX-NET[23], SwinUNETR[20], nnU-Net[22] and $\\mathrm{SegVol}$ on 4 cases from the split test set. For the modeling of pancreas, SegVol is significantly superior to other baseline methods. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "105ZuvpdyW/tmp/a7ac70dda476083cc03e59d155ca3e7aafa8195db27d89bae3838246f95c655d.jpg", "img_caption": ["Figure 13: Visualized spleen and stomach prediction results of 3DUX-NET[23], SwinUNETR[20], nnU-Net[22] and SegVol on 4 cases from the split test set. For the consistency and stability of stomach modeling, SegVol is significantly better than other methods. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "105ZuvpdyW/tmp/bded68a4989d11ee4c463b78c88bf52faa0a822ce1d7a749e77377f705fc4167.jpg", "img_caption": ["Figure 14: Visualized aorta and bladder prediction results of MedSAM[29], SAM(bbox)[28], SAMMED2D[38], SAM-MED3D[39], SAM(points)[28] and SegVol on 4 cases from split test data. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "105ZuvpdyW/tmp/6b99e1c282d1c17c9b519db1bd7435d41d4db211981a49724fc289b620ccdab5.jpg", "img_caption": ["Figure 15: Visualized gall bladder and left kidney prediction results of MedSAM[29], SAM(bbox)[28], SAM-MED2D[38], SAM-MED3D[39], SAM(points)[28] and SegVol on 4 cases from split test data. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "105ZuvpdyW/tmp/35f7cb1094c4f1cdd9073a2e339cd2d54978dab5999537f9600f0b75512a28ae.jpg", "img_caption": ["Figure 16: Visualized liver and prostate/uterus prediction results of MedSAM[29], SAM(bbox)[28], SAM-MED2D[38], SAM-MED3D[39], SAM(points)[28] and SegVol on 4 cases from split test data. "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "105ZuvpdyW/tmp/cd746e49e7e97a76657ec6dc5d5051abb21d70dce7d91979d23d79022bd9508e.jpg", "table_caption": ["Table 9: Comparative experiment results of 3DUX-NET, SwinUNETR, nnU-Net, and $\\mathrm{SegVol}$ on the test set of supervised fine-tuning datasets in terms of Dice score. Dice scores are displayed as \u2018Median values (First quartile, Third quartile)\u2019. "], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "105ZuvpdyW/tmp/7527d8f3d66603a42b616cca6cf2470fd34ee725b4c72223056e98c73e2e4af2.jpg", "img_caption": ["Figure 17: The bar chart illustrates the consistency of SegVol\u2019s performance across different prompt types on the validation set of AMOS22[44] and ULS23[74]. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Generalization performance of SegVol on MRI. We discuss the generalization performance of SegVol on an external MRI dataset. We collect 60 MRI scans annotated with 4 key organ categories from CHAOS[40, 41, 42] dataset and evaluate the generalization ability to unseen modality of SegVol. It achieves median Dice scores of $85.70\\%$ , $80.09\\%$ , $80.04\\%$ , and $81.46\\%$ for liver, spleen, left kidney, and right kidney, respectively. This generalization result demonstrates the robustness of $\\mathrm{SegVol}$ in the face of completely unseen modality data. The detailed scores and visualization results are presented in Table 12 and Figure 18. ", "page_idx": 26}, {"type": "text", "text": "Few-shot fine-tuning experiment on small datasets. To evaluate the few-shot learning ability of our model, we conduct the few-shot fine-tuning experiment on small datasets, FLARE22[56, 57] (40 training cases) and MSD-spleen[56] (32 training cases). Table 8 demonstrates that 1) fine-tuning $\\mathrm{SegVol}$ on dozens of samples works well on easy datasets such as MSD-spleen, in which the few-shot learning performance is close to the joint fine-tuning on all datasets; 2) for challenging datasets such as FLARE22, fine-tuning on all datasets can achieve much better performance. ", "page_idx": 26}, {"type": "table", "img_path": "105ZuvpdyW/tmp/123edc28cc5fa82b4d39efa897a30993ce9c7df91fcdd254072eaff0a83c1657.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "105ZuvpdyW/tmp/674c152ca1d4f0fb19983ad6f726a97b42099765e2ccc1ee59f4cc040e4570ff.jpg", "table_caption": ["Table 11: Dice score and inference time results of ablation study on zoom-out-zoom-in mechanism. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 12: Generalization experiment results of $\\mathrm{SegVol}$ on the MRI set of CHAOS[40, 41, 42] dataset in term of Dice score. Dice scores are displayed as \u2018Median values (First quartile, Third quartile)\u2019. Method Liver Spleen Left Kidney Right Kidney SegVol(5 Points) 0.8091 (0.7376, 0.8554) 0.7496 (0.6990, 0.7872) 0.7216 (0.6125, 0.7869) 0.7174 (0.6052, 0.8090) SegVol(Bbox) 0.8570 (0.8319, 0.8819) 0.8009 (0.7702, 0.8256) 0.8004 (0.7265, 0.8452) 0.8146 (0.7593, 0.8620) ", "page_idx": 29}, {"type": "image", "img_path": "105ZuvpdyW/tmp/64591ec440ae6d410e6ad77af00f7b10fa8d15eb066de4d6ac75764a5528e809.jpg", "img_caption": ["Figure 18: Visualized liver, spleen, and kidney prediction results of $\\mathrm{SegVol}$ on 12 cases from MRI set of CHAOS[40, 41, 42]. For unseen MRI modality, $\\mathrm{SegVol}$ is still able to segment these four organs relatively accurately. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "D Evaluation Metrics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Each subset of the joint dataset is split into $80\\%$ training data and $20\\%$ test data. To ensure the absence of any data leaks, the hash value is utilized to compare the test set and training set. And in the comparative experiments, the model\u2019s parameters are all frozen. ", "page_idx": 29}, {"type": "text", "text": "We use the Dice Similarity Coefficient (Dice score) as a metric to evaluate the model, which is defined as $\\begin{array}{r}{D S C=\\frac{2|X\\cap Y|}{|X|+|Y|}}\\end{array}$ |2X|X|+\u2229|YY  ||. |X \u2229Y | is the cardinality of the intersection of the predicted segmentation sets $X$ and the ground truth sets $Y$ . $|X|$ and $|Y|$ are the cardinalities of sets $X$ and $Y$ respectively. Dice ", "page_idx": 29}, {"type": "text", "text": "score is a commonly used metric for evaluating image segmentation tasks. It measures the degree of similarity between predicted segmentation and true segmentation, making it particularly suitable for evaluating the overlap degree of binary segmentation results. ", "page_idx": 30}, {"type": "text", "text": "E Additional Discussion ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We present $\\mathrm{SegVol}$ , a 3D foundational model for interactive and universal volumetric medical image segmentation. This method has been developed using 90K unlabeled CTs and 25 open-source medical datasets. This results in a universal segmentation tool capable of generating accurate responses for over 200 anatomical targets. Furthermore, SegVol demonstrates state-of-the-art volumetric segmentation performance when compared with both traditional task-specific methods[20, 21, 22, 23] and the recent SAM-like interactive methods[29, 38, 39, 28] in several comparative experiments. Despite its universality and high precision, SegVol maintains a simple architecture compared to other volumetric segmentation methods. ", "page_idx": 30}, {"type": "text", "text": "SegVol\u2019s capability of interactive and precise segmentation makes it a promising clinical aid tool. It can assist clinicians in identifying and quantifying tumor location, size, and shape changes within a patient\u2019s body[1] more accurately and rapidly. This precise monitoring aids clinicians in detecting tumor growth trends, assessing treatment effectiveness, and adjusting treatment plans as needed. Additionally, clinicians can use $\\mathrm{SegVol}$ to accurately identify and segment important structures within a patient\u2019s body, such as organs, blood vessels, or the precise location of tumors and surrounding tissues, using high-resolution 3D images such as CT volumes. These precise segmentation results help clinicians better understand the patient\u2019s anatomical structures, plan surgical pathways, reduce surgical risks, and improve the accuracy and success rate of surgeries[3]. ", "page_idx": 30}, {"type": "text", "text": "While $\\mathrm{SegVol}$ is capable of understanding semantic-prompt composed of sentences, there remains a gap between it and the referring expression segmentation that involves complex semantic information and logical relationships. The establishment of a referring expression segmentation model needs more curated data with spatial annotations with text. Our SegVol provides a foundation for realizing referring segmentation of medical images, and we leave it as future work. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have discussed the limitations of the work in Section 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All of the datasets involved in this work are open-source and accessible, which have been summarized in Section A. The proposed model is described in Section 2.2. The detailed training algorithm is present in Section B. The experimental setup is given in Section 3.1. The trained model and code will be released after the review period. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open Access to Data and Code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All of the datasets involved in this work are open-source and accessible, which have been summarized in Section A. The construction process of data is described in Section 2.1. The trained model and code will be released after the review period. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The detailed experiment setting and information on testing data is described in Section 3.1. The trained model and code will be released after the review period. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All experiments are conducted for multiple times. The violin plots of 32 segmentation tasks are provided in Figure 2 and in Figure 9. Median values, first quartiles, and third quartiles of comparative experiments are present in Table 9 and Table 10. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The information on computer resources is provided in Section 3.1. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Research conducted in the paper conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We contribute a foundation model for universal and interactive volumetric medical image segmentation, which can benefit numerous clinical study and applications. We do not see any obvious negative societal impact of the proposed method and model. Detailed discussion is provided in the Section 4 and Section E. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No model in this paper is with a high risk for misuse. The collected datasets are all open-source and accessible. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The creators or original owners of assets, used in the paper, are all properly credited. The license and terms of use are explicitly mentioned and properly respected. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All of the datasets involved in this work are open-source and accessible, which have been summarized in Section A. The trained model will be released after reviewing. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]