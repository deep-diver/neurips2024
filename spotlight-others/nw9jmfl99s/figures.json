[{"figure_path": "nw9JmfL99s/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Localization in spatial receptive fields (RFs) measured from non-human primate (NHP) primary visual cortex [Rin02, Fig. 2] and in spatiotemporal RFs measured from NHP [dBM98, Fig. 2] and ferret [Sin+18, Fig. 2] primary auditory cortex. (Center) Half-slice of the localized first-layer kernels of AlexNet trained for ImageNet classification [KSH12]. (Right) Localized receptive fields learned from the task of Section 2.3 in 2-D using ICA [HO00] and the soft committee machine (SCM; M1 with fixed second-layer weights) of Section 2.1. Localization\u2014spatial and/or temporal selectivity\u2014appears across settings, as measured by response maximization in biological systems (left) and by inspecting linear filters in artificial systems (center, right).", "description": "This figure shows examples of localized receptive fields in various settings. The left panel displays localized receptive fields from biological systems (non-human primate primary visual and auditory cortices). The center panel presents localized first-layer kernels from AlexNet, a deep neural network trained on ImageNet. The right panel showcases localized receptive fields learned from a data model inspired by natural images using ICA and a soft committee machine.", "section": "1 Introduction"}, {"figure_path": "nw9JmfL99s/figures/figures_3_1.jpg", "caption": "Figure 1: (Left) Localization in spatial receptive fields (RFs) measured from non-human primate (NHP) primary visual cortex [Rin02, Fig. 2] and in spatiotemporal RFs measured from NHP [dBM98, Fig. 2] and ferret [Sin+18, Fig. 2] primary auditory cortex. (Center) Half-slice of the localized first-layer kernels of AlexNet trained for ImageNet classification [KSH12]. (Right) Localized receptive fields learned from the task of Section 2.3 in 2-D using ICA [HO00] and the soft committee machine (SCM; M1 with fixed second-layer weights) of Section 2.1. Localization\u2014spatial and/or temporal selectivity\u2014appears across settings, as measured by response maximization in biological systems (left) and by inspecting linear filters in artificial systems (center, right).", "description": "This figure shows examples of localized receptive fields (RFs) in different neural systems.  The left panel displays examples from biological systems: spatial RFs in NHP primary visual cortex, and spatiotemporal RFs in NHP and ferret primary auditory cortex. The center panel shows localized first-layer kernels from AlexNet, a deep neural network trained on ImageNet. The right panel shows localized receptive fields learned from a specific task using independent component analysis (ICA) and a soft committee machine.  The figure highlights the ubiquity of localized RFs across various neural systems, both biological and artificial.", "section": "1 Introduction"}, {"figure_path": "nw9JmfL99s/figures/figures_6_1.jpg", "caption": "Figure 3: From left and for the same Ising, NLGP, and Kur data models as in Fig. 2: the marginals p(X\u2081), the amplifier defined in Theorem 3.1 and kurtosis \u03ba, and the evolution of simulated receptive fields for the single-neuron model (M2) trained on its data, and lastly the receptive field given by numerically integrating Eq. (5) with expanded to a third-order Taylor approximation for the same data; training or evolution time is indicated by line color (blue for early-time; red for late-time). See Section 4.1 for exposition.", "description": "This figure shows the marginals, amplifier function, and kurtosis for three different data models (Ising, NLGP, Kur).  It then displays how simulated receptive fields evolve over time for a single-neuron model trained on each dataset, comparing the simulations to the results of a third-order Taylor expansion approximation of the theoretical model. The color of the lines represents the training time.", "section": "4 Experimental results"}, {"figure_path": "nw9JmfL99s/figures/figures_7_1.jpg", "caption": "Figure 4: Evolution of receptive fields learned by the single-neuron model (M2), along with sinusoids fit to final states (red dashes) when trained on data from three elliptical distributions: t40 (v = 3) (left), the surface of an ellipse (middle), and a custom elliptical distribution that places its mass near the outside of an ellipse (right). In all cases, the learned receptive field is oscillatory (a sinusoid), as predicted by Proposition 3.3. The l\u2082 distances between the fitted oscillatory weights and empirical RFs, as a ratio of the l\u2082 norm of the empirical RFs, are (left) 9.77%, (center) 3.75%, and (right) 4.14%. See Section 4.3 for exposition.", "description": "This figure shows the results of training a single-neuron model (M2) on three different elliptical distributions.  The left panel shows the results for a t40(v=3) distribution, the middle panel shows results for data sampled from the surface of an ellipse, and the right panel shows results for a custom elliptical distribution concentrated near the ellipse's outer edge.  In all three cases, the learned receptive fields are oscillatory, confirming Proposition 3.3, which states that elliptical data prevent localization in the single neuron model. The red lines show the best-fit sinusoid to the learned weight vectors.", "section": "4.3 Validating Proposition 3.3: Elliptical distributions fail to localize"}, {"figure_path": "nw9JmfL99s/figures/figures_7_2.jpg", "caption": "Figure 5: IPR vs. excess kurtosis for NLGP and Kur data models, with mean and std. dev. across 30 re-initializations for the single-neuron model (M2); error bars are small and may not be visible.", "description": "This figure validates Claim 3.2 from the paper by showing the relationship between the inverse participation ratio (IPR) and excess kurtosis for two data models, NLGP and Kur. The IPR is a measure of localization, with higher values indicating more localized receptive fields. The excess kurtosis is a measure of the non-Gaussianity of the data, with negative values indicating heavier tails than a Gaussian distribution.  The plot shows that as the excess kurtosis becomes more negative (heavier tails), the IPR increases, indicating a stronger tendency towards localization. This supports the claim that negative excess kurtosis in the input data is a necessary condition for the emergence of localized receptive fields in the single-neuron model.", "section": "4.1 Validating Claim 3.2 with positive and negative predictions"}, {"figure_path": "nw9JmfL99s/figures/figures_8_1.jpg", "caption": "Figure 6: (Left, Center) Receptive fields learned by many-neuron (M1) soft committee machines (second-layer weights fixed at) trained on the Kur(10) and Kur(4) datasets, respectively. The models had N = 40 input units, K = 10 hidden units, and an initialization variance of 0.1. (Right) A random subset of 10 components from the 40 learned by the FastICA algorithm from scikit-learn [Hyv99; Ped+11] on the Kur(3) dataset with length-scale correlation values of \u03be\u03bf = 1 and \u00a71 = 3. See Section 4.4 for exposition.", "description": "This figure shows the receptive fields learned by three different models trained on data with different kurtosis values. The left and center panels show the receptive fields learned by a many-neuron model with fixed second-layer weights, trained on data with kurtosis values of 10 and 4 respectively. The right panel shows the receptive fields learned by an ICA model trained on data with kurtosis value of 3. The figure demonstrates that the type of receptive fields learned depends on both the model used and the statistical properties of the data.", "section": "4.4 Validating Proposition 3.3: Elliptical distributions fail to localize"}, {"figure_path": "nw9JmfL99s/figures/figures_9_1.jpg", "caption": "Figure 3: From left and for the same Ising, NLGP, and Kur data models as in Fig. 2: the marginals p(X\u2081), the amplifier defined in Theorem 3.1 and kurtosis K, and the evolution of simulated receptive fields for the single-neuron model (M2) trained on its data, and lastly the receptive field given by numerically integrating Eq. (5) with expanded to a third-order Taylor approximation for the same data; training or evolution time is indicated by line color (blue for early-time; red for late-time). See Section 4.1 for exposition.", "description": "This figure shows a comparison of the marginal distributions, the amplifier function (from Theorem 3.1), the kurtosis, and the learned receptive fields for three different data models (Ising, NLGP, Kur). It also shows the results of numerically integrating Equation 5 (from Lemma 3.1) using a third-order Taylor expansion of the amplifier function. This comparison aims to validate the theoretical model by showing how well it predicts the localization of receptive fields in different scenarios.", "section": "4 Experimental results"}, {"figure_path": "nw9JmfL99s/figures/figures_20_1.jpg", "caption": "Figure 3: From left and for the same Ising, NLGP, and Kur data models as in Fig. 2: the marginals p(X\u2081), the amplifier defined in Theorem 3.1 and kurtosis K, and the evolution of simulated receptive fields for the single-neuron model (M2) trained on its data, and lastly the receptive field given by numerically integrating Eq. (5) with expanded to a third-order Taylor approximation for the same data; training or evolution time is indicated by line color (blue for early-time; red for late-time). See Section 4.1 for exposition.", "description": "This figure shows a comparison of the marginal distributions, the amplifier function (from Theorem 3.1), and the kurtosis for three different data models (Ising, NLGP, and Kur). It also displays the evolution of simulated receptive fields for a single-neuron model trained on each data model, alongside the receptive fields obtained by numerically integrating Equation (5) using a third-order Taylor expansion. The color of the lines indicates the training time (blue for early-time, red for late-time).", "section": "4.1 Validating Claim 3.2 with positive and negative predictions"}]