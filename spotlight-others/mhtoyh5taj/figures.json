[{"figure_path": "mHtOyh5taj/figures/figures_1_1.jpg", "caption": "Figure 1: Illustrations of the motivation of this work. (a) Images with identical rescaled MOS from various IQA datasets exhibit significant variations in perceptual quality. (b) Images that cluster at the same rating level from different IQA datasets display mismatches due to differing subjective testing methodologies. (c) By comparing MOSs within the same dataset, it facilitates the flexible combination of multiple IQA datasets.", "description": "This figure illustrates the challenges of using absolute quality ratings for Image Quality Assessment (IQA).  Panel (a) shows that images with the same Mean Opinion Score (MOS) can have vastly different perceived quality depending on the dataset they come from. Panel (b) highlights the problem of different datasets using different scales for rating quality.  Panel (c) proposes a solution: using relative comparisons within a dataset, allowing for the flexible combination of multiple IQA datasets to improve model training and generalization.", "section": "1 Introduction"}, {"figure_path": "mHtOyh5taj/figures/figures_3_1.jpg", "caption": "Figure 2: Training and inference phash of Compare2Score. (a) The LMM is fine-tuned with instruction-response pairs generated by comparing the MOSs from the same IQA dataset, allowing for a more flexible combination of various IQA datasets. (b) The trained visual quality comparator (i.e., LMM) is utilized to compute the likelihood of a test image being preferred over the anchor images, and then the quality score is derived using MAP estimation.", "description": "This figure illustrates the training and inference phases of the Compare2Score model.  Panel (a) shows how the large multimodal model (LMM) is fine-tuned using comparative instructions derived from Mean Opinion Scores (MOS) within individual IQA datasets. This approach allows for flexible integration of multiple datasets. Panel (b) depicts the inference process: the trained LMM compares a test image against multiple anchor images to determine the likelihood of the test image being preferred, which is then used to calculate a continuous quality score using Maximum a Posteriori (MAP) estimation.", "section": "3 The Compare2Score Framework"}, {"figure_path": "mHtOyh5taj/figures/figures_5_1.jpg", "caption": "Figure 3: Architecture of the proposed Compare2Score. Images are initially processed by an image encoder, followed by token reduction through an abstractor module. The aligned textual and visual embedding are interleaved and processed by the large language model (LLM) decoder to generate precise qualitative comparative levels for paired comparisons.", "description": "This figure illustrates the architecture of the Compare2Score model.  It shows how two images are first processed by an image encoder, then reduced in dimensionality by an abstractor. These reduced representations are combined with textual embeddings, and then processed by a large language model (LLM) decoder. The output of the LLM is the comparison of the image quality relative to the other.", "section": "3.3 Structure: Multi-image LMM as Visual Quality Comparator"}, {"figure_path": "mHtOyh5taj/figures/figures_8_1.jpg", "caption": "Figure 4: Comparisons of SRCC results and running time with different numbers of anchor image per quality interval (\u03b2).", "description": "This figure shows the impact of the number of anchor images per quality interval (\u03b2) on both the SRCC scores (a measure of ranking correlation) and the inference time of the Compare2Score model across six different IQA datasets.  The x-axis represents the number of anchor images (\u03b2), and the y-axis shows the SRCC.  For each dataset, a separate line plot shows the trend of SRCC values as \u03b2 varies. A second y-axis is included to show the running time (in seconds) associated with each \u03b2 value.  The plot demonstrates that increasing \u03b2 beyond a value of 1 does not significantly improve the SRCC scores but substantially increases the running time, indicating that \u03b2=1 provides a good balance between accuracy and efficiency.", "section": "4.3 Ablation Studies"}, {"figure_path": "mHtOyh5taj/figures/figures_16_1.jpg", "caption": "Figure 5: Illustration of the five anchor images selected from KonIQ-10k [7]. (a) MOS = 1.09, \u03c3 = 0.29; (b) MOS = 2.02, \u03c3 = 0.39; (c) MOS = 2.96, \u03c3 = 0.38; (d) MOS = 3.21, \u03c3 = 0.41; (e) MOS = 4.01, \u03c3 = 0.34.", "description": "This figure shows five example anchor images selected from the KonIQ-10k dataset. Each image is labeled with its mean opinion score (MOS) and standard deviation (\u03c3), representing the perceived image quality and variability in subjective ratings. These anchor images represent different levels of visual quality within the dataset, used as references during the soft comparison phase of the Compare2Score framework for quality score inference.", "section": "3.4 Inference Conversion: Adaptive Soft Comparison"}, {"figure_path": "mHtOyh5taj/figures/figures_16_2.jpg", "caption": "Figure 5: Illustration of the five anchor images selected from KonIQ-10k [7]. (a) MOS = 1.09, \u03c3 = 0.29; (b) MOS = 2.02, \u03c3 = 0.39; (c) MOS = 2.96, \u03c3 = 0.38; (d) MOS = 3.21, \u03c3 = 0.41; (e) MOS = 4.01, \u03c3 = 0.34.", "description": "This figure shows five example anchor images selected from the KonIQ-10k dataset.  Each image represents a different level of perceived quality, as indicated by its mean opinion score (MOS) and standard deviation (\u03c3). The images are visually diverse, showcasing the range of quality levels the model considers during inference.", "section": "3.4 Inference Conversion: Adaptive Soft Comparison"}, {"figure_path": "mHtOyh5taj/figures/figures_16_3.jpg", "caption": "Figure 7: Illustration of the five anchor images selected from AGIQA-3K [61]. (a) MOS = 0.73, \u03c3 = 0.10; (b) MOS = 0.95, \u03c3 = 0.12; (c) MOS = 2.27, \u03c3 = 0.14; (d) MOS = 3.41, \u03c3 = 0.16; (e) MOS = 3.96, \u03c3 = 0.17.", "description": "This figure shows five example anchor images selected from the AGIQA-3K dataset to represent different levels of perceived image quality.  Each image is labeled with its mean opinion score (MOS) and standard deviation (\u03c3), indicating the variability in human ratings for that image's quality. The images illustrate a range of visual characteristics and demonstrate that the chosen anchors represent a diverse range of qualities within the dataset.", "section": "3.4 Inference Conversion: Adaptive Soft Comparison"}]