[{"figure_path": "3uI4ceR4iz/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of our SA3DIP with other methods. Methods like SAI3D (bottom) fail to distinguish instances with similar normals when computing superpoints, which accumulate to the final segmentation. Moreover, the part-level 2D segmentation transfers to 3D space, resulting in over-segmented 3D instances. We present a novel pipeline for segmenting any 3D instances, which overcomes the limitations by exploiting additional 3D priors, specifically by incorporating both geometric and textural prior on superpoints computing, and supplementing 3D space constraint provided 3D prior by utilizing a 3D detector.", "description": "This figure compares the proposed SA3DIP method with other existing methods, specifically SAI3D. It highlights the limitations of SAI3D in distinguishing instances with similar normals during superpoint computation, leading to error accumulation and inaccurate final segmentation.  SAI3D also suffers from transferring part-level 2D segmentations to 3D, resulting in over-segmentation. In contrast, SA3DIP leverages additional 3D priors (geometric and textural) and 3D spatial constraints for improved accuracy and reduced over-segmentation.", "section": "1 Introduction"}, {"figure_path": "3uI4ceR4iz/figures/figures_3_1.jpg", "caption": "Figure 2: Overall pipeline. Our approach first integrates both geometric and textural priors for grouping 3D primitives (step A). Corresponding posed masks are generated using SAM. An affinity matrix is then computed based on these 2D-3D results serving as edge weights (step B). Region growing and instance-aware refinement are conducted on the constructed scene graph, utilizing 3D box constraint to address over-segmentation while maintaining the fine-grained outcomes (step C).", "description": "This figure illustrates the overall pipeline of the SA3DIP method. It consists of three main steps: 1. Complementary Primitives Generation: generating finer-grained 3D primitives using both geometric and textural information. 2. Scene Graph Construction: constructing a superpoint graph based on the generated primitives and their relationships, and using 2D masks from SAM to compute edge weights. 3. Region growing & Instance-aware refinement: performing region growing and instance-aware refinement on the constructed graph to obtain final instance segmentation results. The 3D detector is integrated into step 3 to further refine the results by using 3D prior.", "section": "3.1 SA3DIP"}, {"figure_path": "3uI4ceR4iz/figures/figures_5_1.jpg", "caption": "Figure 3: Overview of our proposed ScanNetV2-INS. We present the new benchmark for 3D class-agnostic instance segmentation, which rectifies incomplete annotations and incorporates more instances based on ScanNetV2. Row (a) shows the comparison before and after revision, and row (b) illustrates the object counts per scene between the two benchmarks.", "description": "This figure provides a visual comparison of the original ScanNetV2 dataset and the improved ScanNetV2-INS dataset.  Subfigure (a) shows 3D point cloud scenes with their respective ground truth annotations. The left column displays clean point clouds, the middle shows the original annotations, and the right column depicts the revised, enhanced annotations for ScanNetV2-INS.  Subfigure (b) presents a bar graph comparing the object count distribution in each dataset. The graph shows that ScanNetV2-INS has a higher number of scenes with more objects, indicating a more challenging and representative dataset for 3D instance segmentation.", "section": "3.2 ScanNetV2-INS"}, {"figure_path": "3uI4ceR4iz/figures/figures_8_1.jpg", "caption": "Figure 4: Visual comparison between our method with SAM3D [10], SAMPro3D [9], and SAI3D [8] on ScanNetV2, ScanNetV2-INS, and ScanNet++ dataset. Among all datasets, our method shows the most robust and accurate segmentation.", "description": "This figure shows a visual comparison of the proposed SA3DIP method against three other state-of-the-art methods (SAM3D, SAMPro3D, and SAI3D) and the ground truth on three different datasets (ScanNetV2, ScanNetV2-INS, and ScanNet++).  Each row represents one dataset, and each column shows either the input RGB-D images, the segmentation results produced by each method, or the ground truth segmentation. The red boxes highlight specific regions of interest where the differences between methods are most apparent. The figure demonstrates the superior performance of SA3DIP in generating robust and accurate 3D instance segmentations across various datasets.", "section": "4.2 Results on ScanNet series"}]