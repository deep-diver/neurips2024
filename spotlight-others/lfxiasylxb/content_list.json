[{"type": "text", "text": "In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liam Collins\\* Chandra Family Department of ECE The University of Texas at Austin liamc@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Advait Parulekar\\*   \nChandra Family Department of ECE   \nThe University of Texas at Austin advaitp@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Aryan Mokhtari Chandra Family Department of ECE The University of Texas at Austin mokhtari@austin.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Sujay Sanghavi Chandra Family Department of ECE The University of Texas at Austin sanghavi@mail.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Sanjay Shakkottai Chandra Family Department of ECE The University of Texas at Austin sanjay.shakkottai@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such, that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One of the most compelling behaviors of pretrained transformers is their ability to perform in-context learning (ICL) [1]: determining how to solve an unseen task simply by making a forward pass on input context tokens. Arguably the most critical innovation enabling ICL is the self-attention mechanism [2], which maps each token in an input sequence to a new token using information from all other tokens. A key design choice in this self-attention architecture is of the activation function that controls how much \u201cattention\" a token pays to other tokens. Softmax-activated self-attention (i.e. softmax attention) is most commonly, and successfully, used in practice [1, 3-6]. ", "page_idx": 0}, {"type": "text", "text": "A natural approach to explain ICL adopted by the literature is to equate it with classical machine learning algorithms, primarily variants of gradient descent (GD). Several works have shown that when the ICL tasks are linear regressions and the activation in the attention unit is identity (referred to as linear attention), transformers that implement preconditioned GD during ICL are global optima of the pretraining loss, which is the population loss on ICL tasks [7-9]. In particular, the prediction of such transformers with $l$ linear attention layers equals the prediction of a regressor trained with $l$ preconditioned GD steps on the context examples. However, since these analyses are limited to linear attention and tasks, they do not explain the widespread success of softmax attention at ICL. ", "page_idx": 0}, {"type": "image", "img_path": "lfxIASyLxB/tmp/ae2b6bc43a1b97a3884b5e4ac8188588fa93e767406dac27e1876285f60ae482.jpg", "img_caption": ["Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights - which determine the attention window - as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "More recent work [10] extends these results by showing that for general regression tasks and any attention activation that is a kernel, ICL equates to training a kernel regressor via functional GD in the Reproducing Kernel Hilbert Space (RKHS) induced by the activation. However, this functional GD yields generalization guarantees only when the activation kernel is identical to a kernel that generates the labels, which does not apply to the softmax activation, as it is not a kernel. Further, like the aforementioned studies of the linear setting [7-9], this analysis only shows that pretraining leads to learning the covariate distribution, while the activation implicitly encodes the label distribution needed for accurate predictions. Thus, this line of work has not explained the very fundamental question of what softmax attention learns during pretraining that enables it to perform ICL on a wide variety of downstream tasks. Motivated by this gap in the literature, we ask the following question. ", "page_idx": 1}, {"type": "text", "text": "How does softmax attention learn to perform ICL? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To answer this question, we study general settings in which pretraining and evaluation ICL tasks are regressions that share only Lipschitzness and label noise variance. Specifically, the rate at which their ground-truth labels change along particular directions in the input space, and the variance in the label noise, is similar across tasks. In such settings, we observe that softmax attention acts as a nearest neighbors regressor with an attention window - i.e. neighborhood of points around the query that strongly influence, or \u201cattend to\", the prediction - that adapts to the pretraining tasks. Specifically, our main result is as follows: ", "page_idx": 1}, {"type": "text", "text": "Main Claim: Softmax attention performs ICL by calibrating its attention window to the Lipschitzness and label noise variance of the pretraining tasks. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "While this does not contradict the line of work showing that ICL manifests via a \u201cmeta-learned\" gradient-based algorithm, we show in a general setting that a simpler mechanism can explain the capabilities of a widely accepted model of ICL. ", "page_idx": 1}, {"type": "text", "text": "Outline. We substantiate the above claim via two streams of analysis. To our knowledge, these are the first results showing that softmax attention pretrained on ICL tasks recovers shared structure among the tasks that facilitates ICL on downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "(1) Attention window scale adapts to Lipschitzness and noise variance -- Section 3. We prove that the pretraining-optimal softmax attention estimator scales its attention window inversely with the task Lipschitzness and jointly with the noise level to optimally trade-off bias and variance in its prediction (Theorem 3.4). This requires tight upper and lower bounds on the pretraining ICL loss. While the upper bounds (Lemma C.8) hold for all $L$ -Lipschitz tasks, the lower bounds (Lemma C.9) are more challenging and require considering specific classes of tasks. We consider two classes of generalized linear models (GLMs), and obtain lower bounds via novel concentrations for particular functionals on the distribution of the attention weights for tokens distributed on the hypersphere (Corollary G.5). ", "page_idx": 2}, {"type": "text", "text": "(2) Attention window directions adapt to direction-wise Lipschitzness - Section 4. We prove that when the target function class consists of linear functions that share a common low-dimensional structure, the optimal softmax attention weight matrix from pretraining projects the data onto this subspace (Theorem 4.4). In other words, softmax attention learns to zero-out the zero-Lipschitzness directions in the ambient data space, and thereby reduces the effective dimension of ICL. We prove this via a careful symmetry-based argument to characterize a particular gradient of the ICL loss as positive (Lemmas H.3 and H.4). ", "page_idx": 2}, {"type": "text", "text": "Tightness of results. Our results highlight the importance of shared Lipschitzness across training and test, as well as the critical role of the softmax activation, to ICL. We show that softmax attention pretrained on the setting from Section 3 in-context learns any downstream task with similar Lipschitzness to the pretraining tasks, while changing only the Lipschitzness of the evaluation tasks degrades performance (Theorem 3.5) - implying learning Lipschitzness is both sufficient and necessary for generalization. Further, to emphasize the necessity of the softmax, we show that the minimum ICL loss achievable by linear attention exceeds that achieved by pretrained softmax attention (Theorem 3.6). We verify all of these results with empirical simulations (Section 3.2 and Appendix J). ", "page_idx": 2}, {"type": "text", "text": "Notations. We use (upper-, lower-)case boldface for (matrices, vectors), respectively. We denote the (identity, zero) matrx in $\\mathbb{R}^{d\\times d}$ as $(\\mathbf{I}_{d},\\mathbf{0}_{d\\times d})$ , respectively,the set of column-orthonormal matrices in $\\stackrel{\\cdot}{\\mathbb{R}}^{d\\times k}$ $\\mathbb{O}^{d\\times\\hat{k}}$ and the (coumn space, 2-norm) of a matrix $\\mathbf{B}$ as (col(B), $\\|\\mathbf{B}\\|)$ ,respectivly.We indicate the unit hypersphere in $\\mathbb{R}^{d}$ by $\\mathbb{S}^{d-1}$ and the uniform distribution over $\\mathbb{S}^{d-1}$ as $\\mathcal{U}^{d}$ . We use asymptotic notation $(O,\\Omega)$ to hide constants that depend only on the dimension $d$ ", "page_idx": 2}, {"type": "text", "text": "1.1 Additional Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Numerous recent works have constructed transformers that can implement GD and other machine learning algorithms during ICL [11-15], but it is unclear whether pretraining leads to such transformers. [16] and [13] provide generalization bounds for ICL via tools from algorithmic stability and uniform concentration, respectively. [17] investigate the pretraining statistical complexity of learning a Bayes-optimal predictor for ICL on linear tasks with linear attention. [18-20] study the role of the pretraining data distribution, rather than the learning model, in facilitating ICL. [21] studies the dynamics of a softmax attention unit trained with GD on ICL tasks, but this analysis considers only linear tasks and orthogonal inputs. The connection between ICL with softmax attention and non-parametric regression has been noticed by other works that analyze the ICL performance of a softmax-like kernel regressor [22] and aim to improve upon softmax attention [23-27] rather than explain what it learns during pretraining. Please see Appendix A for further discussion of the large body of related works studying the theory of transformers, ICL and kernel regression. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In-Context Learning (ICL) regression tasks. We study ICL in the regression setting popularized by [28], wherein each task is a regression problem in $\\mathbb{R}^{d}$ . The context for task $t$ consists of a set $n$ featreetorpwiisa $\\{\\pmb{x}_{i}^{(t)},f^{(t)}(\\pmb{x}_{i}^{(t)})+\\epsilon_{i}^{(t)}\\}_{i=1}^{n}$ where $f^{(t)}:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ generates the ground-truth labels for task $t$ and $\\epsilon_{i}^{(t)}$ is label noise. Given this context, the model solves the task if it accurately predicts the label of a query w+1. During pretraining, the model ", "page_idx": 2}, {"type": "text", "text": "observes many such tasks. Then, it is evaluated on a new task with context $\\{\\pmb{x}_{i},f^{(*)}(\\pmb{x}_{i}^{(*)})+\\epsilon_{i}^{(*)}\\}_{i=1}^{n}$ and query n+1.t . We emphasize that the model is trained only on the pretraining tasks, not the evaluation context. Unlike traditional supervised learning, which would involve training on the context $\\{\\pmb{x}_{i},f^{(*)}(\\pmb{x}_{i}^{(*)})+\\epsilon_{i}^{(*)}\\}_{i=1}^{n}$ inordertoprediet $f^{(*)}(\\pmb{x}_{n+1}^{(*)})$   \npass, so there is no training using labels from $f^{(*)}$ . Our inquiry focuses on how ICL is facilitated by the softmax activation in the self-attention unit, which we introduce next. ", "page_idx": 3}, {"type": "text", "text": "The Softmax Attention Unit.  We  consider  a  single  softmax  attention  head $H_{S A}(\\cdot;\\theta)$ $\\mathbb{R}^{(d+1)\\times(n+1)}\\rightarrow\\mathbb{R}^{(d+1)\\times(n+1)}$ parameterized by $\\pmb{\\theta}:=(\\mathbf{W}_{K},\\mathbf{W}_{Q},\\mathbf{W}_{V})$ ,where $\\mathbf{W}_{K},\\mathbf{W}_{Q},\\mathbf{W}_{V}\\in$ $\\mathbb{R}^{(d+1)\\times(d+1)}$ aeql quence of tokens ${\\mathbf Z}=[z_{1},\\ldots,z_{n+1}]\\stackrel{\\cdot}{\\in}z^{(d+1)\\times(n+1)}$ , the atention layer creates a \u201chash map\" where the key-value pairs come from key and value embeddings of the input tokens, $\\left\\{\\mathbf{W}_{K}\\,z_{i}:\\mathbf{W}_{V}\\,z_{i}\\right\\}$ Each token $\\boldsymbol{z}_{i}$ is interpreted as a query. $\\mathbf{W}_{Q}\\,z_{i}$ , and during a pass through the attention layer, this query is matched with the keys $\\{\\mathbf{W}_{K}\\,z_{j}\\}_{j}$ to return an average over the associated values $\\{\\mathbf{W}_{V}\\,z_{j}\\}_{j}$ with a weight determined by the quality of the match (proportional to $e^{(\\mathbf{W}_{K}\\,\\pmb{z}_{j})^{\\top}(\\mathbf{W}_{Q}\\,\\pmb{z}_{i})})$ Specifically, $H_{S A}(\\mathbf{Z};\\bar{\\pmb{\\theta}})=[h_{S A}(\\mathbf{z}_{1},\\pmb{\\dot{\\mathbf{Z}}};\\pmb{\\theta}),\\cdots,\\bar{h_{S A}}(\\mathbf{z}_{n+1},\\mathbf{Z};\\pmb{\\theta})]$ ,where ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{S A}(\\mathbf{z}_{i},\\mathbf{Z};\\theta)=\\frac{\\sum_{j=1}^{n}\\left(\\mathbf{W}_{V}\\mathbf{z}_{j}\\right)\\;e^{(\\mathbf{W}_{K}\\mathbf{z}_{j})^{\\top}\\left(\\mathbf{W}_{Q}\\mathbf{z}_{i}\\right)}}{\\sum_{i=1}^{n}e^{(\\mathbf{W}_{K}\\mathbf{z}_{j})^{\\top}\\left(\\mathbf{W}_{Q}\\mathbf{z}_{i}\\right)}}\\in\\mathbb{R}^{d+1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With slight abuse of notation, we denote $h_{S A}(\\mathbf{z}_{j})=h_{S A}(\\mathbf{z}_{j},\\mathbf{Z};\\mathbf{\\theta})$ when it is not ambiguous. To study how this architecture enables ICL, we follow [28] to formalize ICL as a regression problem. Below we define the tokenization, pretraining objective and evaluation task. ", "page_idx": 3}, {"type": "text", "text": "Tokenization for regression. The learning model encounters token sequences of the form ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Z}:=\\left[{\\begin{array}{c c c c c}{x_{1}}&{x_{2}}&{\\cdots}&{x_{n}}&{x_{n+1}}\\\\ {f(x_{1})+\\epsilon_{1}}&{f(x_{2})+\\epsilon_{1}}&{\\cdots}&{f(x_{n})+\\epsilon_{n}}&{0}\\end{array}}\\right]\\in\\mathbb{R}^{(d+1)\\times(n+1)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the ground-truth labelling function $f$ maps from $\\mathbb{R}^{d}$ to $\\mathbb{R}$ and belongs to some class $\\mathcal{F}$ , each $\\epsilon_{i}$ is mean-zero noise, and the $i$ -th input feature vector $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ is jointly embedded in the same token with its noisy label $f(\\pmb{x}_{i})+\\epsilon_{i}\\in\\mathbb{R}$ . We denote this token $\\mathbf{z}_{i}$ . The ICL task is to accurately predict this label given the $n$ context tokens $\\{(\\pmb{x}_{i},f(\\pmb{x}_{i})+\\epsilon_{i})\\}_{i=1}^{n}$ where $f$ may vary across sequences. The prediction for the label of the $(n\\!+\\!1)$ -th feature vector is the $(d\\!+\\!1)$ -th element of $h_{S A}(\\mathbf{z}_{n+1})$ [10], denoted $h_{S A}(\\mathbf{z}_{n+1})_{d+1}$ . Utimately, the goal is to learn weight matrices such that $h_{S A}(\\mathbf{z}_{n+1})_{d+1}$ is likely to approximate the $(n+1)$ -th label on a random sequence $\\mathbf{Z}$ ", "page_idx": 3}, {"type": "text", "text": "Pretraining protocol. We study what softmax attention learns when its weight matrices are pretrained using sequences of the form of (1). These sequences are randomly generated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf\\sim D(\\mathcal{F}),\\quad x_{1},\\ldots,x_{n+1}\\overset{\\mathrm{i.i.d.}}{\\sim}D_{x}^{\\otimes(n+1)},\\quad\\epsilon_{1},\\ldots,\\epsilon_{n}\\overset{\\mathrm{i.i.d.}}{\\sim}D_{\\epsilon}^{\\otimes(n+1)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D({\\mathcal{F}})$ is a distribution over functions in $\\mathcal{F}$ \uff0c $D_{x}$ is a distribution over $\\mathbb{R}^{d}$ , and $D_{\\epsilon}$ is a distribution over $\\mathbb{R}$ with mean zero and variance $\\sigma^{2}$ . The token embedding sequence $\\mathbf{Z}$ is then constructed as in (1). Given this generative model, the pretraining loss of the parameters $\\pmb{\\theta}=(\\mathbf{W}_{Q},\\mathbf{W}_{K},\\mathbf{W}_{V})$ is the expected squared difference between the prediction of softmax attention and the ground-truth label of the $(n{+}1)$ -th input feature vector in each sequence, namely ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathcal{L}}(\\pmb{\\theta}):=\\mathbb{E}_{f,\\{\\pmb{x}_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left(h_{S A}(\\pmb{\\mathbf{z}}_{n+1})_{d+1}-f(\\pmb{x}_{n+1})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We next reparameterize the attention weights to make (3) more interpretable. For the last column of $\\mathbf{W}_{V}$ , we show in Appendix B that any minimizer of (3) in the settings we consider must have the first $d$ elements of this last column equal to zero. We follow [7, 9, 10] by setting the first $n$ columns of $\\mathbf{W}_{V}$ to zero. As in [10], we fix the $(d\\!+\\!1,d\\!+\\!1)$ -th element of $\\mathbf{W}_{V}$ , here as 1 for simplicity. In the same vein, we follow [7, 10] by setting the $(d\\!+\\!1)$ -th row and column of $\\mathbf{W}_{K}$ and $\\mathbf{W}_{Q}$ equal to zero. To summarize, the reparameterized weights are: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{W}_{V}=\\left[\\mathbf{0}_{d\\times d}\\quad\\mathbf{0}_{d\\times1}\\right],\\quad\\mathbf{W}_{K}=\\left[\\mathbf{M}_{K}\\quad\\mathbf{0}_{d\\times1}\\right],\\quad\\mathbf{W}_{Q}=\\left[\\mathbf{M}_{Q}\\quad\\mathbf{0}_{d\\times1}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\mathbf{M}_{K},\\mathbf{M}_{Q}\\in\\mathbb{R}^{d\\times d}$ . Now, since our goal is to reveal properties of minimizers of the pretraining loss, rather than study the dynamics of optimizing the loss, without loss of generality we can define $\\mathbf{M}:=\\mathbf{M}_{K}^{\\top}\\mathbf{M}_{Q}$ and re-define the pretraining loss (3) as a function of M. Doing so yields: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{M}):=\\mathbb{E}_{f,\\{x_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left(\\frac{\\sum_{i=1}^{n}(f(\\mathbf{x}_{i})+\\epsilon_{i})\\;e^{x_{i}^{\\top}\\mathbf{M}\\,x_{n+1}}}{\\sum_{i=1}^{n}e^{x_{i}^{\\top}\\mathbf{M}\\,x_{n+1}}}-f(\\mathbf{x}_{n+1})\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Interpretation of the pretraining loss. The loss (ICL) clarifies how softmax attention can be interpreted as a nearest neighbors regressor. When $\\mathbf{x}_{i}^{\\top}\\,\\mathbf{M}\\,\\mathbf{x}_{n+1}$ is a proxy for the distance between $\\pmb{x}_{i}$ and $x_{n+1}$ (which we formally show in Section 3 as happening under reasonable assumptions), the softmax attention prediction is a convex combination of the noisy labels with weights determined by the closeness of $\\pmb{x}_{i}$ to $x_{n+1}$ , such that the labels of points closer to $x_{n+1}$ have larger weight. Moreover, the decay in weights on points further from $x_{n+1}$ is exponential and controlled by M, which effectively defines a neighborhood, or attention window, of points around $x_{n+1}$ whose labels have non-trivial weight. More formally, we can think of the attention window defined for a query $x_{n+1}$ as the set AttnWindon $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,$ . As we have observed in Figure 1, our key insight is that pretrained M scales this attention window with the Lipschitzness of the function class. Generally speaking, larger $\\mathbf{M}$ entails averaging over a smaller window and incurring less bias due to the function values of distant tokens in the estimate, while smaller $\\mathbf{M}$ entails averaging over a larger window, resulting in larger bias due to distant token labels, but a smaller noise variance. Figure 2 further depicts this tradeoff. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Connection to non-parametric estimation and the Nadaraya- Watson estimator. A nonparamet", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "ric estimation technique to interpolate between known values of a function is to use a kernel estimator. The Nadaraya-Watson (NW) estimator [29-31] is one such estimator, and interpolates the data as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{N W}(\\pmb{x}_{n+1})=\\sum_{i}\\frac{K(x_{n+1},x_{i})f(x_{i})}{\\sum_{j}K(x_{n+1},x_{j})}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $K(r)=e^{-r^{2}/h}$ for somebandwidth $h$ In Section B.1 we show that optimizing the pretraining loss (ICL) reduces to meta-learning the bandwidth of an NW estimator on a distribution of pretraining tasks. However, to our knowledge, the literature has not determined the optimal bandwidth for the kernel, as there has been no analysis of non-asymptotic lower bounds on the loss, which we need to characterize the optimal solution. A close work to ours is [32], which considers regression on general $L$ -Lipschitz tasks, but this analysis provides only a tight upper bound on the loss. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.1 (Extreme cases). Consider the following two settings. ", "page_idx": 4}, {"type": "text", "text": "(i) Constant functions.If each of the functions the attention unit sees in pretraining is constant, as in the Left column of Figure 1, it is best to consider an infinite attention window, that is, take $\\mathbf{M}=\\mathbf{0}_{d\\times d}$ as this results in a uniform average over all the noisy token labels. ", "page_idx": 4}, {"type": "text", "text": "(i) Rapidly changing functions. If the pretraining functions change rapidly, as in the Right column of Figure 1, attending to a distant token might only corrupt the estimate at the target. For example suppose the input tokens are used to construct Voronoi cells on the surface of the hypersphere, and the label for a new token in a cell is the label of the token used to construct that cell.The optimal estimator attends only to the single nearest token since this incurs error only from label noise. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.2 (Softmax advantage). To further highlight the utility of the softmax, we compare with linear attention [7, 9, 11], whose estimator can be written as $\\begin{array}{r}{{h}_{L A}^{\\dot{\\mathrm{~\\,~}}}(\\pmb{x})=\\dot{\\sum_{i}}(f(\\pmb{x}_{i})+\\epsilon_{i})\\,\\pmb{x}_{i}^{\\top}\\,\\mathbf{M}\\,\\pmb{x}_{i}}\\end{array}$ up to a universal scaling due to the value embedding. This is again a weighted combination of labels, but one that does not allow for adapting an attention window - any scaling of M does not change the relative weights placed on each label - unlike softmax attention.Please see Figure $^{\\,l}$ (MiddleRow) for a comparison of the weights used in the different estimators. ", "page_idx": 4}, {"type": "text", "text": "3  Pretraining Learns Scale of Attention Window ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One of our observations of the attention estimator $h_{S A}$ is that it computes a nearest neighbours regression. We hypothesize that the role of pretraining is to select a neighbourhood within which to select tokens for use in the estimator. In this section we characterize the radius of this neighborhood. ", "page_idx": 4}, {"type": "image", "img_path": "lfxIASyLxB/tmp/334b324379dad215e73a3b93c4e93f680a314148aea48ee160b8e121a4e575cd.jpg", "img_caption": ["Figure 2: From left to right, as we shrink the attention window (shaded in blue), the estimator has lower bias (the expected value of the estimate, depicted in purple, is closer to the ground-truth label, depicted by the white circle) but larger variance (shaded in tan). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Lipschitzness). $A$ function $f:\\mathcal{X}\\to\\mathbb{R}$ has Lipschitzness $L$ if $L$ is the smallest number satisfying $\\bar{f}(\\pmb{x})-f(\\pmb{x}^{\\prime})\\leq L\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|$ for all $(\\pmb{x},\\pmb{x}^{\\prime})\\in\\bar{\\chi}^{2}$ ", "page_idx": 5}, {"type": "text", "text": "The general requirement for the function classes to which our results apply is that the class should be invariant to isometries, each function should be Lipschitz, and the function value at two points should be less correlated as those points get further. These are written formally in Assumption B.4. To be concrete, we work with the following two function classes that satisfy these assumptions (this is shown in Lemmas C.3 and C.7) to derive explicit bounds. ", "page_idx": 5}, {"type": "text", "text": "Denition3.2 (Affne and ReLUFunctionClasses). The function class $\\mathcal{F}_{L}^{a f\\!f}$ and $\\mathcal{F}_{L}^{+}$ arerespectively defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{L}^{a f f}:=\\{f:f(\\mathbf{x})=l\\;{\\mathbf{w}}^{\\top}\\,{\\mathbf{x}}+b,\\;{\\mathbf{w}}\\in\\mathbb{S}^{d-1},b,l\\in[-L,L]\\},}\\\\ &{\\mathcal{F}_{L}^{+}:=\\{f:f({\\mathbf{x}})=l_{1}({\\mathbf{w}}^{\\top}\\,{\\mathbf{x}})_{+}+l_{2}(-{\\mathbf{w}}^{\\top}\\,{\\mathbf{x}})_{+}+b,\\;{\\mathbf{w}}\\in\\mathbb{S}^{d-1},(b,l_{1},l_{2})\\in[-L,L]^{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$D(\\mathcal{F}_{L}^{a\\!f\\!f}),D(\\mathcal{F}_{L}^{+})$ are induced by drawingw $\\sim\\Sigma\\mathcal{U}^{d}$ and $b,l,l_{1},l_{2}\\stackrel{i.i.d.}{\\sim}U n i f([-L,L])$ forsome $\\Sigma\\succ$ ${\\mathbf0}_{d\\times d}$ Note that the max Lipschitzness of any function in these classes is $L_{i}$ and $(z)_{+}:=\\operatorname*{max}(z,0)$ ", "page_idx": 5}, {"type": "text", "text": "Next, we make the following assumption, similar to [7], on the covariate distribution. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.3 (Covariate Distribution). The covariate distribution satisfies $\\boldsymbol{D}_{\\mathbf{x}}=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mathcal{U}}^{d}$ ", "page_idx": 5}, {"type": "text", "text": "Now we are ready to state our main theorem that characterizes minimizers of (ICL). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. Let Assumption 3.3 hold and tasks $f$ be drawn from (Case 1) $D(\\mathcal{F}_{L}^{a f\\!f})$ or (Case 2) $D(\\mathcal{F}_{L}^{+})$ . For $n=\\Omega(1)$ and $\\Omega(n^{-d/2})\\leq\\sigma^{2}\\leq\\mathcal{O}(n L^{2})$ any minimizer of the pretraining loss (ICL) satisfies $\\mathbf{M}^{*}=w_{K Q}\\pmb{\\Sigma}$ where for $\\begin{array}{r}{\\Lambda:=\\frac{n L^{2}}{\\sigma^{2}}}\\end{array}$ a,Q:= $\\begin{array}{r}{\\alpha:=\\frac{1}{d+4}}\\end{array}$ d4 and \u03b2 := $\\begin{array}{r}{\\beta:=\\frac{1}{d+2}}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(C a s e\\,I)\\ \\Omega\\left(\\Lambda^{\\alpha}\\right)\\leq\\vert w_{K Q}\\vert\\leq\\mathcal{O}\\left(\\Lambda^{\\frac{2\\alpha}{1-\\beta}}\\right),\\quad(C a s e\\,2)\\ \\Omega\\left(\\Lambda^{\\beta}\\right)\\leq\\vert w_{K Q}\\vert\\leq\\mathcal{O}\\left(\\Lambda^{2\\beta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 shows that optimizing the pretraining population loss in Equation (ICL) leads to attention key-query parameters that scale with the Lipschitzness of the function class, as well as the noise level and number of in-context samples. These bounds align with our observations from Figures 1 and 2 that softmax attention selects an attention window that shrinks with the function class Lipschitzness, recalling that larger $w_{K Q}$ results in a smaller window. Further, the dependencies of the bounds on $\\sigma^{2}$ and $n$ are also intuitive, since larger noise should encourage wider averaging to average out the noise, andlarger $n$ should encourage a smaller window since more samples makes it more likely that there are samples very close to the query. To our knowledge, this is the first result showing that softmax attention learns properties of the task distribution during pretraining that facilitateICL. ", "page_idx": 5}, {"type": "text", "text": "Learning Lipschitzness is critical to generalization. We next give the following generalization result for downstream tasks. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. Suppose softmax attention is first pretrained on tasks drawn from $D(\\mathcal{F}_{L}^{+})$ andthen testedonanarbitrary $L$ -Lipschitz. task, then the loss on the new task is upper bounded as ${\\mathcal{L}}\\leq$ $\\mathcal{O}\\big(\\frac{L^{2}}{\\Lambda^{\\beta}}\\big)$ .Furthermore, if the new task is instead drawn from $D(\\mathcal{F}_{L^{\\prime}}^{+})$ , the loss is lower bounded as $\\begin{array}{r}{\\mathcal{L}\\geq\\Omega\\big(\\frac{L^{\\prime2}}{\\Lambda^{2\\beta}}\\big)}\\end{array}$ for $L^{\\prime}>L$ and $\\begin{array}{r}{{\\mathcal{L}}\\geq\\Omega\\big(\\frac{\\Lambda^{\\beta d/2}}{n}\\big)}\\end{array}$ for $L^{\\prime}<L$ ", "page_idx": 5}, {"type": "image", "img_path": "lfxIASyLxB/tmp/8500423064c35b007d81eb671ed4dcb60f698022226c8101256405ca8029f097.jpg", "img_caption": ["Figure 3: Spectral norm of M during pretraining with varying $L$ .Each plot shows results for different task and covariate distributions, with (tasks, covariates) drawn from (Left) $(D(\\mathcal{F}_{L}^{+}),\\mathcal{U}^{d})$ (Middle-Left) $(D(\\mathcal{F}_{L}^{+}),\\tilde{\\mathcal{U}}^{d})$ (Middle-Right) $(D(\\mathcal{F}_{L}^{\\mathrm{cos}}),\\mathcal{U}^{d})$ (Right) $(D(\\mathcal{F}_{L}^{\\mathrm{cos}}),\\tilde{\\mathcal{U}}^{d})$ , where $\\tilde{\\mathcal{U}}^{d}$ is a non-isotropic distribution on $\\mathbb{S}^{d-1}$ (see Section 3.2 for its definition). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3.5 shows that pretraining on $D(\\mathcal{F}_{L}^{+})$ yields a model that can in-context learn downstream tasksif andonly $i f$ they have similar Lipschitzness as $L$ . Thus, learning Lipschitzness is both suffcient and necessary for ICL. If the evaluation task Lipschitzness is much larger than that seen in pretraining, the pretrained model will give highly biased estimates. Conversely, if the evaluation Lipschitzness is much lower, the pretrained model will not optimally average the label noise. ", "page_idx": 6}, {"type": "text", "text": "Necessity of Softmax. To further emphasize the importance of the softmax in Theorem 3.4, we next study the performance of an analogous model with the softmax removed. We consider linear self-attention [7, 9, 11], which replaces the softmax activation with an identity operation. In particular, in the in-context regression setting we study, the prediction of $f({\\pmb x}_{n+1})$ by linear attention and the corresponding pretraining loss are given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathfrak{h}_{L A}(\\pmb{x}_{n+1}):=\\sum_{i=1}^{n}(f(\\pmb{x}_{i})+\\epsilon_{i})\\pmb{x}_{i}^{\\top}\\,\\mathbf{M}\\,\\pmb{x}_{n+1},}\\\\ {\\displaystyle\\mathcal{L}_{\\mathrm{LA}}(\\mathbf{M}):=\\mathbb{E}_{f,\\{\\pmb{x}_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left(\\boldsymbol{h}_{L A}(\\pmb{x}_{n+1})-f(\\pmb{x}_{n+1})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As discussed in Remark 2.1, $h_{L A}(\\pmb{x}_{n+1})$ cannot adapt an attention window to the problem setting. We show below that this leads it to large ICL loss when tasks are drawn from $D(\\mathcal{F}_{L}^{+})$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.6 (Lower Bound for Linear Attention). Consider pretraining on $\\mathcal{L}_{L A}$ with tasks $f$ drawn from $D(\\mathcal{F}_{L}^{+})$ and covariates drawnfrom $\\mathcal{U}^{d}$ Then for all $\\mathbf{M}\\sp{\\\"}\\in\\mathbb{R}^{d\\times d}$ $\\mathcal{L}_{L A}(\\mathbf{M})=\\Omega(L^{2})$ ", "page_idx": 6}, {"type": "text", "text": "This lower bound on $\\mathcal{L}_{L A}$ is strictly larger than the upper bound on $\\mathcal{L}$ from Theorem 3.5, up to factors .n $d$ as long as $\\begin{array}{r}{\\frac{\\sigma^{2}}{n}\\leq1}\\end{array}$ , which hols in allreasonable cases. Please see Appendix $\\boldsymbol{\\mathrm{F}}$ for the proof. ", "page_idx": 6}, {"type": "text", "text": "3.1  Proof Sketch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To highlight the key insights of our analysis, in this section we consider a modification of the softmax attention that exhibits important properties of the original. Note that this approximation is for illustration only; the above results use the original softmax attention - see Appendices C, D, E. For now, consider a function class $\\mathcal{F}_{L}:=\\{f:f(\\Breve{\\mathbf{x}})=L\\mathbf{w}^{\\top}\\,\\mathbf{x},\\;\\mathbf{w}\\in$ $\\mathbf{w}\\in\\mathbb{S}^{d-1}\\}$ of linear functions. ", "page_idx": 6}, {"type": "text", "text": "(Temporary) modification of the softmax attention. Rather than averaging over every token with a weight that decays exponentially with distance, we consider a modification which uniformly averages all tokens within a distance specified by $w_{K Q}=\\|\\mathbf{M}\\|$ . From Lemma B.5, without loss of generality (WLOG) we can consider $\\mathbf{M}\\,=\\,w_{K Q}\\mathbf{I}_{d}$ .This means that, ignoring normalization, the weight assigned to $f(\\pmb{x}_{i})$ by the true soft-max attention is $e^{-w_{K Q}\\parallel\\,\\mathbf{x}-\\,\\mathbf{x}_{i}\\,\\parallel^{2}}$ That is, for all $\\pmb{x}_{i}$ satisfying $\\|\\,\\pmb{x}-\\pmb{x}_{i}\\,\\|\\,<\\,1/\\sqrt{w_{K Q}}$ the assigned weights within a constant factor of each other. Meanwhile, for $\\pmb{x}_{i}$ satisfying $\\Vert\\,\\pmb{x}-\\pmb{x}_{i}\\,\\Vert=\\sqrt{c}/\\sqrt{w_{K Q}}$ for $c>1$ , the weights are $e^{-c}$ , decaying exponentially in $c$ This motivtesus to consider ar modifed sortma atentiong given by $\\begin{array}{r}{h_{M S A}(\\pmb{x}):=\\sum_{i}\\frac{f(\\pmb{x}_{i})\\mathbb{1}_{i}}{\\sum_{j}\\mathbb{1}_{j}}}\\end{array}$ whre $\\mathbb{1}_{j}:=\\mathbb{1}\\{\\|\\,\\pmb{x}-\\pmb{x}_{j}\\,\\|<1/\\sqrt{w_{K Q}}\\}.$ ", "page_idx": 6}, {"type": "text", "text": "The In-context Loss. The pretraining loss in Equation ICL can be decomposed as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{K Q}\\mathbf{I}_{d})=\\underbrace{\\mathbb{E}_{f,\\{x_{i}\\}_{i}}\\left(\\sum_{j}\\frac{\\left(f(x_{n+1})-f(x_{j})\\right)\\mathbb{I}_{j}}{\\sum_{j}\\mathbb{I}_{j}}\\right)^{2}}_{=:\\mathcal{L}_{\\mathrm{sganal}}(w_{K Q})}+\\underbrace{\\mathbb{E}_{\\{x_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left(\\sum_{i}\\frac{\\epsilon_{i}\\mathbb{I}_{i}}{\\sum_{j}\\mathbb{I}_{j}}\\right)^{2}}_{=:\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["We first upper and lower bound each of these terms separately, starting with $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Noiseless Estimator Bias. (Please see Appendix C) This term is the squared difference between an unweighted average of the token labels within a radius of $\\textbf{\\em x}$ , and the true label. Take $w_{K Q}=\\Omega(1)$ Then for large $d$ , most of the points $\\pmb{x}_{i}$ satisfying $\\lVert\\,\\mathbfcal{x}-\\mathbfcal{x}_{i}\\,\\rVert\\,\\le\\,^{1}/\\sqrt{w_{K Q}}$ lie on the boundary of the cap, that is, $\\parallel x-x_{i}\\parallel<1/\\sqrt{w_{K Q}}\\implies\\parallel x-x_{i}\\parallel\\approx1/\\sqrt{w_{K Q}}.$ This motivates us to approximate the set of points $\\pmb{x}_{i}$ satisfying the above as coming from a uniform distribution over just the boundary of the cap. The center of mass of a ring of radius $^1/\\sqrt{w_{K Q}}$ embedded on the surface of a hyper-sphere, is $O\\big(1\\big/\\bar{w}_{K Q}\\big)$ from the boundary of a sphere, so the squared bias is $\\Theta\\big(L^{2}\\big/w_{K Q}^{2}\\big)$ ", "page_idx": 7}, {"type": "text", "text": "Noise. (Please see Appendix D for details) Since the noise is independent across tokens, we can write $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})=\\frac{\\sigma^{2^{-}}}{\\sum_{j}\\mathbb{1}_{j}}}\\end{array}$ ', which is elted to the number o tokens found within a / Vvukg radus of r. In Lemma G.1, we derive bounds for the measure of this region. For now, we replace the sum in the denominator with ts expectation. We can bound 1 $\\begin{array}{r}{\\frac{1}{\\sum_{j}\\mathbb{1}_{j}}=\\bar{\\Theta}\\bar{(w_{K Q}^{\\frac{d}{2}}/n)}}\\end{array}$ as long as $w_{K Q}\\lesssim n^{2/d}$ ", "page_idx": 7}, {"type": "text", "text": "Combining the $\\mathcal{L}_{\\mathrm{signal}}$ and $\\mathcal{L}_{\\mathrm{noise}}$ terms. (Please see Appendix $\\boldsymbol{\\mathrm E}$ for details) Overall, we have ${\\mathcal{L}}=$ $\\mathcal{L}_{\\mathrm{signal}}+\\mathcal{L}_{\\mathrm{noise}}$ with $\\mathcal{L}_{\\mathrm{signal}}=\\Theta\\Big(L^{2}\\big/w_{K Q}\\Big)$ and $\\mathcal{L}_{\\mathrm{noise}}=\\Theta\\Big(w_{K Q}^{\\frac{d}{2}}\\sigma^{2}/n\\Big)$ . Minimizing this sum reveals that the optimal $w_{K Q}$ satisfies $w_{K Q}=\\Theta\\Bigl(\\bigl(n L^{2}\\bigl/\\sigma^{2}\\bigr)^{\\frac{2}{d+2}}\\Bigr)$ ", "page_idx": 7}, {"type": "text", "text": "3.2 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We next empirically verify our intuitions and results regarding learning the scale of the attention window. In all cases we use the Adam optimizer with one task sampled per round, use the noise distribution $D_{\\epsilon}=\\mathcal{N}(0,\\sigma^{2})$ , and run 10 trials and plot means and standard deviations over these 10 trials. Please see Appendix J for full details as well as additional results. ", "page_idx": 7}, {"type": "text", "text": "Ablations over $L,\\sigma$ and $n$ We verify whether the relationship between the attention window scale - i.e. $\\lVert\\mathbf{M}\\rVert^{-1}-\\mathrm{and}\\ L,\\sigma$ and $n$ matches our bounds in Theorem 3.4 for the case when tasks are drawn from $D(\\mathcal{F}_{L}^{+})$ and the covariates are drawn from $\\mathcal{U}^{d}$ , as well as whether these relationships generalize to additional function classes and covariate distributions. We train on tasks drawn from $D(\\mathcal{F}_{L}^{+})$ and $D(\\mathcal{F}_{L}^{\\mathrm{cos}})$ , where $\\mathcal{F}_{L}^{\\mathrm{cos}}:=\\{f:f(\\pmb{x})=\\cos(L\\mathbf{w}^{\\top}\\,\\pmb{x})$ \uff0c $\\mathbf{w}\\in\\mathbb{S}^{d-1}\\}$ and $D(\\mathcal{F}_{L}^{\\mathrm{cos}})$ is induced by sampling $\\mathbf{w}\\sim\\mathcal{U}^{d}$ . In all cases we set $d=5$ , and use $(L,\\sigma,n)=(1,0.01,20)$ if not ablating over these parameters, and vary only one of $\\{L,\\sigma,n\\}$ and no other hyperparameters within each plot. ", "page_idx": 7}, {"type": "text", "text": "Attention window scales inversely with $L$ Figure 3 shows that $\\lVert\\bf M\\rVert$ increases with $L$ in various settings. In Figure 3(Left, Middle-Left), tasks are drawn from $D(\\mathcal{F}_{L}^{+})$ , and in Figure 3(Middle-Right, Right), they are drawn $D(\\mathcal{F}_{L}^{\\mathrm{cos}})$ . In Figure 3(Left, Middle-Right), each $\\pmb{x}_{i}$ is drawn from $\\mathcal{U}^{d}$ , whereas in Figure 3(Middle-Left, Right), each $\\pmb{x}_{i}$ is drawn from a non-isotropic distribution $\\tilde{\\mathcal{U}}^{d}$ on $\\mathbb{S}^{d-1}$ defined as follows. First, lt $\\mathbf{S}_{d}:=\\mathrm{diag}([1,\\dots,d])\\in\\mathbb{R}^{d\\times d}$ , then $x\\sim\\bar{\\mathcal{U}}^{d}$ is generated by sampling $\\hat{\\pmb x}\\sim\\mathcal N(\\mathbf0_{d},\\mathbf I_{d})$ , then computing $\\begin{array}{r}{{\\pmb x}\\,=\\,\\frac{{\\bf S}_{d}^{1/2}\\hat{\\bf x}}{\\Vert{\\bf S}_{d}^{1/2}\\hat{\\bf x}\\Vert}}\\end{array}$ Although larger $L$ implies larger $\\|\\nabla_{x}f(x)\\|$ on average across $f$ , it is not clear that it implies larger $\\|\\nabla_{\\mathbf{M}_{K}}\\mathcal{L}(\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q})\\|$ nor $\\|\\nabla_{\\mathbf{M}_{Q}}\\mathcal{L}(\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q})\\|$ so it is surprising that larger $L$ implies larger pretrained M (although it is consistent with our results). ", "page_idx": 7}, {"type": "text", "text": "Attention window scales with $\\sigma$ , inversely with $n$ . Figure 4 shows that the dependence of $\\lVert\\bf M\\rVert$ on $\\sigma$ and $n$ also aligns with Theorem 3.4. As expected, $\\lVert\\bf M\\rVert$ increases slower during pretraining for larger $\\sigma$ (shown in Figures 4(Left, Middle-Left)), since more noise encourages more averaging over a larger window to cancel out the noise. Likewise, $\\lVert\\bf M\\rVert$ increases faster during pretraining for larger $n$ (shown in Figures 4(Middle-Right, Right)), since larger $n$ increases the likelihood that there is a highly informative sample in a small attention window. Here always the covariate distribution is $\\mathcal{U}^{d}$ ", "page_idx": 7}, {"type": "image", "img_path": "lfxIASyLxB/tmp/8622e8cbb5efca60864c1d14d7aec81d42b23b3013ffb28890e385c69650272f.jpg", "img_caption": ["Figure 4: Spectral norm of M during pretraining on tasks drawn from $D(\\mathcal{F}_{1}^{+})$ in Left, Middle-Right and $D(\\mathcal{F}_{1}^{\\mathrm{cos}})$ in Middle-Left, Right. Left, Middle-Left show ablations over the noise standard deviation $\\sigma$ and Middle-Right, Right show ablations over the number of context samples $n$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Learning new tasks in-context. An implication of our work is that for the function classes we consider, the softmax attention estimator does not adapt to the function class beyond its Lipschitzness. We have already seen in Figures 3 and 4 that the growth of $\\lVert\\bf M\\rVert$ during pretraining is similar across different function classes with the same Lipschitzness, as long as $\\sigma$ and $n$ are fixed. Here we verify the conclusion from Theorem 3.5 that for fixed $n$ and $\\sigma$ , the necessary and sufficient condition for downstream generalization, measured by small ICL error, is that the pretraining and downstream tasks have similar Lipschitzness. Figure 5 supports this conclusion. Here we set $d=5,n=200,\\sigma=0.01$ and draw each $\\pmb{x}_{i}$ i.i.d. from $\\mathcal{U}^{d}$ . In Figure 5(Left, Middle-Left, MiddleRight), we train three attention units on tasks drawn from the 1-Lipschitz affine $(D(\\mathcal{F}_{1}^{\\mathrm{aff}}))$ , ReLU $(D(\\mathcal{F}_{1}^{+}))$ , and cosine $(D(\\mathcal{F}_{1}^{\\mathrm{cos}}))$ task distributions. Each plot shows the test ICL error on tasks drawn from a distribution in $\\{D(\\mathcal{F}_{1}^{\\mathrm{aff}}),D(\\mathcal{F}_{1}^{+}),D(\\mathcal{F}_{1}^{\\mathrm{cos}})\\}$ Performance is similar regardless of the pairing of pretraining and test distributions, as the Lipschitzness is the same in all cases, demonstrating that pretraining on tasks with appropriate Lipschitzness is sufficient for generalization. ", "page_idx": 8}, {"type": "text", "text": "Moreover, Figure 5(Right) shows that when the Lipschitzness of the pretraining tasks does not match that of the test tasks, ICL performance degrades sharply, even when the tasks otherwise share similar structure. Here the test task distribution is $\\bar{D}(\\mathcal{F}_{1}^{\\mathrm{cos}})$ , and the pretraining task distributions are $D(\\mathcal{F}_{1}^{\\mathrm{aff}})$ $D(\\mathcal{F}_{0.1}^{\\mathrm{cos}})$ ,and $D(\\mathcal{F}_{10}^{\\mathrm{cos}})$ . The only pretraining distribution that leads to downstream generalization is $D(\\mathcal{F}_{1}^{\\mathrm{aff}})$ since its Lipschitzness matches that of the downstream tasks, despite the fact that it is not a distribution over cosine functions, unlike the other distributions. Thus, these results lend credence to the idea that in addition to being suffcient, pretraining on tasks with appropriate Lipschitzness is necessary for generalization. ", "page_idx": 8}, {"type": "text", "text": "4  Softmax Attention Learns Direction of Attention Window ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Thus far, we have considered distributions over tasks that treat the value of the input data in all directions within the ambient space as equally relevant to its label. However, in practice the ambient dimension of the input data is often much larger than its information content - the labels may change very litle with many features of the data, meaning that such features are spurious. This is generally true of embedded language tokens, whose embedding dimension is typically far larger than the minimum dimension required to store them (logarithmic in the vocabulary size) [1]. Motivated by this, we define a notion of \u201cdirection-wise Lipschitzness\" of a function class to allow for analyzing classes that may depend on some directions within the ambient input data space more than others. ", "page_idx": 8}, {"type": "text", "text": "Definition 4.1 (Direction-wise Lipschitzness of Function Class). The Lipschitzness of a function class $\\mathcal{F}$ with domain $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ in the direction w $\\in\\mathbb{S}^{d-1}$ is defined as as the largest Lipschit constant of all functions in $\\mathcal{F}$ over the domain $\\mathcal{X}$ projected onto w, that is:   \n$i p_{\\mathbf{w}}(\\mathcal{F},\\mathcal{X}):=\\operatorname*{inf}_{L\\in\\mathbb{R}}\\{L:f(\\mathbf{w}\\mathbf{w}^{\\top}\\,\\mathbf{x})-f(\\mathbf{w}\\mathbf{w}^{\\top}\\,\\mathbf{x}^{\\prime})\\le L|\\mathbf{w}^{\\top}\\,\\mathbf{x}-\\mathbf{w}^{\\top}\\,\\mathbf{x}^{\\prime}|\\ \\forall\\ (\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\mathcal{X}^{2},f\\in\\mathcal{F}\\}.$ Using this definition, we analyze function classes consisting of linear functions with parameters lying in asubspace of $\\mathbb{R}^{d}$ asfollows:   \nDeionLlahefco $\\mathcal{F}_{\\mathbf{B}}^{l i n}$ is defned as ${\\mathcal{F}}_{\\mathbf{B}}^{l i n}:=\\{f:$ $f(\\pmb{x})=\\mathbf{a}^{\\top}\\mathbf{B}^{\\top}\\,\\pmb{x},\\ \\mathbf{a}\\in\\mathbb{R}^{k}\\}$ and $D(\\mathcal{F}_{\\mathbf{B}}^{l i n})$ is induced by drawing $\\mathbf{a}\\sim\\mathcal{U}^{k}$   \nwhere $\\mathbf{B}\\in\\mathbb{O}^{d\\times k}$ is a column-wise orthonormal matrx. Since our motivation is settings with low", "page_idx": 8}, {"type": "text", "text": "dimensional structure, we can think of $k\\ll d$ Let $\\mathbf{B}_{\\perp}\\in\\mathbb{O}^{d\\times(d-k)}$ denote a matrix whose columns form an orthonormal basis for the subspace perpendicular to $\\operatorname{col}(\\mathbf{B})$ , and note that the Lipschitzness $\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{lin}}$ in the direction wis $L$ $\\mathbf{w}\\in\\mathrm{col}(\\mathbf{B})$ andOif $\\mathbf{w}\\in\\mathrm{col}(\\mathbf{B}_{\\perp})$ Observe that anyfunction in $\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{lin}}$ canbeledbypngthteo. $\\operatorname{col}(\\mathbf{B})$ \uff0c\uff0c then solving a $k\\ll d$ -dimensional regression. To formally study whether softmax attention recovers $\\operatorname{col}(\\mathbf{B})$ , we assume the covariates are generated as follows. ", "page_idx": 8}, {"type": "image", "img_path": "lfxIASyLxB/tmp/75fd5b40fe81eae9f5a2c693ac88af9bddfd2cd4656a0547c16d208139e4d3fb.jpg", "img_caption": ["Figure 5: Left, Middle-Left, Middle-Right: The test error for softmax attention as it is trained on the distributions over 1-Lipschitz affine, ReLU, and cosine function $(D(\\mathcal{F}_{1}^{\\mathrm{aff}})$ $D(\\mathcal{F}_{1}^{+})$ , and $D(\\mathcal{F}_{1}^{\\mathrm{cos}})$ respectively), where the error is evaluated at each pretraining iteration on 5 tasks drawn from the distributions over the 1-Lipschitz (affine, ReLU, cosine) function classes in (Left, Middle-Left, Middle-Right), respectively. Right: The test error evaluated on tasks drawn from $D(\\mathcal{F}_{1}^{\\mathrm{cos}})$ for three softmax attention trained on tasks drawn from $D(\\mathcal{F}_{1}^{\\mathrm{aff}})$ \uff0c $D(\\mathcal{F}_{0.1}^{\\mathrm{cos}})$ , and $D(\\mathcal{F}_{10}^{\\mathrm{cos}})$ , respectively. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Assumption 4.3 (Covariate Distribution). There are fixed constants $c_{\\mathbf{u}}\\neq0$ and $-\\infty<c_{\\mathbf{v}}<\\infty s.t$ sampling $x_{i}\\sim D_{x}$ isequivalentto $\\pmb{x}_{i}=c_{\\mathbf{u}}\\mathbf{B}\\mathbf{u}_{i}+c_{\\mathbf{v}}\\mathbf{B}_{\\bot}\\mathbf{v}_{i}$ where ${\\bf u}_{i}\\sim\\mathcal{U}^{k}$ and $\\mathbf{v}_{i}\\sim\\mathcal{U}^{d-k}$ ", "page_idx": 9}, {"type": "text", "text": "Assumption 4.3 entails that the data is generated by latent variables $\\mathbf{u}_{i}$ and $\\mathbf{v}_{i}$ that determine labelrelevant and spurious features. This may be interpreted as a continuous analogue of dictionary learning models studied in feature learning works [33, 34]. We require no finite upper bound on $|c_{\\mathbf{v}}|$ nor $\\frac{1}{|c_{\\mathbf{u}}|}$ , so the data may be dominated by spurious features. ", "page_idx": 9}, {"type": "text", "text": "Theorem 4.4. Let $\\mathbf{B}\\in\\mathbb{O}^{d\\times k}$ and consider the pretraining population loss (ICL) with $f\\sim D(\\mathcal{F}_{\\mathbf{B}}^{l i n})$ Suppose Assumption 4.3 holds, as well as at least one of two cases: (Case 1) $\\sigma=0$ or(Case2) $n=2$ Then among all $\\mathbf{M}\\in\\mathcal{M}:=\\left\\{\\mathbf{M}\\in\\mathbb{R}^{d\\times d}:\\mathbf{M}=\\mathbf{M}^{\\top}\\right.$ \uff0c $\\begin{array}{r}{\\|\\mathbf{B}^{\\top}\\mathbf{M}\\mathbf{B}\\|\\leq\\frac{1}{c_{\\mathbf{u}}^{2}}\\}}\\end{array}$ the minimizer of the pretraining population loss (ICL) is $\\mathbf{M}^{*}=c\\mathbf{B}\\mathbf{B}^{\\top}$ for some $c\\in(0,\\frac{1}{c_{\\mathbf{u}}^{2}}]$ ", "page_idx": 9}, {"type": "text", "text": "Theorem 4.4 shows that softmax attention can achieve dimensionality reduction during ICL on any downstream task that has non-zero Lipschitzness only in $\\operatorname{col}(\\mathbf{B})$ by removing the zero-Lipschitzness features while pretraining on $\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{lin}}$ . Removing the zero-Lipschitzness features entails that the nearest neighbor prediction of pretrained softmax attention uses a neighborhood, i.e. attention window, defined strictly by projections of the input onto $\\operatorname{col}(\\mathbf{B})$ . To our knowledge, this is the first result showing that softmax attention pretrained on ICL tasks recovers a shared low-dimensional structure among the tasks. Please see Appendix J for empirical results verifying that softmax attention indeed recovers low-dimensional structure, even for tasks consisting of (nonlinear) generalized linear models. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented, to our knowledge, the first results showing that softmax attention learns shared structure among pretraining tasks that facilitates downstream ICL. Moreover, we have provided empirical evidence suggesting that our conclusions about what softmax attention learns during pretraining generalize to function classes beyond those considered in our analysis. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. 1. The model we use in this work is an attempt to understand a phenomenon that emerges in LLMs, which is that the output of the model can be \u2018primed\u2019 with some examples provided in the context that resembles few-shot learning, even though they are only trained on next token prediction. Establishing a mathematical framework for this remains an interesting question. 2. We consider the output of a single layer of attention. Studying the nature of the solution when this is iterated over multiple trained layers is an interesting future prospect. ", "page_idx": 9}, {"type": "text", "text": "6Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by NSF Grants 2127697, 2019844, 2107037, and 2112471, ARO Grant W911NF2110226, ONR Grant N00014-19-1-2566, the Machine Learning Lab (MLL) at UT Austin, and the Wireless Networking and Communications Group (WNCG) Industrial Affliates Program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.   \n[2]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Mllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1- 113, 2023.   \n[4] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2022.   \n[5]  Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.   \n[6]  Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.   \n[7]  Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning, 2023.   \n[8]  Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.   \n[9]  Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   \n[10]  Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023.   \n[11] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR, 2023.   \n[12] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \n[13]  Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection, 2023.   \n[14]  Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023.   \n[15]  Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos.o Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.   \n[16] Yingcong Li, Muhammed Emrullah Idiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565-19594. PMLR, 2023.   \n[17] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett How many pretraining tasks are needed for in-context learning of linear regresson? arXiv preprint arXiv:2310.08391, 2023.   \n[18] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \n[19]  Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916, 2023.   \n[20] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023.   \n[21] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \n[22]  Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained as kernel regression. arXiv preprint arXiv:2305.12766, 2023.   \n[23] Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan AK Suykens. Primal-attention: Self-attention through asymmetric kernel svd in primal representation. arXiv preprint arXiv:2305.19798, 2023.   \n[24] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: a unifed understanding of transformer's attention via the lens of kernel. arXiv preprint arXiv: 1908.11775, 2019.   \n[25]  Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourierformer: Transformer meets generalized fourier integral theorem. Advances in Neural Information Processing Systems, 35:29319-29335, 2022.   \n[26] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Designing robust transformers using robust kernel density estimation. arXiv preprint arXiv:2210.05794, 2022.   \n[27] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression, 2023.   \n[28] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.   \n[29] Kathryn A Prewitt. A distribution-free theory of nonparametric regression. laszlo gyorf, michael kohler, adam krzyzak, and harro walk. Journal of the American Statistical Association, 98(464):1084-1084, 2003.   \n[30] Elizbar A Nadaraya. On estimating regresson. Theory of Probability & Its Applications, 9(1):141-142, 1964.   \n[31]  Geoffrey S Watson. Smooth regression analysis. Sankhya: The Indian Journal of Statistics, Series A, pages 359-372, 1964.   \n[32] Samuele Tosatto, Riad Akrour, and Jan Peters. An upper bound of the bias of nadaraya-watson kernel regression under lipschitz assumptions. Stats, 4(1):1-17, 2021.   \n[3]  Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In International Conference on Machine Learning, pages 11112-11122. PMLR, 2021.   \n[34]  Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. arXiv preprint arXiv:2206.01717, 2022.   \n[35] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. arXiv preprint arXiv:2306.15063, 2023.   \n[36] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023.   \n[37] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.   \n[38] Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023.   \n[39] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \n[40]  Asher Trockman and J. Zico Kolter. Mimetic initialization of self-attention layers, 2023.   \n[41] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535, 2023.   \n[42]  Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase, 2023.   \n[43]  Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245, 2023.   \n[44]  Samy Jelassi, Michael E. Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure, 2022.   \n[45] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015, 2023.   \n[46]  Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023.   \n[47]  Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism, 2023.   \n[48]  Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566, 2023.   \n[49]  Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. arXiv preprint arXiv:2306.02896, 2023.   \n[50] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.   \n[51]  Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793-5831. PMLR, 2022.   \n[52]  Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.   \n[53] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. arXiv preprint arXiv:2307.11353, 2023.   \n[54]  Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. The Journal of Machine Learning Research, 22(1):3463-3497, 2021.   \n[55]  Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.   \n[56] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020.   \n[57]  Valeri Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764, 2021.   \n[58] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. Advances in Neural Information Processing Systems, 35:12071-12083, 2022.   \n[59]  Zhao Song, Guangyi Xu, and Junze Yin. The expressibility of polynomial based attention scheme, 2023.   \n[60]  Kevin Christian Wibisono and Yixin Wang. On the role of unstructured training data in transformers' in-context learning capabilities. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023.   \n[61]  Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. arXiv preprint arXiv:2310.11685, 2023.   \n[62]  James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of attention, 2020.   \n[63] Hyunjik Kim, George Papamakarios, and Andriy Mnih. Th lipschitz constant of self-attention, 2020.   \n[64]  Herbert Robbins._ A remark on stirling's formula. The American Mathematical Monthly, 62(1):26-29, 1955.   \n[65]  Brian   Knaeble. Variations i on   the   projective   central   limit  theorem. https://arxiv.org/pdf/0904.1048.pdf, 2015.   \n[66]  Elliott H Lieb and Michael Loss. Analysis, volume 14. American Mathematical Soc., 2001.   \n[67]  G.H. Hardy, J.E. Littlewood, and G. Polya. Inequalities. Cambridge Mathematical Library. Cambridge University Press, 1952. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1Introduction 1.1Additional Related Work ", "page_idx": 14}, {"type": "text", "text": "2 Preliminaries 3 ", "page_idx": 14}, {"type": "text", "text": "3 Pretraining Learns Scale of Attention Window ", "page_idx": 14}, {"type": "text", "text": "3.1  Proof Sketch .   \n3.2Experiments . 8 ", "page_idx": 14}, {"type": "text", "text": "4  Softmax Attention Learns Direction of Attention Window 9 ", "page_idx": 14}, {"type": "text", "text": "5 Conclusion 10 ", "page_idx": 14}, {"type": "text", "text": "6  Acknowledgments 11 ", "page_idx": 14}, {"type": "text", "text": "A Additional Related Work 16 ", "page_idx": 14}, {"type": "text", "text": "B Preliminaries 16   \nB.1  Rewriting the Loss 20   \nC The Signal Term 21   \nC.1 Affine functions . 21   \nC.2 ReLU-based functions 24 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D Bounds on Noise Variance 26 ", "page_idx": 14}, {"type": "text", "text": "E  Optimizing the Loss 27   \nE.1 Generalization Bounds 30 ", "page_idx": 14}, {"type": "text", "text": "F  Lower Bound for Linear Attention 31 ", "page_idx": 14}, {"type": "text", "text": "G  Bounds for $g_{p}(r)$ 31   \nG.1  Bounds on Spherical Caps 32   \nG.2  Bounds on $g_{p}(r)$ 34 ", "page_idx": 14}, {"type": "text", "text": "H Attention Window Captures Appropriate Directions 36 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H.1 Proof Sketch. 37   \nH.2 Full Proof 37 ", "page_idx": 14}, {"type": "text", "text": "1 Additional Lemmas 51 ", "page_idx": 14}, {"type": "text", "text": "J  Additional Experiments and Details 52   \nJ.1 Low-Rank Experiments . . 53 ", "page_idx": 14}, {"type": "text", "text": "AAdditional Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Empirical study of ICL. Several works have studied ICL of linear tasks in the framework introduced by [28], and demonstrated that pretrained transformers can mimic the behavior of gradient descent [11-13, 28], Newton's method [14], and certain algorithm selection approaches [13, 16]. [35] studied the same linear setting with the goal of understanding the role of pretraining task diversity, while [36] argued via experiments on general auto-regressive tasks that ICL implicitly constructs a learning objective and optimizes it within one forward pass. Other empirical works have both directly supported [37] and contradicted [38] the hypothesis that ICL is a gradient-based optimization algorithm via experiments on real ICL tasks, while [39] empirically concluded that induction heads with softmax attention are the key mechanism that enables ICL in transformers. Lastly, outside of the context of ICL, [40] noticed that the attention parameter matrices of trained transformers are often close to scaled identities in practice, consistent with our findings on the importance of learning a scale to softmax attention training. ", "page_idx": 15}, {"type": "text", "text": "Transformer training dynamics. [21] and [41] studied the dynamics of softmax attention trained with gradient descent, but assumed orthonormal input features and either linear tasks [21] or that the softmax normalization is a fixed constant [41]. [42] proved that softmax attention with diagonal weight matrices incrementally learns features during gradient-based training. Other work has shown that trained transformers can learn topic structure [43], spatial structure [44], visual features [45] and support vectors [46, 47] in specific settings disjoint from ICL. ", "page_idx": 15}, {"type": "text", "text": "Expressivity of transformers. Multiple works have shown that transformers with linear [11, 36], ReLU [13, 14, 48], and softmax [12, 15] attention are expressive enough to implement generalpurpose machine learning algorithms during ICL, including gradient descent. A series of works have shown the existence of transformers that recover sparse functions of the input data [49-52]. [53] studied the statistical complexity the learning capabilities of attention with random weights. More broadly, [54-59] have analyzed various aspects of the expressivity of transformers. ", "page_idx": 15}, {"type": "text", "text": "Other studies of softmax attention. [60] hypothesized that the role of the softmax in attention is to facilitate a mixture-of-experts algorithm amenable to unstructured training data. [27] formulated a softmax regression problem and analyzed the convergence of a stylized algorithm to solve it. [22] showed that in a setting with ICL regression tasks a la [28], a kernel regressor akin to softmax attention with M equal to the inverse covariance of $\\mathbf{x}$ converges to the Bayes posterior for a new ICL task - in this setting the conditional distribution of the label given the query and $n$ labelled context samples - polynomially with the number of context samples, but did not study what softmax attention learns during pretraining. [61] also compared softmax and linear attention, but focused on softmax's greater capacity to separate data from two classes. [62] and [63] investigate the Lipschitz constant of attention rather thanwhat attentionlearns. ", "page_idx": 15}, {"type": "text", "text": "Non-parametric regression. Our results imply that pretraining softmax attention reduces to the problem of meta-learning the bandwidth of a Nadaraya-Watson estimator with a Gaussian kernel. However, to our knowledge, the non-parametric regression literature has not addressed this problem. The closest work is [32], which only upper bounds the noiseless loss, and only in the limit $n\\to\\infty$ whereas our result characterizes the optimal bandwidth, which requires upper and lower bounds on the noisy loss. ", "page_idx": 15}, {"type": "text", "text": "B Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first justify our claim that the first $d$ rows of the last column of $\\mathbf{W}_{V}$ can be set to $\\mathbf{0}_{d}$ for any optimal choice of parameters. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. If under the function distribution, a function $f$ is equally likely as likely as $-f$ then any optimal solution to $\\mathcal{L}(\\mathbf{W}_{V},\\mathbf{W}_{K},\\mathbf{W}_{Q})$ in $^3$ satisfies ${\\bf W}_{V}=\\left(\\mathbf{0}_{d\\times d}\\quad\\mathbf{0}_{d\\times1}\\right)$ ", "page_idx": 15}, {"type": "text", "text": "Proof. For readability we write $\\begin{array}{r}{\\beta_{i}=e^{-w_{K Q}\\|\\mathbf{\\boldsymbol{x}}_{i}-\\mathbf{\\boldsymbol{x}}_{n+1}\\|^{2}}\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\boldsymbol{x}}_{j}-\\mathbf{\\boldsymbol{x}}_{n+1}\\|^{2}}\\mathrm{~Suppose~}\\mathbf{W}_{\\mathrm{V}}}\\end{array}$ $\\mathbf{W}_{V}=$ $\\left(\\!\\!\\begin{array}{c c}{{\\mathbf{0}_{d\\times d}}}&{{\\mathbf{v}}}\\\\ {{\\mathbf{0}_{1\\times d}}}&{{c}}\\end{array}\\!\\!\\right)$ was optimal, then the loss can be written ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{f,\\left\\{x_{i}\\right\\}}\\left[\\left(\\sum_{i}c\\left(f(\\pmb{x}_{i})+\\epsilon_{i}\\right)\\beta_{i}+\\sum_{i}\\mathbf{v}^{\\top}\\,\\pmb{x}_{i}\\,\\beta_{i}-f(\\pmb{x}_{n+1})\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Butbecause $f$ and $-f$ are equally likely, and because the noise is also symmetric about O, we can write this as ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathcal{L}}={\\frac{1}{2}}\\mathbb{E}_{f,\\{x_{i}\\},\\{\\epsilon_{i}\\}}\\left[\\left(\\sum_{i}c\\left(f(\\mathbf{x}_{i})+\\epsilon_{i}\\right)\\beta_{i}+\\sum_{i}\\mathbf{v}^{\\top}\\,\\mathbf{x}_{i}\\,\\beta_{i}-f(\\mathbf{x}_{n+1})\\right)^{2}\\right]}\\\\ &{\\qquad+\\,{\\cfrac{1}{2}}\\,\\mathbb{E}_{f,\\{x_{i}\\},\\{\\epsilon_{i}\\}}\\left[\\left(\\sum_{i}c\\left((-f)(\\mathbf{x}_{i})-\\epsilon_{i}\\right)\\beta_{i}+\\sum_{i}\\mathbf{v}^{\\top}\\,\\mathbf{x}_{i}\\,\\beta_{i}-(-f)(\\mathbf{x}_{n+1})\\right)^{2}\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can couple the noise $\\{\\epsilon_{i}\\}$ and the data $\\{x_{i}\\}$ in the two summands above to write this as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(A+B+C)^{2}+(-A+B-C)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{A=\\sum_{i}c f(\\pmb{x}_{i})\\beta_{i}-f(\\pmb{x})=-\\left(\\sum_{i}c(-f)(\\pmb{x}_{i})\\beta_{i}\\right)}\\end{array}$ \uff0c $\\begin{array}{r}{B=\\sum_{i}\\mathbf{v}^{\\top}\\,{\\mathbf{x}}_{i}\\,\\beta_{i}}\\end{array}$ , and $\\begin{array}{r}{C=\\sum_{i}c\\epsilon_{i}\\beta_{i}}\\end{array}$ We can set $B=0$ simply by setting $\\mathbf{v}=\\mathbf{0}_{d\\times1}$ , and this has loss ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathbb{E}_{f,\\{x_{i}\\}}\\left[\\left(\\sum_{i}c\\left(f(x_{i})+\\epsilon_{i}\\right)\\beta_{i}-f(x_{n+1})\\right)^{2}\\right]}\\\\ &{\\quad=\\frac{1}{2}\\left(\\mathbb{E}\\left[(A+C)^{2}+(-A-C)^{2}\\right]\\right)\\le\\frac{1}{2}\\left(\\mathbb{E}\\left[(A+B+C)^{2}+(-A+B-C)^{2}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In all of the distributions over functions we consider for pretraining, $f$ is equally likely as $-f$ without loss of generality we set all elements of $\\mathbf{W}_{V}$ besides the $(d+1,d+1)$ -th to O. For simplicity, we set the $(d+1,d+\\dot{1})$ -th element to 1. ", "page_idx": 16}, {"type": "text", "text": "Assumption B.2 (Covariate Distribution). For each token $\\pmb{x}_{\\pmb{\\mathrm{~\\,~}}}$ first we draw $\\tilde{\\pmb{x}}$ as $\\tilde{\\mathbf{x}}\\sim\\mathcal{U}^{d}$ .Then $\\textbf{\\em x}$ is constructed as $\\pmb{x}=\\pmb{\\Sigma}^{1/2}\\tilde{\\pmb{x}}$ ", "page_idx": 16}, {"type": "text", "text": "Definition B.3 (Linear and 2-ReLU Function Classes). The function classes $\\mathcal{F}_{L}^{l i n}$ and $\\mathcal{F}_{L}^{+}$ are respectively defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{L}^{l i n}:=\\{f_{\\mathbf{w}}:f_{\\mathbf{w}}(\\mathbf{x})=l\\mathbf{w}^{\\top}\\,\\mathbf{x}+b,\\,\\mathbf{w}\\in\\mathbb{S}^{d-1},\\,l\\in[-L,L]\\},}\\\\ &{\\mathcal{F}_{L}^{+}:=\\{f_{\\mathbf{w}}:f_{\\mathbf{w}}(\\mathbf{x})=l_{1}\\,R e L U(\\mathbf{w}^{\\top}\\,\\mathbf{x})+l_{2}\\,R e L U(-\\mathbf{w}^{\\top}\\,\\mathbf{x})+b,\\,\\mathbf{w}\\in\\mathbb{S}^{d-1}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$D(\\mathcal{F}_{L}^{l i n}),D(\\mathcal{F}_{L}^{+})$ are induced by drawing w $\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}^{-1})$ and $b,l,l_{1},l_{2}\\sim U n i f([-L,L])$ . We say that these classes are $L$ -Lipschitz, because the maximum Lipschitz. constant for any function in the class is $L$ ", "page_idx": 16}, {"type": "text", "text": "Note that because $\\|\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{i}\\,\\|=1$ always, we have ", "page_idx": 16}, {"type": "text", "text": "2cMn+1 ", "page_idx": 16}, {"type": "equation", "text": "$$\n=\\|\\Sigma^{-1/2}\\,x_{i}\\,\\|^{2}+\\|\\Sigma^{1/2}\\mathbf{M}\\Sigma^{1/2}\\Sigma^{-1/2}\\,x_{n+1}\\|^{2}-\\|\\Sigma^{-1/2}\\,x_{i}\\,-\\Sigma^{1/2}\\mathbf{M}\\Sigma^{1/2}\\Sigma^{-1/2}\\,x_{n+1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\mathbf{M}^{\\prime}=\\boldsymbol{\\Sigma}^{1/2}\\mathbf{M}\\boldsymbol{\\Sigma}^{1/2}$ . This means the attention estimator can be rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{S A}(\\pmb{x}):=\\sum_{i}\\frac{f(\\pmb{x}_{i})e^{\\pmb{x}_{i}^{\\top}\\,\\mathbf{M}\\,\\pmb{x}_{n+1}}}{\\sum_{j}e^{\\pmb{x}_{j}^{\\top}\\,\\mathbf{M}\\,\\pmb{x}_{n+1}}}=\\sum_{i}\\frac{f(\\pmb{x}_{i})e^{-\\|\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{i}\\,-\\,\\mathbf{M}^{\\prime}\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{n+1}\\,\\|^{2}}}{\\sum_{j}e^{-\\|\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{j}\\,-\\,\\mathbf{M}^{\\prime}\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{n+1}\\,\\|^{2}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So the attention a token $x_{n+1}$ places on another $\\pmb{x}_{i}$ is related to the distance between $\\mathbf{M}^{\\prime}\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{n+1}$ and $\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{i}$ It is natural to suppose under some symmery conditions that ${\\bf{M}}^{\\prime}$ best chosen to be a scaled identity matrix so that the attention actually relates to a distance between tokens.Below we discus sufficient conditions for this. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "lfxIASyLxB/tmp/f1ecb1ebe0d29f360f536109252f8cf33d6d6214c01e0c0ffc0132956243d85f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: Comparison between using $\\mathbf{M}$ and $\\omega$ in Lemma B.5. Here we denote $\\pmb{y}:=\\pmb{y}_{n+1}$ Under the attention induced by $\\mathbf{M}$ , the center of attention for $\\textit{\\textbf{y}}$ is actually $\\pmb{y}^{\\prime}$ , and the attention weights are depicted by the light orange shading. Under the attention induced by $\\omega$ , the center of attention for $\\textit{\\textbf{y}}$ is $\\textit{\\textbf{y}}$ and the weights are depicted by the light blue shading. Naturally, using the blue shaded attention should lead to a better estimate of $f(\\mathbf{y})$ under mild regularity conditions. ", "page_idx": 17}, {"type": "text", "text": "Assumption B.4. The function class $\\mathcal{F}$ and distribution $D({\\mathcal{F}})$ satisfy ", "page_idx": 17}, {"type": "text", "text": "1. $|f(\\pmb{x})-f(\\pmb{y})|\\leq L\\|\\pmb{x}-\\pmb{y}\\|_{\\Sigma^{-1}}\\,\\forall\\,\\pmb{x},\\pmb{y}\\in\\mathcal{X}^{2},f\\in\\mathcal{F}$   \n2. $\\mathbb{E}_{f\\sim D(\\mathcal{F})}\\left[f(\\pmb{x})f(\\pmb{y})\\right]=\\rho(\\pmb{x}^{\\top}\\pmb{y})\\,\\forall\\,\\pmb{x},\\pmb{y}\\in\\mathcal{X}^{2}$ for some monotonically increasing $\\rho$ ", "page_idx": 17}, {"type": "text", "text": "3. For any isometry $\\phi$ preserving the unit sphere, and $f\\in\\mathcal F$ , we have $f\\circ\\phi\\in{\\mathcal{F}}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma B.5. Under Assumption B.4, any minimizer of Equation ICL satisfies $\\mathbf{M}^{*}=w_{K Q}\\pmb{\\Sigma}^{-1}$ for somescalar $w_{K Q}\\ge0$ ", "page_idx": 17}, {"type": "text", "text": "Prof. Let $\\{\\pmb{y}_{i}\\}=\\{\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{i}\\}$ . uppose $\\mathbf{M}\\,y_{n+1}\\neq c\\,y_{n+1}$ for any $c>0$ for some $\\pmb{y}_{n+1}$ . Take $c_{{\\pmb y}_{n+1}}=\\|{\\bf M}\\,{\\pmb y}_{n+1}\\|$ $\\begin{array}{r}{{\\bf y}_{n+1}^{\\prime}=\\frac{{\\bf M}\\,{\\bf y}_{n+1}}{c_{{\\bf y}_{n+1}}}}\\end{array}$ tof e $\\omega:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ satisfying $\\omega({\\pmb y}_{n+1})=c_{{\\pmb y}_{n+1}}\\,{\\pmb y}_{n+1}$ Note that this eed not be linear Let $\\phi$ denote a rotation that sends yn+1 to yn+1. ", "page_idx": 17}, {"type": "text", "text": "We show that $\\mathcal{L}(\\mathbf{M})>\\mathcal{L}(\\omega)$ , that is, it is favorable to not rotate $\\pmb{y}_{n+1}$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathbf{M})=\\mathbb{E}_{f,\\pmb{y}_{n+1},\\left\\{\\pmb{y}_{i}\\right\\}}\\left[\\left(f(\\pmb{y}_{n+1})-\\frac{\\sum_{i}f(\\pmb{y}_{i})e^{-\\|\\pmb{y}_{i}-\\mathbf{M}\\pmb{y}_{n+1}\\|^{2}}}{\\sum_{j}e^{-\\|\\pmb{y}_{j}-\\mathbf{M}\\pmb{y}_{n+1}\\|^{2}}}\\right)^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{f,\\pmb{y}_{n+1},\\left\\{\\pmb{y}_{i}\\right\\}}f(\\pmb{y}_{n+1})^{2}+\\mathbb{E}_{f,\\pmb{y}_{n+1},\\left\\{\\pmb{y}_{i}\\right\\}}\\left[\\left(\\frac{\\sum_{i}f(\\pmb{y}_{i})e^{-\\|\\pmb{y}_{i}-\\mathbf{M}\\pmb{y}_{n+1}\\|^{2}}}{\\sum_{j}e^{-\\|\\pmb{y}_{j}-\\mathbf{M}\\pmb{y}_{n+1}\\|^{2}}}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad-\\ 2\\,\\mathbb{E}_{f,\\pmb{y}_{n+1},\\left\\{\\pmb{y}_{i}\\right\\}}\\left[\\sum_{i}\\frac{f(\\pmb{y}_{n+1})f(\\pmb{y}_{i})e^{-\\|\\pmb{y}_{i}-\\mathbf{M}\\pmb{y}_{n+1}\\|^{2}}}{\\sum_{j}e^{-\\|\\pmb{y}_{j}-\\mathbf{M}\\pmb{y}_{n+1}\\|^{2}}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lets compare this with the loss of $\\omega$ . For a depiction of this, please see Figure 6 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\omega)=\\mathbb{E}_{f,y_{n+1},\\{y_{i}\\}}\\left[\\left(f(y_{n+1})-\\frac{\\sum_{i}f(y_{i})e^{-\\|\\mathbf{\\omega}_{y_{i}}-\\omega(y_{n+1})\\|^{2}}}{\\sum_{j}e^{-\\|\\mathbf{\\omega}_{y_{j}}-\\omega(y_{n+1})\\|^{2}}}\\right)^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}_{f,y_{n+1},\\{y_{i}\\}}f(y_{n+1})^{2}+\\mathbb{E}_{f,y_{n+1},\\{y_{i}\\}}\\left[\\left(\\frac{\\sum_{i}f(y_{i})e^{-\\|\\mathbf{\\omega}_{y_{i}}-\\omega(y_{n+1})\\|^{2}}}{\\sum_{j}e^{-\\|\\mathbf{\\omega}_{y_{j}}-\\omega(y_{n+1})\\|^{2}}}\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\;2\\,\\mathbb{E}_{f,\\pmb{y}_{n+1},\\lbrace\\pmb{y}_{i}\\rbrace}\\left[\\sum_{i}\\frac{f(\\pmb{y}_{n+1})f(\\pmb{y}_{i})e^{-\\|\\pmb{y}_{i}-\\omega(\\pmb{y}_{n+1})\\|^{2}}}{\\sum_{j}e^{-\\|\\pmb{y}_{j}-\\omega(\\pmb{y}_{n+1})\\|^{2}}}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "There are three terms to compare. The first in each is identical. The second is also the same: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{f,y_{n+1},\\{y_{n}\\}}\\left[\\left(\\frac{\\sum_{i}f(y_{i})e^{-\\mathrm{i}\\left[y_{i}-M\\mathbf{y}_{n+1}\\right]^{2}}}{\\sum_{j}e^{-\\mathrm{i}\\left[y_{j}-M\\mathbf{y}_{n+1}\\right]^{2}}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}_{y_{n+1}}\\mathbb{E}_{f,\\{y_{n}\\}}\\left[\\left(\\frac{\\sum_{i}f(y_{j})e^{-\\mathrm{i}\\left[y_{i}-M\\mathbf{y}_{n+1}\\right]^{2}}}{\\sum_{j}e^{-\\mathrm{i}\\left[y_{j}-M\\mathbf{y}_{n+1}\\right]^{2}}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}_{y_{n+1}}\\mathbb{E}_{f,\\{y_{n}\\}}\\left[\\left(\\frac{\\sum_{i}f(y_{j})e^{-\\mathrm{i}\\left[y_{i}-\\zeta\\mathbf{y}_{n+1}\\right]^{2}}}{\\sum_{j}e^{-\\mathrm{i}\\left[y_{j}-\\zeta\\mathbf{y}_{n+1}+y_{n+1}\\right]^{2}}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}_{y_{n+1}}\\mathbb{E}_{f,\\{y_{n}\\}}\\left[\\left(\\frac{\\sum_{i}f(y_{i})e^{-\\mathrm{i}\\left[y_{i}-\\zeta\\mathbf{y}_{n+1}\\right]^{2}}}{\\sum_{j}e^{-\\mathrm{i}\\left[y_{j}-\\zeta\\mathbf{y}_{n+1}+y_{n+1}\\right]^{2}}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}_{y_{n+1}}\\mathbb{E}_{f,\\{y_{n}\\}}\\left[\\left(\\frac{\\sum_{i}f(y(y_{i}))e^{-\\mathrm{i}\\left[\\theta(y_{i})-\\zeta\\mathbf{y}_{n+1}+y_{n+1}\\right]^{2}}}{\\sum_{j}e^{-\\mathrm{i}\\left[y_{j}-\\zeta\\mathbf{y}_{n+1}+y_{n+1}\\right]^{2}}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}_{y_{n+1}}\\mathbb{E}_{f,\\{y_{n}\\}}\\left[\\left(\\frac{\\sum_{i}f(y_{i})e^{-\\mathrm{i}\\left[y_{i}-\\zeta\\mathbf{y}_{n+1}+\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The third takes some more work. For any choice of $\\{y_{i}\\}$ , let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{y_{n+1},\\{y_{i}\\}}(y_{*})=\\frac{e^{-\\|\\pmb{y}_{n+1}-\\pmb{y}_{*}\\|^{2}}}{e^{-\\|\\pmb{y}_{n+1}-\\pmb{y}_{*}\\|^{2}}+\\sum_{j}e^{-\\|\\pmb{y}_{n+1}-\\pmb{y}_{i}\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We see that $\\alpha_{{\\pmb y}_{n+1},\\{{\\pmb y}_{i}\\}}({\\pmb y}_{*})$ varies monotonically with $\\pmb{y}_{n+1}^{\\top}\\pmb{y}_{\\ast}$ for all ${\\pmb y}_{n+1},\\{{\\pmb y}_{i}\\}$ . That is, ", "page_idx": 18}, {"type": "equation", "text": "$$\ny_{*}^{\\top}\\,y_{n+1}>y_{*}^{\\prime\\top}\\,y_{n+1}\\implies\\alpha_{y_{n+1},\\{y_{i}\\}}(y_{*})>\\alpha_{y_{n+1},\\{y_{i}\\}}(y_{*}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{f\\in\\mathbb{R}_{n+1},\\{s_{i}\\}}\\Bigg[\\sum_{i=1}^{f}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{f,y_{n+1},\\left\\{y_{i}\\right\\}}\\left[\\sum_{i}\\frac{f(y_{n+1})f(y_{i})e^{-\\left\\|\\mathbf{\\chi}\\right\\|y_{i}-\\omega(y_{n+1})\\left\\|^{2}}}{\\sum_{j}e^{-\\left\\|\\mathbf{\\chi}y_{j}-\\omega(y_{n+1})\\right\\|^{2}}}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{y_{n+1},\\{y_{i}\\}}\\left[\\underset{i}{\\sum}\\frac{\\mathbb{E}_{f}\\left[f(y_{n+1})f(y_{i})\\right]e^{-\\|y_{i}-c_{y_{n+1}}y_{n+1}\\|^{2}}}{\\sum_{j}e^{-\\|y_{j}-c_{y_{n+1}}y_{n+1}\\|^{2}}}\\right]}\\\\ &{=\\mathbb{E}_{y_{n+1},\\{y_{i}\\}}\\left[\\underset{i}{\\sum}\\frac{\\rho(y_{n+1}^{\\top}y_{i})e^{-\\|y_{i}-c_{y_{n+1}}y_{n+1}\\|^{2}}}{\\sum_{j}e^{-\\|y_{j}-c_{y_{n+1}}y_{n+1}\\|^{2}}}\\right]}\\\\ &{=n\\mathbb{E}_{y_{n+1},y_{n},\\{y_{i}\\}_{i=[n-1]}}\\left[\\frac{\\rho(y_{n+1}^{\\top}y_{i})e^{-\\|y_{i}-c_{y_{n+1}}y_{n+1}\\|^{2}}}{e^{-\\|y_{n}-c_{y_{n+1}}y_{n+1}\\|^{2}}+\\sum_{j}e^{-\\|y_{j}-c_{y_{n+1}}y_{n+1}\\|^{2}}}\\right]}\\\\ &{=n\\mathbb{E}_{y_{n+1},y_{n},\\{y_{i}\\}_{i=[n-1]}}\\left[\\rho(y_{n+1}^{\\top}y_{n})c_{x_{n+1},y_{n+1}\\{y_{i}\\}}(y_{n})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Critically for a given ${\\pmb y}_{n+1},\\alpha_{{\\pmb y},\\{{\\pmb y}_{i}\\}}({\\pmb y}_{*})$ can be re-parameterized as $\\alpha_{{\\pmb y}_{n+1},\\{{\\pmb y}_{i}\\}}({\\pmb y}_{*})=\\alpha_{\\{{\\pmb y}_{i}\\}}^{\\prime}({\\pmb y}_{*}-{\\pmb y}_{n+1})$ where $\\alpha_{\\{y_{i}\\}}^{\\prime}$ is symmetric about O and decreasing. Similarly, $\\rho(\\pmb{y}_{n+1}^{\\top}\\pmb{y}_{*})$ can be re-parameterized as $\\rho(\\pmb{y}_{n+1}^{\\top}\\pmb{y}_{\\ast})=\\rho^{\\prime}(\\pmb{y}_{\\ast}-\\pmb{y}_{n+1})$ where $\\alpha^{\\prime},\\rho^{\\prime}$ are symmetric decreasing rearrangement (that is, the set of points $_{\\textit{z}}$ such that $\\rho({\\boldsymbol{x}})>r$ is a ball about the origin). From Lemma I.2 we then have ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{y_{n+1}}\\mathbb{E}\\,y_{*},\\{y_{i}\\}_{i=[n-1]}\\left[\\rho(y_{n+1}^{\\top}\\,y_{*})\\alpha_{c_{y_{n+1}}\\,y_{n+1},\\{y_{i}\\}}(\\phi^{-1}(y_{*}))\\right]}\\\\ &{\\ =\\mathbb{E}_{y_{n+1}}\\mathbb{E}\\,y_{*},\\{y_{i}\\}_{i=[n-1]}\\left[\\rho^{\\prime}(\\|\\,y_{n+1}-y_{*}\\,\\|)\\alpha_{\\{y_{i}\\}}(\\|\\,y_{n+1}-\\phi^{-1}\\,y_{*}\\,\\|)\\right]}\\\\ &{\\ <\\mathbb{E}_{y_{n+1}}\\mathbb{E}\\,y_{*},\\{y_{i}\\}_{i=[n-1]}\\left[\\rho^{\\prime}(\\|\\,y_{n+1}-y_{*}\\,\\|)\\alpha_{\\{y_{i}\\}}(\\|\\,y_{n+1}-y_{*}\\,\\|)\\right]}\\\\ &{\\ =\\mathbb{E}_{y_{n+1}}\\mathbb{E}\\,y_{*},\\{y_{i}\\}_{i=[n-1]}\\left[\\rho(y_{n+1}^{\\top}\\,y_{*})\\alpha_{c_{y_{n+1}}\\,y_{n+1},\\{y_{i}\\}}(y_{*})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So $\\begin{array}{r}{\\mathcal{L}(\\omega)<\\mathcal{L}(\\mathbf{M})}\\end{array}$ . Let ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(c_{y_{n+1}})=\\mathbb{E}_{f,\\{y_{i}\\}}\\left[\\left(f(y_{n+1})-{\\frac{\\sum_{i}f(y_{i})e^{-\\|\\,y_{i}-c_{y_{n+1}}\\,y_{n+1}\\,\\|^{2}}}{\\sum_{j}e^{-\\|\\,y_{j}\\,-c_{y_{n+1}}\\,y_{n+1}\\,\\|^{2}}}}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Observe that $\\mathcal{L}(\\omega)=\\mathbb{E}_{\\pmb{y}_{n+1}}\\,q(c_{\\pmb{y}_{n+1}})$ We might as well set $\\omega$ to be such that $c_{\\pmb{y}_{n+1}}$ is the same for all ${\\pmb y}_{n+1}$ and a minimizer of $q$ , so we have $\\omega({\\pmb y}_{n+1})=c\\,{\\pmb y}_{n+1}$ for all $\\pmb{y}_{n+1}$ which implies $\\omega=c\\mathbf{I}_{d}$ for some $c$ . Because the optimal $\\mathbf{M}^{\\prime}$ is identity, the corresponding optimal $\\mathbf{M}$ is $\\Sigma^{-1}$ \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.1  Rewriting the Loss ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As a result of this, we can take $\\mathbf{M}=w_{K Q}\\boldsymbol{\\Sigma}^{-1}$ and write the attention estimator as ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{S A}(\\pmb{x})=\\sum_{i}\\frac{f(\\pmb{x}_{i})e^{-w_{K Q}\\|\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{i}\\,-\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{n+1}\\;\\|^{2}}}{\\sum_{j}e^{-w_{K Q}\\|\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{j}\\,-\\pmb{\\Sigma}^{-1/2}\\,\\pmb{x}_{n+1}\\;\\|^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This allows us to make the transformation $\\mathcal{X}\\rightarrow\\Sigma^{-1/2}\\,\\mathcal{X}$ . This has the effect of making both the data covariance and the induced function class covariance equal to the identity. Essentially, WLOG we will henceforth consider $\\Sigma=\\mathbf{I}_{d}$ . Henceforth, the estimator will be taken to be ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{S A}(\\pmb{x})=\\sum_{i}\\frac{f(\\pmb{x}_{i})e^{-w_{K Q}\\|\\pmb{x}_{i}-\\pmb{x}_{n+1}\\|^{2}}}{\\sum_{j}e^{-w_{K Q}\\|\\pmb{x}_{j}-\\pmb{x}_{n+1}\\|^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the loss will be parameterized by $w_{K Q}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{K Q})=\\mathbb{E}_{f,\\{x_{i}\\}}\\left[\\left(\\sum_{i}\\frac{\\left(f(x_{i})+\\epsilon_{i}\\right)e^{-w_{K Q}\\|\\mathbf{\\epsilon}_{x_{i}}-\\mathbf{x}_{n+1}\\|^{2}}}{\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\epsilon}_{x_{j}}-\\mathbf{x}_{n+1}\\|^{2}}}-f(\\mathbf{x}_{n+1})\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Because the noise $\\epsilon_{i}$ is independent of everything else, we can decompose this into two terms, a signal term and a noise term as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{K Q})=\\mathbb{E}_{f,\\{x_{i}\\}}\\left[\\left(\\sum_{i}\\frac{\\left(f(x_{n+1})-f(x_{i})\\right)e^{-w_{K Q}\\|\\mathbf{\\,x}_{i}-\\mathbf{\\,x}_{n+1}\\|^{2}}}{\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\,x}_{j}-\\mathbf{\\,x}_{n+1}\\|^{2}}}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n+\\underbrace{\\mathbb{E}_{f,\\left\\{\\boldsymbol{x}_{i}\\right\\}}\\left[\\left(\\sum_{i}\\frac{\\epsilon_{i}e^{-w_{K Q}}\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{n+1}\\|^{2}}{\\sum_{j}e^{-w_{K Q}}\\|\\boldsymbol{x}_{j}-\\boldsymbol{x}_{n+1}\\|^{2}}-f(\\pmb{x}_{n+1})\\right)^{2}\\right]}_{\\mathcal{L}_{\\mathrm{noise}}(\\boldsymbol{w}_{K Q})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We bound the frst term in Appendix $\\mathbf{C}$ and the second in Appendix D. A useful function that we bound in Lemma G.4 and Corrolary G.5 in Appendix $\\mathrm{G}$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{p}(r)=\\sum_{i=1}^{n}\\|\\,\\pmb{x}_{i}-\\pmb{x}\\,\\|^{p}e^{-r\\|\\,\\pmb{x}_{i}^{\\top}-\\pmb{x}^{2}\\,\\|}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We will use this function, particularly for $p=0$ and 1. ", "page_idx": 20}, {"type": "text", "text": "C The Signal Term ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The purpose of this section of the Appendix is to obtain upper and lower bounds on $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})$ Because we work with two different distributions over functions, and because the bounds depend on the distributions, we will make the distribution explicit in the argument to the function ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}))=\\mathbb{E}_{f,\\{x\\}}\\left(f(x_{i})-\\sum_{i}\\frac{f(x_{i})e^{-w_{K Q}\\|\\mathbf{\\psi}_{x_{i}}-\\mathbf{x}_{n+1}\\|^{2}}}{\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\psi}_{x_{j}}-\\mathbf{x}_{n+1}\\|^{2}}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a reminder, we consider the following two distributions over functions. Please see section B.1 to see why we have set the covariance of $\\mathbf{w}$ to be identity. ", "page_idx": 20}, {"type": "text", "text": "Definition C.1 (Affine and 2-ReLU Function Classes). The function classes $\\mathcal{F}_{L}^{a f\\!f}$ and $\\mathcal{F}_{L}^{+}$ are respectively defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{L}^{a f f}:=\\{f:f(\\pmb{x})=l\\mathbf{w}^{\\top}\\,\\pmb{x}+b,\\ \\mathbf{w}\\in\\mathbb{S}^{d-1}\\},}\\\\ &{\\mathcal{F}_{L}^{+}:=\\{f:f(\\pmb{x})=l_{1}\\,R e L U(\\mathbf{w}^{\\top}\\,\\pmb{x})+l_{2}\\,R e L U(-\\mathbf{w}^{\\top}\\,\\pmb{x})+b,\\ \\mathbf{w}\\in\\mathbb{S}^{d-1}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First we have the following trivial bound on $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})$ ", "page_idx": 20}, {"type": "text", "text": "Lemma C.2. For all $w_{K Q}$ we have $\\mathcal{L}_{s i g n a l}(w_{K Q})\\leq4L^{2}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. We have $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})\\le\\mathbb{E}\\left[\\left(\\sum\\frac{f({\\pmb x}_{i})-f({\\pmb x}_{n+1})\\gamma_{i}}{\\sum\\gamma_{i}}\\right)^{2}\\right]}\\end{array}$ for some positive $\\{\\gamma_{i}\\}$ . By Lipschitzness, $f(\\pmb{x}_{i})-f(\\pmb{x}_{n+1})\\leq L\\|\\,\\pmb{x}_{i}-\\bar{\\pmb{x}}_{n+1}\\,\\|\\leq2L$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C.1   Affine functions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we consider the affine function class $\\mathcal{F}_{L}^{\\mathrm{aff}}$ First, we note that this classatisfes Assumption B.4. Lemma C3. The affine cas $\\mathcal{F}_{L}^{a\\!f\\!\\!f}$ inDefniion.2 satisfe Assution4 ", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "text", "text": "2. Because $b$ is independent of $w$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{f}\\left[f(\\pmb{x})f(\\pmb{y})\\right]=\\mathbb{E}_{\\mathbf{w}}\\left[l^{2}\\,\\pmb{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,\\pmb{y}+\\pmb{b}^{2}\\right]=\\mathbb{E}\\,l^{2}\\frac{\\mathbb{E}_{\\mathbf{w}}\\left\\|\\mathbf{w}\\right\\|^{2}}{d}\\,\\pmb{x}^{\\top}\\,\\pmb{y}+\\frac{L^{2}}{3}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "3. w is isotropic, so $\\phi(\\mathbf{w})$ is also supported by the distribution on w. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.4. For affine functions, the signal term is upper bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{s i g n a l}(w_{K Q};D(\\mathcal{F}_{L}^{a f}))\\leq\\left\\{\\begin{array}{l l}{L^{2}\\,\\mathcal{O}\\left(\\frac{1}{w_{K Q}^{2}}+\\frac{w_{K Q}^{\\frac{d}{2}-1}}{n}+\\frac{1}{n}\\right)}&{w_{K Q}\\geq\\frac{d+\\sqrt{d}}{2}}\\\\ {4L^{2}}&{w_{K Q}<\\frac{d+\\sqrt{d}}{2}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. In the interest of readability, we will denote $x_{n+1}$ as $\\textbf{\\em x}$ .Consider $\\tilde{\\pmb{x}}$ such that $\\tilde{\\textbf{\\em x}}=$ $\\begin{array}{r}{\\sum_{i}\\mathbf{\\emx}_{i}\\frac{e^{-2w_{K Q}\\mathbf{\\emx}_{i}^{\\top}\\mathbf{\\emx}}}{\\sum_{j}e^{-2w_{K Q}\\mathbf{\\emx}_{i}^{\\top}\\mathbf{\\emx}}}}\\end{array}$ $\\mathbb{E}\\left[l^{2}\\mathbf{w}^{\\top}(\\pmb{x}-\\tilde{\\pmb{x}})\\right]^{2}$   \ndent of $x,\\{x_{i}\\}$ , we have $\\mathbb{E}\\,l^{2}\\left(\\mathbf{w}^{\\top}(\\pmb{x}-\\tilde{\\pmb{x}})\\right)^{2}=\\mathbb{E}\\,l^{2}\\mathbf{w}\\mathbf{w}^{\\top}(\\pmb{x}-\\tilde{\\pmb{x}})(\\pmb{x}-\\tilde{\\pmb{x}})^{\\top}$ , Now w has a uniformly randomly chosen direction, so its covariance is a multiple of the identity. We have $\\mathbb{E}\\operatorname{Tr}(\\mathbf{ww}^{\\top})=$ $\\begin{array}{r}{\\mathbb{E}\\left\\|\\mathbf{w}\\right\\|^{2}=\\frac{L^{2}}{3}}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}\\,l^{2}\\mathbf{w}\\mathbf{w}^{\\top}\\,=\\,\\frac{L^{2}}{3d}\\mathbf{I}_{d}}\\end{array}$ . Continuing, $\\begin{array}{r}{\\mathbb{E}\\left(\\mathbf{w}^{\\top}({\\pmb x}-\\tilde{{\\pmb x}})\\right)^{2}=\\frac{L^{2}}{3d}\\,\\mathbb{E}\\,\\|\\,{\\pmb x}-\\tilde{{\\pmb x}}\\|^{2}}\\end{array}$ . Take any ${\\pmb x}^{\\prime}\\perp{\\pmb x}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\,\\tilde{\\pmb{x}}^{\\top}\\,\\pmb{x}^{\\prime}=\\mathbb{E}\\sum_{i}\\pmb{x}_{i}^{\\top}\\,\\pmb{x}^{\\prime}\\,\\frac{e^{-2w_{K Q}\\,\\pmb{x}_{i}^{\\top}\\,\\pmb{x}}}{\\sum_{j}e^{-2w_{K Q}\\,\\pmb{x}_{i}^{\\top}\\,\\pmb{x}}}}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\sum_{i}\\mathbb{E}[\\pmb{x}_{i}^{\\top}\\,\\pmb{x}^{\\prime}\\,|\\,\\pmb{x}_{i}^{\\top}]\\frac{e^{-2w_{K Q}\\,\\pmb{x}_{i}^{\\top}\\,\\pmb{x}}}{\\sum_{j}e^{-2w_{K Q}\\,\\pmb{x}_{i}^{\\top}\\,\\pmb{x}}}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Decomposing $\\tilde{\\pmb{x}}$ into an orthogonal and a parallel component, we have $\\begin{array}{r l}{\\mathbb{E}\\parallel\\pmb{x}-\\tilde{\\pmb{x}}\\parallel^{2}}&{{}=}\\end{array}$ $\\mathbb{E}\\|\\,{\\pmb x}-{\\pmb x}\\,{\\pmb x}^{\\top}\\,{\\pmb x}-{\\pmb x}^{\\prime}\\,{\\pmb x}^{\\prime}^{\\top}\\,{\\pmb x}\\|^{2}$ for some $x^{\\prime}\\perp x$ with $\\|\\pmb{x}^{\\prime}\\|=1$ .But ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\,x-x\\,x^{\\top}\\,\\tilde{x}-x^{\\prime}\\,x^{\\prime^{\\top}}\\,\\tilde{x}\\|^{2}}\\\\ &{=\\mathbb{E}\\|\\,x(1-x^{\\top}\\,\\tilde{x})\\|^{2}+\\mathbb{E}\\|\\,x^{\\prime}\\,x^{\\prime^{\\top}}\\,\\tilde{x}\\|^{2}-2\\,\\mathbb{E}\\,x(1-x^{\\top}\\,\\tilde{x})\\tilde{x}^{\\top}\\,x^{\\prime}\\,x^{\\prime^{\\top}}}\\\\ &{=\\mathbb{E}\\|\\,x(1-x^{\\top}\\,\\tilde{x})\\|^{2}+\\mathbb{E}\\,\\|\\,x^{\\prime}\\,x^{\\prime^{\\top}}\\,\\tilde{x}\\|^{2}\\qquad\\quad\\cdot:x^{\\top}\\,x^{\\prime}=0\\implies2\\,\\mathbb{E}\\,x(1-x^{\\top}\\,\\tilde{x})\\tilde{x}^{\\top}\\,x^{\\prime}\\,x^{\\prime^{\\top}}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Case 1: $\\begin{array}{r}{w_{K Q}\\ge\\frac{d+\\sqrt{d}}{2}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Consider first the term $\\mathbb{E}\\,\\|\\,{\\pmb x}(1-{\\pmb x}^{\\top}\\,{\\pmb\\tilde{x}})\\|^{2}=\\mathbb{E}(1-{\\pmb x}^{\\top}\\,{\\pmb\\tilde{x}})^{2}$ . Here we have with probability $\\textstyle1-{\\frac{1}{n}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-{\\pmb x}^{\\top}\\,{\\tilde{\\pmb x}}=\\frac{\\sum\\left(1-{\\pmb x}^{\\top}\\,{\\pmb x}_{i}\\right)e^{-w_{K Q}}\\left\\|\\,{\\pmb x}-{\\pmb x}_{i}\\,\\right\\|^{2}}{\\sum e^{-w_{K Q}}\\left\\|\\,{\\pmb x}-{\\pmb x}_{i}\\,\\right\\|^{2}}=\\frac{g_{2}\\left(w_{K Q}\\right)}{2g_{0}\\left(w_{K Q}\\right)}}\\\\ &{\\ \\ \\leq\\frac{\\overline{{C_{b}}}n\\,\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}+1}}{2C_{b}n\\,\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}}}\\leq\\frac{\\overline{{C_{b}}}}{\\underline{{C_{b}}}}\\frac{1}{w_{K Q}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The other term $\\mathbb{E}\\,\\|\\,{\\pmb x}^{\\prime}\\,{\\pmb x}^{\\prime\\,\\top}\\,{\\pmb x}\\|^{2}=\\mathbb{E}({\\pmb x}^{\\prime\\,\\top}\\,{\\pmb\\tilde{x}})^{2}$ is the component of the bias in the direction orthogonal to $\\textbf{\\em x}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x^{\\prime\\prime}\\,\\bar{x})^{2}=\\left(\\frac{\\sum_{i}x^{\\prime\\prime}\\,x_{i}\\,e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}\\,e^{\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}{\\sum_{i}e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}\\right)^{2}}\\\\ &{\\qquad\\leq\\left(\\frac{\\sum_{i}x^{\\prime\\prime}\\,x_{i}\\,e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}\\,e^{\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}{\\sum_{i}e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}\\right)^{2}}\\\\ &{\\qquad\\leq\\left(\\frac{\\sum_{i}x^{\\prime\\prime}\\,x_{i}\\,e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}\\,e^{\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}{\\sum_{i}e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}\\right)^{2}}\\\\ &{\\qquad\\leq\\frac{\\sum_{i}\\left(1-\\left(x^{\\prime}\\,x_{i}\\right)^{2}\\right)\\,e^{-2\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}\\,e^{\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}{\\left(\\sum_{i}e^{-\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}\\,e^{\\arg[\\mathbf{z}_{i}]}\\right)^{2}}}\\\\ &{\\qquad\\leq\\frac{\\sum_{i}2\\left(1-x^{\\prime\\prime}\\,x_{i}\\right)\\,e^{-2\\arg[\\mathbf{z}_{i}-\\mathbf{z}]}}{\\left(\\sum_{i}e^{-\\arg[\\mathbf{z}_{i}]}-\\mathbf{z}\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Popoviciu's Variance inequality ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq{\\frac{\\sum_{i}\\|\\mathbf{\\nabla}x_{i}-\\mathbf{x}\\|^{2}e^{-2w_{K Q}\\|\\mathbf{\\nabla}x_{i}-\\mathbf{x}\\|^{2}}}{\\left(\\sum_{i}e^{-w_{K Q}\\|\\mathbf{\\nabla}x_{i}-\\mathbf{x}\\|^{2}}\\right)^{2}}}={\\frac{g_{2}(2w_{K Q})}{g_{0}^{2}(w_{K Q})}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With probability $\\textstyle1-{\\frac{1}{n}}$ , when $w_{K Q}\\ge d+\\sqrt{d}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{g_{2}(2w_{K Q})}{g_{0}(w_{K Q})^{2}}\\leq\\frac{\\overline{{c_{g}}}n\\left(\\frac{1}{2w_{K Q}}\\right)^{\\frac{d}{2}+1}}{\\left(\\underline{{c}}_{g}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}}\\right)^{2}}\\leq\\frac{\\overline{{c_{g}}}w_{K Q}^{\\frac{d}{2}-1}}{\\underline{{c_{g}}}^{2}2^{\\frac{d}{2}+1}n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Putting toghqatio1lad 1wehavewithpabilty $\\textstyle1-{\\frac{1}{n}}$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}))\\leq\\mathcal{O}\\left(\\frac{L^{2}}{3d}\\left(\\frac{1}{w_{K Q}}+\\frac{w_{K Q}^{\\frac{d}{2}-1}}{n}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The signal bias is upper bounded by $4L^{2}$ always (Lemma C.2). The overall upper-bound on the expectation is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}))\\leq\\mathcal{O}\\left(\\frac{L^{2}}{3d}\\left(\\frac{1}{w_{K Q}}+\\frac{w_{K Q}^{\\frac{d}{2}-1}}{n}+4\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case 2: $\\begin{array}{r}{w_{K Q}<\\frac{d+\\sqrt{d}}{2}}\\end{array}$ We always have $\\mathcal{L}(w_{K Q})\\leq4L^{2}$ from Lemma C.2. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. For affine functions, the signal term is lower bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{s i g n a l}(w_{K Q};D(\\mathcal{F}_{L}^{a f f}))\\geq\\left\\{\\Omega\\left(\\frac{L^{2}}{w_{K Q}^{2}}\\right)\\right.\\quad w_{K Q}>\\frac{d+\\sqrt{d}}{2}}\\\\ {\\Omega\\left(1\\right)\\quad\\quad\\quad\\quad\\left.w_{K Q}<\\frac{d+\\sqrt{d}}{2}\\right.}\\end{array}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Similar to Equation (10), for $\\begin{array}{r}{\\tilde{\\pmb{x}}=\\sum_{i}\\pmb{x}_{i}\\,\\frac{e^{-2w_{K Q}\\,\\pmb{x}_{i}^{\\top}\\,\\pmb{x}}}{\\sum_{j}e^{-2w_{K Q}\\,\\pmb{x}_{i}^{\\top}\\,\\pmb{x}}}}\\end{array}$ \u2211; e-2wkq \u03b1 \u03b1, We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{signal}}(w_{K Q};D({\\mathcal{F}}_{L}^{\\mathrm{aff}}))\\geq{\\frac{L^{2}}{3d}}\\operatorname{\\mathbb{E}}\\|\\,x(1-x^{\\top}\\,\\tilde{x})\\|^{2}={\\frac{L^{2}}{3d}}\\operatorname{\\mathbb{E}}(1-x^{\\top}\\,\\tilde{x})^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now consider the term $1-x^{\\top}\\,\\tilde{x}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\sum(1-\\pmb{x}^{\\top}\\,\\pmb{x}_{i})e^{-w_{K Q}\\,\\parallel\\,\\pmb{x}\\,-\\,\\pmb{x}_{i}\\parallel^{2}}}{\\sum e^{-w_{K Q}\\,\\parallel\\,\\pmb{x}\\,-\\,\\pmb{x}_{i}\\parallel^{2}}}\\geq\\frac{g_{2}(w_{K Q})}{2g_{0}(w_{K Q})}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case 1: $\\begin{array}{r}{w_{K Q}\\ge\\frac{d+\\sqrt{d}}{2}}\\end{array}$ Here we have from Corollary G.5, with probability $1-1/n$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\sum(1-{\\pmb x}^{\\top}\\,{\\pmb x}_{i})e^{-w_{K Q}\\,\\parallel\\,{\\pmb x}\\,-\\,{\\pmb x}_{i}\\,\\parallel^{2}}}{\\sum e^{-w_{K Q}\\,\\parallel\\,{\\pmb x}\\,-\\,{\\pmb x}_{i}\\,\\parallel^{2}}}\\ge\\frac{\\frac{C_{b}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}+1}}{2\\overline{C_{b}n}\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}}}\\ge\\frac{C_{b}}{2\\overline{C_{b}}}\\frac{1}{w_{K Q}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With probability $1/n\\leq{\\frac{1}{2}}$ the lowest we can have is $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})=0$ , so overall we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})\\geq\\frac{L^{2}}{24d}\\left(\\frac{C_{b}}{\\overline{{C_{b}}}}\\frac{1}{w_{K Q}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case2: $\\textstyle{\\frac{d+{\\sqrt{d}}}{4}}\\leq w_{K Q}\\leq{\\frac{d+{\\sqrt{d}}}{2}}$ From ColaryG ., wit robabity $\\textstyle1-{\\frac{1}{n}}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\sum(1-x^{\\top}\\,{\\pmb x}_{i})e^{-w_{K Q}\\,\\|\\,{\\pmb x}\\,-\\,{\\pmb x}_{i}\\|^{2}}}{\\sum e^{-w_{K Q}\\,\\|\\,{\\pmb x}\\,-\\,{\\pmb x}_{i}\\|^{2}}}\\geq\\frac{C_{b}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}+1}}{2\\overline{C_{b}}n e^{-2w_{K Q}}}\\geq\\frac{C_{b}}{2\\overline{C_{b}}}\\frac{e^{2w_{K Q}}}{w_{K Q}^{\\frac{d}{2}+1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With probability $1/n\\leq{\\frac{1}{2}}$ the lowest we can have is $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}^{\\mathrm{aff}}))=0$ , so overall we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}^{\\mathrm{aff}}))\\geq\\frac{L^{2}}{24d}\\left(\\frac{C_{b}}{\\overline{{C_{b}}}}\\frac{e^{2w_{K Q}}}{w_{K Q}^{\\frac{d}{2}+1}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case 3: $\\textstyle{\\frac{d+{\\sqrt{d}}}{4}}>w_{K Q}$ From CorollaryG.5, withprobability $\\textstyle1-{\\frac{1}{n}}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\sum(1-\\pmb{x}^{\\top}\\,\\pmb{x}_{i})e^{-w_{K Q}\\,\\parallel\\,\\pmb{x}\\,-\\,\\pmb{x}_{i}\\parallel^{2}}}{\\sum e^{-w_{K Q}\\,\\parallel\\,\\pmb{x}\\,-\\,\\pmb{x}_{i}\\parallel^{2}}}\\ge\\frac{C_{b}n e^{-4w_{K Q}}}{2\\overline{{C_{b}}}n e^{-2w_{K Q}}}\\ge\\frac{C_{b}}{2\\overline{{C_{b}}}}e^{-2w_{K Q}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With probability $1/n\\leq{\\frac{1}{2}}$ the lowest we can have is $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}^{\\mathrm{aff}}))=0$ so overall we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}^{\\mathrm{aff}}))\\geq\\frac{L^{2}}{24d}\\left(\\frac{C_{b}}{\\overline{{C_{b}}}}e^{-2w_{K Q}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Corollary C.6. Combining the above, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nL^{2}\\,\\mathcal{O}\\left(\\frac{1}{\\left(w_{K Q}+1\\right)^{2}}\\right)\\leq\\mathcal{L}_{s i g n a l}(w_{K Q};D(\\mathcal{F}_{L}^{a f}))\\leq L^{2}\\,\\mathcal{O}\\left(\\frac{1}{w_{K Q}^{2}}+\\frac{w_{K Q}^{\\frac{d}{2}-1}}{n}+\\frac{1}{n}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can now perturb these bounds in the case ofthe ReLU-based function class $\\mathcal{F}_{L}^{+}$ ", "page_idx": 23}, {"type": "text", "text": "C.2  ReLU-based functions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Consider the function class ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{F}_{L}^{+}=\\{l_{1}\\mathrm{ReLU}(\\mathbf{w}^{\\top}\\,\\mathbf{x})+l_{2}\\mathrm{ReLU}(-\\mathbf{w}^{\\top}\\,\\mathbf{x})+b:\\mathbf{w}\\in\\mathbb{S}^{d-1},b,l_{1},l_{2}\\in[-L,L]\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathrm{ReLU}(z)\\,:=\\,(z)_{+}\\,:=\\,\\operatorname*{max}(z,0)$ . Consider a distributions on $\\mathcal{F}_{L}^{+}$ ,namely $D(\\mathcal{F}_{L}^{+})$ . Let $D(\\mathcal{F}_{L}^{+})$ be induced by $\\mathbf{w}\\sim\\mathcal{U}^{d},b,l_{1},l_{2}\\sim\\mathrm{Unif}[-L,L]$ . That is, a vector w is drawn uniformly on the unit hypersphere. Then two norms are selected, $l_{1},l_{2}$ , and the overall function is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{{\\bf w},l_{1},l_{2}}({\\pmb x})=l_{1}\\mathrm{ReLU}({\\bf w}^{\\top}\\,{\\pmb x})+l_{2}\\mathrm{ReLU}(-{\\bf w}^{\\top}\\,{\\pmb x})+b,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that it follows one affine rule in one halfspace, and another affine rule in the opposite halfspace.   \nPlease see section B.1 to see why we have set the covariance of $\\mathbf{w}$ to be identity. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.7. The class $\\mathcal{F}_{L}^{+}$ and distribution $D(\\mathcal{F}_{L}^{+})$ defined above satisfy Assumption B.4. ", "page_idx": 23}, {"type": "text", "text": "Proof 1. Each function is defined as being piece-wise $L$ -Lipschitz, and it is continuous, so it is also $L$ -Lipschitz overall. ", "page_idx": 23}, {"type": "text", "text": "2. With probability 1 - 2arcos(a) the points $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ are such that $(\\mathbf{w}^{\\top}\\,\\mathbf{x})(\\mathbf{w}^{\\top}\\,\\mathbf{y})<0$ (that is, they are on opposite sides of the hyperplane defining the two pieces of the ReLU). Because the bias $b$ is independent of the other parameters, we have as in the proof of Lemma C.3 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{f}\\left[f(\\mathbf{x})f(\\pmb{y})\\right]=\\frac{L^{2}}{3}+\\mathbb{E}_{\\mathbf{w}}\\left[l_{1}^{2}\\,\\mathbf{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,{y}\\right]\\left(\\mathbf{w}^{\\top}\\,{x}\\right)(\\mathbf{w}^{\\top}\\,{y})\\geq0\\big]\\mathbb{P}[(\\mathbf{w}^{\\top}\\,{x})(\\mathbf{w}^{\\top}\\,{y})\\geq0]}\\\\ &{\\qquad\\qquad\\qquad+\\mathbb{E}_{\\mathbf{w}}\\left[l_{1}l_{2}\\,\\mathbf{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,{y}\\big|\\left(\\mathbf{w}^{\\top}\\,{x}\\right)(\\mathbf{w}^{\\top}\\,{y})<0\\right]\\mathbb{P}[(\\mathbf{w}^{\\top}\\,{x})(\\mathbf{w}^{\\top}\\,{y})<0]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{L^{2}}{3}+\\mathbb{E}_{\\mathbf{w}}\\left[l_{1}^{2}\\,\\mathbf{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,{y}\\big|\\,{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,{y}>0\\right]\\left(2\\frac{\\mathrm{arccos}\\left(\\mathbf{x}^{\\top}\\,{y}\\right)}{\\pi}\\right)\\,\\,\\,\\cdot\\,l_{1}\\pm l_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{\\overline{{\\mathbf{\\emx}}}=\\mathbf{\\beta}\\frac{\\mathbf{\\partial}x}{\\Vert\\mathbf{\\emx}\\Vert}}\\end{array}$ for any vector $\\textbf{\\em x}$ . Consider a re-parameterization of the pair $\\left({\\pmb x},{\\pmb y}\\right)$ as $\\xi_{\\theta}(x,{\\pmb y})\\rightarrow(\\overline{{{x+y}}},\\overline{{{x-y}}})$ .Because $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ are on the unit sphere, this is a bijection as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\xi_{\\theta}^{-1}({\\pmb x},{\\pmb y})=\\left(\\frac{1+\\theta}{2}\\,{\\pmb x}+\\frac{1-\\theta}{2}\\,{\\pmb y},\\frac{1+\\theta}{2}\\,{\\pmb x}-\\frac{1-\\theta}{2}\\,{\\pmb y}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "That is, for any $x,y,\\xi_{x^{\\top}y}^{-1}(\\xi_{x^{\\top}y}(x,y))=(x,y)$ . The push-forward of $\\xi$ is also uniform, that is for $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}$ satisfying $x^{\\top}\\,y=\\theta$ $\\xi_{\\theta}(x,y)$ is distributed as $\\boldsymbol{\\mathcal{U}}^{d}\\times\\boldsymbol{\\mathcal{U}}^{d-1}$ . For any $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}$ \uff0c let $\\xi_{\\theta}^{-1}(x,{\\pmb y})\\,=\\,({\\pmb x}_{\\theta},{\\pmb y}_{\\theta})$ : Then we have $\\mathbb{E}_{f}\\left[f(\\pmb{x}_{\\theta})f(\\pmb{y}_{\\theta})\\right]$ is a decreasing function of $\\theta$ Finally, for $\\theta\\,\\leq\\,\\theta^{\\prime}$ $\\theta^{\\prime},\\,L^{2}\\,{\\pmb x}_{\\theta}^{\\top}\\,{\\bf w}{\\bf w}^{\\top}\\,{\\pmb y}_{\\theta}\\,>\\,\\bar{L}^{2}\\,{\\pmb x}_{\\theta^{\\prime}}^{\\top}\\,{\\bf w}{\\bf w}^{\\top}\\,{\\pmb y}_{\\theta^{\\prime}}$ $\\pmb{x}_{\\theta}^{\\top}$ ww $^{\\top}\\pmb{y}_{\\theta}\\;<\\;0\\;\\;\\;\\Longrightarrow$ $\\mathbf{\\Delta}\\mathbf{x}_{\\theta^{\\prime}}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,\\mathbf{{\\pmby}}_{\\theta^{\\prime}}<0$ The product f two positive ncreasing fuctions isitself non-increasig. Since we have both Ew [L2\u03b1T wwT y\u03b1T wwT y>0] and 2arco(a ) are increasing functions of $\\boldsymbol{x}^{\\intercal}\\boldsymbol{y}$ , we also have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}}\\left[L^{2}\\,{\\pmb{x}}^{\\top}\\,{\\mathbf{w}}{\\mathbf{w}}^{\\top}\\,{\\pmb{y}}\\,{\\pmb{\\vert}}\\,{\\pmb{x}}^{\\top}\\,{\\mathbf{w}}{\\mathbf{w}}^{\\top}\\,{\\pmb{y}}>0\\right]\\left(\\frac{2\\operatorname{arccos}\\!\\left({\\pmb{x}}^{\\top}\\,{\\pmb{y}}\\right)}{\\pi}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is an increasing function of $\\boldsymbol{x}^{\\intercal}\\boldsymbol{y}$ since $\\mathbb{E}_{\\mathbf{w}}\\left[L^{2}\\,\\mathbf{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,\\mathbf{y}\\right|\\mathbf{x}^{\\top}\\,\\mathbf{w}\\mathbf{w}^{\\top}\\,\\mathbf{y}\\,>\\,0\\right]\\,\\ge\\,0$ and $\\left({\\frac{2\\operatorname{arccos}\\!\\left(\\mathbf{x}^{\\top}\\,\\mathbf{y}\\right)}{\\pi}}\\right)\\geq0.$ ", "page_idx": 24}, {"type": "text", "text": "3. w is distributed uniformly on the hypersphere, so $\\phi(\\mathbf{w})$ is also also distributed uniformly on the hypersphere for any isometry $\\phi$ that preserves the origin. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.8. The signal term is upper bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{s i g n a l}(w_{K Q};D(\\mathcal{F}_{L}^{+}))\\leq\\left\\{\\begin{array}{l l}{L^{2}\\,\\mathcal{O}\\left(\\frac{1}{w_{K Q}}+\\frac{1}{n}\\right)}&{w_{K Q}\\geq\\frac{d+\\sqrt{d}}{2}}\\\\ {4L^{2}}&{w_{K Q}<\\frac{d+\\sqrt{d}}{2}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D)=\\mathbb{E}_{f,\\{x_{i}\\}}\\left(\\frac{\\sum_{i}\\left(f\\left(x_{i}\\right)-f\\left(x_{n}\\right)\\right)e^{-w_{K Q}\\left\\Vert\\mathbf{\\sigma}_{x_{i}}-\\mathbf{x}_{n}\\right\\Vert^{2}}}{\\sum_{i}e^{-w_{K Q}\\left\\Vert\\mathbf{\\sigma}_{x_{i}}-\\mathbf{x}_{n}\\right\\Vert^{2}}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{f,\\{x_{i}\\}}\\left(\\frac{\\sum_{i}L\\left\\Vert\\mathbf{\\sigma}_{x_{i}}-\\mathbf{x}_{n}\\right\\Vert e^{-w_{K Q}\\left\\Vert\\mathbf{\\sigma}_{x_{i}}-\\mathbf{x}_{n}\\right\\Vert^{2}}}{\\sum_{i}e^{-w_{K Q}\\left\\Vert\\mathbf{\\sigma}_{x_{i}}-\\mathbf{x}_{n}\\right\\Vert^{2}}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(L\\frac{g_{1}\\left(w_{K Q}\\right)}{g_{0}\\left(w_{K Q}\\right)}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With probability $\\textstyle1-{\\frac{1}{n}}$ , when $\\begin{array}{r}{w_{K Q}\\ge\\frac{d+\\sqrt{d}}{2}}\\end{array}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{g_{1}(w_{K Q})}{g_{0}(w_{K Q})}\\leq\\frac{\\overline{{C_{b}}}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d+1}{2}}}{\\underline{{C_{b}}}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}}}\\leq\\frac{\\overline{{C_{b}}}}{\\underline{{C_{b}}}}\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We always have $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q})\\leq4L^{2}$ from Lemma C.2. So the overall upper bound is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D)\\leq L^{2}\\left(\\frac{1}{w_{K Q}}+\\frac{4}{n}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For wkq \u2265 dtva, , as before, we always have $\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D)\\leq4L^{2}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma C.9. The signal term is lower bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s i g n a l}(w_{K Q};D(\\mathcal{F}_{L}^{+}))\\geq\\mathcal{L}_{s i g n a l}(w_{K Q};D(\\mathcal{F}_{L}^{a f f}))/2\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Again for readability we will write $x_{n+1}$ as $\\textbf{\\em x}$ .For any $f\\,\\in\\,{\\mathcal F}_{L}^{+}$ let $f_{x,\\mathrm{aff}}$ denote the corresponding affine function that is equal to $f$ in the halfspace containing $\\textbf{\\em x}$ , that is if $f(\\pmb{x}^{\\prime})=$ $l_{1}\\mathrm{ReLU}(\\mathbf{w}^{\\top}\\,\\mathbf{x}^{\\prime})+l_{2}\\mathrm{ReLU}(-\\mathbf{w}^{\\top}\\,\\mathbf{x}^{\\prime})+b$ ,and WLOG $\\mathbf{w}^{\\top}\\,\\pmb{x}^{\\prime}>0$ then $f_{x,\\mathrm{aff}}(\\mathbf{x}^{\\prime})=l_{1}\\mathbf{w}^{\\top}\\,\\mathbf{x}^{\\prime}\\,{+}b$ Note that $f_{x,\\mathrm{aff}}$ comes from a w selected from the unit sphere and $b,l\\in[-L,L]$ exactly as $f\\sim$ ", "page_idx": 24}, {"type": "text", "text": "$D(\\mathcal{F}_{L})$ , so it is actually statistically indistinguishable from a sample from $D(\\mathcal{F}_{L}^{\\mathrm{aff}})$ , the distribution over affine functions in Definition 3.2 (and the object of Lemma C.5). The error of the nonlinear estimator can be written as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{f,\\mathbf{x},\\{\\mathbf{x}_{i}\\}}\\left[\\left(\\sum_{i}f(\\mathbf{x}_{i})\\gamma_{i}-f(\\mathbf{x}_{n})\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\gamma_{i}\\;=\\;\\frac{e^{-w_{K Q}\\;\\|\\;\\mathbf{x}-\\mathbf{x}_{i}\\;\\|_{\\mathbf{\\Sigma}}^{2}}-1}{\\sum_{j}e^{-w_{K Q}\\;\\|\\;\\mathbf{x}-\\mathbf{x}_{j}\\;\\|_{\\mathbf{\\Sigma}}^{2}-1}}}\\end{array}$ $A=\\{i:(\\pmb{x}_{i}^{\\top}\\,\\mathbf{w})(\\pmb{x}^{\\top}\\,\\mathbf{w})<0\\}$ denote the set of points on the opposite side to $\\textbf{\\em x}$ of the hyperplane defining the function. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\alpha\\in\\mathbb{N}_{F}}\\alpha_{0,0}(B(T_{1}))}\\\\ &{=\\mathcal{E}_{\\alpha,1}\\left(\\displaystyle\\sum_{s\\geq0}\\left(\\sum_{i=1}^{n}f_{i,s}(x_{i})-f_{i,s}(x_{i})\\right)^{s}\\right)}\\\\ &{=\\mathcal{E}_{\\alpha,1}\\left(\\prod_{s\\geq0}\\left(\\sum_{i=1}^{n}f_{i,s}(x_{i})\\right)+\\frac{\\sum_{i=1}^{n}(f_{i}(x_{i})-f_{i,s}(x_{i}))-f_{i,s}(x_{i})}{\\sum_{s\\geq0}^{n}f_{i,s}(x_{i})}\\right)^{s}}\\\\ &{\\qquad=\\mathcal{E}_{\\alpha,1}\\mathcal{E}_{\\alpha}\\left(\\left[\\sum_{s\\geq0}\\left(f_{i,s}(x_{i})-f_{i,s}(x_{i})\\right)^{s}\\right]+x_{\\alpha}\\left[\\left(\\sum_{s\\geq0}\\left(f_{i,s}(x_{i})\\right)\\right)^{s}\\right]\\right.}\\\\ &{\\qquad=\\mathcal{E}_{\\alpha,1}\\delta_{i,1}^{\\alpha}\\Bigg[\\Bigg.\\Bigg(\\sum_{s\\geq0}\\left(\\sum_{i=1}^{n}f_{i,s}(x_{i})\\right)-f_{i,s}(x_{i})\\Bigg)^{s}\\Bigg]+x_{\\alpha}\\left[\\sum_{s\\geq0}\\left(\\sum_{i=1}^{n}f_{i,s}(x_{i})\\right)^{s}\\right]}\\\\ &{\\qquad\\qquad\\quad-2\\delta_{i,j}\\left(\\sum_{s\\geq0}\\left(f_{i,s}(x_{i})\\right)-f_{i,s}(x_{i})\\right)\\left(\\sum_{s\\geq0}\\left(f_{i,s}(x_{i})\\right)+\\delta_{i,j}\\left(\\sum_{s\\geq0}\\left(f_{i,s}(x_{i})\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\left.\\sum_{i=1}^{n}\\delta_{i,i+1}\\left(\\sum_{s\\geq0}\\left(\\sum_{i=1}^{n}f_{i,s}(x_{i})\\right)^{s}\\right)+\\mathbb{E}_{\\alpha,i+1}\\left(\\sum_{s\\geq0}\\left(f_{i,s}(x_{i})\\right)\\right)^{s}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\sum_{i=1}^{n}\\sum_{s\\geq0}\\left(\\sum_{i=1}^{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here the third equality holds because $f(\\pmb{x}_{i})$ is independent of $f_{\\mathbf{x},\\mathrm{aff}}(\\pmb{x}_{j})$ if $i\\in A,j\\not\\in A$ ", "page_idx": 25}, {"type": "text", "text": "Let $\\begin{array}{r}{q=\\mathbb{E}_{f,x,\\{x_{i}\\}}\\left(\\sum_{i\\in A}f(x_{i})\\gamma_{i}\\right)\\right)^{2}=\\mathbb{E}_{f,x,\\{x_{i}\\}}\\left(\\sum_{i\\in A}f_{x,\\mathrm{aff}}(x_{i})\\gamma_{i}\\right)\\right)^{2}.}\\end{array}$ Then from the above we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{f,x,\\{x_{i}\\}}\\left[\\left(\\sum_{i}f(\\pmb{x}_{i})\\gamma_{i}-f(\\pmb{x})\\right)^{2}\\right]\\ge(\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}))(w_{K Q})-q)^{2}+q^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which has minimum at $q=\\mathcal{L}_{\\mathrm{signal}}(w_{K Q};D(\\mathcal{F}_{L}))/2$ , completing the proof. ", "page_idx": 25}, {"type": "text", "text": "D Bounds on Noise Variance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we obtain upper and lower bounds on the variance of the estimator due to label noise. There are three relevant parameters: $d$ , the ambient dimension of the data; $w_{K Q}$ , the scaling induced ", "page_idx": 25}, {"type": "text", "text": "by the attention layer; and $n$ , the number of tokens. Recall that the noise term is ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{noise}}(w_{K Q})=\\mathbb{E}_{f,\\{x_{i}\\}}\\left[\\left(\\sum_{i}{\\frac{\\epsilon_{i}e^{-w_{K Q}\\|\\mathbf{\\,x}_{i}-\\mathbf{x}_{n+1}\\|^{2}}}{\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\,x}_{j}-\\mathbf{x}_{n+1}\\|^{2}}}}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Because the $\\epsilon_{i}$ are independent, this can further be simplified as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})=\\sigma^{2}\\mathbb{E}_{\\{x_{i}\\}}\\left[\\sum_{i}{\\frac{e^{-2w_{K Q}\\|\\mathbf{\\,x}_{i}-\\mathbf{x}_{n+1}\\|^{2}}}{\\left(\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\,x}_{j}-\\mathbf{x}_{n+1}\\|^{2}}\\right)^{2}}}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma D.1. The noise term is bounded for $\\begin{array}{r}{d+\\sqrt{d}\\leq w_{K Q}\\leq\\left(\\frac{n}{45\\sqrt{d}\\log n}\\right)^{\\frac{2}{d}}a s}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Omega\\left(\\frac{\\sigma^{2}w_{K Q}^{\\frac{d}{2}}}{n}\\right)\\leq\\mathcal{L}_{n o i s e}(w_{K Q})\\leq\\mathcal{O}\\left(\\frac{\\sigma^{2}\\left(1+w_{K Q}^{\\frac{d}{2}}\\right)}{n}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{noise}}(w_{K Q})=\\sigma^{2}\\operatorname{\\mathbb{E}}\\left[\\sum_{i}{\\frac{e^{-2w_{K Q}\\|\\mathbf{\\epsilon}_{x_{i}}-\\mathbf{x}_{n}\\|^{2}}}{\\left(\\sum_{j}e^{-w_{K Q}\\|\\mathbf{\\epsilon}_{x_{j}}-\\mathbf{x}_{n}\\|^{2}}\\right)^{2}}}\\right]=\\sigma^{2}\\operatorname{\\mathbb{E}}\\left[{\\frac{g_{0}(2w_{K Q})}{g_{0}(w_{K Q})^{2}}}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using Lemma G.5, we have with probability at least $\\textstyle1-{\\frac{1}{n}}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{g_{0}(2w_{K Q})}{g_{0}(w_{K Q})^{2}}\\leq\\frac{\\overline{{c_{n}}}n\\left(\\frac{1}{w_{2K Q}}\\right)^{\\frac{d}{2}}}{\\left(\\underline{{c_{n}}}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}}\\right)^{2}}\\leq\\frac{\\overline{{c_{n}}}}{\\underline{{c_{n}}}^{2}}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and similarly ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{g_{0}(2w_{K Q})}{g_{0}(w_{K Q})^{2}}\\geq\\frac{\\underline{{c_{n}}}n\\left(\\frac{1}{w_{2K Q}}\\right)^{\\frac{d}{2}}}{\\left(\\overline{{c_{n}}}n\\left(\\frac{1}{w_{K Q}}\\right)^{\\frac{d}{2}}\\right)^{2}}\\leq\\frac{\\underline{{c_{n}}}}{\\overline{{c_{n}}}^{2}}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, in the worst case, we have $0\\leq\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})\\leq1$ ", "page_idx": 26}, {"type": "text", "text": "Finally, we show that the noise term is monotonic in $w_{K Q}$ ", "page_idx": 26}, {"type": "text", "text": "Lemma D.2. $\\mathcal{L}_{n o i s e}(w)>\\mathcal{L}_{n o i s e}(w^{\\prime})\\iff w>w^{\\prime}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. Let $a_{i}=e^{-w^{\\prime}\\|x_{i}-x_{n+1}\\|^{2}},b_{i}=e^{-\\left(w-w^{\\prime}\\right)\\|x_{i}-x_{n+1}\\|^{2}}$ The result follows from Lemma L3 because $\\{a_{i}\\}$ and $\\{b_{i}\\}$ satisfy $a_{i}>a_{j}\\iff b_{i}>b_{j}\\iff\\left\\|x_{i}-x_{n+1}\\right\\|<\\left\\|x_{j}-x_{n+1}\\right\\|$ \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E Optimizing the Loss ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For the nonlinear function class $\\mathcal{F}_{L}^{+}$ , we have the following. ", "page_idx": 26}, {"type": "text", "text": "Theorem E.1. Suppose the functions seen in pretraining are drawn from $D(\\mathcal{F}_{L}^{+})$ as in Definition 3.2, the covariates are drawn as Assumption 3.3, $\\begin{array}{r}{n=\\Omega\\left(\\frac{L\\log n}{\\sigma}\\right)^{d}}\\end{array}$ and $n^{\\frac{2}{d+2}}=\\Omega(1)$ ,thentheoptimal Msatisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{M}=w_{K Q}\\mathbf{I}_{d}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $w_{K Q}$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Omega\\left(\\left(n L^{2}\\right)^{\\frac{1}{d+2}}\\right)\\leq w_{K Q}\\leq\\mathcal{O}\\left(\\left(n L^{2}\\right)^{\\frac{2}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "lfxIASyLxB/tmp/c47f9cd82f2d41766d1c9b19b94b3c48c6f6a4703b9549b29b7f806f08093dac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 7: Left: Rough upper and lower bounds for the bias term (shaded region), along with the noise variance (gray). Right: Overall upper and lower bound for the in-context loss. The horizontal dashed line establishes an upper bound for the optimal loss, while the vertical dashed lines establish lower and upper bounds for the parameter $w_{K Q}$ that can attain the optimal loss. ", "page_idx": 27}, {"type": "text", "text": "Proof. We consider three regions in which the optimal value could potentially lie and see that only the third region is viable. ", "page_idx": 27}, {"type": "text", "text": "Case1. $w_{K Q}\\leq d+\\sqrt{d}$ : In this case, the signal term lower bounds the optimal loss by Lemma C.5 as $\\Omega(1)$ ", "page_idx": 27}, {"type": "text", "text": "Case 2. $\\begin{array}{r}{w_{K Q}>\\Omega\\left(\\frac{n}{\\log n}\\right)^{\\frac{2}{d}}}\\end{array}$ . In this case, the noise term lower bounds the optimal loss. From Lemma $w_{K Q}$ soi therange $\\begin{array}{r}{w_{K Q}>\\Omega(\\left(\\frac{n}{\\log n}\\right)^{\\frac{2}{d}}}\\end{array}$ 1S lower boundby $\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})$ $\\begin{array}{r}{w_{K Q}=\\Omega(\\left(\\frac{n}{\\log n}\\right)^{\\frac{2}{d}}}\\end{array}$ .whichis $\\textstyle\\Omega\\left({\\frac{\\sigma^{2}}{\\log n}}\\right)$ ", "page_idx": 27}, {"type": "text", "text": "Case3. d + \u221aa \u2264 wkq \u2264 S2 (1gn) By combining Lemmas C.8, C.9, and D.1 , we obtain the following overall bound on the loss: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underline{{c}}\\left(\\frac{L^{2}}{\\left(w_{K Q}+1\\right)^{2}}+\\frac{\\sigma^{2}w_{K Q}^{\\frac{d}{2}}}{n}\\right)\\le\\mathcal{L}(w_{K Q})\\le\\bar{c}\\left(\\frac{L^{2}}{w_{K Q}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}+\\frac{\\sigma^{2}+L^{2}}{n}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some constants $\\overline{{c}},\\underline{{c}}$ that only depend on $d$ . In the range $w_{K Q}\\,\\geq\\,d+{\\sqrt{d}}$ we have $w_{K Q}>1$ and $w_{K Q}\\le n$ , so the upper bound can be relaxed as $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})\\leq2\\overline{c}\\left(\\frac{L^{2}}{n}+\\frac{\\sigma^{2}w_{K Q}^{\\frac{d}{2}}}{n}\\right).}\\end{array}$ , which is minimized at $\\begin{array}{r}{w_{K Q}=\\left(\\frac{n L^{2}}{\\sigma^{2}d}\\right)^{\\frac{2}{d+2}}}\\end{array}$ . He it is upper bounded by 4 ( aLa-2) . We note frst of all that for large enough $n$ (as long as $\\begin{array}{r}{n=\\Omega\\left(\\frac{\\sigma\\log n}{L}\\right)^{d}}\\end{array}$ and $n^{\\frac{2}{d+2}}=\\Omega(1))$ this is lower than the lower bounds we got in Case 1 and Case 2, so this is indeed the region of global optimal solution. From Lemma C.9 we have $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})\\geq\\frac{L^{2}}{w_{K Q}^{2}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\\geq\\frac{L^{2}}{w_{K Q}^{2}}}\\end{array}$ which gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{L^{2}}{w_{K Q}^{2}}\\leq\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})\\leq4\\overline{c}L^{2}\\left(\\frac{\\sigma^{2}d}{n L^{2}}\\right)^{\\frac{2}{d+2}}}\\\\ {\\implies\\left(\\frac{n L^{2}}{d\\sigma^{2}}\\right)^{\\frac{1}{d+2}}\\sqrt{\\frac{\\underline{c}}{4\\overline{c}}}\\leq w_{K Q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for the upper bound, we similarly also have $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{noise}}(w_{K Q})\\geq\\frac{L^{2}}{w_{K Q}^{2}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\\geq\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}}\\end{array}$ which gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n4\\overline{{c}}\\left(\\frac{d L^{d}\\sigma^{2}}{n}\\right)^{\\frac{2}{d+2}}\\geq\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Longrightarrow w_{K Q}\\leq\\left(\\frac{n L^{2}}{\\sigma^{2}}\\right)^{\\frac{2}{d+2}}\\left(4_{-d}^{\\overline{{c}}}{}{}^{\\!\\frac{2}{d+2}}\\right)^{\\frac{2}{d}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Of course, for this to not be vacuous we need ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\frac{n L^{2}}{\\sigma^{2}}\\right)^{\\frac{2}{d+2}}\\left(4{\\frac{\\overline{{c}}}{\\underline{{c}}}}d^{\\frac{2}{d+2}}\\right)^{\\frac{2}{d}}\\leq\\left({\\frac{1}{45{\\sqrt{d}}}}{\\frac{n}{\\log n}}\\right)^{\\frac{2}{d}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We will again hide constants that depend only on $d$ and write this as ", "page_idx": 28}, {"type": "equation", "text": "$$\nc_{1}\\left({\\frac{n L^{2}}{\\sigma^{2}}}\\right)^{\\frac{2}{d+2}}\\leq c_{2}\\left({\\frac{n}{\\log n}}\\right)^{\\frac{2}{d}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which is true as long as $\\begin{array}{r}{n>\\left(\\frac{L\\log n}{\\sigma}\\right)^{d}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "For the affne function class $\\mathcal{F}_{L}^{\\mathrm{aff}}$ we have the following ", "page_idx": 28}, {"type": "text", "text": "Theorem E.2. If the functions seen in pretraining are drawn from $D(\\mathcal{F}_{L}^{a f\\!\\!f})$ as in Definition 3.2, and the noise variance $\\sigma^{2}$ andLiphscitz.constant $L$ satisfies $\\begin{array}{r}{n\\geq\\left(\\frac{L\\log^{2}n}{\\sigma}\\right)^{d+2}}\\end{array}$ ,and $n^{\\frac{2}{d}}\\geq\\Omega(1)$ ,and the covariates are drawn as Assumption 3.3, the optimal M satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{M}=w_{K Q}\\mathbf{I}_{d}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $w_{K Q}$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Omega\\left(\\left(n L^{2}\\right)^{\\frac{1}{d+4}}\\right)\\leq w_{K Q}\\leq\\mathcal{O}\\left(\\left(n L^{2}\\right)^{\\frac{2\\left(d+2\\right)}{d\\left(d+4\\right)}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Again we work with three cases. ", "page_idx": 28}, {"type": "text", "text": "Case 1. $w_{K Q}\\leq d+\\sqrt{d}$ . Again in this case we have a lower bound to the signal term of $\\Omega(1)$ ", "page_idx": 28}, {"type": "text", "text": "Case 2. $\\begin{array}{r}{w_{K Q}\\geq\\Omega\\left(\\frac{n}{\\log n}\\right)^{\\frac{2}{d}}}\\end{array}$ . Again we have a lower bound of $\\textstyle\\Omega\\left({\\frac{\\sigma^{2}}{\\log n}}\\right)$ ", "page_idx": 28}, {"type": "text", "text": "Case 3. $d+{\\sqrt{d}}\\leq w_{K Q}\\leq\\Omega\\left({\\frac{n}{\\log n}}\\right)^{\\frac{2}{d}}$ Combining Lemmas C.4, C.5, D.1 is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underline{{c}}\\left(\\frac{L^{2}}{\\left(w_{K Q}+1\\right)^{2}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\\right)\\le\\mathcal{L}(w_{K Q})\\le\\bar{c}\\left(\\frac{L^{2}}{w_{K Q}^{2}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}+L^{2}\\frac{w_{K Q}^{\\frac{d}{2}-1}}{n}+\\frac{L^{2}+\\sigma^{2}}{n}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We will minimize the upper bound. First suppose ${\\begin{array}{r}{{\\frac{L^{2}}{\\sigma^{2}}}\\geq w_{K Q}}\\end{array}}$ for the $w_{K Q}$ that minimizes the upper bound. Then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{K Q})\\leq\\overline{{c}}\\left(\\frac{L^{2}}{w_{K Q}^{2}}+\\frac{\\sigma^{2}}{n}+2L^{2}\\frac{w_{K Q}^{\\frac{d}{2}-1}}{n}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This upper bound is minimized at $w_{K Q}\\;=\\;n^{\\frac{2}{d+2}}$ . However, this contradicts the constraint that $\\begin{array}{r}{w_{K Q}\\leq\\frac{L^{2}}{\\sigma^{2}}}\\end{array}$ , when n a2 $\\begin{array}{r}{n^{\\frac{2}{d+2}}\\geq{\\frac{L^{2}}{\\sigma^{2}}}}\\end{array}$ , as we assume. So we have $\\begin{array}{r}{w_{K Q}\\geq\\frac{L^{2}}{\\sigma^{2}}}\\end{array}$ for the minimizer. This means the upper bound is no more than ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{K Q})\\leq\\overline{{c}}\\left(\\frac{L^{2}}{w_{K Q}^{2}}+\\sigma^{2}\\frac{2w_{K Q}^{\\frac{d}{2}}}{n}+\\frac{\\sigma^{2}+L^{2}}{n}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This upper bound is minimized at $\\begin{array}{r}{w_{K Q}=\\left(\\frac{n L^{2}}{\\sigma^{2}d}\\right)^{\\frac{2}{d+4}}}\\end{array}$ where it is upper bounded by ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{noise}}(w_{K Q})\\leq4L^{2}{\\overline{{c}}}\\left({\\frac{\\sigma^{2}d}{n L^{2}}}\\right)^{\\frac{2}{d+4}}+{\\frac{L^{2}}{n}}\\leq5L^{2}{\\overline{{c}}}\\left({\\frac{\\sigma^{2}d}{n L^{2}}}\\right)^{\\frac{2}{d+4}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "whenever $\\begin{array}{r}{n\\geq\\frac{L^{2}}{\\sigma^{2}}}\\end{array}$ . We see that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(w_{K Q})\\geq c\\left(\\frac{L^{2}}{w_{K Q}^{2}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{2}{2}}}{n}\\right)\\geq c\\frac{L^{2}}{w_{K Q}^{2}}}\\\\ &{\\qquad\\implies c\\frac{L^{2}}{w_{K Q}^{2}}\\leq5L^{2}\\bar{c}\\left(\\frac{\\sigma^{2}d}{n L^{2}}\\right)^{\\frac{2}{d+4}}}\\\\ &{\\qquad\\implies\\left(\\frac{n L^{2}}{\\sigma^{2}}\\right)^{\\frac{1}{d+4}}\\sqrt{\\frac{c}{5\\bar{c}}}\\left(\\frac{1}{d}\\right)^{\\frac{1}{d+4}}\\leq w_{K Q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for the upper bound, we similarly also have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}(w_{K Q})\\geq\\!\\underline{{c}}\\left(\\frac{L^{2}}{w_{K Q}^{2}}+\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\\right)\\geq\\underline{{c}}\\sigma^{2}\\frac{w_{K Q}^{\\frac{d}{2}}}{n}}\\\\ &{}&{\\Longrightarrow w_{K Q}\\leq\\left(\\frac{n L^{2}}{\\sigma^{2}}\\right)^{\\frac{2(d+2)}{d(d+4)}}\\left(\\underset{\\mathcal{L}}{\\overleftarrow{c}}d^{\\frac{2}{d+4}}\\right)^{\\frac{2}{d}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Of course, for this to not be vacuous we need ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(\\frac{n L^{2}}{\\sigma^{2}}\\right)^{\\frac{2(d+2)}{d(d+4)}}\\left(5_{-}^{\\overline{{C}}}d^{\\frac{2}{d+4}}\\right)^{\\frac{2}{d}}\\leq\\left(\\frac{1}{45\\sqrt{d}}\\frac{n}{\\log n}\\right)^{\\frac{2}{d}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We will again hide constants that depend only on $d$ and write this as ", "page_idx": 29}, {"type": "equation", "text": "$$\nc_{1}\\left(\\frac{n L^{2}}{\\sigma^{2}}\\right)^{\\frac{2(d+2)}{d(d+4)}}\\leq c_{2}\\left(\\frac{n}{\\log n}\\right)^{\\frac{2}{d}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which again is true as long as $\\begin{array}{r}{n=\\Omega\\left(\\frac{L\\log^{2}n}{\\sigma}\\right)^{d+2}}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "E.1  Generalization Bounds ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We conclude this section with a proof of the generalization error on a new $L$ -Lipschitz task. ", "page_idx": 29}, {"type": "text", "text": "Theorem E.3. Suppose our attention is first pretrained on tasks drawn from $D(\\mathcal{F}_{L}^{+})$ and then tested on an arbitrary $L$ iph $\\begin{array}{r}{\\mathcal{L}\\leq\\mathcal{O}\\left(\\frac{L^{2}}{\\Lambda^{\\beta}}\\right)}\\end{array}$ Furthermore, if the new task is instead drawn from $D(\\mathcal{F}_{L^{\\prime}}^{+})$ , the loss is lower bounded as $\\mathcal{L}\\mathrm{~\\geq~}$ $\\operatorname*{min}\\{\\Omega(\\frac{L^{\\prime2}}{\\Lambda^{2\\beta}}),\\Omega(\\frac{\\Lambda^{\\beta d/2}}{n})\\}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. We know from Theorem E.2 that $\\Omega(\\Lambda^{\\beta})\\le w_{K Q}\\le\\mathcal{O}(\\Lambda^{2\\beta})$ . The upper bound for $\\mathcal{L}(w_{K Q})$ \uff0c Which is $\\mathcal{O}\\big(\\frac{L^{2}}{w_{K Q}}+\\frac{w_{K Q}^{\\frac{d}{2}}}{n}\\big)$ , is a convex function for $d\\geq2$ so in any range it atains is maximum vale $\\begin{array}{r l}{\\mathcal{O}(\\operatorname*{max}\\{\\frac{L^{2}}{\\Lambda^{\\beta}}+\\frac{\\Lambda^{d\\beta/2}}{n},\\frac{L^{2}}{\\Lambda^{2\\beta}}+\\frac{\\Lambda^{d\\beta}}{n}\\})=}&{{}}\\end{array}$ $\\begin{array}{r}{\\mathcal{O}(\\frac{L^{2}}{\\Lambda^{\\beta}}+\\frac{\\Lambda^{d\\beta/2}}{n}+\\frac{L^{2}}{\\Lambda^{2\\beta}}+\\frac{\\Lambda^{d\\beta}}{n})=\\mathcal{O}(\\frac{L^{2}}{\\Lambda^{\\beta}})}\\end{array}$ for large enough $n$ Now consider testing on a new task from $D(F_{L^{\\prime}}^{+})$ . The ICL loss for $\\Omega\\left(\\Lambda^{\\beta}\\right)\\leq w_{K Q}\\leq\\mathcal{O}\\left(\\Lambda^{2\\beta}\\right)$ is bounded below as $\\Omega\\big(\\frac{L^{\\prime2}}{\\Lambda^{2\\beta}}\\big)$ and $\\Omega\\big(\\frac{\\Lambda^{\\beta d/2}}{n}\\big)$ \u53e3 ", "page_idx": 29}, {"type": "text", "text": "The implication of this is that if $L^{\\prime}\\gg L$ , the error scales as $\\left(L^{\\prime}\\right)^{2}$ rather than $(L^{\\prime})^{\\frac{2d}{d+2}}$ while for $L^{\\prime}\\ll\\bar{L}$ , the error is lower bounded by a constant. ", "page_idx": 29}, {"type": "text", "text": "F  Lower Bound for Linear Attention ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section we prove Theorem 3.6. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.1. Consider the function distributions $D(\\mathcal{F}_{L})$ and $D(\\mathcal{F}_{L}^{+})$ described in Definition 3.2. Wehave $\\mathcal{L}_{L A}\\geq\\Omega(L^{2})$ , that is, the ICL error is lower bounded as $\\Omega(L^{2})$ ", "page_idx": 30}, {"type": "text", "text": "Proof. We start by decomposing the ICL loss into a bias dependent term and a cenetered term. For $f\\in\\mathcal{F}_{L}\\in\\{\\mathcal{F}_{L}^{\\mathrm{aff}},\\mathcal{F}_{L}^{+}\\}$ let $\\overline{{f}}$ denote the centered function $f-\\mathbb{E}_{\\mathbf{x}}\\,f$ .Let $f^{\\prime}$ denote the fip of $f$ about its expected value, so $f^{\\prime}=\\mathbb{E}_{x}\\,f-{\\overline{{f}}}$ . We observe that $\\overline{{f}}$ is independent of $\\mathbb{E}_{x}\\,f$ . For linear attention, we have, for $f\\sim D(\\mathcal{F}_{L})$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{\\ensuremath{\\beta}}}}&{=\\mathrm{\\ensuremath{\\mathbb{E}}}_{f(\\varepsilon),\\star(\\varepsilon)_{+}(\\varepsilon)_{+}}\\left[\\left(B_{\\varepsilon,\\lambda}(\\alpha_{n+1})-f(\\alpha_{n+1})\\right)^{2}\\right]}\\\\ &{=\\mathrm{\\ensuremath{\\mathbb{E}}}_{f(\\varepsilon)_{+},\\varepsilon_{+}(\\varepsilon_{+})_{+}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\left((f(\\alpha_{\\varepsilon})+\\varepsilon_{i})x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}\\right)-f(\\alpha_{n+1})\\right)^{2}\\right]}\\\\ &{=\\mathrm{\\ensuremath{\\mathbb{E}}}_{f(\\varepsilon)_{+},\\varepsilon_{+}(\\varepsilon_{+})_{+}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\left(\\widehat{f}(\\alpha_{\\varepsilon})x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}+\\varepsilon_{i}x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}+\\mathrm{\\ensuremath{\\mathbb{E}}}_{\\varepsilon}f x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}\\right)-f(\\alpha_{n+1})\\right)^{2}\\right]}\\\\ &{=\\mathrm{\\ensuremath{\\mathbb{E}}}_{f(\\varepsilon)_{+},\\varepsilon_{+}(\\varepsilon_{+})_{+}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\widehat{f}(\\alpha_{\\varepsilon})x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}+\\varepsilon_{i}x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}\\right)-f(\\alpha_{n+1})\\right)^{2}\\right]\\qquad\\mathrm{\\ensuremath{\\mathbb{I}}}(\\mathrm{\\ensuremath{\\mathbb{B}}})}\\\\ &{\\ \\ \\ \\ \\ +\\mathrm{\\ensuremath{\\mathbb{E}}}_{f(\\varepsilon)_{+},\\varepsilon_{+}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\left(\\widehat{f}(\\alpha_{\\varepsilon})x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}\\right)^{2}\\right)^{2}\\right]}\\\\ &{\\ge\\mathrm{\\ensuremath{\\mathbb{E}}}_{f(\\varepsilon_{+})_{+},\\varepsilon_{+}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\widehat{f}(\\alpha_{\\varepsilon})x_{i}^{\\top}\\mathbf{M}\\alpha_{n+1}+\\varepsilon_{\\varepsilon}x_{i}^{\\top}\\mathbf{M}\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By symmetry, this is also equal to the same expression using $f^{\\prime}$ instead of $f$ , since $f$ and $f^{\\prime}$ are distributed identically. Besides, $\\mathbb{E}_{x}\\,f=\\mathbb{E}_{x}\\,f^{\\prime}$ and $\\epsilon$ is symmetric about the origin, so ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\ensuremath{\\mathbbE}}_{\\mathrm{LA}}(\\mathbf{M})\\geq{\\ensuremath{\\mathbb E}}_{f,\\{x_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\left(f^{\\prime}(x_{i})x_{i}^{\\top}\\,\\mathbf{M}\\,x_{n+1}+\\epsilon_{i}x_{i}^{\\top}\\,\\mathbf{M}\\,x_{n+1}\\right)-f^{\\prime}(x_{n+1})-{\\ensuremath{\\mathbb E}}_{x}\\,f^{\\prime}\\right)^{2}\\right]}\\\\ &{\\qquad={\\ensuremath{\\mathbb E}}_{f,\\{x_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\left(f^{\\prime}(x_{i})x_{i}^{\\top}\\,\\mathbf{M}\\,x_{n+1}-\\epsilon_{i}x_{i}^{\\top}\\,\\mathbf{M}\\,x_{n+1}\\right)-f^{\\prime}(x_{n+1})-{\\ensuremath{\\mathbb E}}_{x}\\,f\\right)^{2}\\right]}\\\\ &{\\qquad={\\ensuremath{\\mathbb E}}_{f,\\{x_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left[\\left(-\\left(\\displaystyle\\sum_{i=1}^{n}\\left(\\overline{{f}}(x_{i})x_{i}^{\\top}\\,\\mathbf{M}\\,x_{n+1}+\\epsilon_{i}x_{i}^{\\top}\\,\\mathbf{M}\\,x_{n+1}\\right)-\\overline{{f}}(x_{n+1})\\right)-{\\ensuremath{\\mathbb E}}_{x}\\,f\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\begin{array}{r}{A=\\sum_{i=1}^{n}\\left(\\overline{{f}}({x}_{i}){x}_{i}^{\\top}\\operatorname{M}{x}_{n+1}+{\\epsilon}_{i}{x}_{i}^{\\top}\\operatorname{M}{x}_{n+1}\\right)-\\overline{{f}}({x}_{n+1})}\\end{array}$ and $B=\\mathbb{E}_{x}\\,f$ . Then we see that $\\begin{array}{r}{\\dot{\\b\\Sigma}_{L A}({\\bf M})\\geq\\frac{1}{2}\\,\\mathbb{E}(A+B)^{2}+\\frac{1}{2}\\,\\mathbb{E}(-A+B)^{2}=\\mathbb{E}\\,A^{2}+\\mathbb{E}\\,B^{2}.}\\end{array}$ Meanwhile, $\\mathbb{E}\\left(\\mathbb{E}_{x}\\,f\\right)^{2}$ is just the variance o the signal term in $D(\\mathcal{F}_{L}^{\\mathrm{aff}})$ $D(\\mathcal{F}_{L}^{+})$ , Which is $\\frac{L^{2}}{3}$ So $\\begin{array}{r}{\\mathcal{L}_{L A}(\\mathbf{M})\\geq\\frac{L^{2}}{3}}\\end{array}$ \u53e3 ", "page_idx": 30}, {"type": "text", "text": "G Bounds for $g_{p}(r)$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The purpose of this section is to obtain upper and lower bounds on ", "page_idx": 30}, {"type": "equation", "text": "$$\ng_{p}(r)=\\sum_{i=1}^{n}\\|\\,\\pmb{x}_{i}-\\pmb{x}\\,\\|^{p}e^{-r\\|\\,\\pmb{x}_{i}^{\\top}-\\pmb{x}\\,\\|^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $p=0,1/2,1$ . For this, we will need high probability upper and lower bounds on the number of points in a spherical cap under a uniform distribution over the hypersphere. Consider $n$ points $\\{x_{i}\\}$ drawn uniformly from $\\sigma_{d-1}$ , the uniform measure over $S_{d-1}$ , the $d-$ dimensional hypersphere. The measure of the $\\epsilon-$ spherical cap around $x\\in S_{d-1}$ \uff0c $C(\\epsilon,x)=\\{x^{\\prime}:x^{\\prime\\top}\\,x>1-\\dot{\\epsilon}\\}$ is denoted by $\\sigma_{\\epsilon}$ ", "page_idx": 31}, {"type": "text", "text": "G.1  Bounds on Spherical Caps ", "text_level": 1, "page_idx": 31}, {"type": "image", "img_path": "lfxIASyLxB/tmp/a6ae41682fc7d683765bc7ac1ffce686a1b7354be24727f191d734b14e6a3dd9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 8: The surface area of the purple hemisphere is used to upper bound the surface area of $C(\\frac{i}{r})$ while the volume of the green hypersphere is used as a lower bound. Points in the orange region are $S_{i+1}\\setminus S_{i}$ , and their count is $N_{i+1}-N_{i}$ ", "page_idx": 31}, {"type": "text", "text": "Lemma G.1. The area of the spherical cap $C(\\epsilon)$ $\\sigma_{\\epsilon}$ isbounded as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{(2\\epsilon-\\epsilon^{2})^{\\frac{d-1}{2}}}{\\sqrt{2d\\pi}}\\le\\sigma_{\\epsilon}\\le(2\\epsilon-\\epsilon^{2})^{\\frac{d}{2}}\\le(2\\epsilon)^{\\frac{d-1}{2}}\\,e^{-\\epsilon d/4}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We derive a lower bound as follows. We replace the surface area of a spherical cap in $S_{d-1}$ with a $d-1$ dimensional ball of the same boundary. Let $V_{d}$ denote the volume of a $d$ dimensional ball (that is, $\\begin{array}{r}{V_{3}(r)\\,=\\,\\frac{4}{3}\\pi r^{3},}\\end{array}$ ), and let $A_{d}$ denote the surface area of a $d$ dimensional sphere (so $A_{3}(a)=4\\pi r^{2};$ 0. It is known that ", "page_idx": 31}, {"type": "equation", "text": "$$\nV_{d}(r)={\\frac{\\pi^{\\frac{d}{2}}}{\\Gamma({\\frac{d}{2}}+1)}}r^{d},{\\mathrm{~and~}}A_{d}(r)={\\frac{2\\pi^{\\frac{d}{2}}}{\\Gamma({\\frac{d}{2}})}}r^{d-1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{\\epsilon}\\geq\\frac{V_{d-1}\\left((1-(1-\\epsilon)^{2})^{\\frac{1}{2}}\\right)}{A_{d}(1)}}\\\\ &{\\quad=\\frac{(1-(1-\\epsilon)^{2})^{\\frac{d-1}{2}}}{2\\sqrt{\\pi}}\\frac{\\Gamma\\left(\\frac{d}{2}\\right)}{\\Gamma\\left(\\frac{d+1}{2}\\right)}}\\\\ &{\\quad\\geq\\frac{(1-(1-\\epsilon)^{2})^{\\frac{d}{2}}}{\\sqrt{d\\pi}}}\\\\ &{\\quad=\\frac{(2\\epsilon-\\epsilon^{2})^{\\frac{d-1}{2}}}{\\sqrt{2d\\pi}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma G.6 ", "page_idx": 31}, {"type": "text", "text": "The upper bound is similar. This time we replace the cap with the surface of a hemisphere with the same boundary.We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sigma_{\\epsilon}\\leq\\frac{A_{d}\\left((1-(1-\\epsilon)^{2})^{\\frac{1}{2}}\\right)}{2A_{d}(1)}=\\frac{(1-(1-\\epsilon)^{2})^{\\frac{d-1}{2}}}{2}\\leq(2\\epsilon-\\epsilon^{2})^{\\frac{d-1}{2}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We will also need upper and lower bounds on a discretized version of the incomplete gamma function. Definition G.2. Denote by $\\gamma(d,\\alpha,m)$ the expression $\\begin{array}{r}{\\gamma(d,\\alpha,m)=\\sum_{i=1}^{m}i^{d}e^{-\\alpha i}}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "We have the following ", "page_idx": 32}, {"type": "text", "text": "Lemma G.3. For $d>5,1\\leq\\alpha\\leq2$ , the incomplete Gamma function is bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\int_{m^{d}}\\!\\!e^{-\\alpha m-1/2}\\leq\\gamma(d,\\alpha,m)\\leq m^{d+1}e^{-\\alpha m-1/2}}&{m<d+\\sqrt{d}}\\\\ &{\\left\\{\\frac{\\Gamma(d+1)}{2\\alpha^{d+1}}\\leq\\gamma(d,\\alpha,m)\\leq\\frac{2\\Gamma(d+1)}{\\alpha^{d+1}}\\right.}&{m\\geq d+\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We compare with the Gamma function ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Gamma(d+1)=\\int_{0}^{\\infty}t^{d}e^{-t}d t.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\int_{0}^{\\infty}t^{d}e^{-\\alpha t}d t\\:=\\:\\frac{1}{\\alpha^{d+1}}\\int_{0}^{\\infty}t^{d}e^{-t}d t\\:=\\:\\frac{1}{\\alpha^{d+1}}\\Gamma(d+1)}\\end{array}$ . Because the function $t^{d}e^{-\\alpha t}$ is uni-modal with maximum $\\left(\\frac{d}{\\alpha e}\\right)^{d}$ , we have from Lemma I.1 ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}i^{d}e^{-\\alpha i}+\\left({\\frac{d}{\\alpha e}}\\right)^{d}+\\sum_{i=m}^{\\infty}i^{d}e^{-\\alpha i}\\geq\\int_{0}^{\\infty}t^{d}e^{-\\alpha t}d t={\\frac{1}{\\alpha^{d+1}}}\\Gamma(d+1)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now suppose $\\begin{array}{r}{m\\geq\\frac{d+\\sqrt{d}}{\\alpha}}\\end{array}$ . Then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{m=0}^{\\infty}i^{q}e^{-\\alpha i}\\leq\\displaystyle\\sum_{i=0}^{\\infty}i^{q}e^{-\\alpha i}}}\\\\ &{=\\displaystyle\\sum_{i=\\frac{\\alpha+i}{2}\\alpha}^{\\infty}\\left(\\frac{d+\\sqrt{d}}{\\alpha}\\right)^{i}e^{-(d+\\sqrt{d})}\\displaystyle\\prod_{j=0}^{i-\\frac{i-\\alpha+2i}{2}}\\left[\\frac{1}{e^{\\alpha}}\\left(\\frac{\\frac{d+\\sqrt{d}}{\\alpha}+j+1}{\\frac{d+\\sqrt{d}}{\\alpha}+j}\\right)^{i}\\right]}\\\\ &{\\leq\\displaystyle\\sum_{i=\\frac{\\alpha+2i}{2}}^{\\infty}\\left(\\frac{d+\\sqrt{d}}{\\alpha}\\right)^{i}e^{-(d+\\sqrt{d})}\\displaystyle\\prod_{j=0}^{i-\\frac{i-\\alpha+2i}{2}}\\left[\\frac{1}{e^{\\alpha}}\\left(\\frac{\\frac{d+\\sqrt{d}}{\\alpha}+1}{\\frac{d+\\sqrt{d}}{\\alpha}}\\right)^{q}\\right]}\\\\ &{\\leq\\displaystyle\\sum_{i=\\frac{\\alpha+2i}{2}}^{\\infty}\\left(\\frac{d+\\sqrt{d}}{\\alpha}\\right)^{i}e^{-(d+\\sqrt{2})}\\left(e^{-\\frac{d+\\sqrt{d}}{\\alpha+2}}\\right)^{i-\\frac{\\alpha+2i}{2}}}\\\\ &{=\\left(\\frac{d+\\sqrt{d}}{\\alpha}\\right)^{i}e^{-(d+\\sqrt{d})}\\frac{1}{1-e^{-\\alpha\\sqrt{d}/(d+\\sqrt{d})}}\\leq\\left(\\frac{d}{\\alpha e}\\right)^{i}\\frac{2\\sqrt{d}}{\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "the frst equalit folows eae $\\textstyle{\\frac{d+{\\sqrt{d}}}{\\alpha}}\\leq m$ the second folows because $\\begin{array}{r}{\\frac{2d+1}{2d}\\geq\\frac{2d+j+1}{2d+j}}\\end{array}$ ,the last folows bcause $\\begin{array}{r}{\\left(1+\\frac{\\sqrt{d}}{d\\alpha}\\right)^{d}\\leq e^{\\frac{\\sqrt{d}}{\\alpha}}}\\end{array}$ and ${\\frac{1}{1-e^{x-x}}}\\leq2x$ for $x\\leq2$ . Over all, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}i^{d}e^{-i}+\\left(2{\\frac{\\sqrt{d}}{\\alpha}}+1\\right)\\left({\\frac{d}{\\alpha e}}\\right)^{d}\\geq\\int_{0}^{\\infty}t^{d}e^{-t}d t={\\frac{\\Gamma(d+1)}{\\alpha^{d+1}}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "While for the upper bound we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}i^{d}e^{-\\alpha i}-\\left({\\frac{d}{\\alpha e}}\\right)^{d}\\leq\\int_{0}^{\\infty}t^{d}e^{-\\alpha t}d t={\\frac{\\Gamma(d+1)}{\\alpha^{d+1}}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, we use Lemma G.3, specifically that $\\begin{array}{r}{\\left(\\frac{d}{\\alpha e}\\right)^{d}\\,\\leq\\,\\frac{1}{\\alpha^{d+1}}\\sqrt{2\\pi d}\\left(\\frac{d}{e}\\right)^{d}\\,\\leq\\,\\frac{\\Gamma(d+1)}{\\alpha^{d+1}}}\\end{array}$ to yield the desired result. ", "page_idx": 32}, {"type": "text", "text": "For $\\begin{array}{r}{m<\\frac{d+\\sqrt{d}}{\\alpha}}\\end{array}$ , we have from Lemma G.7 that $m^{d}e^{-\\alpha m}\\geq\\frac{1}{\\sqrt{e}}i^{d}e^{-\\alpha i}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}i^{d}e^{-\\alpha i}\\geq m^{d}e^{-\\alpha m-\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}i^{d}e^{-\\alpha i}\\leq m^{d+1}e^{-\\alpha m-\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "G.2  Bounds on $g_{p}(r)$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma G.4. Suppose $\\{x_{i}\\}$ are drawn independently and uniformly from the unit hypersphere. For $\\begin{array}{r}{\\frac{n}{\\log n}\\geq45\\sqrt{d}r^{\\frac{d}{2}},n>5,d>2,p\\leq2,}\\end{array}$ we have $\\begin{array}{r}{g_{p}(r)=\\sum_{i=1}^{n}\\|\\pmb{x}_{i}-\\pmb{x}\\|^{p}e^{-r\\|\\pmb{x}_{i}^{\\top}-\\pmb{x}\\|^{2}}}\\end{array}$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n(1-e^{\\frac{p}{2}-2})\\frac{n2^{\\frac{p}{2}}}{\\sqrt{8e^{4}\\pi d}}\\left(\\frac{1}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\gamma(\\frac{d}{2}+\\frac{p}{2},2,r)\\le g_{p}(r)\\le3n\\left(\\frac{2}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\gamma(\\frac{d}{2}+\\frac{p}{2},2,r)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Wwith probablity at least $\\textstyle1-{\\frac{1}{2n}}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. For $0~\\le~i~\\le~r$ let $N_{i}$ denote the number, and $S_{i}$ denote the set, of points satisfying $\\begin{array}{r}{1-\\frac{i}{r}\\leq\\pmb{x}_{i}^{\\top}\\,\\pmb{x}\\iff\\|\\,\\pmb{x}_{i}-\\pmb{x}\\,\\|\\leq\\left(\\frac{2i}{r}\\right)^{\\frac{1}{2}}}\\end{array}$ . Also denote by $N_{-1}$ the points satisfying $x_{i}^{\\top}\\,x<0$ , and let $S_{-1}$ denote this set. Note that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g_{p}(\\boldsymbol{r})=\\sum_{i=0}^{n}\\|\\,\\mathbf{x}_{i}^{\\top}-\\pmb{x}\\,\\|^{p}e^{-r\\|\\,\\mathbf{x}_{i}^{\\top}-\\pmb{x}\\|^{2}}}\\\\ {\\displaystyle\\qquad=\\sum_{i=0}^{r-1}\\sum_{j\\in S_{i+1}\\setminus S_{i}}\\|\\,\\pmb{x}_{i}^{\\top}-\\pmb{x}\\,\\|^{p}e^{-r\\|\\,\\mathbf{x}_{j}^{\\top}-\\pmb{x}\\|^{2}}+\\sum_{j\\in S_{-1}}\\|\\,\\pmb{x}_{i}^{\\top}-\\pmb{x}\\,\\|^{p}e^{-r\\|\\,\\mathbf{x}_{j}^{\\top}-\\pmb{x}\\|^{2}}}\\\\ {\\displaystyle\\qquad\\leq\\sum_{i=0}^{r-1}\\left(\\frac{2(i+1)}{r}\\right)^{\\frac{p}{2}}e^{-2i}\\left(N_{i+1}-N_{i}\\right)+2^{p}e^{-2r}N_{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Similarly, ", "page_idx": 33}, {"type": "equation", "text": "$$\nh(r)\\geq\\sum_{i=0}^{r-1}\\left({\\frac{2i}{r}}\\right)^{\\frac{p}{2}}e^{-2(i+1)}\\left(N_{i+1}-N_{i}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that because $N_{i}>0$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{r-1}\\left({\\frac{2(i+1)}{r}}\\right)^{\\frac{p}{2}}N_{i+1}e^{-2i}\\geq\\sum_{i=0}^{r-1}\\left({\\frac{2(i+1)}{r}}\\right)^{\\frac{p}{2}}\\left(N_{i+1}-N_{i}\\right)e^{-2i}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "And similarly, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\because i=0\\implies{\\frac{2i}{r}}=0\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=0}^{r-1}\\left(\\frac{2i}{r}\\right)^{\\frac{p}{2}}N_{i+1}e^{-2i}=\\sum_{i=1}^{r-1}\\left(\\frac{2i}{r}\\right)^{\\frac{p}{2}}\\sum_{j=0}^{i}\\left(N_{j+1}-N_{j}\\right)e^{-2i}}}\\\\ &{}&{=\\sum_{j=1}^{r-1}\\left(N_{j+1}-N_{j}\\right)\\sum_{i=j}^{r-1}\\left(\\frac{2i}{r}\\right)^{\\frac{p}{2}}e^{-2i}}\\\\ &{}&{\\leq\\sum_{j=1}^{r-1}\\left(N_{j+1}-N_{j}\\right)\\sum_{i=j}^{\\infty}\\left(\\frac{2i}{r}\\right)^{\\frac{p}{2}}e^{-2i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\sum_{j=1}^{r-1}(N_{j+1}-N_{j})\\displaystyle\\sum_{i=j}^{\\infty}\\left(\\frac{2j}{r}\\right)^{\\frac{r}{2}}e^{-2j}\\left(\\frac{\\left(j+\\frac{1}{j}\\right)^{\\frac{r}{2}}}{e^{2}}\\right)^{i-j}}&{\\because i<j\\left(\\frac{j+1}{j}\\right)^{i-j}}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{r-1}(N_{j+1}-N_{j})\\displaystyle\\sum_{i=j}^{\\infty}\\left(\\frac{2j}{r}\\right)^{\\frac{r}{2}}e^{-2j}\\left(e^{\\frac{p}{2}-2}\\right)^{i-j}}&{\\qquad\\because\\;1+x\\le e^{x}}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{r-1}(N_{j+1}-N_{j})\\displaystyle\\sum_{i=j}^{\\infty}\\left(\\frac{2j}{r}\\right)^{\\frac{r}{2}}e^{-2j}\\left(e^{\\frac{p}{2}-2}\\right)^{i-j}}&{\\qquad\\because\\;j\\ge1}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{r-1}(N_{j+1}-N_{j})\\left(\\frac{2j}{r}\\right)^{\\frac{r}{2}}e^{-2j}\\displaystyle\\frac{1}{1-e^{\\frac{p}{2}-2}}}&{\\qquad\\cdot\\;p<4}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and so ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(1-e^{\\frac p2-2}\\right)\\sum_{i=0}^{r-1}\\left(\\frac{2i}r\\right)^{\\frac p2}N_{i+1}e^{-2(i+1)}\\le\\sum_{j=0}^{r-1}\\left(N_{j+1}-N_{j}\\right)\\left(\\frac{2i}r\\right)^{\\frac p2}e^{-2(i+1)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By a Chermoff bound for Binomial random variables, we have with probability $\\textstyle1-{\\frac{r}{n^{2}}}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{i}=n\\sigma_{\\frac{i}{r}}\\le n\\sigma_{\\frac{i}{r}}+\\sqrt{6n\\log n\\sigma_{\\frac{i}{r}}}\\le2n\\sigma_{\\frac{i}{r}}\\,\\forall r\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{i}=n\\sigma_{\\frac{i}{r}}\\geq n\\sigma_{\\frac{i}{r}}-\\sqrt{4n\\log n\\sigma_{\\frac{i}{r}}}\\leq\\frac{1}{2}n\\sigma_{\\frac{i}{r}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Whenever ", "page_idx": 34}, {"type": "equation", "text": "$$\nn\\sigma_{\\frac{i}{r}}\\geq16\\log n\\,{\\forall i\\leftarrow{\\frac{1}{\\sqrt{2\\pi d}}}\\left({\\frac{1}{r}}\\right)^{\\frac{d}{2}}}\\geq{\\frac{16\\log n}{n}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{-1}\\leq n\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Over all we have with probability $\\textstyle1-{\\frac{r}{n^{2}}}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(r)\\le\\displaystyle\\sum_{i=0}^{r-1}\\left(\\frac{2(i+1)}{r}\\right)^{\\frac{r}{2}}N_{i+1}e^{-2i}+2^{p}N_{-1}e^{-2r}}\\\\ &{\\qquad\\le n\\displaystyle\\sum_{i=0}^{r-1}2e^{-2i}\\left(\\frac{2(i+1)}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}+2^{p}e^{-2r}n}\\\\ &{\\qquad=2n e^{2}\\left(\\frac{2}{r}\\right)^{\\frac{q}{2}+\\frac{p}{2}}\\displaystyle\\sum_{i=1}^{r}i^{\\frac{d}{2}+\\frac{p}{2}}e^{-2i}+2^{p}e^{-2r}n}\\\\ &{\\qquad=2n e^{2}\\left(\\frac{2}{r}\\right)^{\\frac{q}{2}+\\frac{p}{2}}\\gamma(\\frac{d}{2}+\\frac{p}{2},2,r)+2^{p}e^{-2r}n}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We always have for $p\\leq2$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{2n e^{2}\\left(\\displaystyle\\frac{2}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\gamma(\\displaystyle\\frac{d}{2}+\\displaystyle\\frac{p}{2},2,r)\\geq2^{p}e^{-2r}n}}\\\\ {{\\leftarrow\\left(\\displaystyle\\frac{d}{2}\\right)^{\\frac{d}{2}}e^{2r}2^{-p}\\geq r^{\\frac{d+p}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "So at last, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{p}(r)\\leq16n\\left({\\frac{2}{r}}\\right)^{\\frac{d}{2}+{\\frac{p}{2}}}\\gamma({\\frac{d}{2}}+{\\frac{p}{2}},2,r)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We obtain a lower bound in the same way. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h(r)\\geq\\displaystyle\\left(1-e^{\\frac{p}{2}-2}\\right)\\sum_{i=0}^{r-1}{\\left(\\frac{2i}{r}\\right)^{\\frac{p}{2}}}e^{-2(i+1)}\\frac{n}{2\\sqrt{2\\pi d}}\\left(\\frac{i}{r}\\right)^{\\frac{d}{2}}}}\\\\ {{\\geq\\displaystyle\\left(1-e^{\\frac{p}{2}-2}\\right)\\frac{n2^{\\frac{p}{2}}}{\\sqrt{8e^{4}\\pi d}}\\left(\\frac{1}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\sum_{i=0}^{r-1}e^{-2i}i^{\\frac{d}{2}+\\frac{p}{2}}}}\\\\ {{\\geq\\displaystyle\\left(1-e^{\\frac{p}{2}-2}\\right)\\frac{n2^{\\frac{p}{2}}}{\\sqrt{8e^{4}\\pi d}}\\left(\\frac{1}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\gamma(\\frac{d}{2}+\\frac{p}{2},2,r)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n(1-e^{\\frac{p}{2}-2})\\frac{n2^{\\frac{p}{2}}}{\\sqrt{8e^{4}\\pi d}}\\left(\\frac{1}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\gamma(\\frac{d}{2}+\\frac{p}{2},2,r)\\le h(r)\\le3n\\left(\\frac{2}{r}\\right)^{\\frac{d}{2}+\\frac{p}{2}}\\gamma(\\frac{d}{2}+\\frac{p}{2},2,r)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with probability $\\begin{array}{r}{1-\\frac{r}{n^{2}}\\geq1-\\frac{1}{2n}}\\end{array}$ when $\\textstyle{\\frac{n}{\\log n}}\\geq45{\\sqrt{d}}r^{\\frac{d}{2}}$ ", "page_idx": 35}, {"type": "text", "text": "It will be useful to simplify this bound in regimes that we are interested in ", "page_idx": 35}, {"type": "text", "text": "Corollary G.5. Suppose $\\{x_{i}\\}$ are drawn independently and uniformly from the unit hypersphere. For $\\textstyle{\\frac{n}{\\log n}}\\geq45{\\sqrt{d}}r^{\\frac{d}{2}},n>5$ \uff0c $p\\leq2\\leq d;$ we have $\\begin{array}{r}{g_{p}(r)=\\sum_{i=1}^{n}\\|\\,\\pmb{x}_{i}-\\pmb{x}\\,\\|^{p}e^{-r\\|\\pmb{x}_{i}^{\\top}-\\pmb{x}\\|^{2}}}\\end{array}$ satisfies with probability $\\textstyle1-{\\frac{1}{2n}}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{g_{p}(r)=\\Theta\\left(\\frac{n}{r^{\\frac{d+p}{2}}}\\right)\\quad r\\geq\\frac{d+\\sqrt{d}}{2}}\\\\ {g_{p}(r)=\\Theta\\left(n e^{-2r}\\right)\\quad r<\\frac{d+\\sqrt{d}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The following bounds are known for the Gamma function. ", "page_idx": 35}, {"type": "text", "text": "Lemma G.6. The Gamma function satisfies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l.\\ \\sqrt{2\\pi d}\\left(\\frac{d}{e}\\right)^{d}\\leq\\Gamma(d+1)\\leq e\\sqrt{2\\pi d}\\left(\\frac{d}{e}\\right)^{d}}\\\\ &{2.\\ \\frac{\\Gamma(x+\\frac{1}{2})}{\\Gamma(x+1)}\\geq\\frac{1}{\\sqrt{x+0.5}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. 1. Please see [64]. 2. Please see [65]. ", "page_idx": 35}, {"type": "text", "text": "Lemma G.7. The following inequality holds: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(1+{\\frac{1}{\\sqrt{d}}}\\right)^{d}e^{-{\\sqrt{d}}}\\geq e^{-{\\frac{1}{2}}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Take the logarithm of both sides, we have that this is equivalent to ", "page_idx": 35}, {"type": "equation", "text": "$$\nd\\log\\left(1+{\\frac{1}{\\sqrt{d}}}\\right)\\geq{\\sqrt{d}}-{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "$\\log(1+x)$ $\\begin{array}{r}{\\log\\!\\left(1+\\frac{1}{\\sqrt{d}}\\right)=\\sum_{i}(-1)^{i+1}\\frac{1}{i\\sqrt{d}^{i}}}\\end{array}$ $d>1$ $i=2$ with just the fist two terms: $\\begin{array}{r}{\\log\\!\\left(1+\\frac{1}{\\sqrt{d}}\\right)\\geq\\frac{1}{\\sqrt{d}}-\\frac{1}{2d}}\\end{array}$ \u53e3 ", "page_idx": 35}, {"type": "text", "text": "H Attention Window Captures Appropriate Directions ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section we prove Theorem 4.4, which entails showing that if the Lipschitzness of the function class is zero in some directions, one-layer self-attention learns to ignore these directions when the function class consists of linear functions. First, we give a brief sketch of the proof. ", "page_idx": 35}, {"type": "text", "text": "H.1 Proof Sketch ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We briefly sketch the proof of Theorem 4.4. WLOG we write $\\mathbf{M}=\\mathbf{B}\\mathbf{F}+\\mathbf{B}_{\\perp}\\mathbf{G}$ where $\\mathbf{F}:=\\mathbf{B}^{\\top}\\mathbf{M}$ and $\\mathbf{G}:=\\bar{\\mathbf{B}}_{\\perp}^{\\top}\\mathbf{M}$ . Lemma H.2 leverages the rotational symmetry of $\\mathcal{F}_{\\mathbf{B}}$ in $\\operatorname{col}(\\mathbf{B})$ to show that the loss is minimized over $(\\mathbf{F},\\mathbf{G})$ at $(\\mathbf{F},\\mathbf{G})=(c\\mathbf{B}^{\\top},c^{\\prime}\\mathbf{B}_{\\bot})$ for some constants $c,c^{\\prime}$ . It remains to show that $\\mathcal{L}(c\\mathbf{B}\\mathbf{B}^{\\top}+c^{\\prime}\\mathbf{B}_{\\bot}\\mathbf{B}_{\\bot}^{\\top})>\\mathcal{L}(c\\mathbf{B}\\mathbf{B}^{\\top})$ whenever $c^{\\prime}$ is nonzero. Intuitively, if the attention estimator incorporates the closeness of $\\mathbf{B}_{\\perp}^{\\top}\\,\\pmb{x}_{i}$ and $\\mathbf{B}_{\\perp}^{\\top}\\,{\\pmb x}_{n+1}$ into its weighting scheme via nonzero $\\mathbf{Q}$ , this may improperly up- or down-weight $f(\\pmb{x}_{i})$ , since projections of $\\pmb{x}_{i}$ onto $\\mathrm{col}(\\mathbf{B}_{\\perp})$ do not carry any information about the closeness of $f(\\pmb{x}_{i})$ and $f({\\pmb x}_{n+1})$ ", "page_idx": 36}, {"type": "text", "text": "Using this intuition, we show that for any fixed $c^{\\prime}$ and $\\{\\mathbf{v}_{i}\\}_{i}$ such that $c^{\\prime}\\mathbf{v}_{i}^{\\top}\\mathbf{v}_{n+1}\\neq\\mathbf{v}_{i^{\\prime}}^{\\top}\\mathbf{Q}\\mathbf{v}_{n+1}$ for some $i,i^{\\prime}$ , the attention estimator improperly up-weights $f(\\pmb{x}_{1})$ , where $1\\,\\in\\,\\arg\\operatorname*{max}_{i}c^{\\prime}\\mathbf{v}_{i}^{\\top}\\mathbf{v}_{n+1}$ WLOG. In particular, the version of the pretraining population loss (ICL) with expectation over a, $\\{{\\mathbf{u}}_{i}\\}_{i}$ and $\\{\\epsilon_{i}\\}_{i}$ is reduced by reducing $c^{\\prime}\\mathbf{v}_{1}^{\\top}\\mathbf{v}_{n+1}$ . The only way to ensure all $\\{c^{\\prime}\\mathbf{v}_{i}^{\\top}\\mathbf{v}_{n+1}\\}_{i}$ are equal for all instances of $\\{{\\bf v}_{i}\\}_{i}$ is to set $c^{\\prime}=0$ , so this $c^{\\prime}$ must be optimal. ", "page_idx": 36}, {"type": "text", "text": "To show that reducing $c^{\\prime}\\mathbf{v}_{1}^{\\top}\\mathbf{v}_{n+1}$ reduces the loss with fixed $\\{\\mathbf{v}_{i}\\}_{i}$ , we define $\\alpha_{i}:=e^{c_{\\mathbf{v}}c^{\\prime}\\mathbf{v}_{i}^{\\top}\\mathbf{v}_{n+1}}$ for all $i\\in[n]$ and show the loss\u2019 partial derivative with respect to $\\alpha_{1}$ is positive, i.e. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\alpha_{1}}\\left(\\tilde{\\mathcal{L}}(c,\\{\\alpha_{i}\\}_{i}):=\\mathbb{E}_{\\mathbf{a},\\{\\mathbf{u}_{i}\\}_{i},\\{\\epsilon_{i}\\}_{i}}\\left[\\left(\\frac{\\sum_{i=1}^{n}(\\mathbf{a}^{\\top}\\mathbf{u}_{i}-\\mathbf{a}^{\\top}\\mathbf{u}_{n+1}+\\epsilon_{i})e^{c c_{\\mathbf{u}}^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\sum_{i=1}^{n}e^{c c_{\\mathbf{u}}^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{n+1}}\\alpha_{i}}\\right)^{2}\\right]\\right)>0.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This requires a careful symmetry-based argument as the expectation over $\\{{\\mathbf{u}}_{i}\\}_{i}$ cannot be evaluated in closed-form. To overcome this, we fix all $\\mathbf{u}_{i}$ but $\\mathbf{u}_{1}$ and one other $\\mathbf{u}_{i^{\\prime}}\\neq\\ensuremath{\\mathbf{u}}_{n+1}$ with $\\alpha_{i^{\\prime}}<\\alpha_{1}$ .We show the expectation over $(\\mathbf{u}_{1},\\mathbf{u}_{i^{\\prime}})$ can be written as an integral over $(\\mathbf{y}_{1},\\mathbf{y}_{2})\\in\\mathbb{S}^{k-1}\\times\\mathbb{S}^{k-1}$ of a sum of the derivatives at each of the four assignments of $(\\mathbf{u}_{1},\\mathbf{u}_{i^{\\prime}})$ to $(\\mathbf{y}_{1},\\mathbf{y}_{2})$ , and show that this sum is always positive. Intuitively, any \\*bad\u201d assignment for which increasing $\\alpha_{1}$ reduces the loss is outweighed by the other assignments, which favor smaller $\\alpha_{1}$ . For example, if ${\\bf y}_{1}={\\bf u}_{n+1}\\neq{\\bf y}_{2}$ , and $\\mathbf{u}_{1}=\\mathbf{y}_{1}$ and $\\mathbf{u}_{i^{\\prime}}=\\mathbf{y}_{2}$ , we observe from (21) that increasing $\\alpha_{1}$ can reduce the loss. However, the cumulative increase in the loss on the other three assignments due to increasing $\\alpha_{1}$ is always greater. ", "page_idx": 36}, {"type": "text", "text": "H.2 Full Proof ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We now prove Theorem 4.4 in full detail. ", "page_idx": 36}, {"type": "text", "text": "Lemma H.1. For any $\\mathbf{u}\\in\\mathbb{S}^{k-1}$ and $\\alpha_{1},\\ldots,\\alpha_{n}$ such that $\\operatorname*{min}_{i}\\alpha_{i}>0$ and any $c_{a},c_{u}\\in\\mathbb{R}\\setminus\\{0\\}$ define ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(c):=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{u})e^{c_{u}^{2}c\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c_{u}^{2}c\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{i}\\alpha_{j}}{(\\sum_{i=1}^{n}e^{c_{u}^{2}c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right]}\\\\ &{\\qquad+\\sigma^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c_{u}^{2}c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}^{2}}{(\\sum_{i=1}^{n}e^{c_{u}^{2}c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then for any $\\delta>0,\\,0\\not\\in\\arg\\operatorname*{min}_{0\\leq c\\leq\\delta}J(c)$ ", "page_idx": 36}, {"type": "text", "text": "Proof. We show that there exists some arbitrarily small $\\epsilon>0$ such that $J(\\epsilon)<J(0)$ by showing Ic=o < 0. We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=2c_{u}^{4}\\mathbb{E}_{\\{{\\mathbf{u}}_{i}\\}_{i\\in[n]}}\\left[\\displaystyle\\sum_{i=1}^{n}\\sum_{i^{\\prime}=1}^{n}\\sum_{i^{\\prime\\prime}=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{i^{\\prime}}-\\mathbf{u})(\\mathbf{u}_{i}^{\\top}\\mathbf{u}-\\mathbf{u}_{i^{\\prime\\prime}}^{\\top}\\mathbf{u})\\frac{e^{c_{u}^{2}c(\\mathbf{u}_{i}+\\mathbf{u}_{i^{\\prime}}+\\mathbf{u}_{i^{\\prime\\prime}})^{\\top}u}\\alpha_{i}\\alpha_{i^{\\prime}}\\alpha_{i^{\\prime\\prime}}}{(\\sum_{i=1}^{n}e^{c_{u}^{2}c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{3}}\\right]}\\\\ &{\\phantom{=}+2\\sigma^{2}c_{u}^{2}\\mathbb{E}_{\\{{\\mathbf{u}}_{i}\\}_{i\\in[n]}}\\left[\\displaystyle\\sum_{i=1}^{n}\\sum_{i^{\\prime}=1}^{n}\\sum_{i^{\\prime\\prime}=1}^{n}(\\mathbf{u}_{i}^{\\top}\\mathbf{u}-\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u})\\frac{e^{c_{u}^{2}c(2\\mathbf{u}_{i}+\\mathbf{u}_{i^{\\prime}}+\\mathbf{u}_{i^{\\prime\\prime}})^{\\top}\\mathbf{u}}\\alpha_{i}^{2}\\alpha_{i^{\\prime}}\\alpha_{i^{\\prime\\prime}}}{(\\sum_{i=1}^{n}e^{c_{u}^{2}c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{4}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Setting $c=0$ results in ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left.\\frac{d J(c)}{d c}\\right|_{c=0}=\\frac{2c_{a}^{2}c_{u}^{4}}{(\\sum_{i=1}^{n}\\alpha_{i})^{3}}\\sum_{i=1}^{n}\\sum_{i^{\\prime}=1}^{n}\\sum_{i^{\\prime\\prime}=1}^{n}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{i^{\\prime}}-\\mathbf{u})(\\mathbf{u}_{i}^{\\top}\\mathbf{u}-\\mathbf{u}_{i^{\\prime\\prime}}^{\\top}\\mathbf{u})\\alpha_{i}\\alpha_{i^{\\prime}}\\alpha_{i^{\\prime\\prime}}\\right]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\left.\\frac{2\\sigma^{2}c_{u}^{2}}{(\\sum_{i=1}^{n}\\alpha_{i})^{4}}\\sum_{i=1}^{n}\\sum_{i^{\\prime}=1}^{n}\\sum_{i^{\\prime\\prime}=1}^{n}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[(\\mathbf{u}_{i}^{\\top}\\mathbf{u}-\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u})\\alpha_{i}^{2}\\alpha_{i^{\\prime}}\\alpha_{i^{\\prime\\prime}}\\right]\\right.}\\\\ &{=\\frac{2c_{a}^{2}c_{u}^{4}}{(\\sum_{i=1}^{n}\\alpha_{i})^{3}}\\sum_{i=1}^{n}\\sum_{i^{\\prime}=1}^{n}\\sum_{i^{\\prime\\prime}=1}^{n}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{i^{\\prime}}-\\mathbf{u})(\\mathbf{u}_{i}^{\\top}\\mathbf{u}-\\mathbf{u}_{i^{\\prime\\prime}}^{\\top}\\mathbf{u})\\alpha_{i}\\alpha_{i^{\\prime}}\\alpha_{i^{\\prime\\prime}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{-\\frac{2{\\mathcal L}_{\\mathrm{c}}^{2}+\\alpha_{1}^{2}}{(\\sum_{k=0}^{\\infty})^{\\frac{3}{2}}}\\Biggr\\Delta_{k}^{2}\\Biggl\\Delta_{k}^{2}\\Biggl\\Delta_{1}\\Biggl\\{\\left[u_{k}^{T}-u_{k}^{T}\\right]\\left(u_{k}^{T}-u_{k}^{T}\\right)\\left(u_{k}^{T}-u_{k}^{T}\\right)\\Biggr\\}}\\\\ &{-\\frac{2{\\mathcal L}_{\\mathrm{c}}^{2}+\\alpha_{1}^{2}}{(\\sum_{k=0}^{\\infty})^{\\frac{3}{2}}}\\Biggr\\Delta_{k}^{2}\\sum_{i=1}^{\\infty}\\sum_{\\alpha_{k-1}}\\Biggl\\{\\left[u_{k}^{T}+u_{k}^{T}\\right]\\left(u_{k}^{T}-u_{k}^{T}\\right)\\left(u_{k}^{T}-u_{k}^{T}\\right)\\sin_{\\alpha}u_{i}-u_{i}^{T}\\right.\\Biggr.}\\\\ &{\\left.-\\frac{2{\\mathcal L}_{\\mathrm{c}}^{2}+\\alpha_{1}^{2}}{(\\sum_{k=0}^{\\infty})^{\\frac{3}{2}}}\\right\\Delta_{k}^{2}\\Biggl\\{\\left[u_{k}^{T}-u_{k}^{T}\\right]\\left(u_{k}^{T}-u_{k}^{T}\\right)\\left(u_{k}^{T}-u_{k}^{T}\\right)\\left(u_{k}^{T}-u_{k}^{T}\\right)\\Biggr\\}}\\\\ &{-\\frac{2{\\mathcal L}_{\\mathrm{c}}^{2}+\\alpha_{1}^{2}}{(\\sum_{k=0}^{\\infty})^{\\frac{3}{2}}}\\Biggr\\Delta_{k}^{2}\\Biggl\\{\\left[u_{k}^{T}-u_{k}^{T}\\right]\\left(u_{k}^{T}-u_{k}^{T}\\right)\\left(u_{k}^{T}-u_{k}^{T}\\right)\\sin_{\\alpha}u_{i}-u_{i}^{T}\\Biggr\\}}\\\\ &{\\times\\sum_{k=0}^{\\infty}\\sum_{\\alpha_{k-1}}^{\\infty}\\sum_{\\alpha_{k-1}}^{\\infty}\\bigg\\{\\left[u_{k}^{T}-u_{k}^{T}\\right]\\left(u_{k}^{T}\\right)^{2}\\left(u_{k}^{T}-u_{k}^{T \n$$$$\n\\begin{array}{r l}&{\\quad_{1\\leq i\\leq n}-\\frac{1}{2}\\sum_{k\\geq n+1}^{n}\\alpha_{i}}\\\\ &{=-\\frac{2\\sum_{k\\geq n}^{n}\\alpha_{i}^{k}}{(1+\\alpha_{i}^{1})^{3}}\\sum_{k\\geq n}^{\\infty}\\alpha_{i}}\\\\ &{\\times\\left(\\frac{\\displaystyle\\sum_{l=i+1}^{n}\\sum_{s\\geq n}^{n}\\mathbb{E}\\left(\\alpha_{l}\\log\\log\\log\\log\\log\\log\\pi_{k}\\right)-\\frac{1}{\\sqrt{1+\\alpha_{i}^{n}}}\\sum_{l=1}^{n}\\mathbb{E}\\left(\\alpha_{l}\\log\\log\\log(\\log\\log(\\pi_{k})\\log\\log(\\log(\\log\\log(\\pi_{k})\\log\\log(\\log(\\pi_{k})\\log\\log(\\log(\\pi_{k})\\log\\log(\\pi_{k})))}\\right)}\\\\ &{-\\frac{2\\sqrt{2+\\alpha_{i}^{n}}}{(1+\\alpha_{i}^{1})^{3}})\\frac{\\displaystyle\\sum_{l=1}^{n}\\sum_{s\\geq n}^{n}\\sum_{i\\geq n}^{n}\\mathbb{E}\\left(\\alpha_{l}\\log\\log\\left(\\log(\\log(\\log(\\log(\\pi_{k}\\log\\log(\\log(\\pi_{k})\\log\\log(\\pi_{k})\\log\\pi_{k})\\right)}\\\\ &{-\\frac{2\\sqrt{2+\\alpha_{i}^{n}}}{(1+\\alpha_{i}^{1})^{3}})\\log(\\log(\\log(\\pi_{k})\\log(\\log(\\log(\\pi_{k})\\log\\log(\\pi_{k})\\log\\theta_{0}))}\\\\ &{=-\\frac{2\\sqrt{2+\\alpha_{i}^{n}}}{(1+\\alpha_{i}^{1})^{3}})\\frac{\\displaystyle\\sum_{l=1}^{n}\\sum_{s\\geq n}^{n}\\sum_{i\\geq n}^{n}\\mathbb{E}\\left(\\alpha_{l}\\log(\\log(\\log(\\log(\\pi_{k})\\log(\\log(\\pi_{k})\\log(\\theta_{0})\\log(\\pi_{k})\\log(\\log(\\pi_{k})\\log(\\pi_{k})\\log(\\log(\\pi_{k})\\log(\\pi_{k})\\log(\\log(\\pi_{k}))}\\\\ &{+\\frac{2\\sqrt{2+\\alpha_{i}^{n}}}{(1+\\alpha_{i}^{1})^{3}})\\frac{\\displaystyle\\sum_{l=1}^{n}\\sum_{s\\geq n}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (22) follows since $\\mathbb{E}[{\\bf u}_{i}]={\\bf0}_{k}$ , (23) similarly follows since odd moments of uniform random variables on the hypersphere are zero, (24) follows by the i.i.d.-ness of the $\\mathbf{u}_{i}$ 's, (25) follows since $\\begin{array}{r}{\\mathbb{E}[{\\bf u}_{i}{\\bf u}_{i}^{\\top}]={\\frac{1}{k}}{\\bf I}_{k}}\\end{array}$ and $\\mathbf{u}^{\\mathsf{T}}\\mathbf{u}=1$ , and (26) follows since $\\operatorname*{min}_{i}\\alpha_{i}>0$ . This completes the proof. ", "page_idx": 37}, {"type": "text", "text": "Lemma H.2. Consider any $\\mathbf{B}\\,\\in\\,\\mathbb{O}^{d\\times k}$ and resulting function class $\\mathcal{F}_{\\mathbf{B}}^{l i n}$ Consider the training population loss $\\mathcal{L}$ defined in (ICL), and tasks drawn from $D(\\mathcal{F}_{\\mathbf{B}}^{l i n})$ such that $\\mathbb{E}_{\\mathbf{a}}[\\mathbf{a}\\mathbf{a}^{\\top}]=c_{a}^{2}\\mathbf{I}_{k}$ for some $c_{a}\\neq0$ and let $\\mathbf{M}:=\\mathbf{M}_{K}^{\\top}\\mathbf{M}_{Q}$ be optimized over the domain $\\mathcal{M}_{\\widehat{c}}:=\\left\\{\\mathbf{M}\\in\\mathbb{R}^{d\\times d}:\\mathbf{M}=\\right.$ $\\begin{array}{r}{\\mathbf{M}^{\\top},\\|\\mathbf{B}^{\\top}\\mathbf{M}\\mathbf{B}\\|_{2}\\leq\\frac{\\hat{c}}{c_{u}^{2}}\\}}\\end{array}$ for any $\\hat{c}>0$ Then any ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbf{M}^{*}\\in\\arg\\operatorname*{min}_{\\mathbf{M}\\in\\mathcal{M}_{\\hat{c}}}\\mathcal{L}(\\mathbf{M})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "satisfes $\\mathbf{M}^{*}=c_{1}^{*}\\mathbf{B}\\mathbf{B}^{T}+c_{2}^{*}\\mathbf{B}_{\\perp}\\mathbf{B}_{\\perp}^{\\top}$ for some $\\begin{array}{r}{c_{1}^{*}:|c_{1}^{*}|\\in(0,\\frac{\\hat{c}}{c_{u}^{2}}]}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "Proof. Without loss of generality (WLOG), we can decompose ${\\bf M}={\\bf B}{\\bf F}\\!+\\!{\\bf B}_{\\perp}{\\bf G}$ where $\\mathbf{F}:=\\mathbf{B}^{\\top}\\mathbf{M}$ and $\\mathbf{G}:=\\mathbf{B}_{\\perp}^{\\top}\\mathbf{M}$ . Recall that for each $i\\in[n+1]$ \uff0c $\\mathbf{x}_{i}=c_{u}\\mathbf{B}\\mathbf{u}_{i}+c_{v}\\mathbf{B}_{\\perp}\\mathbf{v}_{i}$ . Thus, for each $i\\in[n]$ ", "page_idx": 37}, {"type": "text", "text": "we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e^{\\mathbf{x}_{i}^{\\top}\\mathbf{M}\\mathbf{x}_{n+1}}=e^{\\mathbf{x}_{i}^{\\top}\\mathbf{B}\\mathbf{F}\\mathbf{x}_{n+1}}e^{\\mathbf{x}_{i}^{\\top}\\mathbf{B}_{\\bot}\\mathbf{G}\\mathbf{x}_{n+1}}}\\\\ &{\\qquad\\qquad=e^{c_{u}\\mathbf{u}_{i}^{\\top}\\mathbf{F}\\mathbf{x}_{n+1}}e^{c_{v}\\mathbf{v}_{i}^{\\top}\\mathbf{G}\\mathbf{x}_{n+1}}}\\\\ &{\\qquad\\qquad=e^{c_{u}\\mathbf{u}_{i}^{\\top}\\mathbf{F}\\mathbf{x}_{n+1}}\\alpha_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where, fo each $i\\in[n]$ $\\alpha_{i}:=e^{c_{v}\\mathbf{v}_{i}^{\\top}\\mathbf{G}\\mathbf{x}_{n+1}}$ For ease of notation, dente $\\mathbf{x}=\\mathbf{x}_{n+1}$ ", "page_idx": 38}, {"type": "text", "text": "We start by expanding the square and using the linearity of the expectation to re-write the population loss as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathbf{M})}\\\\ &{=\\mathbb{E}\\frac{1}{{\\mathbf{R}}_{+}(\\mathbf{M})+[\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})]+[\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})]+[\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})]^{\\mathsf{T}}\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})}}\\\\ &{\\qquad\\qquad\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{a}^{\\mathsf{t}}\\mathbf{M}^{\\mathsf{T}}\\mathbf{1}_{i}-\\mathbf{a}^{\\mathsf{T}}\\mathbf{1}_{i})\\mathbf{T}\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})}{(\\sum_{i=1}^{n}\\mathbf{r}_{i}^{\\mathsf{T}}\\mathbf{A})^{\\mathsf{T}}\\mathbf{1}_{i}\\mathbf{r}_{i}^{\\mathsf{T}}}-\\mathbf{a}^{\\mathsf{T}}\\mathbf{1}_{{\\hat{R}}}^{\\mathsf{T}}\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})\\mathbf{s}^{\\mathsf{T}}\\mathbf{1}_{{i}}\\right]}\\\\ &{=\\mathrm{e}^{2\\mathbf{E}_{\\mathbf{M},\\mathbf{a}}}[\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}}),}\\\\ &{\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{a}^{\\mathsf{t}}\\mathbf{M}^{\\mathsf{T}}\\mathbf{1}_{i}-\\mathbf{a}^{\\mathsf{T}}\\mathbf{1}_{i})\\mathbf{a}^{\\mathsf{T}}\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})-\\mathbf{a}^{\\mathsf{T}}\\mathbf{u}^{\\mathsf{T}}\\mathbf{1}_{{\\hat{R}}}(\\mathbf{\\hat{z}})\\mathbf{s}^{\\mathsf{T}}\\mathbf{a}^{\\mathsf{T}}\\mathbf{s}^{\\mathsf{T}}\\mathbf{a}^{\\mathsf{T}}\\right]}{(\\sum_{i=1}^{n}\\mathbf{r}_{i}^{\\mathsf{T}}\\mathbf{a}^{\\mathsf{T}}\\mathbf{M}^{\\mathsf{T}}\\mathbf{1}_{i})^{2}}}\\\\ &{\\qquad\\qquad+\\sigma^{2}\\mathbb{E}_{\\mathbf{a},\\mathbf{a}}[ \n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "WLOG we can write $\\mathbf{Fx}\\,=\\,\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}\\|\\mathbf{Fx}\\|_{2}$ for some rotation matrix $\\mathbf{R}(\\mathbf{F}\\mathbf{x})\\,\\in\\,\\mathbb{O}^{k\\times k}$ . Denote $C_{1}(\\mathbf{F}\\mathbf{x}):=\\|\\mathbf{F}\\mathbf{x}\\|_{2}$ . Then we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\bar{\\mathbf\\Xi}}_{\\mathrm{signal}}(\\mathbf{M},\\mathbf{x})}\\\\ &{=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\left(\\mathbf{u}_{i}-\\mathbf{u}\\right)^{\\top}\\left(\\mathbf{u}_{j}-\\mathbf{u}\\right)e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}+c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{i}\\alpha_{j}}{\\left(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{i}}\\right)^{2}}\\right]}\\\\ &{=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\left(\\mathbf{u}_{i}-\\mathbf{u}\\right)^{\\top}\\mathbf{R}\\left(\\mathbf{Fx}\\right)\\mathbf{R}\\left(\\mathbf{Fx}\\right)^{\\top}\\left(\\mathbf{u}_{j}-\\mathbf{u}\\right)e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}+c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}}{\\left(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}}\\right)^{2}}}{\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}}\\mathbf{R}(\\mathbf{Fx})^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{{\\mathbf{u}}_{i}\\},\\{{\\mathbf{v}}_{i}\\}}\\left[\\frac{1}{(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{\\alpha_{i}}})^{2}}\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\quad\\times\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\left((\\mathbf{R}(\\mathbf{Fx})^{\\top}\\mathbf{u}_{i}-\\mathbf{R}(\\mathbf{Fx})^{\\top}\\mathbf{u})^{\\top}(\\mathbf{R}(\\mathbf{Fx})^{\\top}\\mathbf{u}_{j}-\\mathbf{R}(\\mathbf{Fx})^{\\top}\\mathbf{u})\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.\\times e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}+c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}_{\\alpha_{i}}\\alpha_{j}}\\right)\\right]}\\\\ &{\\displaystyle=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{{\\mathbf{u}}_{i}\\},\\{{\\mathbf{v}}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{R}(\\mathbf{Fx})^{\\top}\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{R}(\\mathbf{Fx})^{\\top}\\mathbf{u})e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{u}_{\\alpha_{i}}}{(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}})^{2}}\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{i}}\\right]^{2}\\!\\!\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where (30) follows since $\\mathbf{R}(\\mathbf{F}\\mathbf{x})\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\top}=\\mathbf{I}_{k}$ and (31) follows since the distribution of $\\mathbf{u}_{i}$ is the same as the distribution of $\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\top}\\mathbf{u}_{i}$ for any rotation $\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\top}$ .Define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{v}):=\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\top}\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\top}\\mathbf{u})e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{j}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for any $\\mathbf{F}\\in\\mathbb{R}^{k\\times d}$ We have $\\mathcal{L}_{\\mathrm{signal}}(\\mathbf{M})=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\mathbf{u},\\mathbf{v}}[g(\\mathbf{F},\\mathbf{u},\\mathbf{v})]$ and Note that if $\\mathbf{F}^{\\prime}=c\\mathbf{B}^{\\top}$ , then $\\mathbf{R}_{\\mathbf{F}^{\\prime}\\mathbf{x}}=\\mathbf{I}_{k}$ and $C_{1}(\\mathbf{F^{\\prime}x})=c_{u}c$ Thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathbf{F},\\mathbf{u},\\mathbf{v})-g(\\frac{\\mathbf{F}(\\mathbf{F},\\mathbf{u})}{\\hbar}\\mathbf{B}^{\\prime},\\mathbf{u},\\mathbf{v})}\\\\ &{={\\mathbb{E}}_{(\\mathbf{u},\\mathbf{v}),[\\mathbf{v}]}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\mathbf{(u}_{i}-\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\mathsf{T}}\\mathbf{u})^{\\mathsf{T}}(\\mathbf{u}_{j}-\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\mathsf{T}}\\mathbf{u})^{\\mathsf{T}}e^{-E_{i}\\mathbf{F}\\mathbf{x})\\mathbf{F}\\mathbf{u}}\\mathrm{e}^{i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}^{-E_{i}\\mathbf{F}\\mathbf{u}}\\mathrm{e}^{i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}_{i,j}\\right]}\\\\ &{\\qquad-{\\mathbb{E}}_{(\\mathbf{u},\\mathbf{v}),[\\mathbf{v}]}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\mathsf{T}}(\\mathbf{u}_{j}-\\mathbf{u})^{\\mathsf{T}}(\\mathbf{u}_{i}-\\mathbf{u})^{\\mathsf{T}}(\\mathbf{F}\\mathbf{x})^{\\mathsf{T}}\\mathbf{u}\\mathrm{e}^{i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}^{-E_{i}\\mathbf{u}}\\mathrm{e}^{i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}_{i,j}\\right]}\\\\ &{\\qquad-{\\mathbb{E}}_{(\\mathbf{u},\\mathbf{v}),[\\mathbf{v}]}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\mathsf{T}}(\\mathbf{B}^{\\mathsf{T}}\\mathbf{x})^{\\mathsf{T}}(\\mathbf{u}_{j}-\\mathbf{u})^{\\mathsf{T}}(\\mathbf{F}\\mathbf{x})^{\\mathsf{T}}\\mathbf{u}\\mathrm{e}^{i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}^{-i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}^{i E\\mathbf{D}\\mathbf{u}}^{\\mathsf{T}}\\mathrm{e}_{i,j}\\right]}\\\\ &{={\\mathbb{\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\hat{\\mathbf{u}}:=\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\mathbf{u}_{i}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}}{\\sum_{i\\equiv1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}}\\right]$ and WLOG write $\\mathbf{u}_{i}=\\mathbf{p}_{\\mathbf{u}_{i}}+\\mathbf{q}_{\\mathbf{u}_{i}}$ , where $\\mathbf{p}_{\\mathbf{u}_{i}}:=$ $\\mathbf{u}\\mathbf{u}^{\\top}\\mathbf{u}_{i}$ and $\\mathbf{q}_{\\mathbf{u}_{i}}:=(\\mathbf{I}_{k}-\\mathbf{u}\\mathbf{u}^{\\top})\\mathbf{u}_{i}$ . Note that for any $\\mathbf{u}_{i}=\\mathbf{p}_{\\mathbf{u}_{i}}+\\mathbf{q}_{\\mathbf{u}_{i}}$ \uff0c $\\mathbf{u}_{i}^{\\prime}:=\\mathbf{p}_{\\mathbf{u}_{i}}-\\mathbf{q}_{\\mathbf{u}_{i}}$ occurs with equal probability, and flipping $\\mathbf{q}_{\\mathbf{u}_{i}}$ does not change any exponent or $\\alpha_{i}$ in (32). Thus ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{u}}=\\mathbb{E}_{\\{(\\mathbf{p}_{\\mathbf{u}_{i}},\\mathbf{q}_{\\mathbf{u}_{i}})\\}_{i\\in[n]},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\left(\\mathbf{p}_{\\mathbf{u}_{i}}+\\mathbf{q}_{\\mathbf{u}_{i}}\\right)e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}\\right]}\\\\ &{\\phantom{\\hat{\\mathbf{u}}=\\frac{1}{2}\\mathbb{E}_{\\{(\\mathbf{p}_{\\mathbf{u}_{i}},\\mathbf{q}_{\\mathbf{u}_{i}})\\}_{i},\\left\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\left(2\\mathbf{p}_{\\mathbf{u}_{i}}+\\mathbf{q}_{\\mathbf{u}_{i}}-\\mathbf{q}_{\\mathbf{u}_{i}}\\right)e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}\\right]}\\\\ &{\\phantom{\\hat{\\mathbf{u}}=\\mathbb{E}_{\\{\\mathbf{p}_{\\mathbf{u}_{i}}\\}_{i},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\mathbf{p}_{\\mathbf{u}_{i}}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}\\right]}\\\\ &{\\phantom{\\hat{\\mathbf{u}}=\\mathbb{E}_{\\{\\mathbf{p}_{\\mathbf{u}_{i}}\\}_{i},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\mathbf{p}_{\\mathbf{u}_{i}}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i} \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r}{\\tilde{c}:=\\mathbb{E}_{\\left\\{\\mathbf{u}_{i}\\right\\},\\left\\{\\mathbf{v}_{i}\\right\\}}\\left[\\frac{\\sum_{i=1}^{n}\\mathbf{u}_{i}^{\\top}\\mathbf{u}\\,e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}\\right]}\\end{array}$ . Note that for any $\\mathbf{u}_{i}$ \uff0c $-\\mathbf{u}_{i}$ occurs with equal probability, so ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{c}=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\mathbf{u}^{\\top}\\mathbf{u}_{i}\\mathbf{\\Sigma}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\displaystyle\\sum_{j=1}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{j}}\\right]}\\\\ &{\\quad=\\displaystyle\\frac{1}{2}\\sum_{i=1}^{n}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\mathbf{u}_{i}^{\\top}\\mathbf{u}\\mathbf{\\Sigma}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}}{e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}+\\sum_{j=1,j\\neq i}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{j}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,-\\,\\frac{\\mathbf{u}_{i}^{\\top}\\mathbf{u}\\,e^{-c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{e^{-c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}+\\sum_{j=1,j\\ne i}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{u}_{\\alpha_{j}}}}\\Bigg]}\\\\ &{=\\frac{1}{2}\\sum_{i=1}^{n}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\Bigg[\\mathbf{u}_{i}^{\\top}\\mathbf{u}\\Bigg(\\frac{e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}}{e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}+\\sum_{j=1,j\\ne i}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{j}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,e^{-c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,\\,-\\,\\frac{e^{-c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}}{e^{-c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}+\\sum_{j=1,j\\ne i}^{n}e^{c_{u}C_{1}(\\mathbf{Fx})\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{j}}\\Bigg)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since $\\alpha_{i}>0$ and $\\bar{c}_{u,v}>0$ by definition, $e^{c_{u}C_{1}\\left(\\mathbf{F}\\mathbf{x}\\right)\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}$ is monotonically increasing in $\\mathbf{u}_{i}^{\\top}\\mathbf{u}$ Also, $\\begin{array}{r}{f(x):=\\frac{x}{x+c}}\\end{array}$ is monotonical inereasing for $x>0$ for all $c>0$ Thus we have tht ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overbrace{\\mathrm{\\te}^{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}{\\bf u}}}^{\\substack{c_{u}\\cdots\\rightarrow\\mathrm{\\t}}}e^{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}{\\bf u}_{\\alpha_{i}}}}\\\\ &{\\overbrace{e^{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}{\\bf u}}\\alpha_{i}+\\sum_{j=1,j\\ne i}^{n}e^{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{j}^{\\top}{\\bf u}}\\alpha_{j}}^{\\substack{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}{\\bf u}}}-\\frac{e^{-c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}{\\bf u}}\\alpha_{i}+\\sum_{j=1,j\\ne i}^{n}e^{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}}}{e^{-c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{i}^{\\top}{\\bf u}}\\alpha_{i}+\\sum_{j=1,j\\ne i}^{n}e^{c_{u}C_{1}({\\bf F}{\\bf x}){\\bf u}_{j}^{\\top}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and thereby $\\tilde{c}>0$ . Therefore, $\\arg\\operatorname*{max}_{\\mathbf{u}^{\\prime}\\in\\mathbb{S}^{k-1}}(\\mathbf{u}^{\\prime})^{\\top}\\hat{\\mathbf{u}}=\\mathbf{u}$ , in particular $\\mathbf{u}^{\\top}\\hat{\\mathbf{u}}>\\mathbf{u}^{\\top}\\mathbf{R}(\\mathbf{F}\\mathbf{x})^{\\top}\\hat{\\mathbf{u}}$ whenever $\\mathbf{R}(\\mathbf{F}\\mathbf{x})\\mathbf{u_{\\mathrm{~}}}\\neq\\mathbf{\\deltau_{\\mathrm{~}}}$ . so (32) is strictly positive if $\\mathbf{R}(\\mathbf{F}\\mathbf{x})\\mathbf{u_{\\mathrm{~}}}\\neq\\mathbf{\\deltau_{\\mathrm{~}}}$ Thus, for any $\\mathbf{u},\\mathbf{v}$ such that $\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}\\neq\\mathbf{u}$ $\\begin{array}{r}{g(\\mathbf{F},\\mathbf{u},\\mathbf{v})>g(\\frac{C_{1}(\\mathbf{F}\\mathbf{x})}{c_{u}}\\mathbf{B}^{\\top},\\mathbf{u},\\mathbf{v})}\\end{array}$ Ch(Fx) BT , u,v). Also, for any u, v such that R(Fx)u = u, $\\begin{array}{r}{g(\\mathbf{F},\\mathbf{u},\\mathbf{v})=g(\\frac{C_{1}(\\mathbf{F}\\mathbf{x})}{c_{u}}\\mathbf{B}^{\\top},\\mathbf{u},\\mathbf{v})}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Next we need to account for $\\tilde{\\mathcal{L}}_{\\mathrm{noise}}(\\mathbf{M},\\mathbf{x})$ . Again writing $\\mathbf{Fx}=\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}\\|\\mathbf{Fx}\\|_{2}$ and $C_{1}(\\mathbf{Fx})=$ $\\|\\mathbf{Fx}\\|_{2}$ and using the rotational invariance of $\\mathbf{u}_{i}$ , we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{noise}}(\\mathbf{M})=\\sigma^{2}\\mathbb{E}_{\\mathbf{x},\\{\\mathbf{x}_{i}\\}_{i\\in[n]}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c_{u}\\mathbf{u}_{i}^{\\top}\\mathbf{F}\\mathbf{x}}\\alpha_{i}^{2}}{(\\sum_{i=1}^{n}e^{c_{u}\\mathbf{u}_{i}^{\\top}\\mathbf{F}\\mathbf{x}}\\alpha_{i})^{2}}\\right]}\\\\ &{\\qquad\\qquad=\\sigma^{2}\\mathbb{E}_{\\mathbf{u},\\mathbf{v},\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{\\alpha_{i}^{2}}}}{(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{R}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{\\alpha_{i}}})^{2}}\\right]}\\\\ &{\\qquad\\qquad=\\sigma^{2}\\mathbb{E}_{\\mathbf{u},\\mathbf{v},\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}^{2}}{(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where (36) follows using the rotational invariance of $\\mathbf{u}_{i}$ . So, returning to (29), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\footnotesize\\mathrm{\\boldmath~\\hat{z}(M)~}=\\mathbb{E}_{\\mathbf{x}}[\\tilde{\\mathcal{L}}_{\\mathbf{s}\\mathbf{i}\\mathbf{p}\\mathbf{a}\\mathbf{i}}(\\mathbf{B}\\mathbf{F}+\\mathbf{B}_{\\bot}\\mathbf{G},\\mathbf{x})+\\tilde{\\mathcal{L}}_{\\mathbf{n}\\mathbf{i}\\mathbf{s}\\mathbf{e}}(\\mathbf{B}\\mathbf{F}+\\mathbf{B}_{\\bot}\\mathbf{G},\\mathbf{x})]\\quad}&{}\\\\ &{}&{\\geq\\mathbb{E}_{\\mathbf{u},\\mathbf{v}}\\Bigg[c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}\\left(\\mathbf{u}_{j}-\\mathbf{u}\\right)e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{j}^{\\top}\\mathbf{u}_{\\alpha_{i}\\alpha_{j}}}}{\\left(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}\\right)^{2}}\\right]}\\\\ &{}&{+\\,\\sigma^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}}{\\left(\\sum_{i=1}^{n}e^{c_{u}C_{1}(\\mathbf{F}\\mathbf{x})\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{\\alpha_{i}}}\\right)^{2}}\\right]\\Bigg]\\quad}&{\\quad(37)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where (37) is strict if $\\mathbf{R}(\\mathbf{Fx})\\mathbf{u}\\neq\\mathbf{u}$ for some $\\mathbf{u},\\mathbf{v}$ , which is equivalent to saying that ${\\bf F}\\notin\\{c^{\\prime}{\\bf B}^{\\top},c^{\\prime}>$ $0\\}$ ", "page_idx": 40}, {"type": "text", "text": "Next, recall that we have defned $\\alpha_{i}:=e^{c_{v}\\mathbf{v}_{i}^{\\top}\\mathbf{G}\\mathbf{x}}$ . Using a similar argument as earlie, by the rotational invariance of the $\\mathbf{v}_{i}$ 's, for any fixed $\\mathbf{x}$ we can write $\\bar{\\alpha_{i}}=e^{c_{v}C_{2}(\\mathbf{G}\\bar{\\mathbf{x}})\\mathbf{v}_{i}^{\\top}\\mathbf{e}_{1}}$ where $C_{2}(\\mathbf{Gx}):=\\|\\mathbf{Gx}\\|_{2}$ and $\\mathbf{e}_{1}$ is the first standard basis vector. ", "page_idx": 40}, {"type": "text", "text": "Next, for $c_{1},c_{2}\\geq0$ and some fixed $\\mathbf{u},\\mathbf{v}$ , define ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\mathbf{u},\\mathbf{v},c_{1},c_{2}):=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{u})e^{c_{u}c_{1}\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c_{u}c_{1}\\mathbf{u}_{j}^{\\top}\\mathbf{u}}e^{c_{v}c_{2}\\mathbf{v}_{i}^{\\top}\\mathbf{e}_{1}+c_{v}c_{2}\\mathbf{v}_{i}^{\\top}\\mathbf{u}_{j}^{\\top}}}{(\\sum_{i=1}^{n}e^{c_{u}c_{1}\\mathbf{u}_{i}^{\\top}\\mathbf{u}}e^{c_{v}c_{2}\\mathbf{v}_{i}^{\\top}\\mathbf{e}_{1}})^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n+\\,\\sigma^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\},\\{\\mathbf{v}_{i}\\}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c_{u}c_{1}\\mathbf{u}_{i}^{\\top}\\mathbf{u}}e^{2c_{v}c_{2}\\mathbf{v}_{i}^{\\top}\\mathbf{e}_{1}}}{(\\sum_{i=1}^{n}e^{c_{u}c_{1}\\mathbf{u}_{i}^{\\top}\\mathbf{u}}e^{c_{v}c_{2}\\mathbf{v}_{i}^{\\top}\\mathbf{e}_{1}})^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and let ", "page_idx": 41}, {"type": "equation", "text": "$$\n(C_{1}^{*}(\\mathbf{u},\\mathbf{v}),C_{2}^{*}(\\mathbf{u},\\mathbf{v}))\\in\\arg\\operatorname*{min}_{(c_{1},c_{2}):0\\leq c_{1}\\leq\\frac{\\hat{c}}{c_{u}^{2}},c_{2}\\geq0}H(\\mathbf{u},\\mathbf{v},c_{1},c_{2})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Since $H$ does not vary with $\\mathbf{v}$ , we have $(C_{1}^{*}(\\mathbf{u},\\mathbf{v}),C_{2}^{*}(\\mathbf{u},\\mathbf{v}))\\ =\\ (C_{1}^{*}(\\mathbf{u}),C_{2}^{*}(\\mathbf{u}))$ WLOG. In fact, $H$ does not vary with $\\mathbf{u}$ either, due to the rotational invariance of the $\\mathbf{u}_{i}$ 's.'So, we have $(C_{1}^{*}({\\mathbf{u}},{\\mathbf{v}}),C_{2}^{*}({\\mathbf{u}},{\\mathbf{v}}))\\ {^{\\prime}}=\\ (C_{1}^{*},C_{2}^{*})$ WLOG, i.e. there is a single pair $(C_{1}^{*},C_{2}^{*})$ that minimizes ${\\cal H}(\\mathbf{u},\\mathbf{v},c_{1},c_{2})$ over $c_{1},c_{2}$ for all $\\mathbf{u}\\in\\mathbb{S}^{k-1}$ and $\\mathbf{v}\\in\\mathbb{S}^{d-k-1}$ ", "page_idx": 41}, {"type": "text", "text": "Thus $\\mathbf{F}^{*}=C_{1}^{*}\\mathbf{B}^{\\top}$ and $\\mathbf{G}^{*}$ satisfies $\\|\\mathbf{G}^{*}\\mathbf{x}\\|=C_{2}^{*}$ for all $\\mathbf{x}$ , which implies, using also that $\\mathbf{M}$ is symmetric, that ${\\bf G}^{*}=C_{2}^{*}{\\bf B}_{\\perp}^{\\top}$ \u53e3 ", "page_idx": 41}, {"type": "text", "text": "Lemma H.3. Consider any $\\alpha:=[\\alpha_{1},\\ldots,\\alpha_{n}]$ such that $\\alpha_{1}=\\operatorname*{max}_{i}\\alpha_{i}$ and $\\alpha_{1}>\\operatorname*{min}_{i}\\alpha_{i}>0$ Further, let $c\\in(0,2]$ Define ", "page_idx": 41}, {"type": "equation", "text": "$$\nH_{s i g n a l}(\\mathbf{u},\\alpha):=\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{u})e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{i}\\alpha_{j}}{(\\sum_{i=1}^{n}e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\partial H_{s i g n a l}(\\mathbf{u},\\alpha)}{\\partial\\alpha_{1}}>0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. We first compute (Uing thelneartyof thextaonn thequttr obtain: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cdots\\quad\\infty_{(\\mathbf{u}),\\mathbf{i}\\neq\\mathbf{n}}^{\\mathcal{I}}}\\\\ &{=\\mathbb{E}_{\\{\\mathbf{u}\\}_{1}\\neq\\mathbf{n}}^{\\mathcal{I}}\\left[\\frac{\\partial}{\\partial\\alpha_{1}}\\sum_{j=1}^{\\infty}\\mathrm{tr}_{0}(\\mathbf{u}_{\\ t}-\\mathbf{u})^{\\top}(\\mathbf{u}_{\\theta}-\\mathbf{u})e^{\\mathrm{i}\\theta_{1}\\cdot\\mathbf{u}\\cdot\\mathbf{u}\\cdot\\mathbf{u}}^{j\\mathbf{u}}u_{0,\\theta_{2}}\\right]}\\\\ &{=2\\mathbb{E}_{\\{\\mathbf{u}\\}_{1},\\cdots\\;\\mathbf{u}}^{\\mathcal{I}}\\left[\\frac{\\sqrt{\\sum_{j=1}^{n}\\mathbf{u}_{j}^{\\top}\\mathbf{u}_{\\theta_{1}}}\\mathrm{tr}_{0}(\\mathbf{u}_{1}-\\mathbf{u})^{\\top}(\\mathbf{u}_{1}-\\mathbf{u})e^{\\mathrm{i}\\theta_{1}\\cdot\\mathbf{u}\\cdot\\mathbf{u}\\cdot\\mathbf{u}}^{j\\mathbf{u}}u_{0,\\theta_{2}}}{(\\sum_{j=1}^{n}\\mathbf{u}_{j}^{\\top}+\\mathbf{u}^{\\top})^{\\theta_{1}}}\\right]}\\\\ &{\\quad-2\\mathbb{E}_{\\{\\mathbf{u}\\}_{1},\\cdots,\\;\\mathbf{u}}^{\\mathcal{I}}\\left[\\frac{(\\sum_{j=1}^{n}\\mathbf{u}_{j}^{\\top}+\\mathbf{u}_{1})^{\\top}(\\mathbf{u}_{1}-\\mathbf{u})^{\\top}(\\mathbf{u}_{1}-\\mathbf{u})^{\\top}(\\mathbf{u}_{1}-\\mathbf{u})}{(\\sum_{j=1}^{n}\\mathbf{u}_{j}^{\\top}+\\mathbf{u}_{1}^{\\top})^{\\theta_{1}}}\\right]\\sum_{m_{1}\\in\\theta_{2}\\}e^{\\mathrm{i}\\theta_{1}\\cdot\\mathbf{u}}^{j\\mathbf{u}}u_{0,\\theta_{2}}\\mathrm{tr}_{1}^{m}\\right]}\\\\ &{=2\\mathbb{E}_{\\{\\mathbf{u}\\}_{1},\\cdots,\\;\\mathbf{u}}^{\\mathcal{I}}\\left[\\frac{(\\sum_{j=1}^{n}\\mathbf{u}_{j}^{\\top}\\mathbf{u}_{\\theta_{1}})\\left(\\sum_{j=1}^{\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where ", "page_idx": 41}, {"type": "equation", "text": "$$\nS_{i,j}:=\\alpha_{i}\\alpha_{j}\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{\\big(\\mathbf{u}_{1}-\\mathbf{u}_{i}\\big)^{\\top}\\big(\\mathbf{u}_{j}-\\mathbf{u}\\big)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{\\big(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}}\\big)^{3}}\\right].\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Note that terms with $i=1$ do not appear in (41). We analyze $S_{i,1}+S_{i,i}$ and each $S_{i,j},j\\notin\\{1,i\\}$ separately, and will ultimately show that each of these terms is positive. We start with the latter case as it is easier to handle. For $j\\notin\\{1,i\\}$ ,wehave ", "page_idx": 41}, {"type": "equation", "text": "$$\nS_{i,j}=\\alpha_{i}\\alpha_{j}\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{(\\mathbf{u}_{1}-\\mathbf{u}_{i})^{\\top}(\\mathbf{u}_{j}-\\mathbf{u})e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ }&{=\\alpha_{i}\\alpha_{j}\\mathbb{E}_{\\{\\mathbf{u}_{t}\\}_{r}\\in[\\mathrm{s}]}\\left[\\frac{\\left(\\mathbf{u}_{1}-\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{u}\\mathbf{u}^{\\top}\\left(\\mathbf{u}_{j}-\\mathbf{u}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{\\left(\\sum_{i^{\\prime}=1}^{n}e^{c(\\mathbf{u}_{i^{\\prime}})^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}}\\right)^{3}}\\right]}\\\\ &{\\ \\ \\ +\\alpha_{i}\\alpha_{j}\\underbrace{\\mathbb{E}_{\\{\\mathbf{u}_{t}\\}_{r}\\in[\\mathrm{s}]}\\left[\\frac{\\mathbf{u}_{1}^{\\top}\\left(\\mathbf{I}_{k}-\\mathbf{u}_{1}^{\\top}\\right)\\left(\\mathbf{u}_{j}-\\mathbf{u}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{\\left(\\sum_{i^{\\prime}=1}^{n}e^{c(\\mathbf{u}_{i^{\\prime}})^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}}\\right)^{3}}\\right]}_{\\mathrm{~\\~=~0~}}}\\\\ &{\\ \\ \\ -\\alpha_{i}\\alpha_{j}\\mathbb{E}_{\\{\\mathbf{u}_{t}\\}_{r}\\in[\\mathrm{s}]}\\left[\\frac{\\mathbf{u}_{1}^{\\top}\\left(\\mathbf{I}_{k}-\\mathbf{u}_{1}^{\\top}\\right)\\left(\\mathbf{u}_{j}-\\mathbf{u}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{\\left(\\sum_{i^{\\prime}=1}^{n}e^{c(\\mathbf{u}_{i^{\\prime}})^{3}\\alpha_{i^{\\prime}}}\\right)^{3}}\\right]}\\\\ &{=\\alpha_{i}\\alpha_{j}\\mathbb{E}_{\\{\\mathbf{u}_{t}\\}_{r}\\in[\\mathrm{s}]}\\left[\\frac{\\left(\\mathbf{u}_{1}^{\\top}\\mathbf{u}-\\mathbf{u}_{1\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the latter two terms in (42) are zero by the same argument as in (33): fipping the component of either $\\mathbf{u}_{1}$ or $\\mathbf{u}_{i}$ perpendicular to $\\mathbf{u}$ does not change any of the values in any exponent, and each flip occurs with equal probability. Next, note that if $\\alpha_{i}=\\alpha_{1}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{\\bar{z}}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{\\mathbf{u}_{1}^{\\top}\\mathbf{u}(\\mathbf{u}_{j}^{\\top}\\mathbf{u}-1)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right]=\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{\\mathbf{u}_{i}^{\\top}\\mathbf{u}(\\mathbf{u}_{j}^{\\top}\\mathbf{u}-1)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u}+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "thus $S_{i,j}\\,=\\,0$ . Otherwise, $\\alpha_{i}<\\alpha_{1}$ by definition of $\\alpha_{1}$ , and there must be some such $\\alpha_{i}$ , since if not, there would be some $c^{\\prime}\\in\\mathbb{R}_{+}$ such that $\\alpha=c^{\\prime}\\alpha^{*}$ . For the case $\\alpha_{i}<\\alpha_{1}$ , we use a symmetry argument to show that $S_{i,j}>0$ ", "page_idx": 42}, {"type": "text", "text": "First we define additional notations. Let $\\bar{U}_{1,i}\\,:=\\,\\{{\\mathbf{u}}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]\\backslash\\{1,i\\}}$ , and for any $(a,b)\\,\\in\\,[-1,1]^{2}$ define ", "page_idx": 42}, {"type": "equation", "text": "$$\nf_{a,b}(\\bar{U}_{1,i}):=\\frac{(a-b)(\\mathbf{u}_{j}^{\\top}\\mathbf{u}-1)e^{c(a+b+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}{(e^{c a}\\alpha_{1}+e^{c b}\\alpha_{i}+\\sum_{i^{\\prime}\\neq1,i}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In particular, for any $a\\in[-1,1]$ , define $p_{a}:=\\mathbb{P}_{\\mathbf{u}_{1}}[\\mathbf{u}_{1}^{\\top}\\mathbf{u}=a]$ . Since $\\mathbf{u}_{1}$ and $\\mathbf{u}_{i}$ are i.i.d., we have $\\mathbb{P}_{\\mathbf{u}_{1},\\mathbf{u}_{i}}[\\mathbf{u}_{1}^{\\top}\\mathbf{u}=a,\\mathbf{u}_{i}^{\\top}\\mathbf{u}=b]=\\mathbb{P}_{\\mathbf{u}_{1},\\mathbf{u}_{i}}[\\mathbf{u}_{1}^{\\top}\\mathbf{u}=b,\\mathbf{u}_{i}^{\\top}\\mathbf{u}=a]=p_{a}p_{b}$ for any $(a,b)\\in[-1,1]^{2}$ Thus, by the law of total expectation we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{S_{i,j}=\\alpha_{i}\\alpha_{j}\\mathbb{E}_{\\bar{U}_{1,i}}\\left[\\displaystyle\\int_{-1}^{1}\\int_{-1}^{1}f_{a,b}(\\bar{U}_{1,i})p_{a}p_{b}\\ d a\\ d b\\right]}\\\\ {\\quad=\\frac{\\alpha_{i}\\alpha_{j}}{2}\\mathbb{E}_{\\bar{U}_{1,i}}\\left[\\displaystyle\\int_{-1}^{1}\\int_{-1}^{1}(f_{a,b}(\\bar{U}_{1,i})+f_{b,a}(\\bar{U}_{1,i}))p_{a}p_{b}\\ d a\\ d b\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Next we show that for any instance of $a,b$ and $\\bar{U}_{1,i}$ \uff0c $f_{a,b}(\\bar{U}_{1,i})+f_{b,a}(\\bar{U}_{1,i})$ is positive. We have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{a,b}(\\bar{U}_{1,i})+f_{b,a}(\\bar{U}_{1,i})}\\\\ &{=(a-b)(\\mathbf{u}_{j}^{\\top}\\mathbf{u}-1)e^{c(a+b+\\mathbf{u}_{j}^{\\top}\\mathbf{u})}}\\\\ &{\\qquad\\times\\left(\\frac{1}{(e^{c a}\\alpha_{1}+e^{c b}\\alpha_{i}+\\sum_{i^{\\prime}\\neq1,i}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}-\\frac{1}{(e^{c b}\\alpha_{1}+e^{c a}\\alpha_{i}+\\sum_{i^{\\prime}\\neq1,i}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with equality only if $a=b$ or $\\mathbf{u}_{j}=\\mathbf{u}$ , since $\\mathbf{u}_{j}^{\\top}\\mathbf{u}\\leq1$ with equality only if $\\mathbf{u}_{j}=\\mathbf{u}$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\na>b\\iff(e^{c a}\\alpha_{1}+e^{c b}\\alpha_{i}+\\sum_{i^{\\prime}\\neq1,i}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}>(e^{c b}\\alpha_{1}+e^{c a}\\alpha_{i}+\\sum_{i^{\\prime}\\neq1,i}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "due to $\\alpha_{1}>\\alpha_{i}$ and $\\alpha_{i^{\\prime}}>0$ for all $i^{\\prime}$ . So we have $S_{i,j}>0$ ", "page_idx": 42}, {"type": "text", "text": "Next we analyze $S_{i,1}+S_{i,i}$ . In these cases we cannot immediately drop the components of $\\mathbf{u}_{1}$ and $\\mathbf{u}_{i}$ that are perpendicular to $\\mathbf{u}$ We have: ", "page_idx": 42}, {"type": "equation", "text": "$$\nS_{i,1}+S_{i,i}=\\alpha_{i}\\alpha_{1}\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{(\\mathbf{u}_{1}-\\mathbf{u}_{i})^{\\top}(\\mathbf{u}_{1}-\\mathbf{u})e^{c(2\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u})}}{(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\alpha_{i}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{r}\\}_{r},\\tau_{i}^{\\star}\\in[n]}\\left[\\frac{\\left(\\mathbf{u}_{1}-\\mathbf{u}_{i}\\right)^{\\top}\\left(\\mathbf{u}_{1}-\\mathbf{u}\\right)e^{\\tau\\left(\\mathbf{u}_{1}^{\\top}+\\mathbf{u}_{2}^{\\top}\\mathbf{u}_{1}^{\\top}+2\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{1}^{\\top}\\right)}}{\\left(\\sum_{i^{\\prime}=1}^{n}e^{\\tau\\left(\\mathbf{u}_{i^{\\prime}}^{\\top}+\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}_{2}^{\\top}+\\mathbf{u}_{i^{\\prime}}^{\\top}\\right)^{3}}\\right)}\\right]}\\\\ &{=\\alpha_{i}\\alpha_{1}\\mathbb{E}_{\\{\\mathbf{u}_{r}\\}_{r},\\tau_{i^{\\star}\\in[n]}}\\left[\\frac{\\left(1-\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{1}-\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u}\\right)e^{\\tau\\left(2\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{1}^{\\top}\\right)}}{\\left(\\sum_{i^{\\prime}=1}^{n}e^{\\tau\\left(\\mathbf{u}_{1}^{\\top}+\\mathbf{u}_{2}^{\\top}\\mathbf{u}_{2}^{\\top}+\\mathbf{u}_{i^{\\prime}}^{\\top}\\right)^{3}}\\right)}\\right]}\\\\ &{\\quad+\\alpha_{i}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{r}\\}_{r},\\tau_{i^{\\star}\\in[n]}}\\left[\\frac{\\left(\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{1}-1-\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u}\\right)e^{\\tau\\left(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+2\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{2}^{\\top}\\mathbf{u}_{1}^{\\top}+2\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{2}^{\\top}\\right)}}{\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now we can split $\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{1}$ into the product of the components of $\\mathbf{u}_{i}$ $\\mathbf{u}_{1}$ in the direction $\\mathbf{u}$ and the product of their components in the perpendicular subspace as before. Doing so yields ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{i,1}+S_{i,i}=\\alpha_{i}\\mathbb{E}_{\\{\\mathbf{u}_{r}\\}_{s^{\\prime}\\in[\\mathrm{in}}}\\left[\\frac{\\left(\\mathbf{u}_{1}^{\\top}\\mathbf{u}-\\mathbf{u}_{1}^{\\top}\\mathbf{u}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u})}\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{i}\\right)}{\\left(\\sum_{t=1}^{n}e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{t}\\right)^{3}}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\alpha_{i}\\mathbb{E}_{\\{\\mathbf{u}_{t}\\}_{s^{\\prime}\\in[\\mathrm{in}}}\\left[\\frac{\\left(1-\\mathbf{u}_{1}^{\\top}\\mathbf{u}\\mathbf{u}^{\\top}\\mathbf{u}_{1}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u})}\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}-e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{i}\\right)}{\\left(\\sum_{t=1}^{n}e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{t}\\right)^{3}}\\right]}\\\\ &{\\qquad\\qquad-\\alpha_{i}\\mathbb{E}_{\\{\\mathbf{u}_{t}\\}_{s^{\\prime}\\in[\\mathrm{in}}}\\left[\\frac{\\mathbf{u}_{1}^{\\top}\\left(\\mathbf{I}_{k}-\\mathbf{u}_{1}^{\\top}\\mathbf{u}\\right)\\mathbf{u}_{1}e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{1}^{\\top}\\mathbf{u})}\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}-e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{i}\\right)}{\\left(\\sum_{t=1}^{n}e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{t}\\right)^{3}}\\right]}\\\\ &{\\qquad=\\alpha_{i}\\mathbb{E}_{\\{\\mathbf{u}_{r}\\}_{s^{\\prime}\\in[\\mathrm \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Next, define ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{a,b}(\\bar{U}_{1,i}):=\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{\\left(\\mathbf{u}_{i}^{\\top}\\mathbf{u}-\\mathbf{u}_{1}^{\\top}\\mathbf{u}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u})}\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}\\right)}{(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right]}\\\\ &{\\phantom{m m m m m m m m}+\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{\\left(1-\\mathbf{u}_{i}^{\\top}\\mathbf{u}\\mathbf{u}^{\\top}\\mathbf{u}_{1}\\right)e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{i}^{\\top}\\mathbf{u})}\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}-e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}\\right)}{(\\sum_{i^{\\prime}=1}^{n}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}})^{3}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We argue similarly as in the previous case, except that here we must include additional terms. ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle S_{i,1}+S_{i,i}}\\\\ {\\displaystyle=\\frac{\\alpha_{i}}{2}\\mathbb{E}_{\\bar{U}_{1,i}}\\left[\\int_{-1}^{1}\\int_{-1}^{1}(g_{a,b}(\\bar{U}_{1,i})+g_{b,a}(\\bar{U}_{1,i}))p_{a}p_{b}\\;d a\\;d b\\right]}\\\\ {\\displaystyle=\\frac{\\alpha_{i}}{2}\\mathbb{E}_{\\bar{U}_{1,i}}\\left[\\int_{-1}^{1}\\int_{-1}^{1}G_{a,b}(\\bar{U}_{1,i})p_{a}p_{b}\\;d a\\;d b\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\nG_{a,b}(\\bar{U}_{1,i}):=g_{a,b}(\\bar{U}_{1,i})+g_{b,a}(\\bar{U}_{1,i})\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We show that for any $(a,b)\\,\\in\\,[-1,1]^{2}$ and any $\\bar{U}_{1,i}$ \uff0c $G_{a,b}(\\bar{U}_{1,i})$ is positive, which implies that $S_{i,1}+S_{i,i}$ is positive by (46). ", "page_idx": 43}, {"type": "text", "text": "First, note that if $b=a$ for any $a\\in[-1,1]$ and $\\bar{U}_{1,i}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\ng_{a,a}(\\bar{U}_{1,i})=\\mathbb{E}_{\\{\\mathbf{u}_{i^{\\prime}}\\}_{i^{\\prime}\\in[n]}}\\left[\\frac{(1-a^{2})e^{3c a}(\\alpha_{1}-\\alpha_{i})}{((\\alpha_{1}+\\alpha_{i})e^{c a}+\\sum_{i^{\\prime}\\in[n]\\backslash\\{1,i\\}}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}_{\\alpha_{i^{\\prime}}})^{3}}}\\right]\\geq0\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "since each term inside the expectation is nonnegative, as $a^{2}\\leq1$ and $\\alpha_{1}>\\alpha_{i}$ . Note that this implies $G_{a,b}\\geq0$ when $a=b$ , so WLOG we consider $b\\neq a$ for the remainder of the proof. Now we focus on showing (61). Throughout, we will make use of the notation ", "page_idx": 44}, {"type": "equation", "text": "$$\nd_{a,b}:=e^{c a}\\alpha_{1}+e^{c b}\\alpha_{i}+\\sum_{i^{\\prime}\\in[n]\\setminus\\{1,i\\}}e^{c\\mathbf{u}_{i^{\\prime}}^{\\top}\\mathbf{u}}\\alpha_{i^{\\prime}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which represents the cube root of the denominator in all terms when $\\mathbf{u}_{1}^{\\top}\\mathbf{u}=a$ and $\\mathbf{u}_{i}^{\\top}\\mathbf{u}=b$ and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\gamma_{a,b}:=1-a b+a-b.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using this notation, we can rewrite ", "page_idx": 44}, {"type": "equation", "text": "$$\ng_{a,b}(\\bar{U}_{1,i})=e^{c(a+b)}\\frac{e^{c a}\\gamma_{b,a}\\alpha_{1}-e^{c b}\\gamma_{a,b}\\alpha_{i}}{d_{a,b}^{3}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{a,b}(\\bar{U}_{1,i})+g_{b,a}(\\bar{U}_{1,i})}\\\\ &{\\;=e^{c(a+b)}\\frac{e^{c a}\\gamma_{b,a}\\alpha_{1}-e^{c b}\\gamma_{a,b}\\alpha_{i}}{d_{a,b}^{3}}+e^{c(a+b)}\\frac{e^{c b}\\gamma_{a,b}\\alpha_{1}-e^{c a}\\gamma_{b,a}\\alpha_{i}}{d_{b,a}^{3}}}\\\\ &{\\;=e^{c(a+b)}d_{a,b}^{-3}d_{b,a}^{-3}\\Big(\\alpha_{1}\\left(e^{c a}\\gamma_{b,a}d_{b,a}^{3}+e^{c b}\\gamma_{a,b}d_{a,b}^{3}\\right)-\\alpha_{i}\\left(e^{c a}\\gamma_{b,a}d_{a,b}^{3}+e^{c b}\\gamma_{a,b}d_{b,a}^{3}\\right)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Note that $e^{c(a+b)}d_{a,b}^{-3}d_{b,a}^{-3}>0$ This term can be rearranged as: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{1}\\left(e^{c a}\\gamma_{b,a}d_{b,a}^{3}+e^{c b}\\gamma_{a,b}d_{a,b}^{3}\\right)-\\alpha_{i}\\left(e^{c a}\\gamma_{b,a}d_{a,b}^{3}+e^{c b}\\gamma_{a,b}d_{b,a}^{3}\\right)}\\\\ &{=(\\alpha_{1}-\\alpha_{i})\\left(e^{c a}\\gamma_{b,a}d_{b,a}^{3}+e^{c b}\\gamma_{a,b}d_{a,b}^{3}\\right)}\\\\ &{\\quad+\\alpha_{i}\\left(e^{c a}\\gamma_{b,a}d_{b,a}^{3}+e^{c b}\\gamma_{a,b}d_{a,b}^{3}-e^{c a}\\gamma_{b,a}d_{a,b}^{3}-e^{c b}\\gamma_{a,b}d_{b,a}^{3}\\right)}\\\\ &{=\\underbrace{\\left(\\alpha_{1}-\\alpha_{i}\\right)\\left(e^{c a}\\gamma_{b,a}d_{b,a}^{3}+e^{c b}\\gamma_{a,b}d_{a,b}^{3}\\right)}_{=:T_{1}}+\\underbrace{\\alpha_{i}\\left(d_{b,a}^{3}-d_{a,b}^{3}\\right)\\left(e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}\\right)}_{=:T_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "First we show that $T_{1}$ is positive by analyzing $\\gamma_{a,b}$ and $^{\\gamma_{b,a}}$ . For any $(a,b)\\in[-1,1]^{2}$ such that $a\\neq b$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial b}(\\gamma_{a,b})=\\frac{\\partial}{\\partial b}(1-a b+a-b)=-1-a\\leq0\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "with equality holding if and only if $a=-1$ .If $a=-1$ , we have $\\gamma_{a,b}=1+b-1-b=0$ for all $b\\in[-1,1]$ . Otherwise, (52) shows that $\\gamma_{a,b}$ is strictly decreasing with $b$ , so it is minimized over $b\\in[-1,1]$ at $b=1$ . When $b=1$ , we have $\\gamma_{a,b}=1-a+a-1=0$ for all $a$ . So, $\\gamma_{a,b}\\geq0$ with equality holding if and only if $a=-1$ or $b=1$ . Note that by symmetry, this implies $\\gamma_{b,a}\\geq0$ with equality holding if and only if $a=1$ or $b=-1$ . So, we can have both $\\gamma_{a,b}=0$ and $\\gamma_{b,a}=0$ if and only if $a=b=-1$ or $a=b=1$ . However, we have $a\\neq b$ , so at least one of $\\gamma_{a,b}$ and $^{\\gamma_{b,a}}$ are strictly positive, and $T_{1}$ is strictly positive (using also that $\\alpha_{1}>\\alpha_{i}$ ", "page_idx": 44}, {"type": "text", "text": "We next show that $T_{2}$ is positive. Observe that ", "page_idx": 44}, {"type": "equation", "text": "$$\nd_{b,a}^{3}-d_{a,b}^{3}>0\\iff b>a\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "since $\\alpha_{1}>\\alpha_{i}$ , so it remains to show ", "page_idx": 44}, {"type": "equation", "text": "$$\nb>a\\implies e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}>0.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where ", "page_idx": 44}, {"type": "equation", "text": "$$\ne^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}=e^{c a}(1-a b-a+b)-e^{c b}(1-a b+a-b).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We first show the forward direction, namely $b>a\\implies e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}>0$ ", "page_idx": 44}, {"type": "text", "text": "Note that if $b=a$ \uff0c $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}=0$ . So, if we can show that for any fixed $a$ $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}$ is increasing with $b$ as long as $b\\geq a$ , then we will have $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}>0$ for $b>a$ . To show $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}$ is increasing, we take its partial derivative with respet t $b$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial b}\\left(e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}\\right)=e^{c a}(1-a)+e^{c b}(1+a+c b-c a-c+c a b)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We would like to show that the RHS of (56) is nonnegative. To do so, we show that its partial derivative with respect to $a$ is positive, so it achieves minimum value at $a=-1$ , at which point the value is positive. We have: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial}{\\partial a}\\left(\\frac{\\partial}{\\partial b}\\left(e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}\\right)\\right)=e^{c a}(c-c a-1)+e^{c b}(1-c+c b)}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle=q(b)-q(a)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $q(x):=e^{c x}(1+c x-c)$ . Note that $q(x)$ is monotonically increasing in $x\\in[-1,1]$ ; to see this, observe that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x}q(x)=e^{c x}(1+c x-c)c+e^{c x}c=e^{c x}(2+c x-c)c\\geq0\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the inequality follows since $c\\in\\left(0,2\\right]$ and $x\\in[-1,1]$ . Therefore, since $b>a$ , we have q(b) - q(a) \u22650 and  ( $\\begin{array}{r}{\\frac{\\partial}{\\partial a}\\left(\\frac{\\partial}{\\partial b}\\left(e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}\\right)\\right)\\ge0}\\end{array}$ from (57). As a result, $\\begin{array}{r}{\\frac{\\partial}{\\partial b}\\left(e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}\\right)}\\end{array}$ achieves minimum value at $a=-1$ . At this point, using (56) we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial}{\\partial b}\\left(e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}\\right)=2e^{-c}+e^{c b}(c b+c-c-c b)}}\\\\ {{\\displaystyle=2e^{-c}}}\\\\ {{\\displaystyle>0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This implies that the minimum value of $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}$ over $b\\in[a,1]$ is achieved at $b=a$ , and we know this value is zero, so we have that $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}>0$ when $b-a$ ", "page_idx": 45}, {"type": "text", "text": "To show the backward direction of (54), namely $e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}>0\\implies b>a$ note that the converse, namely $a>b\\implies e^{c a}\\gamma_{b,a}-e^{c b}\\gamma_{a,b}<0$ , follows by the same argument as above with $a$ and $b$ swapped. Therefore, we have $T_{2}>0$ as desired. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Lemma H.4. Consider any $\\begin{array}{r}{\\alpha:=\\left[\\alpha_{1},\\alpha_{2}\\right]}\\end{array}$ such that $\\alpha_{1}>\\alpha_{2}>0$ Further, let $c\\in(0,1]$ . Define ", "page_idx": 45}, {"type": "equation", "text": "$$\nH_{n o i s e}(\\mathbf{u},\\boldsymbol{\\alpha}):=\\mathbb{E}_{\\mathbf{u}_{1},\\mathbf{u}_{2}}\\left[\\frac{e^{2c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}^{2}+e^{2c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}^{2}}{(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2})^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\partial H_{n o i s e}(\\mathbf{u},\\alpha)}{\\partial\\alpha_{1}}>0\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. We have ", "page_idx": 45}, {"type": "equation", "text": "$$\nH_{\\mathrm{noise}}(\\mathbf{u},\\alpha):=\\mathbb{E}_{\\mathbf{u}_{1},\\mathbf{u}_{2}}\\left[\\frac{e^{2c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}^{2}+e^{2c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}^{2}}{\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}\\right)^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Since $n=2$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial{H_{\\mathrm{noise}}(\\mathbf{u},\\boldsymbol{\\alpha})}}{\\partial\\alpha_{1}}=\\mathbb{E}_{\\mathbf{u}_{1},\\mathbf{u}_{2}}\\left[\\frac{\\partial}{\\partial\\alpha_{1}}\\frac{e^{2c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}^{2}+e^{2c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}^{2}}{\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}\\right)^{2}}\\right]}\\\\ &{\\phantom{\\frac{\\partial{H_{\\mathrm{noise}}(\\mathbf{u},\\mathbf{u}_{2})}}{\\partial\\alpha_{1}}=\\mathbb{E}_{\\mathbf{u}_{1},\\mathbf{u}_{2}}\\left[\\frac{2e^{2c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}\\right)^{2}}{\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}\\right)^{4}}\\right]}\\\\ &{\\phantom{\\frac{\\partial{H_{\\mathrm{noise}}(\\mathbf{u},\\mathbf{u}_{2})}}{\\partial\\alpha_{1}}=\\mathbb{E}_{\\mathbf{u}_{1},\\mathbf{u}_{2}}\\left[\\frac{2\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}\\right)e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\left(e^{2c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}^{2}+e^{2c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{2}^{2}\\right)}{\\left(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2}\\right)^{4}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n=2\\alpha_{2}\\mathbb{E}_{\\mathbf{u}_{1},\\mathbf{u}_{2}}\\left[\\frac{e^{c(\\mathbf{u}_{1}^{\\top}\\mathbf{u}+\\mathbf{u}_{2}^{\\top}\\mathbf{u})}(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}-e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2})}{(e^{c\\mathbf{u}_{1}^{\\top}\\mathbf{u}}\\alpha_{1}+e^{c\\mathbf{u}_{2}^{\\top}\\mathbf{u}}\\alpha_{2})^{3}}\\right]\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Define N := Eu,u2 $\\begin{array}{r}{N:=\\mathbb{E}_{{\\mathbf{u}}_{1},{\\mathbf{u}}_{2}}\\left[\\frac{e^{c({\\mathbf{u}}_{1}^{\\top}{\\mathbf{u}}+{\\mathbf{u}}_{2}^{\\top}{\\mathbf{u}})}(e^{c{\\mathbf{u}}_{1}^{\\top}{\\mathbf{u}}}\\alpha_{1}-e^{c{\\mathbf{u}}_{2}^{\\top}{\\mathbf{u}}}\\alpha_{2})}{(e^{c{\\mathbf{u}}_{1}^{\\top}{\\mathbf{u}}}\\alpha_{1}+e^{c{\\mathbf{u}}_{2}^{\\top}{\\mathbf{u}}}\\alpha_{2})^{3}}\\right]}\\end{array}$ and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d_{a,b}:=e^{c a}\\alpha_{1}+e^{c b}\\alpha_{2}}}\\\\ {{h_{a,b}:=e^{c(a+b)}\\displaystyle\\frac{e^{c a}\\alpha_{1}-e^{c b}\\alpha_{i}}{d_{a,b}^{3}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Now, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{V}:=\\displaystyle\\int_{-1}^{1}\\int_{-1}^{1}h_{n,\\beta}\\|\\tilde{p}_{n}\\mathrm{d}\\boldsymbol{\\Lambda}\\,}&{}\\\\ {=\\displaystyle\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}\\big(h_{n,\\beta}+h_{n,\\epsilon}\\big)_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}\\big(h_{n,\\beta}+h_{n,\\epsilon}\\big)_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}\\,}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}\\big(h_{n,\\beta}+h_{n,\\epsilon}\\big)_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\Big[\\epsilon\\,\\boldsymbol{\\mu}^{*}\\ b\\big]\\,\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\bigg\\{\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}\\big(h_{n,\\beta}+h_{n,\\epsilon}\\big)_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}(h_{n,\\beta}+h_{n,\\epsilon})_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\Big\\{\\epsilon\\,\\boldsymbol{\\mu}^{*}\\ b\\big\\}\\,\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\bigg\\{\\int_{-1}^{1}h_{n,\\epsilon}\\eta_{n,\\beta}^{2}\\mathrm{d}\\boldsymbol{\\Lambda}}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}(h_{n,\\beta}+h_{n,\\epsilon})_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\Big\\{\\epsilon\\,\\boldsymbol{\\mu}^{*}\\ b\\big\\}\\,\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\bigg\\{\\int_{-1}^{1}h_{n,\\epsilon}\\eta_{n,\\beta}^{2}\\mathrm{d}\\boldsymbol{\\Lambda}+\\frac{1}{2}\\int_{-1}^{1}h_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}\\,}\\\\ &{=\\displaystyle\\frac{1}{2}\\int_{-1}^{1}\\int_{-1}^{1}\\big(h_{n,\\beta}+h_{n,\\epsilon}\\big)_{n,\\beta}\\mathrm{d}\\boldsymbol{\\Lambda}\\,\\bigg\\{\\epsilon\\,\\boldsymbol{\\mu}^ \n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where ", "page_idx": 46}, {"type": "equation", "text": "$$\nH_{a,b}:=p_{a}p_{b}(h_{a,b}+h_{b,a})+\\frac{p_{a}^{2}}{2}h_{a,a}+\\frac{p_{b}^{2}}{2}h_{b,b}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We will show that for any $(a,b)\\in[-1,1]^{2}$ and $(p_{a},p_{b})\\in[0,1]^{2},$ ${\\cal H}_{a,b}$ is positive, which implies that $N_{i}$ is positive by (59). To do this, assuming $h_{a,a}$ is nonnegative for any $a$ , it is sufficient to show ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\tilde{H}_{a,b}:=h_{a,b}+h_{b,a}+\\sqrt{h_{a,a}h_{b,b}}>0,\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "since this implies $h_{a,b}+h_{b,a}>-\\sqrt{h_{a,a}h_{b,b}}$ and thus, from (60), ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{H_{a,b}>-p_{a}p_{b}\\sqrt{h_{a,a}h_{b,b}}+\\frac{p_{a}^{2}}{2}h_{a,a}+\\frac{p_{b}^{2}}{2}h_{b,b}}}\\\\ &{=\\left(p_{a}\\sqrt{\\frac{h_{a,a}}{2}}-p_{b}\\sqrt{\\frac{h_{b,b}}{2}}\\right)^{2}}\\\\ &{\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Before showing (61), we need to confirm that $h_{a,a}$ is not negative for all $a\\in[-1,1]$ .Wehave ", "page_idx": 46}, {"type": "equation", "text": "$$\nh_{a,a}=\\frac{e^{3c a}(\\alpha_{1}-\\alpha_{2})}{d_{a,a}^{3}}\\geq0\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "since each term inside the expectation is nonnegative, as $\\alpha_{1}>\\alpha_{2}$ . Note that this implies $H_{a,b}\\geq0$ when $a=b$ soWLOGweconsider $a>b$ for the remainder of the proof. ", "page_idx": 47}, {"type": "text", "text": "Note that ", "page_idx": 47}, {"type": "equation", "text": "$$\nh_{a,a}h_{b,b}=\\frac{e^{3c(a+b)}(\\alpha_{1}-\\alpha_{i})^{2}}{e^{3c(a+b)}(\\alpha_{1}+\\alpha_{2})^{6}}=\\frac{(\\alpha_{1}-\\alpha_{2})^{2}}{(\\alpha_{1}+\\alpha_{2})^{6}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using this, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{H}_{a,b}=h_{a,b}+h_{b,a}+\\sqrt{h_{a,a}h_{b,b}}}\\\\ &{\\qquad=\\frac{e^{2c a+c b}\\alpha_{1}-e^{2c b+c a}\\alpha_{2}}{d_{a,b}^{3}}+\\frac{e^{2c b+c a}\\alpha_{1}-e^{2c a+c b}\\alpha_{2}}{d_{b,a}^{3}}+\\frac{\\alpha_{1}-\\alpha_{2}}{\\left(\\alpha_{1}+\\alpha_{2}\\right)^{3}}}\\\\ &{\\qquad=d_{a,b}^{-3}d_{b,a}^{-3}e^{c\\left(a+b\\right)}\\left(\\alpha_{1}+\\alpha_{2}\\right)^{3}}\\\\ &{\\qquad\\quad\\times\\left(\\underbrace{\\left(e^{c a}\\alpha_{1}-e^{c b}\\alpha_{2}\\right)d_{b,a}^{3}\\left(\\alpha_{1}+\\alpha_{2}\\right)^{3}+\\left(e^{c b}\\alpha_{1}-e^{c a}\\alpha_{2}\\right)d_{a,b}^{3}\\left(\\alpha_{1}+\\alpha_{2}\\right)^{3}}_{=:P}}\\\\ &{\\qquad\\qquad\\qquad\\underbrace{+e^{-c\\left(a+b\\right)}d_{a,b}^{3}d_{b,a}^{3}\\left(\\alpha_{1}-\\alpha_{2}\\right)}_{=:P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "To show that $\\tilde{H}_{a,b}$ is positive, we need to show that $P$ is positive. Without loss of generality we can consider $\\alpha_{1}=1$ and $\\alpha_{2}\\in(0,1)$ by dividing the numerator and denominator of $H_{\\mathrm{noise}}$ by $\\alpha_{1}^{2}$ . Thus, for the remainder of the proof we treat $\\alpha_{1}$ as 1 and write $\\alpha:=\\alpha_{2}$ for ease of notation. Using this notation we can expand $P$ as follows: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P=(e^{-\\alpha}-e^{\\alpha})\\phi_{1}\\phi_{3}^{*}(1+\\alpha)^{3}+(e^{\\beta}-e^{\\alpha})\\omega_{3}^{*}(1+\\alpha)^{3}+e^{-\\alpha(\\alpha+\\beta)}\\phi_{3}^{*}(\\bar{u}_{3}^{*})(1-\\alpha)}\\\\ &{=(e^{-\\alpha}e^{\\alpha})\\omega_{2}(e^{\\beta}+e^{\\alpha}\\alpha^{3})^{3}(1+\\alpha)^{3}+(e^{\\beta}-e^{\\alpha})(e^{\\alpha}+e^{\\beta}\\alpha)^{3}(1+\\alpha)^{3}}\\\\ &{\\quad+e^{-(\\alpha+\\beta)}(e^{\\alpha}+e^{\\alpha}\\alpha^{3})^{3}(e^{\\beta}+e^{\\alpha}\\alpha)^{3}(1-\\alpha)}\\\\ &{=(e^{\\alpha}-e^{\\alpha})+e^{\\alpha}\\phi_{1}^{*}(-\\alpha)\\left(\\alpha^{3}(1-\\alpha)\\right)}\\\\ &{\\quad+(e^{\\alpha}+e^{\\alpha})\\left(-\\alpha-5\\alpha^{3}+5\\alpha^{4}+\\alpha^{6}\\right)}\\\\ &{\\quad+(e^{5\\alpha+\\beta}+e^{5\\alpha+\\alpha})\\left(1+6\\alpha+10\\alpha^{3}-10\\alpha^{4}-6\\alpha^{6}-\\alpha^{7}\\right)}\\\\ &{\\quad+e^{2\\alpha+2\\beta}\\left(1+5\\alpha+27\\alpha^{2}+3\\alpha^{3}-3\\alpha^{4}-27\\alpha^{5}-56\\alpha^{2}-\\alpha^{7}\\right)}\\\\ &{=(1-\\alpha)\\times\\left((e^{6\\alpha-\\beta}+e^{5\\alpha-\\alpha})\\alpha^{3}\\right.}\\\\ &{\\quad+\\left.(e^{4\\alpha}+e^{4\\beta})(-\\alpha-\\alpha^{2}-6\\alpha^{3}-\\alpha^{4}-\\alpha^{5})\\right.}\\\\ &{\\quad+\\left.(e^{5\\alpha+\\beta}+e^{3\\alpha+\\alpha})\\left(1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+7\\alpha^{5}+\\alpha^{6}\\right)}\\\\ &{\\quad+e^{2\\alpha+2\\beta}\\left(1+6\\alpha+3\\alpha^{2}+36\\alpha^{3}+3\\alpha^{4}+6\\alpha^{5}+\\alpha^{6}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Recall that $1-\\alpha>0$ , so we need to show that the sum of the remaining terms is positive. These terms can be written as a polynomial in $y:=e^{c(a-b)}$ as folows: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P(1-\\alpha)^{-1}e^{c a-5c b}=y^{6}\\alpha^{3}}&{}\\\\ {\\qquad\\qquad+y^{5}\\left(-\\alpha-\\alpha^{2}-6\\alpha^{3}-\\alpha^{4}-\\alpha^{5}\\right)}\\\\ {\\qquad\\qquad+y^{4}\\left(1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+7\\alpha^{5}+\\alpha^{6}\\right)}\\\\ {\\qquad\\qquad+y^{3}\\left(1+6\\alpha+33\\alpha^{2}+36\\alpha^{3}+33\\alpha^{4}+6\\alpha^{5}+\\alpha^{6}\\right)}\\\\ {\\qquad\\qquad+y^{2}\\left(1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+7\\alpha^{5}+\\alpha^{6}\\right)}\\\\ {\\qquad\\qquad+y\\left(-\\alpha-\\alpha^{2}-6\\alpha^{3}-\\alpha^{4}-\\alpha^{5}\\right)}\\\\ {\\qquad\\qquad+\\alpha^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Weknowthat $y^{6}>y^{5}>\\cdots>1$ since $a>b$ We also have that $\\alpha<1$ . Using these facts we next show that the sum of the third and smaller-order terms in the RHS of (66) is positive. ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(*):=y^{3}\\left(1+6\\alpha+33\\alpha^{2}+36\\alpha^{3}+33\\alpha^{4}+6\\alpha^{5}+\\alpha^{6}\\right)}&{}\\\\ {\\qquad+y^{2}\\left(1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+7\\alpha^{5}+\\alpha^{6}\\right)}\\\\ {\\qquad+y\\left(-\\alpha-\\alpha^{2}-6\\alpha^{3}-\\alpha^{4}-\\alpha^{5}\\right)}\\\\ {\\qquad+\\alpha^{3}}\\\\ {>y\\left(1+6\\alpha+33\\alpha^{2}+36\\alpha^{3}+33\\alpha^{4}+6\\alpha^{5}+\\alpha^{6}\\right)}\\\\ {\\qquad+y\\left(1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+7\\alpha^{5}+\\alpha^{6}\\right)}\\\\ {\\qquad+y\\left(-\\alpha-\\alpha^{2}-6\\alpha^{3}-\\alpha^{4}-\\alpha^{5}\\right)}\\\\ {\\qquad+\\alpha^{3}}\\\\ {>y\\left(2+12\\alpha+39\\alpha^{2}+47\\alpha^{3}+39\\alpha^{4}+12\\alpha^{5}+1\\alpha^{6}\\right)}\\\\ {\\qquad0}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Next we show that the sum of the sixth-, fifth, and fourth-order terms is positive. Let $a_{6}:=\\alpha^{3}$ $a_{5}:=\\alpha+\\alpha^{2}+6\\alpha^{3}+\\alpha^{4}+\\alpha^{5}$ , and $a_{4}:=1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+\\dot{7}\\alpha^{5}+\\alpha^{6}$ , so the sum of the sixth-, fifth-, and fourth-order terms is $y^{6}a_{6}-y^{5}a_{5}+y^{4}a_{4}$ . Note that $32a_{6}<a_{4}$ since $\\alpha<1$ and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{5}-4a_{6}=\\alpha+\\alpha^{2}+2\\alpha^{3}+\\alpha^{4}+\\alpha^{5}}\\\\ &{\\phantom{a a a a a a}=\\frac{1}{7.5}\\left(7.5\\alpha+7.5\\alpha^{2}+15\\alpha^{3}+7.5\\alpha^{4}+7.5\\alpha^{5}\\right)}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}<\\frac{1}{7.5}\\left(1+7\\alpha+7\\alpha^{2}+17\\alpha^{3}+7\\alpha^{4}+7\\alpha^{5}+\\alpha^{6}\\right)}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}=\\frac{a_{4}}{7.5}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "thus $\\begin{array}{r}{a_{5}<\\frac{a_{4}}{7.5}+4a_{6}}\\end{array}$ . Also, $y=e^{c(a-b)}\\leq e^{2}<7.5$ since $c\\leq1$ . Therefore, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{y^{\\delta}a_{6}-y^{5}a_{5}+y^{4}a_{4}=y^{4}\\left(y^{2}a_{6}-y a_{5}+a_{4}\\right)}&{}\\\\ {>y^{4}\\left(y^{2}a_{6}-4y a_{6}-y\\frac{a_{4}}{7.5}+a_{4}\\right)}&{}\\\\ {>y^{4}\\left(y^{2}a_{6}-4y a_{6}+a_{4}\\left(1-\\frac{y}{7.5}\\right)\\right)}&{}\\\\ {>y^{6}\\left(y^{2}a_{6}-4y a_{6}+a_{4}\\left(1-\\frac{y}{7.5}\\right)\\right)}&{}\\\\ {>y^{4}\\left(y^{2}a_{6}-4y a_{6}+32a_{6}\\left(1-\\frac{y}{7.5}\\right)\\right)}&{}\\\\ {=y^{4}a_{6}\\left(y^{2}-\\frac{62}{7.5}y+32\\right)}&{}\\\\ {>y^{4}a_{6}\\left(-\\frac{1}{4}\\left(\\frac{62}{7.5}\\right)^{2}+32\\right)}&{}\\\\ {>0}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where (68) follows by minimizing the terms inside the parentheses over $y$ .Thus, we have $\\tilde{H}_{a,b}>0$ which completes the proof. ", "page_idx": 48}, {"type": "text", "text": "Now we can finally prove Theorem 4.4. We prove a slightly stronger result, formally stated as follows. Theorem H.5. Consider any $\\mathbf{B}\\in\\mathbb{O}^{d\\times k}$ and the corresponding function class $\\mathcal{F}_{\\mathbf{B}}^{l i n}$ as defned in (4.2). Suppose tasks are drawn from $D(\\mathcal{F}_{\\mathbf{B}}^{l i n})$ and Assumption 4.3 holds. Recallthe pretraining population loss: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{M})=\\mathbb{E}_{f,\\{\\mathbf{x}_{i}\\}_{i\\in[n+1]},\\{\\epsilon_{i}\\}_{i\\in[n]}}\\left[\\left(\\frac{\\sum_{i=1}^{n}(f(\\mathbf{x}_{i})-f(\\mathbf{x}_{n+1})+\\epsilon_{i})e^{\\mathbf{x}_{i}^{\\top}\\mathbf{M}\\mathbf{x}_{n+1}}}{\\sum_{i=1}^{n}e^{\\mathbf{x}_{i}^{\\top}\\mathbf{M}\\mathbf{x}_{n+1}}}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Consider two cases: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 Case 1: $\\sigma=0$ $n>1$ . Then define $C_{p}:=2$ \u00b7 Case 2: $\\sigma>0$ $n=2$ . Then define $C_{p}:=1$ ", "page_idx": 49}, {"type": "text", "text": "Then in each case, among all $\\begin{array}{r}{\\mathbf{M}\\,\\in\\,\\mathcal{M}\\,:=\\,\\{\\mathbf{M}\\,\\in\\,\\mathbb{R}^{d\\times d}:\\,\\mathbf{M}\\,=\\,\\mathbf{M},\\|\\mathbf{B}^{\\top}\\mathbf{M}\\mathbf{B}\\|_{2}\\,\\le\\,\\frac{C_{p}}{c_{u}^{2}}\\}}\\end{array}$ any minimizer $\\mathbf{M}^{*}$ of(70) satisfies $\\mathbf{M}^{*}=c\\mathbf{B}\\mathbf{B}^{\\top}$ for some $\\begin{array}{r}{c\\in(0,\\frac{C_{p}}{c_{u}^{2}}]}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "Proof. From Lemma H.2, we have $\\mathbf{M}^{*}=c_{p}\\mathbf{B}\\mathbf{B}^{\\top}+\\tilde{c}m a t h b f B_{\\perp}\\mathbf{B}_{\\perp}^{\\top}$ for some $\\tilde{c}\\in\\mathbb{R}$ and some $\\begin{array}{r}{c_{p}\\in(0,\\frac{C_{p}}{c_{u}^{2}}]}\\end{array}$ , where $C_{p}=2$ in Case 1 and $C_{p}=1$ in Case 2. Suppose that $\\tilde{c}\\neq0$ . Then it remains to show that $\\bar{\\mathcal{L}}(c_{p}\\mathbf{B}\\mathbf{B}^{\\top}+\\tilde{c}\\mathbf{B}_{\\perp}\\mathbf{B}_{\\perp}^{\\top})>\\mathcal{L}(c_{p}\\mathbf{B}\\mathbf{B}^{\\top})$ ", "page_idx": 49}, {"type": "text", "text": "We start by establishing the same notations as in the proof of Lemma H.2. For each $i\\in[n+1]$ $\\mathbf{x}_{i}=c_{u}B\\mathbf{u}_{i}+c_{v}\\mathbf{B}_{\\perp}\\mathbf{v}_{i}$ .Thus, for each $i\\in[n]$ ,wehave ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e^{\\mathbf{x}_{i}^{\\top}\\mathbf{M}\\mathbf{x}_{n+1}}=e^{c_{p}\\mathbf{x}_{i}^{\\top}\\mathbf{B}\\mathbf{B}^{\\top}\\mathbf{x}_{n+1}}e^{c^{\\prime}\\mathbf{x}_{i}^{\\top}\\mathbf{B}_{\\bot}\\mathbf{B}_{\\bot}^{\\top}\\mathbf{x}_{n+1}}}\\\\ &{\\qquad\\qquad=e^{c_{p}c_{u}^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{n+1}}e^{c_{v}^{2}\\tilde{c}\\mathbf{v}_{i}^{\\top}\\mathbf{v}_{n+1}}}\\\\ &{\\qquad\\qquad=e^{c_{p}c_{u}^{2}\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{n+1}}\\alpha_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where, for each $i\\in[n]$ \uff0c $\\alpha_{i}:=e^{c_{v}^{2}\\tilde{c}\\mathbf v_{i}^{\\top}\\mathbf v_{n+1}}$ For ease of notation, dente $\\mathbf{x}=\\mathbf{x}_{n+1}$ $\\mathbf{u}:=\\mathbf{u}_{n+1}$ and $c=c_{p}c_{u}^{2}$ Also,not tha for any $\\mathbf{x}_{i}$ \uff0c\uff0c $f(\\mathbf{x}_{i})=\\mathbf{a}^{\\top}\\mathbf{B}^{\\top}\\mathbf{x}_{i}=c_{u}\\mathbf{a}^{\\top}\\mathbf{u}_{i}$ and that drawing $f\\sim D(\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{lin}})$ is equivalent to drawing $\\mathbf{a}\\sim D_{\\mathbf{a}}$ for some distribution $D_{\\mathbf{a}}$ over $\\mathbb{R}^{k}$ such that $\\mathbb{E}_{\\mathbf{a}\\sim D_{\\mathbf{a}}}[\\mathbf{a}\\mathbf{a}^{T}]=c_{a}^{2}\\mathbf{I}_{k}$ Using this, we have: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(c_{p}\\mathbf{B}\\mathbf{B}^{\\top}++\\bar{c}\\mathbf{B}_{\\bot}\\mathbf{B}_{\\bot}^{\\top})}\\\\ &{=\\mathbb{E}_{\\mathbf{a},\\mathbf{u}_{\\parallel},\\{\\mathbf{u}_{\\bot}\\},\\{\\alpha_{1}\\}_{\\ell}(n_{1}),\\xi_{1}(n_{1});\\ell_{1}(n_{1});\\ell_{1}(n_{1})}\\left[\\frac{\\left(\\sum_{i=1}^{n}(c_{\\mathbf{a}}\\mathbf{a}^{\\top}\\mathbf{u}_{i}-c_{\\mathbf{a}}\\mathbf{a}^{\\top}\\mathbf{u}+c_{i})e^{c_{\\mathbf{a}}\\top}\\mathbf{u}\\cdot\\mathbf{a}_{i}\\right)^{2}}{(\\sum_{i=1}^{n}e^{c_{\\mathbf{a}}\\top}\\mathbf{u}_{i})^{2}}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbf{a},\\{\\mathbf{u}_{\\parallel},\\mathbf{c}_{\\{i\\}},\\{\\alpha_{1}\\}_{\\ell}(n_{1});\\ell_{1}(n_{1})\\}}}\\\\ &{\\qquad\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\mathbb{E}_{\\mathbf{a},\\{c}_{i}\\in[\\mathbf{a},\\mathbf{u}_{j}]}\\left([c_{\\mathbf{a}}\\mathbf{a}^{\\top}\\mathbf{u}_{i}-c_{\\mathbf{a}}\\mathbf{a}^{\\top}\\mathbf{u}+c_{i})(c_{\\mathbf{a}}\\mathbf{a}^{\\top}\\mathbf{u}_{j}-c_{\\mathbf{a}}\\mathbf{a}^{\\top}\\mathbf{u}+c_{j})[\\mathbf{e}^{c_{\\mathbf{a}}\\top}\\mathbf{u}+\\mathbf{a}_{i}^{\\top}\\mathbf{a}\\mathbf{a}_{i}^{\\top}]\\right.}{(\\sum_{i=1}^{n}e^{c_{\\mathbf{a}}\\top}\\mathbf{u})^{2}}\\right.}\\\\ &{=\\mathbb{E}_{\\mathbf{a},\\{\\mathbf{u}_{\\parallel},\\mathbf{c}_{\\{i\\}},\\{\\alpha_{1}\\}_{\\ell}(n_{1})}}(\\sum_{i=1}^{n}\\mathbf{(a}_{j}-\\mathbf{u})e^{c_{\\mathbf{a}}\\top}\\mathbf{u}+\\mathbf{c}_{i}^{\\top}\\mathbf{a}_{i})^{\\top}\\mathbf{a}_{i \n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\pmb{\\alpha}:=[\\alpha_{1},\\ldots,\\alpha_{n}]$ and ", "page_idx": 49}, {"type": "equation", "text": "$$\nH(\\mathbf{u},\\alpha)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n:=\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[c_{a}^{2}c_{u}^{2}\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{u})e^{\\mathbf{cu}_{i}^{\\top}\\mathbf{u}+\\mathbf{cu}_{j}^{\\top}\\mathbf{u}}\\alpha_{i}\\alpha_{j}}{(\\sum_{i=1}^{n}e^{\\mathbf{cu}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}+\\sigma^{2}\\frac{\\sum_{i=1}^{n}e^{2\\mathbf{cu}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}^{2}}{(\\sum_{i=1}^{n}e^{\\mathbf{cu}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Define $\\pmb{\\alpha}^{*}=[1,\\dots,1]\\in\\mathbb{R}^{n}$ .We proceed by showing that for any $\\mathbf{u}\\in\\mathbb{S}^{d-1}$ , all $\\alpha\\in\\mathbb{R}_{+}^{n}$ satisfy ", "page_idx": 49}, {"type": "text", "text": "This implies $\\mathcal{L}(c_{p}\\mathbf{B}\\mathbf{B}^{\\top}+\\tilde{c}\\mathbf{B}_{\\perp}\\mathbf{B}_{\\perp}^{\\top})>\\mathcal{L}(c_{p}\\mathbf{B}\\mathbf{B}^{\\top})$ , since ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\alpha}\\big(\\{\\alpha=c^{\\prime}\\alpha^{*}\\mathrm{~for~some~}c^{\\prime}\\in\\mathbb{R}_{+}\\}\\big)=1\\iff\\tilde{c}=0,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "which implies that $\\tilde{c}=0$ is the unique argument that achieves the minimal value of $\\mathcal{L}(c_{p}\\mathbf{B}\\mathbf{B}^{\\top}+$ $\\tilde{c}\\mathbf{B}_{\\perp}\\mathbf{B}_{\\perp}^{\\top},$ over $\\tilde{c}\\in\\mathbb{R}$ (and this value is $\\mathbb{E}_{\\mathbf{u}}\\left[H(\\mathbf{u},\\pmb{\\alpha}^{*})\\right])$ ", "page_idx": 50}, {"type": "text", "text": "Proving $(i)$ istrivialail $H(\\mathbf{u},\\alpha)=H(\\mathbf{u},c^{\\prime}\\alpha)$ for all $\\mathbf{u}\\in\\mathbb{S}^{d-1}$ \uff0c $\\alpha\\in\\mathbb{R}_{+}^{n}$ \uff0c and $c^{\\prime}\\in\\mathbb{R}_{+}$ ", "page_idx": 50}, {"type": "text", "text": "Proving $(i i)$ is more involved. Consider any $\\alpha\\neq c^{\\prime}\\alpha^{*}$ for any $c^{\\prime}\\in\\mathbb{R}_{+}$ . WLOG let $1\\in\\arg\\operatorname*{max}_{i}\\alpha_{i}$ We show that the partial derivative of $H(\\mathbf{u},\\alpha)$ with respect to $\\alpha_{1}$ is strictly positive, which means that $H(\\mathbf{u},\\alpha)$ can be reduced by reducing $\\alpha_{1}$ by some $\\epsilon>0$ . We can repeat this argument, repeatedly reducing $\\operatorname*{max}_{i}\\alpha_{i}$ at each step and thereby reducing the loss, until we reach an $\\alpha^{\\prime}$ satisfying $\\pmb{\\alpha}^{\\prime}=$ $c^{\\prime}\\alpha^{*}$ . Since the loss is reduced at each step, we have that $H(\\mathbf{u},\\alpha)>H(\\mathbf{u},\\alpha^{*})$ ", "page_idx": 50}, {"type": "text", "text": "To show that the partial derivative of $H(\\mathbf{u},\\alpha)$ with respect to $\\alpha_{1}$ is strictly positive, we decompose $\\begin{array}{r}{\\frac{\\partial H(\\mathbf{u},\\pmb{\\alpha})}{\\partial\\alpha_{1}}=\\frac{\\partial H_{\\mathrm{signal}}(\\mathbf{u},\\pmb{\\alpha})}{\\partial\\alpha_{1}}+\\frac{\\partial H_{\\mathrm{noise}}(\\mathbf{u},\\pmb{\\alpha})}{\\partial\\alpha_{1}}}\\end{array}$ aHnoise(u,\u03b1a) , where ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{signal}}(\\mathbf{u},\\alpha):=c_{a}^{2}c_{u}^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\mathbf{u}_{i}-\\mathbf{u})^{\\top}(\\mathbf{u}_{j}-\\mathbf{u})e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}+c\\mathbf{u}_{j}^{\\top}\\mathbf{u}}\\alpha_{i}\\alpha_{j}}{(\\sum_{i=1}^{n}e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right]}\\\\ &{H_{\\mathrm{noise}}(\\mathbf{u},\\alpha):=\\sigma^{2}\\mathbb{E}_{\\{\\mathbf{u}_{i}\\}_{i\\in[n]}}\\left[\\frac{\\sum_{i=1}^{n}e^{2c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i}^{2}}{(\\sum_{i=1}^{n}e^{c\\mathbf{u}_{i}^{\\top}\\mathbf{u}}\\alpha_{i})^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By Lemma H.3, we have $\\frac{\\partial H_{\\mathrm{signal}}(\\mathbf{u},\\pmb{\\alpha})}{\\partial\\alpha_{1}}\\,>\\,0$ > 0. If o = O we are done, otherwise we have n = 2 and oHmo(u.) > by Llema H.4. This completes the prof. \u53e3 ", "page_idx": 50}, {"type": "text", "text": "Additional Lemmas ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Lemma I.1. Consider a continuous unimodal function $f$ Thenwehave ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\infty}f(i)-\\operatorname*{max}f\\leq\\int_{0}^{\\infty}f(t)d t\\leq\\sum_{i=1}^{\\infty}f(i)+\\operatorname*{max}f\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof Let $T$ denote the point that achieves the maximum of $f$ Thenweknowthat $f(t)\\geq f(\\lfloor t\\rfloor)$ for $t<T$ While $f(t)\\geq f(\\lceil t\\rceil)$ for $t>T$ .Thismeans $\\begin{array}{r}{\\int_{i-1}^{i}f(t)d t\\le f(i)\\le\\int_{i}^{i+1}f(t)d t}\\end{array}$ for $t\\leq\\lfloor T\\rfloor$ and $\\begin{array}{r}{\\int_{i-1}^{i}f(t)d t\\geq f(i)\\geq\\int_{i}^{i+1}f(t)d t}\\end{array}$ for $t\\geq\\lceil T\\rceil$ So ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=0}^{\\infty}f(i)=\\sum_{i=0}^{[T]}f(i)+\\sum_{i=[T]}^{\\infty}f(i)}\\\\ {\\displaystyle~~~~~~~\\leq\\sum_{i=0}^{[T]}\\int_{i}^{i+1}f(i)d t+\\sum_{[T]}^{\\infty}\\int_{i-1}^{i}f(t)d t}\\\\ {\\displaystyle~~~~~~\\leq\\sum_{i=0}^{\\infty}\\int_{i}^{i+1}f(t)d t+\\int_{\\{T\\}}^{\\lceil T\\rceil}f(t)d t}\\\\ {\\displaystyle~~~~~~\\leq\\int_{0}^{\\infty}f(t)d t+\\operatorname*{max}_{\\ell}f}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Similarly we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{\\infty}f(i)=\\sum_{i=1}^{\\lfloor T\\rfloor}f(i)+\\sum_{i=\\lceil T\\rceil}^{\\infty}f(i)}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{\\lfloor T\\rfloor}\\int_{i-1}^{i}f(t)d t+\\displaystyle\\sum_{\\lceil T\\rceil}^{\\infty}\\int_{i}^{i+1}f(t)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{i=1}^{\\infty}\\int_{i-1}^{i}f(t)d t-\\int_{\\lfloor T\\rfloor}^{\\lceil T\\rceil}f(t)d t}\\\\ {\\displaystyle\\leq\\int_{0}^{\\infty}f(t)d t-\\operatorname*{max}f}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma I.2. If $f$ and $g$ are nonnegative measurable real functions, then ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\int f(x)g(x)d x\\leq\\int f^{*}(x)g^{*}(x)d x\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $f^{*},g^{*}$ are the symmetric decreasing rearrangements of $f$ and $g$ ", "page_idx": 51}, {"type": "text", "text": "Proof. Please see [66] or [67]. ", "page_idx": 51}, {"type": "text", "text": "Lemma I.3. Suppose $\\{a_{i}\\},\\{b_{i}\\}$ are sorted the same way, $a_{i}>a_{j}\\iff b_{i}>b_{j}$ . Then we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\sum a_{i}^{2}}{\\left(\\sum a_{i}\\right)^{2}}<\\frac{\\sum a_{i}^{2}b_{i}^{2}}{\\left(\\sum a_{i}b_{i}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. Cross multiplying and expanding, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{a_{i}^{2})(\\sum a_{i}b_{i})^{2}<(\\sum_{a_{i}}a_{i}^{2}b_{i}^{2})(\\sum a_{i})^{2}}}}\\\\ {{\\displaystyle\\iff\\sum_{i,j,k}a_{i}b_{i}a_{j}b_{j}a_{k}^{2}<\\sum_{i,j,k}a_{i}^{2}b_{i}^{2}a_{j}a_{k}}}\\\\ {{\\displaystyle\\iff\\frac{1}{3}\\sum_{i,j,k}a_{i}b_{i}a_{j}b_{j}a_{k}^{2}+a_{j}b_{j}a_{k}b_{k}a_{i}^{2}+a_{k}b_{k}a_{i}b_{i}a_{j}^{2}<\\frac{1}{3}\\sum_{i,j,k}a_{i}^{2}b_{i}^{2}a_{j}a_{k}+a_{j}^{2}b_{j}^{2}a_{k}a_{i}+a_{k}^{2}b_{k}^{2}a_{i}a_{j}}}\\\\ {{\\displaystyle\\iff\\frac{1}{3}\\sum_{i,j,k}a_{i}^{2}b_{i}^{2}a_{j}a_{k}+a_{j}^{2}b_{j}^{2}a_{k}a_{i}+a_{k}^{2}b_{k}^{2}a_{i}a_{j}-\\left(a_{i}b_{i}a_{j}b_{j}a_{k}^{2}+a_{j}b_{j}a_{k}b_{k}a_{i}^{2}+a_{k}b_{k}a_{i}b_{i}a_{j}^{2}\\right)>0}}\\\\ {{\\displaystyle\\iff\\frac{1}{3}\\sum_{i,j,k}a_{i}a_{j}a_{k}\\left(a_{i}b_{i}^{2}+a_{j}b_{j}^{2}+a_{k}b_{k}^{2}-a_{i}b_{j}b_{k}-a_{j}b_{k}b_{i}-a_{k}b_{i}b_{j}\\right)>0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "The last of which follows from the rearrangement inequality [67]. ", "page_idx": 51}, {"type": "text", "text": "J  Additional Experiments and Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "All experiments were run in Google Colab in a CPU runtime. We used a random seed of O in all cases. All training was executed in PyTorch with the Adam optimizer. We tuned learning rates in $\\{10^{-3},10^{-2},10^{-\\overline{{1}}}\\}$ separately for linear and softmax attention, andwe initilized $\\mathbf{M}_{K}$ and $\\mathbf{M}_{Q}$ by setting each to $0.001{\\bf I}_{d}$ , and tie the weights of ${{\\bf{M}}_{K}}$ and $\\mathbf{M}_{Q}$ to speed up training. ", "page_idx": 51}, {"type": "text", "text": "Figure 1. The upper row depicts our functions, which increase in Lipschitzness from left to right The black curve depicts the ground truth, while the gray dots depict the noisy training samples. The shaded region represents the attention window. The middle row depicts the attention weights for softmax and linear attention. We remark that the softmax is able to adapt to the Lipschitzness while linear is not. The bottom row depicts the ICL error as a function of the context length $n$ for Linear and ReLU pretraining using Linear and Softmax attention. That is, at each iteration, a context is drawn from a non-linear regression (defined below) consisting of a randomly phase shifted cosine function. The ICL task is to predict the function value at a randomly chosen query on the unit circle. Each point in the plot depicts the ICL error of a pretrained attention unit (using softmax (blue) or linear (orange) activation) at the end of 15000 iterations with learning rate $10^{-3}$ .Weuse $d=2$ and a distribution $D(\\mathcal{F}_{\\nu,\\mathrm{hills}})$ . Here we define ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\nu,\\mathrm{hills}}=\\left\\{\\nu\\cos\\left(\\theta-b\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and a distribution $D(\\mathcal{F}_{\\nu,\\mathrm{hills}})$ is induced by drawing $b$ uniformly from $[-\\pi,\\pi]$ Weuse $\\nu=0,1.5,6$ for the left, middle and right plots in the bottom row, respectively. ", "page_idx": 51}, {"type": "image", "img_path": "lfxIASyLxB/tmp/a80fd883584de9d2516308cd7a232df631f9d984c21c2fb559b9af159eabf7e0.jpg", "img_caption": ["Figure 9: Representation learning error $\\left(\\rho({\\bf M},{\\bf B})\\right)$ and test ICL error (mean squared error) during pretraining softmax and linear atntion on tasks from eft: $\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{aff}}$ Center: $\\mathcal{F}_{\\mathbf{B}}^{2}$ , and Right: $\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{cos}}$ "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "Figures 3, 4, 5. In all cases, we use an exponentially decaying learning rate schedule with factor 0.999. In Figures 3 and 5 we use initial learning rate 0.1 and in Figure 4 we use an initial learning rate 0.01. Moreover, in all cases besides those with varying $n$ in Figure 4, we compute gradients with respect to the ICL loss evaluated on $N:=\\lfloor{\\sqrt{n}}\\rfloor$ query samples per task (that is, each context input to the attention unit has $n+N$ samples, of which $n$ are labeled, and the other $N$ labels are inferred). When $n$ varies in Figure 4, we use $N=1$ . In Figure 5 we show smoothed test ICL errors with smoothing rate 0.01. ", "page_idx": 52}, {"type": "text", "text": "J.1 Low-Rank Experiments ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Due to our results in Section 3 showing that softmax attention can learn an appropriate attention window scale when pretrained on nonlinear tasks, we hypothesize that it can also learn the appropriate directions during pretraining on nonlinear tasks. To test this, we consider tasks drawn from lowrank versins of affnequaratic and csine fuction las, in pariular: ${\\mathcal{F}}_{\\mathbf{B}}^{\\mathrm{aff}}:=\\{f:f(\\mathbf{x})=$ $\\mathbf{a}^{\\top}\\mathbf{B}^{\\top}\\,\\mathbf{x}+2,\\mathbf{a}\\in\\mathbb{S}^{k-1}\\}$ ${\\mathcal{F}}_{\\mathbf{B}}^{2}:=\\{f:f({\\pmb x})=(\\mathbf{a}^{\\top}\\mathbf{B}^{\\top}\\,{\\pmb x})^{2},\\mathbf{a}\\in\\mathbb{S}^{k-1}\\}$ and ${\\mathcal{F}}_{\\mathbf{B}}^{\\mathrm{cos}}:=\\{f:f(\\mathbf{x})=$ $\\cos(4\\mathbf{a}^{\\top}\\mathbf{B}^{\\top}\\,\\mathbf{x}),\\mathbf{a}\\in\\mathbb{S}^{k-1}\\big\\}$ . Each task distribution $D(\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{aff}}),D(\\mathcal{F}_{\\mathbf{B}}^{2}),D(\\mathcal{F}_{\\mathbf{B}}^{\\mathrm{cos}})$ is induced by drawing $\\mathbf{a}\\sim\\mathcal{U}^{k}$ . We train $\\mathbf{M}_{K}$ and $\\mathbf{M}_{Q}$ with Adam with learning rate tuned separately for softmax and linear atention. We set $d=10$ $k=2$ $n=50$ and $\\sigma=0.01$ We draw $\\{{\\pmb x}_{i}\\}_{i=1}^{n+1}$ i.d.from anon-uniform distribution on $\\mathbb{S}^{d-1}$ for each task, and draw one task per training iteration. We draw $\\mathbf{B}$ randomly at the start of each trial, and repeat each trial 5 times and plots means and standard deviations over the 5 trials. We capture the extent to which the learned $\\mathbf{M}=\\mathbf{M}_{K}^{\\top}\\mathbf{M}_{Q}$ recovers $\\operatorname{col}(\\mathbf{B})$ via the metric $\\begin{array}{r}{\\rho(\\mathbf{M},\\mathbf{B}):=\\frac{\\|\\mathbf{B}_{\\perp}^{\\top}\\mathbf{M}\\mathbf{B}_{\\perp}\\|_{2}}{\\sigma_{\\operatorname*{min}}(\\mathbf{B}^{\\top}\\mathbf{M}\\mathbf{B})}}\\end{array}$ M, where min (A) is the minimum singular value of A.For tst err, we compute the average squared error on 500 random tasks drawn from the same distribution as the (pre)training tasks. Please see Appendix J for more details. ", "page_idx": 52}, {"type": "text", "text": "We randomly generate $\\mathbf{B}$ on each trial by first sampling each element of $\\hat{\\bf B}$ i.i.d. from the standard normal distribution, then take its QR decomposition to obtain $\\mathbf{B}$ . To draw the covariates, we draw a random matrx $\\tilde{\\mathbf{J}}\\in\\mathbb{R}^{d\\times d}$ by sampling each element i.i.d. from the standard normal distribution. Then, we compute $\\mathbf{J}=(\\tilde{\\mathbf{J}}^{\\top}\\tilde{\\mathbf{J}})^{1/2}$ . Then we draw $\\widetilde{\\pmb{x}}_{i}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{I}_{d})$ and set $\\begin{array}{r}{\\pmb{x}_{i}=\\frac{\\mathbf{J}\\tilde{\\pmb{x}}_{i}}{\\|\\mathbf{J}\\tilde{\\pmb{x}}_{i}\\|}}\\end{array}$ ", "page_idx": 52}, {"type": "text", "text": "Results. Figure 9 shows that softmax attention recovers the low-rank structure when tasks are drawn from each of the three function classes, which leads to test error improving with the quality of the learned subspace. In contrast, linear attention does not learn any meaningful structure in these cases. ", "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Our abstract is consistent with our introduction. In the introduction, we point to the places in the paper in which we substantiate our claims. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We provide a discussion in Section 5. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: Assumptions are specified before all theorem statements. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We provide details in Sections 3.2 and J. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Please see supplementary material. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We provide details in Sections 3.2 and J. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental seting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: Please see results in Sections 3.2 and J. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: Please see Section J. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: we have read the Code of Ethics and ensured conformity. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: This is primarily an analysis of an already existing algorithm. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 56}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faitheffort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Ganidelinec. ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]