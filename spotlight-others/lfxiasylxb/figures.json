[{"figure_path": "lfxIASyLxB/figures/figures_1_1.jpg", "caption": "Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights \u2013 which determine the attention window \u2013 as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details.", "description": "The figure shows how softmax attention adapts to the Lipschitzness of the target function in an in-context learning setting. The top row shows three example functions with varying Lipschitzness. The middle row displays the attention weights assigned by softmax and linear attention mechanisms, revealing how softmax attention dynamically adjusts its window size based on Lipschitzness. The bottom row illustrates how the In-context learning (ICL) error varies with the number of context samples for different settings, highlighting the superior performance of softmax attention when adapting to Lipschitzness.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_5_1.jpg", "caption": "Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights \u2013 which determine the attention window \u2013 as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details.", "description": "This figure shows how the softmax attention mechanism adapts its attention window to the Lipschitzness of the target function and the noise level in the training data.  The top row displays three regression tasks with increasing Lipschitzness. The middle row compares the attention weights assigned by the softmax and linear attention models. The bottom row shows the in-context learning (ICL) error, demonstrating that softmax attention achieves lower error by adapting its window size.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_6_1.jpg", "caption": "Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights \u2013 which determine the attention window \u2013 as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details.", "description": "This figure shows how the softmax attention mechanism adapts to different function Lipschitzness and noise levels in an in-context learning (ICL) setting. The top row illustrates three regression tasks with varying Lipschitzness, showing how the optimal attention window (the region of input space that influences the prediction) shrinks as Lipschitzness increases. The middle row compares the attention weights assigned by softmax and linear attention, demonstrating the adaptive nature of softmax attention. Finally, the bottom row illustrates how ICL error changes with the number of context samples, showing that softmax attention achieves lower error by adapting to the function Lipschitzness.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_8_1.jpg", "caption": "Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights \u2013 which determine the attention window \u2013 as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details.", "description": "This figure shows how the softmax attention mechanism adapts to different function Lipschitzness and noise levels. The top row illustrates three target functions with increasing Lipschitzness, along with noisy training data.  The middle row plots the attention weights assigned to each data point in the context by the softmax and linear attention models.  The softmax model's weights show adaptation to Lipschitzness, widening their focus for less smooth functions. The bottom row presents the in-context learning (ICL) error for each setting as the number of context samples increases.  The results show that softmax attention achieves lower error by adapting its focus to the function's smoothness and data quality.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_9_1.jpg", "caption": "Figure 5: Left, Middle-Left, Middle-Right: The test error for softmax attention as it is trained on the distributions over 1-Lipschitz affine, ReLU, and cosine function (D(Faff), D(F+), and D(Fcos), respectively), where the error is evaluated at each pretraining iteration on 5 tasks drawn from the distributions over the 1-Lipschitz (affine, ReLU, cosine) function classes in (Left, Middle-Left, Middle-Right), respectively. Right: The test error evaluated on tasks drawn from D(Fcos) for three softmax attention trained on tasks drawn from D(Faff), D(F+), and D(Fcos), respectively.", "description": "This figure shows the test ICL error for softmax attention trained on different function classes, with the same Lipschitz constant (L=1).  The left three plots demonstrate that when both pretraining and test tasks have the same Lipschitz constant, test error is low regardless of the specific function class. The rightmost plot shows the importance of having matching Lipschitz constants between pretraining and test tasks; using a mismatch leads to high error.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_17_1.jpg", "caption": "Figure 6: Comparison between using M and w in Lemma B.5. Here we denote y := yn+1. Under the attention induced by M, the center of attention for y is actually y', and the attention weights are depicted by the light orange shading. Under the attention induced by w, the center of attention for y is y and the weights are depicted by the light blue shading. Naturally, using the blue shaded attention should lead to a better estimate of f(y) under mild regularity conditions.", "description": "This figure compares two methods for estimating the value of a function at a given point, using either a matrix M or a vector w to weight the contributions of nearby points. The figure shows that using w leads to a more accurate estimate.", "section": "B Preliminaries"}, {"figure_path": "lfxIASyLxB/figures/figures_27_1.jpg", "caption": "Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights \u2013 which determine the attention window \u2013 as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details.", "description": "The figure shows how the softmax attention mechanism adapts to different function Lipschitzness and noise levels in in-context learning. The top row displays target functions with varying Lipschitzness. The middle row compares attention weights between softmax and linear attention mechanisms, highlighting the softmax's ability to adapt the attention window size based on Lipschitzness.  The bottom row shows how the ICL error varies with the number of context samples under different settings, demonstrating the effectiveness of softmax attention when it adapts to the function's characteristics.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_31_1.jpg", "caption": "Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle Row: Attention weights \u2013 which determine the attention window \u2013 as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. Bottom Row: ICL error versus number of context samples for the three settings. Adapting to function Lipschitzness leads softmax attention to achieve small error. Please see Remark 2.1 and Appendix J for further discussion and details.", "description": "This figure shows how softmax attention adapts to the Lipschitzness of the target function in an in-context learning setting. The top row illustrates three regression tasks with varying Lipschitzness. The middle row compares attention weights for softmax and linear attention, showing that softmax attention adjusts its window based on Lipschitzness, while linear attention does not. The bottom row demonstrates that this adaptability improves ICL performance.", "section": "3 Pretraining Learns Scale of Attention Window"}, {"figure_path": "lfxIASyLxB/figures/figures_52_1.jpg", "caption": "Figure 9: Representation learning error (\u03c1(M, B)) and test ICL error (mean squared error) during pretraining softmax and linear attention on tasks from Left: Faff, Center: Fb, and Right: Fos.", "description": "This figure shows the results of experiments comparing softmax and linear attention mechanisms in a low-rank setting. Three different function classes (affine, quadratic, and cosine) were used to generate tasks.  The plots show both the representation learning error, measuring how well the attention mechanism learns the low-dimensional structure of the tasks (\u03c1(M,B)), and the test ICL error, measuring the performance of the pretrained attention mechanism on unseen tasks. The results indicate that softmax attention effectively learns the low-rank structure, leading to improved performance, while linear attention does not.", "section": "3.2 Experiments"}]