{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of in-context learning, introducing the concept and demonstrating the capability of language models to perform few-shot learning."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation of the models used in in-context learning."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023-12-01", "reason": "This paper provides a theoretical explanation of how Transformers implement preconditioned gradient descent, which is relevant to understanding the mechanism of in-context learning."}, {"fullname_first_author": "Ruiqi Zhang", "paper_title": "Trained transformers learn linear models in-context", "publication_date": "2023-12-01", "reason": "This paper demonstrates theoretically that pretrained transformers learn linear models during in-context learning, which is relevant to the paper's focus on understanding the mechanism of in-context learning."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-12-01", "reason": "This paper explores the connection between in-context learning and gradient descent, providing insights into the learning mechanisms of Transformers."}]}