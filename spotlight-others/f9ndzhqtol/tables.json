[{"figure_path": "F9NDzHQtOl/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of the approximate time complexity (cf. Definition 2.1) of different implementations of diffusion models. \u03b7 is a small parameter that controls the smooth approximation of the data distribution (cf. Section 3.1.1).", "description": "This table compares the approximate time complexity of different implementations of diffusion models from existing research papers. The approximate time complexity is defined as the number of unparallelizable evaluations of the learned neural network-based score function.  It shows how the time complexity scales with the data dimension (d) and a smoothing parameter (\u03b7). The table includes results for SDE and ODE implementations, both with and without parallel sampling techniques.  The results highlight the improvement in computational efficiency achieved by parallel sampling in reducing the time complexity.", "section": "2 Preliminaries"}, {"figure_path": "F9NDzHQtOl/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of the approximate time complexity (cf. Definition 2.1) of different implementations of diffusion models. \u03b7 is a small parameter that controls the smooth approximation of the data distribution (cf. Section 3.1.1).", "description": "This table compares the approximate time complexity of various implementations of diffusion models from different research papers.  The complexity is expressed in terms of its dependence on the data dimension *d* and a smoothing parameter \u03b7.  Different implementations (SDE, ODE with/without corrections, and parallel sampling variants) are contrasted, showing a range of complexities from polynomial in *d* to polylogarithmic in *d*. The table highlights the significant improvement in computational efficiency achieved by the proposed method in the paper.", "section": "2 Preliminaries"}]