[{"figure_path": "5G7MRfPngt/figures/figures_1_1.jpg", "caption": "Figure 1: ICAL (In-Context Abstraction Learning) is a method for efficient agent learning from both noisy visual demonstrations and human feedback using large language / vision models. Left: The agent can take in a video demonstration, and generate a refined example with language annotations to be used later by the VLM via in-context learning. Right: Humans provide feedback, correct errors and supply additional knowledge.", "description": "This figure illustrates the In-Context Abstraction Learning (ICAL) method.  The left side shows how the agent processes a noisy video demonstration, generating refined examples with added linguistic annotations for later use in vision-language models (VLMs). The right side demonstrates the human-in-the-loop aspect, where humans provide feedback to correct errors and contribute additional knowledge, leading to improved agent learning.", "section": "1 Introduction"}, {"figure_path": "5G7MRfPngt/figures/figures_3_1.jpg", "caption": "Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. Top: Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. Bottom: An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).", "description": "This figure illustrates the In-Context Abstraction Learning (ICAL) process.  The top half shows the process of transforming a noisy trajectory (a sequence of actions and observations from a demonstration) into a refined trajectory by using a Vision-Language Model (VLM). This involves correcting inefficient actions and adding annotations such as causal relationships, object state changes, and subgoals. This refined trajectory is then executed in the environment, with human feedback used to further improve the trajectory if it fails.  The bottom half provides a visual comparison of the raw, noisy trajectory and the final, optimized trajectory produced by ICAL, highlighting the improvements made through abstraction and feedback.", "section": "3 In-Context Abstraction Learning (ICAL)"}, {"figure_path": "5G7MRfPngt/figures/figures_3_2.jpg", "caption": "Figure 1: ICAL (In-Context Abstraction Learning) is a method for efficient agent learning from both noisy visual demonstrations and human feedback using large language / vision models. Left: The agent can take in a video demonstration, and generate a refined example with language annotations to be used later by the VLM via in-context learning. Right: Humans provide feedback, correct errors and supply additional knowledge.", "description": "This figure illustrates the In-Context Abstraction Learning (ICAL) method. The left side shows how the agent processes a noisy visual demonstration, generating refined examples with language annotations for later use by a Vision-Language Model (VLM). The right side shows the human-in-the-loop process where humans provide feedback, correct errors, and supply additional knowledge, improving the agent's learning efficiency.", "section": "3 In-Context Abstraction Learning (ICAL)"}, {"figure_path": "5G7MRfPngt/figures/figures_5_1.jpg", "caption": "Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. Top: Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. Bottom: An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).", "description": "This figure illustrates the ICAL process. The top half shows how a noisy trajectory is transformed into a refined trajectory with added language annotations using a VLM.  The bottom half shows an example of a raw, noisy trajectory compared to the final, optimized abstracted trajectory created by ICAL. The process involves both VLM processing and human-in-the-loop feedback.", "section": "3 In-Context Abstraction Learning (ICAL)"}, {"figure_path": "5G7MRfPngt/figures/figures_7_1.jpg", "caption": "Figure 4: ICAL enables greater success on training tasks. Tasks successfully completed by ICAL over number of interactions when using the ICAL method with kinesthetic or visual demonstrations, and when replaying the kinesthetic or visual demonstrations directly.", "description": "This figure shows the cumulative number of successfully completed tasks as a function of the number of interactions with the environment.  The results for four conditions are displayed:  (1) ICAL using visual demonstrations; (2) directly replaying the original visual demonstrations; (3) ICAL using kinesthetic demonstrations; and (4) directly replaying the original kinesthetic demonstrations. The plot demonstrates that the ICAL method, which generates its own improved examples, outperforms the strategy of simply replaying the original demonstrations. The gap between ICAL and the replay-only conditions grows as the number of interactions increases, highlighting ICAL's ability to improve performance over time.", "section": "4.2 ICAL beats written & unchanged demonstrations in household instruction following"}, {"figure_path": "5G7MRfPngt/figures/figures_8_1.jpg", "caption": "Figure 4: ICAL enables greater success on training tasks. Tasks successfully completed by ICAL over number of interactions when using the ICAL method with kinesthetic or visual demonstrations, and when replaying the kinesthetic or visual demonstrations directly.", "description": "This figure shows the cumulative number of successfully completed tasks plotted against the number of interactions for four different conditions: ICAL with kinesthetic demonstrations, replaying kinesthetic demonstrations, ICAL with visual demonstrations, and replaying visual demonstrations.  It demonstrates that ICAL significantly improves task success rate compared to simply replaying the original (noisy) demonstrations. The improvement is consistent across both kinesthetic and visual demonstration types.", "section": "4.2 ICAL beats written & unchanged demonstrations in household instruction following"}, {"figure_path": "5G7MRfPngt/figures/figures_9_1.jpg", "caption": "Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. Top: Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. Bottom: An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).", "description": "This figure illustrates the In-Context Abstraction Learning (ICAL) process. The top panel shows the process of transforming a noisy trajectory into a refined example with the help of a VLM and human feedback. The bottom panel provides a visual comparison of a raw, noisy trajectory and its abstracted counterpart after the ICAL process has been applied.", "section": "3 In-Context Abstraction Learning (ICAL)"}, {"figure_path": "5G7MRfPngt/figures/figures_17_1.jpg", "caption": "Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. Top: Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. Bottom: An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).", "description": "This figure illustrates the In-Context Abstraction Learning (ICAL) process. The top part shows the overall workflow: ICAL takes a noisy trajectory (a sequence of actions and observations), uses a Vision-Language Model (VLM) to optimize the actions and add annotations, executes the optimized trajectory in an environment (getting human feedback for any errors), and stores successful examples in a memory. The bottom part displays a concrete example: on the left, a raw, noisy trajectory, and on the right, the same trajectory after it has been processed by ICAL, showing optimized actions and added annotations (like summaries, task decomposition, and explanations).", "section": "3 In-Context Abstraction Learning (ICAL)"}, {"figure_path": "5G7MRfPngt/figures/figures_21_1.jpg", "caption": "Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. Top: Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. Bottom: An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).", "description": "This figure illustrates the ICAL process. The top half shows how a noisy trajectory is transformed into useful abstractions using a VLM and human feedback. The bottom half provides a before-and-after comparison of a raw, noisy trajectory and the refined, abstracted example produced by ICAL.", "section": "3 In-Context Abstraction Learning (ICAL)"}, {"figure_path": "5G7MRfPngt/figures/figures_29_1.jpg", "caption": "Figure 2: ICAL transforms raw experience into useful abstractions for in-context learning. Top: Given a noisy trajectory, It prompts a VLM to optimize actions and add language annotations. The optimized trajectory is executed, incorporating human feedback on failures. Successful examples are stored for future VLM in-context action generation. Bottom: An example of the raw, noisy trajectory (left), and the final abstracted example after ICAL (right).", "description": "This figure illustrates the In-Context Abstraction Learning (ICAL) process. The top half shows how a noisy trajectory is processed by a Vision-Language Model (VLM) to generate optimized actions and add language annotations.  These are then executed in an environment, with human feedback incorporated to correct errors. Successful examples are stored in a memory. The bottom half provides a before-and-after comparison of a raw, noisy trajectory and its ICAL-refined version, highlighting the transformation and abstraction involved.", "section": "3 In-Context Abstraction Learning (ICAL)"}]