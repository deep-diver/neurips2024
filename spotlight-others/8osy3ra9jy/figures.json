[{"figure_path": "8oSY3rA9jY/figures/figures_1_1.jpg", "caption": "Figure 1: Edge Pruning disentangles the residual stream and optimizes continuous masks on the read operations via gradient descent. Discretizing the masks to {0, 1} yields the final circuit. The full model corresponds to the case where all masks equal 1.", "description": "This figure illustrates the Edge Pruning method. (a) shows a regular Transformer with a residual stream. (b) shows the modified Transformer architecture where the residual stream is replaced with a disentangled residual stream, allowing for the introduction of learnable binary masks on the edges. These masks control which components are read from. (c) shows the resulting sparse circuit after optimizing the masks and discretizing them to 0 or 1.  The full model is represented by the case where all masks are 1.", "section": "Method: Edge Pruning"}, {"figure_path": "8oSY3rA9jY/figures/figures_5_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares three methods (ACDC, EAP, and Edge Pruning) for finding circuits in a language model, evaluating their faithfulness to the full model. Faithfulness is measured by KL divergence, a lower score indicating better faithfulness.  The results are shown across four different tasks (IOI-t1, IOI, GT, and GP), with varying levels of edge sparsity in the circuits. Edge Pruning consistently demonstrates higher faithfulness, especially for the IOI and GT tasks, meaning that the pruned circuits accurately represent the full model's behavior.", "section": "4 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_6_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares three methods (ACDC, EAP, and Edge Pruning) for circuit discovery based on their faithfulness to the full model's predictions.  Faithfulness is measured using KL divergence, a metric that quantifies the difference between the probability distributions of model outputs and circuit outputs. Lower KL divergence indicates higher faithfulness. The figure shows that Edge Pruning consistently achieves lower KL divergence than the other methods across four different tasks (IOI-t1, IOI, GT, and GP), indicating that it produces more faithful circuits. Notably, while Edge Pruning and ACDC are comparable on IOI-t1 and GP at lower sparsities, Edge Pruning significantly outperforms them on IOI and GT, particularly at higher sparsities.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_7_1.jpg", "caption": "Figure 4: The canonical ground-truth circuits for the Tracr-compiled xproportion and reverse programs. Edge Pruning recovers both circuits perfectly.", "description": "This figure shows the ground truth circuits for two programs compiled using Tracr, a tool that compiles programs into Transformers. The figure demonstrates that Edge Pruning, the method proposed in the paper, is able to perfectly recover these ground truth circuits, highlighting its accuracy and effectiveness.", "section": "4 Experiments"}, {"figure_path": "8oSY3rA9jY/figures/figures_14_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares the faithfulness of three circuit discovery methods (ACDC, EAP, and Edge Pruning) across four different tasks (IOI-t1, IOI, GT, and GP).  Faithfulness is measured by the KL divergence between the full model's predictions and the predictions of the generated circuits; lower KL divergence indicates higher faithfulness.  The x-axis represents the sparsity of the circuit (percentage of edges retained), and the y-axis represents the KL divergence.  The results show that Edge Pruning consistently achieves lower KL divergence (higher faithfulness) than ACDC and EAP, particularly at higher sparsities, demonstrating its superior ability to find sparse circuits that accurately represent the behavior of the full model.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_16_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares three methods (ACDC, EAP, and Edge Pruning) for circuit discovery based on their faithfulness to the full model, measured by KL divergence.  Lower KL divergence indicates higher faithfulness. The x-axis represents the edge sparsity of the discovered circuit (higher is more sparse). The y-axis shows the KL divergence between the full model's predictions and the circuit's predictions. The figure includes four subplots, each representing a different task: IOI-t1 (a single template version of Indirect Object Identification), IOI (a multi-template version of Indirect Object Identification), GT (Greater Than), and GP (Gendered Pronoun). Edge Pruning demonstrates consistently lower KL divergence across various sparsities, indicating better faithfulness to the model, particularly for the IOI and GT tasks.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_16_2.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares three different methods (ACDC, EAP, and Edge Pruning) for discovering circuits in a transformer model, based on how faithfully the discovered circuit represents the behavior of the full model.  The faithfulness is measured using KL divergence, with lower values indicating higher faithfulness. The figure shows that Edge Pruning consistently achieves better faithfulness, particularly on more complex tasks (IOI and GT).  Specifically, it demonstrates superior performance at higher sparsities (i.e., when the circuit is more sparse), showcasing its ability to find concise yet accurate representations of model behavior.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_17_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares three methods (ACDC, EAP, and Edge Pruning) for discovering circuits in a transformer model based on their faithfulness.  Faithfulness is measured by the Kullback-Leibler (KL) divergence between the full model's predictions and the predictions of the discovered circuit. Lower KL divergence indicates higher faithfulness.  The x-axis represents the sparsity (percentage of edges removed) of the discovered circuit. The y-axis represents the KL divergence.  The plots show that Edge Pruning consistently achieves lower KL divergence (higher faithfulness) than ACDC and EAP across four different tasks (IOI-t1, IOI, GT, and GP) and particularly at higher sparsities.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_18_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure compares three methods (ACDC, EAP, and Edge Pruning) for circuit discovery in terms of their faithfulness to the full model.  Faithfulness is measured using the Kullback-Leibler (KL) divergence, which quantifies the difference between the full model's predictions and the predictions of the sparse circuit produced by each method. Lower KL divergence indicates higher faithfulness.  The figure shows that across four different tasks (IOI-t1, IOI, GT, and GP), Edge Pruning consistently achieves lower KL divergence, especially at higher sparsity levels (more edges pruned), demonstrating its superior faithfulness to the full model.", "section": "4 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_18_2.jpg", "caption": "Figure 9: The sparsities of obtained circuits are remarkably consistent across 12 seeds.", "description": "This figure shows the consistency of Edge Pruning's results across multiple random initializations.  Three histograms display the distribution of sparsity values obtained from running the algorithm 12 times on IOI, GT, and GP tasks, respectively. The consistency in the spread of sparsity values obtained suggests that the algorithm's outcome is not overly sensitive to random initialization.", "section": "D How consistent are the circuits found by Edge Pruning?"}, {"figure_path": "8oSY3rA9jY/figures/figures_19_1.jpg", "caption": "Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (lower is better). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.", "description": "This figure shows the faithfulness of three different circuit discovery methods (ACDC, EAP, and Edge Pruning) across four different tasks (IOI-t1, IOI, GT, and GP). Faithfulness is measured using the KL divergence between the full model's predictions and the predictions of the discovered circuits. Lower KL divergence indicates higher faithfulness.  The x-axis represents the sparsity of the circuit (percentage of edges removed). The y-axis shows the KL divergence. Edge Pruning consistently shows lower KL divergence (higher faithfulness) compared to ACDC and EAP, particularly at higher sparsities.  The results highlight the superior performance of Edge Pruning in accurately capturing the model's behavior with significantly fewer edges.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/figures/figures_20_1.jpg", "caption": "Figure 1: Edge Pruning disentangles the residual stream and optimizes continuous masks on the read operations via gradient descent. Discretizing the masks to {0, 1} yields the final circuit. The full model corresponds to the case where all masks equal 1.", "description": "This figure illustrates the Edge Pruning method. It shows how the residual stream in a standard Transformer is disentangled, allowing for the introduction of edge masks. These masks control which components are read from, enabling the optimization of sparse circuits using gradient-based techniques. The final circuit is obtained by discretizing the continuous masks to binary values (0 or 1), with the full model corresponding to a scenario where all masks are set to 1.", "section": "Method: Edge Pruning"}, {"figure_path": "8oSY3rA9jY/figures/figures_21_1.jpg", "caption": "Figure 1: Edge Pruning disentangles the residual stream and optimizes continuous masks on the read operations via gradient descent. Discretizing the masks to {0, 1} yields the final circuit. The full model corresponds to the case where all masks equal 1.", "description": "The figure illustrates the process of Edge Pruning, a method for finding sparse computational subgraphs (circuits) within Transformer models.  It shows three stages: (a) a regular Transformer with a residual stream; (b) the introduction of learnable binary masks that control which components are read from the residual stream, optimized using gradient descent; and (c) the resulting sparse circuit obtained by discretizing the masks. The full model is represented by the scenario where all masks are 1.", "section": "Method: Edge Pruning"}, {"figure_path": "8oSY3rA9jY/figures/figures_21_2.jpg", "caption": "Figure 1: Edge Pruning disentangles the residual stream and optimizes continuous masks on the read operations via gradient descent. Discretizing the masks to {0, 1} yields the final circuit. The full model corresponds to the case where all masks equal 1.", "description": "This figure illustrates the process of Edge Pruning.  (a) shows a regular Transformer with a residual stream. (b) shows how Edge Pruning modifies the architecture by introducing learnable binary masks to control the flow of information between components. These masks are optimized using gradient descent.  (c) shows the resulting sparse circuit after discretizing the masks to 0 or 1, where 1 indicates an active edge and 0 indicates a pruned edge.", "section": "Method: Edge Pruning"}]