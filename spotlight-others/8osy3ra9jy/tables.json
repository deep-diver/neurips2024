[{"figure_path": "8oSY3rA9jY/tables/tables_6_1.jpg", "caption": "Table 1: Scaling to a larger IOI dataset: ACDC improves with more examples but its runtime scales prohibitively. EAP is fast but cannot perform as well. Edge Pruning scales effectively to 100K examples, where it is the fastest and most faithful method. All runs use one NVIDIA H100 GPU.", "description": "This table compares three methods (ACDC, EAP, and Edge Pruning) for circuit discovery on a larger dataset (100k examples).  It shows how the KL divergence, runtime, and sparsity of the obtained circuits change as the number of examples increases.  The results highlight the scalability and efficiency of Edge Pruning compared to other methods.", "section": "4.2 Results"}, {"figure_path": "8oSY3rA9jY/tables/tables_8_1.jpg", "caption": "Table 2: Edge pruning finds circuits with 0.03-0.04% of the edges in CodeLlama-13B that match the performance of the full model. The circuits perform well in cross-evaluation and overlap highly, hinting that the same mechanisms explain large parts of instruction-prompted and few-shot behavior.", "description": "This table presents the results of applying Edge Pruning to the CodeLlama-13B model on a Boolean Expressions task.  It shows the number of edges used in the full model and in circuits discovered using instruction prompting and few-shot prompting, along with their respective accuracy and exact match percentages.  A comparison is also made of the intersection of the instruction-prompted and few-shot circuits. The low number of edges in the discovered circuits (less than 0.04% of the full model's edges) demonstrates the effectiveness of Edge Pruning in identifying sparse, high-performing subgraphs. The high performance and overlap of the instruction and few-shot circuits suggest that the underlying mechanisms for both prompting methods are largely shared.", "section": "5 Case Study: Scaling to 13B Parameters"}]