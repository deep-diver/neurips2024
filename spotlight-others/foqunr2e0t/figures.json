[{"figure_path": "fOQunr2E0T/figures/figures_0_1.jpg", "caption": "Figure 1: Generalization ability of our approach (sDTM) compared with baselines across various out-of-distribution shifts, averaged over different datasets. See \u00a75.", "description": "This figure compares the generalization performance of the proposed Sparse Differentiable Tree Machine (sDTM) model against several baselines across various out-of-distribution scenarios.  The baselines include a standard Transformer, a Relative Universal Transformer, and NQG (a hybrid neurosymbolic model). The different generalization scenarios tested are: IID (in-distribution), 0-Shot Lexical (seeing a word only once during training), 1-Shot Lexical (seeing a word once in the training data), Length/Structural (generalizing to sequences longer than those seen during training), MCD (maximum compound divergence, which means similar unigram distributions but vastly different n-gram frequencies), and Template (holding back abstract n-grams from training). The figure visualizes the relative performance of each model across these scenarios, allowing for a comprehensive evaluation of their generalization abilities.", "section": "5 Results"}, {"figure_path": "fOQunr2E0T/figures/figures_2_1.jpg", "caption": "Figure 2: An example representation using Sparse Coordinate Trees (SCT). The values are N-dimensional vectors, and the tree positional indices are integer representations of positions in the tree. The absent child nodes of \"The\" (indices 4 and 6) are skipped with SCT.", "description": "This figure illustrates how Sparse Coordinate Trees (SCT) represent a tree structure using a sparse vector encoding.  Each node in the tree (e.g., \"The\", \"fun\", \"person\") is associated with an N-dimensional vector (the \"Values\").  Instead of explicitly representing every position in the tree, SCT only stores vectors for the existing nodes.  The \"Tree positional index\" column shows the integer representation of each node's position within the tree. This approach improves efficiency by avoiding the storage of zero-valued vectors for missing children nodes.", "section": "3 Differentiable Tree Operations Over Sparse Coordinate Trees"}, {"figure_path": "fOQunr2E0T/figures/figures_3_1.jpg", "caption": "Figure 3: Left: Performing left (orange) and right (blue). Right: visualizing the left transformation which results in DP being placed at the root. Tree positional indices of 0 and their corresponding values are discarded.", "description": "This figure demonstrates the Sparse Coordinate Trees (SCT) representation and the \"left\" and \"right\" tree operations.  The left panel shows a tree structure and how the left and right operations shift the tree structure. The right panel shows the effect of the left operation: it shifts the tree structure to the left resulting in DP becoming the root.  Indices (representing tree positions) with value 0 are discarded during the operation. This illustrates the core mechanism of how tree manipulations are performed efficiently and differentiably in the Sparse Differentiable Tree Machine (sDTM).", "section": "3.1 Differentiable Tree Operations"}, {"figure_path": "fOQunr2E0T/figures/figures_6_1.jpg", "caption": "Figure 5: Left: The memory state is initialized as a sequence of trees where only the root node contains a token. Right: An output sequence is embedded in a tree using the left-aligned uniform-depth (LAUD) scheme. <NT> and <EOB> are special tokens not in the original output sequence.", "description": "This figure illustrates how the Sparse Differentiable Tree Machine (sDTM) handles sequential inputs and outputs in a sequence-to-sequence (seq2seq) task. The left panel shows the initial memory state, where each input token is represented as a separate tree with only a root node. The right panel shows how an output sequence is converted into a tree structure using the left-aligned uniform-depth (LAUD) approach.  This method simplifies the processing of sequences by structuring them as trees for easier handling by the sDTM.", "section": "4.5 Handling Sequential Inputs and Outputs"}, {"figure_path": "fOQunr2E0T/figures/figures_16_1.jpg", "caption": "Figure 6: Adapted from Soulos et al. [67]. One step of DTM is expanded to show how the agent produces the input to the interpreter. The interpreter then writes the output to memory and encodes the output for the agent. Parts of the architecture with learnable parameters are indicated in yellow. The agent uses three linear transformations on top of a standard Transformer encoder layer to parameterize the inputs to the interpreter.", "description": "This figure illustrates one step in the Differentiable Tree Machine (DTM) process, showing how the agent, interpreter, and tree memory interact.  The agent, using a transformer encoder, processes information to determine which tree operations to perform (left, right, or cons). The interpreter then executes these operations, updating the tree memory.  The updated tree memory and operation information is then encoded and passed back to the agent for the next step in the process.  The diagram highlights the learnable parameters within the model.", "section": "4 The Sparse Differentiable Tree Machine (sDTM)"}, {"figure_path": "fOQunr2E0T/figures/figures_19_1.jpg", "caption": "Figure 7: An input and output pair from Active Logical.", "description": "This figure shows an example of input and output pairs for the Active-Logical task.  The input is a parse tree representing the sentence \"The dog ate the treat\" in standard syntactic structure. The output is a parse tree representing the same semantic information, but in a logical form.  This demonstrates the model's ability to transform between active voice and logical form representations of the same sentence.", "section": "5.2 Performance Regression (Active Logical)"}, {"figure_path": "fOQunr2E0T/figures/figures_19_2.jpg", "caption": "Figure 3: Left: Performing left (orange) and right (blue). Right: visualizing the left transformation which results in DP being placed at the root. Tree positional indices of 0 and their corresponding values are discarded.", "description": "This figure demonstrates the operations of \"left\" and \"right\" on Sparse Coordinate Trees (SCT).  The left side shows how these operations are performed on an example tree, modifying its structure by shifting subtrees. The right side visually explains the effect of the \"left\" operation, showing how it moves a subtree to become the root node, with indices corresponding to absent nodes being removed from the representation.  The figure highlights the efficiency of SCT in vector space compared to traditional tensor product representations by explicitly showing how empty nodes are not explicitly stored.", "section": "3.1 Differentiable Tree Operations"}, {"figure_path": "fOQunr2E0T/figures/figures_19_3.jpg", "caption": "Figure 9: An input and output pair from GeoQuery.", "description": "This figure shows an example of input and output pair from the GeoQuery dataset. The input is a natural language question: \"what is the capital city of the largest state in the m0\". The output is a tree-structured representation of the corresponding SQL query. The tree visually represents the compositional structure of the query, breaking down the question into smaller, more manageable components.", "section": "5.4 Seq2Tree (GeoQuery)"}]