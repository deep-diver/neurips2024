[{"figure_path": "kf80ZS3fVy/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons of existing knowledge editing methods and UniKE.", "description": "This figure compares three different knowledge editing methods: Intrinsic Knowledge Editing, External Knowledge Resorting, and UniKE (the proposed method).  It shows the type of knowledge used (intrinsic, external or both), the form the knowledge takes (parametric neurons, descriptive examples, or unified vectorized key-value pairs), and the resulting locality and generality of each approach.  UniKE aims to improve upon the limitations of the other two methods by unifying both intrinsic and external knowledge into a single, unified framework.", "section": "1 Introduction"}, {"figure_path": "kf80ZS3fVy/figures/figures_3_1.jpg", "caption": "Figure 3: (a) We develop a unified view for multimodal editing, with both intrinsic and external knowledge represented as vectorized key-value memory. (b) We disentangle the knowledge representation into the semantic and truthfulness spaces, further enhancing the knowledge collaboration.", "description": "This figure illustrates the UniKE framework. (a) shows the unified view of multimodal knowledge editing, representing both intrinsic and external knowledge as vectorized key-value memories.  The intrinsic knowledge is integrated into the internal key-value memory (FFN), while external knowledge is incorporated as external key-value memory through feature shifting in self-attention. (b) details the knowledge collaboration enhancement.  It disentangles knowledge representations into semantic and truthfulness spaces using contrastive learning. This allows the model to leverage the strengths of both intrinsic and external knowledge editing synergistically, improving generality and locality.", "section": "3 Method"}, {"figure_path": "kf80ZS3fVy/figures/figures_8_1.jpg", "caption": "Figure 4: (a) Performance for IKE & Latent-IKE (both combined with intrinsic knowledge editing) with different number of in-context examples or hidden states on E-VQA. (b) Performance with different fixed value of \u03b1 and our dynamic \u03b1. (c) Performance with different \u03b6 treatments.", "description": "This figure analyzes the impact of different components of UniKE on its performance. (a) compares the performance of IKE and Latent-IKE (both combined with intrinsic knowledge editing) using various numbers of in-context examples/hidden states. (b) evaluates the effect of using a fixed versus a dynamic \u03b1 (scaling factor controlling the inclusion magnitude of in-context knowledge). (c) shows the results with different treatments of \u03b6 (editing direction derived from the truthfulness space), demonstrating how each component contributes to the overall effectiveness of the method.", "section": "4.5 In-Depth Analysis"}, {"figure_path": "kf80ZS3fVy/figures/figures_8_2.jpg", "caption": "Figure 5: (a) Visualization of different knowledge spaces. (b) A qualitative example.", "description": "This figure visualizes the disentangled knowledge representations in semantic and truthfulness spaces using t-SNE for dimensionality reduction.  (a) shows distinct clustering of positive and negative hidden states within the truthfulness space while similar distributions in the semantic space.  (b) provides a qualitative example demonstrating how UniKE performs multimodal editing by correcting factual errors and generalizing to similar scenarios while maintaining accuracy for irrelevant examples.  It showcases UniKE\u2019s ability to maintain reliability, generality, and locality during editing. ", "section": "3.3 Enhanced Collaboration with Knowledge Disentangling"}, {"figure_path": "kf80ZS3fVy/figures/figures_15_1.jpg", "caption": "Figure 3: (a) We develop a unified view for multimodal editing, with both intrinsic and external knowledge represented as vectorized key-value memory. (b) We disentangle the knowledge representation into the semantic and truthfulness spaces, further enhancing the knowledge collaboration.", "description": "This figure illustrates the UniKE framework.  (a) shows the unified view of multimodal knowledge editing, where both intrinsic and extrinsic knowledge are represented as vectorized key-value pairs.  (b) details how UniKE disentangles knowledge representations into semantic and truthfulness spaces to enhance collaboration between intrinsic and extrinsic editing methods. This disentanglement allows for a more controlled and effective editing process, improving the reliability, generality, and locality of the resulting model.", "section": "3 Method"}, {"figure_path": "kf80ZS3fVy/figures/figures_17_1.jpg", "caption": "Figure 1: Comparisons of existing knowledge editing methods and UniKE.", "description": "This figure compares three different approaches to knowledge editing: intrinsic knowledge editing, external knowledge resorting, and UniKE.  Intrinsic knowledge editing modifies the model's parameters directly. External knowledge resorting leverages external knowledge sources (e.g., in-context learning) to influence the model's output. UniKE, proposed in this paper, aims to unify both methods, combining their strengths to provide reliable, general, and localized editing. The figure displays the different types of knowledge used by each method, how the knowledge is represented (parametric neurons or unified vectorized key-value pairs), and their resulting impact on locality and generality.", "section": "1 Introduction"}]