[{"type": "text", "text": "Overcoming Common Flaws in the Evaluation of Selective Classification Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jeremias Traub1,2 Till J. Bungert1,2,6 Carsten T. L\u00fcth1,2 Michael Baumgartner2,3,6 Klaus H. Maier-Hein2,3,5,6,7 Lena Maier-Hein2,4,6,7 Paul F. J\u00e4ger1,2 ", "page_idx": 0}, {"type": "text", "text": "1German Cancer Research Center (DKFZ) Heidelberg, Interactive Machine Learning Group, Germany 2Helmholtz Imaging, DKFZ Heidelberg, Germany 3DKFZ Heidelberg, Division of Medical Image Computing (MIC), Germany 4DKFZ Heidelberg, Division of Intelligent Medical Systems (IMSY), Germany 5Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, 69120 Heidelberg, Germany 6Faculty of Mathematics and Computer Science, University of Heidelberg, Germany 7National Center for Tumor Diseases (NCT) Heidelberg jeremias.traub@dkfz-heidelberg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Selective Classification, wherein models can reject low-confidence predictions, promises reliable translation of machine-learning based classification systems to real-world scenarios such as clinical diagnostics. While current evaluation of these systems typically assumes fixed working points based on pre-defined rejection thresholds, methodological progress requires benchmarking the general performance of systems akin to the AUROC in standard classification. In this work, we define 5 requirements for multi-threshold metrics in selective classification regarding task alignment, interpretability, and flexibility, and show how current approaches fail to meet them. We propose the Area under the Generalized Risk Coverage curve (AUGRC), which meets all requirements and can be directly interpreted as the average risk of undetected failures. We empirically demonstrate the relevance of AUGRC on a comprehensive benchmark spanning 6 data sets and 13 confidence scoring functions. We find that the proposed metric substantially changes metric rankings on 5 out of the 6 data sets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Selective Classification (SC) is increasingly recognized as a crucial component for the reliable deployment of machine learning-based classification systems in real-world scenarios, such as clinical diagnostics [Dvijotham et al., 2023, Leibig et al., 2022, Dembrower et al., 2020, Yala et al., 2019]. The core idea of SC is to equip models with the ability to reject low-confidence predictions, thereby enhancing the overall reliability and safety of the system [Geifman and El-Yaniv, 2017, Chow, 1957, El-Yaniv et al., 2010]. This is typically achieved through three key components: a classifier that makes the predictions, a confidence scoring function (CSF) that assesses the reliability of these predictions, and a rejection threshold $\\tau$ that determines when to reject a prediction based on its confidence score. ", "page_idx": 0}, {"type": "text", "text": "In evaluating SC systems, two primary concepts are essential: risk and coverage. Risk expresses the classifier\u2019s error potential and is typically measured by its misclassification rate. Coverage, on the other hand, indicates the proportion of instances where the model makes a prediction rather than rejecting it. An effective SC system aims to minimize risk while maintaining high coverage, ensuring that the model provides accurate predictions for as many instances as possible. ", "page_idx": 0}, {"type": "text", "text": "Current evaluation of SC systems often focuses on fixed working points defined by pre-set rejection thresholds[Geifman and El-Yaniv, 2017, Liu et al., 2019, Geifman and El-Yaniv, 2019]. For instance, a common evaluation metric might be the selective risk at a given coverage of $70\\%$ [Geifman and El-Yaniv, 2019], which communicates the potential risk associated with a specific confidence score c and selection threshold $\\tau$ to a patient or end-user. While this method is useful for communicating risk in specific instances, it does not provide a comprehensive evaluation of the system\u2019s overall performance. In standard classification, this is analogous to the need for the Area Under the Receiver Operating Characteristic (AUROC) curve rather than evaluating sensitivity at a specific specificity, which can be highly misleading when assessing general classifier capabilities [MaierHein et al., 2022]. The AUROC provides a holistic view of the classifier\u2019s performance across all possible thresholds, thereby driving methodological progress. Similarly, SC requires a multithreshold metric that aggregates performance across all rejection thresholds to fully benchmark the system\u2019s capabilities. ", "page_idx": 1}, {"type": "text", "text": "Several current approaches attempt to address this need for multi-threshold metrics in SC. The Area Under the Risk Coverage curve (AURC) is the most prevalent of these Geifman et al. [2018]. However, we demonstrate in this work that AURC has significant limitations as it fails to adequately translate the risk from specific working points into a meaningful aggregated evaluation score. This shortcoming hampers the ability to benchmark and improve SC methodologies effectively. ", "page_idx": 1}, {"type": "text", "text": "In our work, we aim to bridge this gap by providing a robust evaluation framework for SC. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "Refined Task Formulation: We are the first to provide a comprehensive SC evaluation framework, explicitly deriving meaningful task formulations for different evaluation and application scenarios such as working point versus multi-threshold evaluation. ", "page_idx": 1}, {"type": "text", "text": "Formulation of Requirements: We define five critical requirements for multi-threshold metrics in SC, focusing on task suitability, interpretability, and flexibility. We assess current multi-threshold metrics against our five requirements and demonstrate their shortcomings. ", "page_idx": 1}, {"type": "text", "text": "Proposal of AUGRC: We introduce the Area Under the Generalized Risk Coverage (AUGRC) curve, a new metric designed to overcome the flaws of current multi-threshold metrics for SC. AUGRC meets all five requirements, providing a comprehensive and interpretable measure of SC system performance. ", "page_idx": 1}, {"type": "text", "text": "Empirical Validation: We empirically demonstrate the relevance and effectiveness of AUGRC through a comprehensive benchmark spanning 6 datasets and 13 confidence scoring functions. ", "page_idx": 1}, {"type": "text", "text": "In summary, our work presents a significant advancement in the evaluation of SC systems, offering a more reliable and interpretable metric that can drive further methodological progress in the field. ", "page_idx": 1}, {"type": "text", "text": "2 Refined Task Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Selective Classification (SC) systems consist of a classifier $m:\\mathcal X\\rightarrow\\mathcal Y$ , which outputs a prediction and a confidence scoring function (CSF) $g:\\mathcal{X}\\to\\mathbb{R}$ , which outputs a confidence score associated with the prediction. Assuming a supervised training setup, let $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ be a dataset containing $N$ independent samples from the source distribution $P(X,{\\dot{Y}})$ over $\\mathcal X\\times\\mathcal Y$ . Given a rejection threshold $\\tau$ , the model prediction is accepted only if the corresponding score is larger than $\\tau$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n(m,g)(x):={\\binom{m(x),\\quad{\\mathrm{if~}}g(x)\\geq\\tau}{\\mathrm{reject},\\quad{\\mathrm{otherwise}}}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Given an error function $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^{+}$ , the overall risk of the classifier $m$ is given by $R(m):=$ $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\ell(m(x),y)}\\end{array}$ . The error function thereby contains the measure of classification performance suitable for the task at hand. Commonly, SC literature assumes a $0/1$ error corresponding to the failure indicator variable $Y_{\\mathrm{f}}$ with $y_{\\mathrm{f},i}(x_{i},y_{i},m)\\,:=\\,\\mathbb{I}[y_{i}\\,\\neq\\,m(x_{i})]$ . In this case, the overall risk represents the probability of misclassification $P(Y_{\\mathrm{f}}=1)$ . By rejecting low-confidence predictions, the selective classifier can decrease the risk of misclassifications at the cost of letting the classifier make predictions on only a fraction of the dataset. This fraction is denoted as the coverage $:=$ $P(g(x)\\;\\geq\\;\\tau)$ , with the respective empirical estimator $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{I}(g(x_{i})\\,\\geq\\,\\tau)}\\end{array}$ . Evaluation of SC ", "page_idx": 1}, {"type": "image", "img_path": "2TktDpGqNM/tmp/c55d9aa5d0e3ea1afe08502afd749b3f4e06269b4e1e360a049d3a80f2fd40a7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: The AUGRC metric based on Generalized Risk overcomes common flaws in current evaluation of Selective classification (SC). a) Refined task definition for SC. Analogously to standard classification, we distinguish between holistic evaluation for method development and benchmarking using multi-threshold metrics versus evaluation of specific application scenarios at pre-determined working points. The current most prevalent multi-threshold metric in SC, AURC, is based on Selective Risk, a concept for working point evaluation that is not suitable for aggregation over rejection thresholds (red arrow). To fill this gap, we formulate the new concept of Generalized Risk and a corresponding metric, AUGRC (green arrow). b) We formalize our perspective on SC evaluation by identifying five key requirements for multi-threshold metrics and analyze how previous metrics fail to fulfill them. Abbreviations, CSF: Confidence Scoring Function, AU(G)RC: Area Under the (Generalized) Risk Coverage curve. ", "page_idx": 2}, {"type": "text", "text": "systems often focuses on preventing \"silent\", i.e. undetected failures for which both $y_{\\mathrm{f},i}\\,=\\,1\\$ and $\\bar{g}(x_{i})\\ge\\tau$ . ", "page_idx": 2}, {"type": "text", "text": "Deploying a SC system in practice requires the selection of a fixed rejection threshold $\\tau$ . In the following refined evaluation protocol, we distinguish between the well-established application-specific task formulation where SC models are evaluated at individual working points (Section 2.1) and SC method development where evaluation is independent of individual working points (Section 2.2). ", "page_idx": 2}, {"type": "text", "text": "2.1 Evaluating SC systems in applied settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Current Selective Classification evaluation commonly reports the selective risk of the system at a given coverage (\u201crisk $@$ coverage\u201d), which implies a pre-determined cutoff on the risk or coverage (e.g. \"maximum risk of $5\\%\"$ \") leading to a working point of the system defined by a rejection threshold $\\tau$ . The selective risk is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Selective\\;Risk}_{(m,g)}(\\tau):=\\frac{\\sum_{i=1}^{N}\\ell(m(x_{i}),y_{i})\\cdot\\mathbb{I}(g(x_{i})\\geq\\tau)}{\\sum_{i=1}^{N}\\mathbb{I}(g(x_{i})\\geq\\tau)}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This formulation only considers accepted predictions with $c>\\tau$ . For a binary failure error, this risk is an empirical estimator for the conditional probability $P(Y_{\\mathrm{f}}=1|g(x)\\geq\\tau)$ . Thus, this metric effectively communicates the risk of a \"silent\" failure for a specific prediction of classifier $m$ that has been accepted $\\left(c>\\tau\\right)$ at a pre-defined threshold $\\tau$ . This information may be useful in applied medical settings, e.g. to communicate the risk of misclassification for a patient p whose associated prediction has been accepted by a given SC system $(c_{p}>\\tau)$ . Aside from the selective risk, other performance measurements for working point selection include for example Classification quality and Rejection quality [Condessa et al., 2017]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2 Evaluating SC systems for method development and benchmarking ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Evaluating the general performance of a Selective Classification system requires a multi-threshold metric for a more comprehensive view compared to a working point analysis based on a fixed singular rejection threshold, analogous to binary classification tasks, where the Area Under the Receiver Operating Characteristic (AUROC) is used to assess the entire system performance across multiple classifier thresholds. However, the working point analysis can not be simply translated to multithreshold aggregation, as it is based on Selective Risk, which only considers the risk w.r.t accepted predictions, assuming that a specific selection has already occurred $\\operatorname{\\cal{P}}(Y_{\\mathrm{{f}}}\\;=\\;1\\;\\;|\\;\\;g(x)\\;\\geq\\;\\tau))$ . This evaluation ignores rejected cases and thus contradicts the holistic assessment, where we are interested in the general risk for any prediction causing a silent failure independent of the future selection decision of individual predictions. This holistic assessment is reflected by the joint probability $P(Y_{\\mathrm{f}}=1,g(x)\\geq\\tau)$ , which reflects the risk of silent failure for any prediction processed by the system including cases that are potentially rejected in the future. ", "page_idx": 3}, {"type": "text", "text": "To address the discrepancy between the need for holistic assessment versus the selective risk\u2019s focus on already selected cases, we formulate the generalized risk: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Generalized~Risk}(m,g)(\\tau):=\\frac{1}{N}\\sum_{i=1}^{N}\\ell(m(x_{i}),y_{i})\\cdot\\mathbb{I}[g(x_{i})\\geq\\tau].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This metric reflects the joint probability of misclassification and acceptance by the confidence score threshold. By aggregating this risk over multiple rejection thresholds, we can evaluate the overall performance of the SC system. ", "page_idx": 3}, {"type": "text", "text": "2.3 Requirements for Selective Classification multi-threshold metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the refined task definition given above, we can formulate concrete five requirements R1-R5 for multi-threshold metrics in Selective Classification: ", "page_idx": 3}, {"type": "text", "text": "R1: Task Alignment. The general goal in SC is to prevent silent failures either by preventing failures in the first place (via classifier performance), or by detecting the remaining failures (via CSF ranking quality). As argued in J\u00e4ger et al. [2023], it is crucial to jointly evaluate both aspects of an SC system, since the choice of CSF generally affects the underlying classifier. R2: Monotonicity. The metric should be monotonic w.r.t both evaluated factors stated in R1, i.e. improving on one of the two factors while keeping the other one fixed results in an improved metric value. Note that R2 does not make assumptions about how the two factors are combined, but it represents a minimum requirement for meaningful comparison and optimization of the metric. R3: Ranking Interpretability. Interpretability is a crucial component of a metric [Maier-Hein et al., 2022]. AUROC is the de facto standard ranking metric for binary classification tasks, providing an intuitive assessment of ranking quality which is proportional to the number of permutations needed for establishing an optimal ranking. This can also be interpreted as the \"probability of a positive sample having a higher score than a negative one\". Ideally, the SC metric should follow this intuitive assessment of rankings. R4: CSF Flexibility. The metric should be applicable to arbitrary choices of CSFs. This includes external CSFs, i.e. scores that are not based directly on the classifier output. R5: Error Flexibility. Current SC literature largely focuses on 0/1 error (i.e. 1 \u2212accuracy) in their risk computation. However, in many real-world scenarios, accuracy is not an adequate classification metric, such as in the presence of class imbalance and for pixel-level classification. Thus, the SC metric should be flexible w.r.t the choice of error function. ", "page_idx": 3}, {"type": "text", "text": "2.4 Current multi-threshold metrics in SC do not fulfill requirements R1-R5 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "AURC: Geifman et al. [2018] derive the AURC as the Area under the Selective Risk-Coverage curve. This metric is the most prevalent multi-threshold metric in SC [Geifman et al., 2018, J\u00e4ger et al., 2023, Bungert et al., 2023, Cheng et al., 2023, Zhu et al., 2023a, Varshney et al., 2020, Naushad and Voiculescu, 2024, Van Landeghem et al., 2024, 2023, Zhu et al., 2022, Xin et al., ", "page_idx": 3}, {"type": "text", "text": "2021, Yoshikawa and Okazaki, 2023, Ding et al., 2020, Zhu et al., 2023b, Galil and El-Yaniv, 2021, Franc et al., 2023, Cen et al., 2023, Xia and Bouganis, 2022, Cattelan and Silva, 2023, Tran et al., 2022, Kim et al., 2023, Ashukha et al., 2020, Xia et al., 2024]. For the 0/1-error, AURC can be expressed through the following integral: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{AURC}=\\int_{0}^{1}P(Y_{\\mathrm{f}}=1|g(x)\\geq\\tau)\\,\\mathrm{d}P(g(x)\\geq\\tau)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This integral effectively aggregates the Selective Risk (Equation 2) over all fractions of accepted predictions (i.e. coverage). However, as discussed in Section 2.2, the Selective Risk is not suitable for aggregation over rejection thresholds to holistically assess a SC system. This inadequacy effectively leads to an excessive over-weighting of high-confidence failures in AURC and in return to breaching the requirements of monotonicity (R2) and ranking interpretability (R3). We provide empirical evidence for these shortcomings on toy data (Section 3) and real-world data (Section 4.2). Further, current SC literature usually employs the 0/1 error function, even though the related Accuracy metric is well known to be an unsuitable classification performance measure for many applications. For example, Balanced Accuracy is used to address class imbalance and metrics such as the Dice Score are employed for segmentation. Moving beyond the 0/1 error yields a higher variance in distinct error values and may thus amplify the shortcoming of over-weighting high-confidence failures. As more sophisticated error functions being are an important direction of future SC research, it is crucial to ensure the compatibility of metrics in SC evaluation. ", "page_idx": 4}, {"type": "text", "text": "e-AURC: Geifman et al. [2018] further introduce the excess-AURC as the difference to the AURC of an optimal confidence scoring function (denoted as AURC ) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{e}{\\mathrm{-}}\\mathrm{AURC}=\\mathrm{AURC}-\\mathrm{AURC^{\\ast}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It relies on the intuition that subtracting the optimal AURC eliminates the contribution of the overall classification performance and collapses it to a pure ranking metric [Geifman et al., 2018, Galil et al., 2023, J\u00e4ger et al., 2023]. However, several works demonstrated that this intuition does not hold and that the e-AURC is still sensitive to the overall classification performance [Galil et al., 2023, Cattelan and Silva, 2023]. We attribute this deviation from the intended behavior to the shortcomings of the underlying AURC, as we find the desired isolation of classifier performance in our improved metric formulation in Section 3. e-AURC further inherits the shortcomings of AURC w.r.t monotonicity (R2), ranking interpretability (R3), and error flexibility (R5). ", "page_idx": 4}, {"type": "text", "text": "NLL / Brier Score: Importantly, proper scoring rules such as the Negative-Log-Likelihood and the Brier Score are technically not multi-threshold metrics. Yet, we include them here as they also aim for a holistic performance assessment, i.e. assessment beyond individual working points, by evaluating a general meaningfulness of scores based on the softmax-output of the classifier [Ovadia et al., 2019]. Thereby, they jointly assess ranking and calibration of scores, which dilutes the focus on ranking quality in the context of SC [J\u00e4ger et al., 2023]. In our formulation, the calibration aspect in these metrics breaks the required monotonicity w.r.t SC evaluation (R2). Further, they are not applicable to CSFs beyond those that are directly based on the classifier output (R4). AUROCf: The \"failure version\" of the standard AUROC assesses the correctness of predictions with a binary failure label [J\u00e4ger et al., 2023]. Based on the AUROC, it provides an intuitive ranking quality assessment. However, it ignores the underlying classifier performance (R1, R2), and is restricted to binary error functions (R5). OC-AUC: Kivlichan et al. [2021] introduce the Oracle-Model Collaborative AUC, where first a fixed threshold is applied on the confidence scores, above which error values are set to zero. Then, the $\\mathrm{AUROC_{f}}$ (or the Average Precision) are evaluated. This metric is also reported in [Dehghani et al., 2023, Tran et al., 2022], with a review fraction of $0.5\\%$ . OC-AUC is subject to the same pitfalls as $\\mathrm{AUROC_{f}}$ (R1, R2, R5). AUROC-AURC: Pugnana and Ruggieri [2023] propose the AUROC-AURC where the Selective Risk is defined as the classification (not failure) AUROC computed over the set of accepted predictions. However, it is only applicable to binary classification models with binary error functions (R5). Further, in employing Selective Risk it inherits the pitfalls described for AURC regarding monotonicity (R2) and ranking interpretability (R3). NAURC: Cattelan and Silva [2023] propose the Normalized-AURC, a min-max scaled version of the e-AURC, and claim that this modification eliminates its lack of interpretability. However, given the linear relationship with the AURC, it does not fulfill the requirements R2 and R3. F1-AUC: Malinin and Gales [2020], Malinin et al. [2021] introduce the notion of Error-Retention Curves, which corresponds to that of Generalized Risk Coverage Curves. However, in Malinin et al. [2021] the authors propose an F1-AUC metric which is only applicable to binary error functions (R5) and breaks monotonicity (R2), as it decreases with increasing accuracy for accuracy values above $\\approx0.56$ (see Appendix A.1.3 for a detailed explanation.) ARC: Accuracy-Rejection-Curves [Nadeem et al., 2009, Ashukha et al., 2020, Condessa et al., 2017] and the associated AUC directly correspond to the AURC and are therefore subject to the same pitfalls (R2, R3). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "2TktDpGqNM/tmp/6da915b67715bfb7cf1b128fdd3f269be71d96206c2026d7e4d2ee81face43c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: The proposed AUGRC metric resolves shortcomings of AURC. All figures are based on rankings of predictions according to descending associated confidence scores induced by a CSF. All AURC, e-AURC, and AUGRC values are scaled by $\\times1000$ . a) shows the contribution of an individual failure case on the AURC and AUGRC metrics depending on its ranking position (for technical details, see Section A.1.1). While AUGRC reflects the intuitive behavior of weighing the failure cases proportional to their ranking position, the AURC puts excessive weight on highconfidence failures. b-d) Toy example of three CSFs ranking 20 predictions to show how AUGRC resolves the broken monotonicity requirement (R2) of AURC. Despite equal $\\mathrm{{AUROC}_{f}}$ and equal acc in CSF-1 and CSF-2, the AURC improves. And AURC even improves in CSF-3, which features lower $\\mathrm{{AUROC}_{f}}$ and lower acc compared to CSF-1. e-f) The corresponding risk-coverage curves reveal that the non-intuitive behavior of AURC is due to the excessive effect of the high-confidence failure of CSF-1 on the Selective Risk, which is resolved in the Generalized Risk. ", "page_idx": 5}, {"type": "text", "text": "3 Area under the Generalized Risk Coverage Curve ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Section 2.2, we illustrate that aggregating SC performance across working points requires to shift the perspective from the Selective Risk to the Generalized Risk (Equation 3) as an holistic assessment of the risk of silent failures for all predictions, before the rejection decision is made. We propose to evaluate SC methods via the Area under the Generalized Risk Coverage Curve (AUGRC). For the binary failure error it becomes an empirical estimator of the following expression: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{AUGRC}=\\int_{0}^{1}P(Y_{\\mathrm{f}}=1,\\,g(x)\\geq\\tau)\\,\\mathrm{d}P(g(x)\\geq\\tau)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This metric evaluates SC models in terms of the rate of silent failures averaged over working points and thus provides a practical measurement that is directly interpretable. It is bounded to the $[0,1/2]$ interval, whereby lower AUGRC corresponds to better SC performance. The AUGRC is not subject to the shortcomings of the AURC, and we can derive a direct relationship to $\\mathrm{{AUROC}_{f}}$ and acc (the derivation and visualizations are shown in Equation 8 and Figure 5): ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathrm{AUGRC}}=(1-{\\mathrm{AUROC}}_{\\mathrm{f}})\\cdot{\\mathrm{acc}}\\cdot(1-{\\mathrm{acc}})+{\\frac{1}{2}}(1-{\\mathrm{acc}})^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To illustrate, when drawing two random samples, it corresponds to the probability that either both are failure cases or that one is a failure and has a higher confidence score than the non-failure. This is reflected by the second and first term of Equation 7, respectively. As an example, a CSF that outputs random scores yields $\\mathrm{{AUROC}_{f}\\,=\\,\\frac{1}{2}}$ , and hence $\\scriptstyle\\mathrm{AUGRC}^{\\prime}\\ =\\ {\\frac{1}{2}}(1\\ -\\ \\mathrm{acc})$ . The AUGRC for an optimal CSF $(\\mathrm{AUROC}_{\\mathrm{f}}\\,=1)$ ) is given by the second term in Equation 7, hence subtracting the optimal SC performance yields a pure ranking measure re-scaled by the probability of finding a positive/negative pair. This overcomes the lack of interpretability of AURC and e-AURC (R3). Monotonicity (R2) is ensured since the partial gradients w.r.t. both $\\mathrm{{AUROC}_{f}}$ and acc are always negative. Further, it can accommodate arbitrary classification metrics via the error function $\\ell$ (R4) as well as arbitrary CSF (R5). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Figure 2a depicts the metric contribution of individual failure cases depending on their ranking position. This shows empirically how the excessive over-weighting of high-confidence failures in AURC is resolved by AUGRC, and how the intuitive ranking assessment of $\\mathrm{{AUROC}_{f}}$ is established (see Section A.1.1 for a detailed derivation). We further showcase how AUGRC resolves the broken monotonicity requirement (R2) of AURC in Figures 2 b-d. Despite equal $\\mathrm{{AUROC}_{f}}$ and equal acc in CSF-1 and CSF-2, the AURC improves. And AURC even improves in CSF-3, which features lower $\\mathrm{{AUROC}_{f}}$ and lower acc compared to CSF-1. Figures 2 e-f depict the associated risk-coverage curves and reveal that the non-intuitive behavior of AURC is due to the excessive effect of the highconfidence failure of CSF-1 on the Selective Risk, which is resolved in the Generalized Risk. In Section 4.2 we demonstrate implications of AURC\u2019s shortcomings on real-world data. ", "page_idx": 6}, {"type": "text", "text": "4 Empirical Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct an empirical study based on the existing FD-Shifts benchmarking framework [J\u00e4ger et al., 2023], which compares various CSFs across a broad range of datasets. The focus of our study is not on the performance of individual methods (CSFs) but rather on the ranking of methods based on AUGRC evaluation as compared to AURC. We utilize the same experimental setup as in J\u00e4ger et al. [2023] with the addition of CSF scores based on temperature-scaled classifier logits. Extending the benchmark by recent SC methods, e.g. Feng et al. [2022], is an interesting direction of future work. A detailed overview of the datasets and methods used can be found in Appendix A.2. The code for reproducing our results and a PyTorch implementation of the AUGRC are available at: https://github.com/IML-DKFZ/fd-shifts. ", "page_idx": 6}, {"type": "text", "text": "4.1 Comparing Method Rankings of AUGRC and AURC ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To study the relevance of AUGRC, we investigate the changes induced by AUGRC on rankings of CSFs compared to AURC method rankings. ", "page_idx": 6}, {"type": "text", "text": "CSF Rankings with AUGRC substantially deviate from AURC rankings. Figure 3 illustrates the method ranking differences for all i.i.d. test datasets, showing changes in ranks across all CSFs. Notably, AUGRC induces changes in the top-3 methods (out of 13) on 5 out of 6 datasets. To ensure that these ranking changes are not due to variability in the test data, we evaluate the metrics on 500 bootstrap samples from the test dataset and derive the compared method rankings based on the average rank across these samples (\"rank-then-aggregate\"). The size of each bootstrap sample corresponds to the size of the test dataset. Metric values for each bootstrap sample are averaged over 5 training initializations (2 for BREEDS and 10 for CAMELYON-17-Wilds). We analyze the robustness of the resulting method rankings for AURC and AUGRC separately, based on statistical significance. To that end, we perform pairwise CSF comparisons in form of a one-sided Wilcoxon signed-rank test [Wilcoxon, 1992] on the bootstrap samples. To control the family-wise error rate at a $5\\%$ significance level, we apply the Holm correction for multiple testing per metric and dataset, following [Wiesenfarth et al., 2021] (see Figure 7 for the results without correction for multiple testing). The resulting significance maps displayed in Figure 3 indicate stable method rankings for both AURC and AUGRC. This suggests that the observed ranking differences are induced by the conceptual difference of AUGRC compared to AURC and, as a consequence, that the shortcomings of current metrics and respective solutions discussed in Section 2, are not only conceptually sound, but also highly relevant in practice. For the results in Figure 3, we select the DeepGamblers reward parameter and whether to use dropout for both metrics separately on the validation dataset (details in Table 1 and Table 2). ", "page_idx": 6}, {"type": "text", "text": "The shortcomings of AURC affect CSF comparison across datasets and distribution shifts. The method rankings for all datasets and distribution shifts, based on the original test datasets, are shown in Table 4, which also includes results for equal hyperparameters for AURC and AUGRC. Results for the bootstrap-based method ranking analysis for distribution shifts are displayed in Figure 6. The substantial differences in method rankings across all datasets and distribution shifts underline the relevance of AUGRC for SC evaluation. The complete AUGRC results on the FD-Shifts benchmark are shown in Table 3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In few cases, as shown in Figure 3, we observe that a CSF A is ranked worse than a neighboring CSF B even though CSF A\u2019s ranks are significantly superior to those of CSF B based on the Wilcoxon statistic over the bootstrap samples. This discrepancy can occur if the rank variability in CSF A is larger than in CSF B. While this does not affect our conclusions regarding the relevance of the AUGRC, it indicates that selecting a single best CSF for application based solely on method rankings should be approached with caution. We recommend closer inspection of the individual CSF performance in such cases, although this is not the focus of our study. ", "page_idx": 7}, {"type": "image", "img_path": "2TktDpGqNM/tmp/59e23566768d5429d482d93101206dc6b45216fd913eeefce070b6f5afe6f7bd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Substantial differences in method rankings for AUGRC and AURC. On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed AUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests based on the bootstrap samples at $5\\%$ significance level with adjustment for multiple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position $(x,y)$ , filled entries indicate that metric values of CSF $y$ are ranked significantly better than those from CSF $x$ (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal. ", "page_idx": 7}, {"type": "text", "text": "Figure 4 gives an in-depth look at how the conceptual shortcomings of AURC affect method assessment on real-world data. The example uses the confidence like reservation score of the DeepGamblers method (DG-Res) [Liu et al., 2019] and the Monte Carlo Dropout-based predictive entropy (MCD-PE) [Gal and Ghahramani, 2016] as CSFs on the CIFAR-10 test dataset. Despite DG-Res having higher classification performance and ranking quality than MCD-PE, the AURC erroneously favors MCD-PE over DG-Res. This violates the monotonicity requirement (R2). This can be attributed to the excessive contributions of only few high-confidence failures, which aligns with the theoretical findings on failure contributions shown in Figure 2 (R3). In this example, the highconfidence failures are associated with high label ambiguity or incorrect labeling, suggesting that the AURC may exacerbate the influence of label noise in practice. ", "page_idx": 8}, {"type": "image", "img_path": "2TktDpGqNM/tmp/3dfa21a48c039a5ed4cdef543c8a3fcacf58f3879a8a597668c1953ebe6cd751.jpg", "img_caption": ["Figure 4: The conceptual shortcomings of AURC affects method assessment in practice. We illustrate the practical effects of excessive weight high-confidence failures in AURC by comparing the performance of two CSFs, DG-Res and MCD-PE, on the CIFAR-10 test dataset. (a) shows the coverage curves based on Selective Risk and Generalized Risk for both CSFs. The AURC violates the monotonicity requirement (R2) in practice, favoring MCD-PE despite a lower classification performance and ranking quality compared to DG-Res. (b) displays the images associated with the top- $k$ high-confidence failures. For DG-Res, the four failures correspond to the first four peaks in the Selective Risk curve, up to coverage $\\approx0.27$ (the total number of failures is 446). Only a few high-confidence failures significantly increase the AURC. For both CSFs, the images associated with high-confidence failures exhibit high label ambiguity or are incorrectly labeled, indicating that the AURC may amplify the influence of label noise in practice. AURC and AUGRC values are scaled by $\\times1000$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Despite the increasing relevance of SC for reliable translation of machine learning systems to realworld application, we find that the current metrics have limitations in providing the comprehensive assessment needed to guide the methodological progress of SC systems. ", "page_idx": 8}, {"type": "text", "text": "In this work, we establish a systematic SC evaluation framework, thereby promoting the adoption of more comprehensive, interpretable, and task-aligned metrics for comparative benchmarking of SC systems. ", "page_idx": 8}, {"type": "text", "text": "We find that none of the existing multi-threshold metrics, particularly the AURC, meet the key requirements we identified for comprehensive SC evaluation, leading to deviations from intended and intuitive performance assessment behaviors. To address this, we introduce the AUGRC as a suitable metric for comprehensive SC method evaluation. Substantial differences in method rankings between AURC and AUGRC, demonstrated through extensive empirical studies, highlight the importance of selecting the right SC metric. Thus, we propose the adoption of AUGRC for meaningful SC performance evaluation. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "When evaluating specific applications for which certain risk or coverage intervals are known to be irrelevant, adaptations such as a partial AUGRC (analogous to the partial AUROC) may be considered. Further, investigating the relation between confidence ranking and calibration and on evaluating SC in settings where calibrated scores are of interest is an interesting direction of future work. Overall, our proposed evaluation framework provides a solid basis for future work in Selective Classification, including developing novel methods as well as analyzing the properties of individual methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly funded by Helmholtz Imaging (HI), a platform of the Helmholtz Incubator on Information and Data Science. We thank Lukas Klein, Maximilian Zenk and Fabian Isensee for insightful discussions and feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Krishnamurthy Dvijotham, Jim Winkens, Melih Barsbey, Sumedh Ghaisas, Robert Stanforth, Nick Pawlowski, Patricia Strachan, Zahra Ahmed, Shekoofeh Azizi, Yoram Bachrach, et al. Enhancing the reliability and accuracy of ai-enabled diagnosis via complementarity-driven deferral to clinicians. Nature Medicine, 29(7): 1814\u20131820, 2023.   \nChristian Leibig, Moritz Brehmer, Stefan Bunk, Danalyn Byng, Katja Pinker, and Lale Umutlu. Combining the strengths of radiologists and ai for breast cancer screening: a retrospective analysis. The Lancet Digital Health, 4(7):e507\u2013e519, 2022.   \nKarin Dembrower, Erik W\u00e5hlin, Yue Liu, Mattie Salim, Kevin Smith, Peter Lindholm, Martin Eklund, and Fredrik Strand. Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study. The Lancet Digital Health, 2 (9):e468\u2013e474, 2020.   \nAdam Yala, Tal Schuster, Randy Miles, Regina Barzilay, and Constance Lehman. A deep learning model to triage screening mammograms: a simulation study. Radiology, 293(1):38\u201346, 2019.   \nYonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. Advances in neural information processing systems, 30, 2017.   \nChi-Keung Chow. An optimum character recognition system using decision functions. IRE Transactions on Electronic Computers, (4):247\u2013254, 1957.   \nRan El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010.   \nZiyin Liu, Zhikang Wang, Paul Pu Liang, Russ R Salakhutdinov, Louis-Philippe Morency, and Masahito Ueda. Deep gamblers: Learning to abstain with portfolio theory. Advances in Neural Information Processing Systems, 32, 2019.   \nYonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject option. In International conference on machine learning, pages 2151\u20132159. PMLR, 2019.   \nLena Maier-Hein, Bjoern Menze, et al. Metrics reloaded: Pitfalls and recommendations for image analysis validation. arXiv. org, (2206.01653), 2022.   \nYonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep neural classifiers. arXiv preprint arXiv:1805.08206, 2018.   \nFilipe Condessa, Jos\u00e9 Bioucas-Dias, and Jelena Kovac\u02c7evic\u00b4. Performance measures for classification systems with rejection. Pattern Recognition, 63:437\u2013450, 2017.   \nPaul F J\u00e4ger, Carsten L\u00fcth, Lukas Klein, and Till Bungert. A call to reflect on evaluation practices for failure detection in image classification. In ICLR 2023, 2023.   \nTill J Bungert, Levin Kobelke, and Paul F Jaeger. Understanding silent failures in medical image classification. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 400\u2013 410. Springer, 2023.   \nZhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. Unified classification and rejection: A one-versus-all framework. arXiv preprint arXiv:2311.13355, 2023.   \nFei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. Openmix: Exploring outlier samples for misclassification detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12074\u201312083, 2023a.   \nNeeraj Varshney, Swaroop Mishra, and Chitta Baral. Towards improving selective prediction ability of nlp systems. arXiv preprint arXiv:2008.09371, 2020.   \nJunayed Naushad and ID Voiculescu. Super-trustscore: reliable failure detection for automated skin lesion diagnosis. 2024.   \nJordy Van Landeghem, Sanket Biswas, Matthew Blaschko, and Marie-Francine Moens. Beyond document page classification: Design, datasets, and challenges. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2962\u20132972, 2024.   \nJordy Van Landeghem, Rub\u00e8n Tito, \u0141ukasz Borchmann, Micha\u0142 Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Micka\u00ebl Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19528\u201319540, 2023.   \nFei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. Rethinking confidence calibration for failure prediction. In European Conference on Computer Vision, pages 518\u2013536. Springer, 2022.   \nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. The art of abstention: Selective prediction and error regularization for natural language processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1040\u20131051, 2021.   \nHiyori Yoshikawa and Naoaki Okazaki. Selective-lama: Selective prediction for confidence-aware evaluation of language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2017\u20132028, 2023.   \nYukun Ding, Jinglan Liu, Jinjun Xiong, and Yiyu Shi. Revisiting the evaluation of uncertainty estimation and its application to explore model complexity-uncertainty trade-off. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 4\u20135, 2020.   \nFei Zhu, Xu-Yao Zhang, Zhen Cheng, and Cheng-Lin Liu. Revisiting confidence estimation: Towards reliable failure prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023b.   \nIdo Galil and Ran El-Yaniv. Disrupting deep uncertainty estimation without harming accuracy. Advances in Neural Information Processing Systems, 34:21285\u201321296, 2021.   \nVojtech Franc, Daniel Prusa, and Vaclav Voracek. Optimal strategies for reject option classifiers. Journal of Machine Learning Research, 24(11):1\u201349, 2023.   \nJun Cen, Di Luan, Shiwei Zhang, Yixuan Pei, Yingya Zhang, Deli Zhao, Shaojie Shen, and Qifeng Chen. The devil is in the wrongly-classified samples: Towards unified open-set recognition. arXiv preprint arXiv:2302.04002, 2023.   \nGuoxuan Xia and Christos-Savvas Bouganis. Augmenting softmax information for selective classification with out-of-distribution data. In Proceedings of the Asian Conference on Computer Vision, pages 1995\u20132012, 2022.   \nLu\u00eds Felipe Prates Cattelan and Danilo Silva. How to fix a broken confidence estimator: Evaluating post-hoc methods for selective classification with deep neural networks. 2023.   \nDustin Tran, Jeremiah Liu, Michael W Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, et al. Plex: Towards reliability using pretrained large model extensions. arXiv preprint arXiv:2207.07411, 2022.   \nJihyo Kim, Jiin Koo, and Sangheum Hwang. A unified benchmark for the unknown detection capability of deep neural networks. Expert Systems with Applications, 229:120461, 2023.   \nArsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. arXiv preprint arXiv:2002.06470, 2020.   \nGuoxuan Xia, Olivier Laurent, Gianni Franchi, and Christos-Savvas Bouganis. Understanding why label smoothing degrades selective classification and how to fix it. arXiv preprint arXiv:2403.14715, 2024.   \nIdo Galil, Mohammed Dabbah, and Ran El-Yaniv. What can we learn from the selective prediction and uncertainty estimation performance of 523 imagenet classifiers. arXiv preprint arXiv:2302.11874, 2023.   \nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019.   \nIan D Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasserman. Measuring and improving model-moderator collaboration using uncertainty estimation. arXiv preprint arXiv:2107.04212, 2021.   \nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \nAndrea Pugnana and Salvatore Ruggieri. Auc-based selective classification. In International Conference on Artificial Intelligence and Statistics, pages 2494\u20132514. PMLR, 2023.   \nAndrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650, 2020.   \nAndrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455, 2021.   \nMalik Sajjad Ahmed Nadeem, Jean-Daniel Zucker, and Blaise Hanczar. Accuracy-rejection curves (arcs) for comparing classification methods with a reject option. In Machine Learning in Systems Biology, pages 65\u201381. PMLR, 2009.   \nLeo Feng, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, and Amir Abdi. Towards better selective classification. arXiv preprint arXiv:2206.09034, 2022.   \nFrank Wilcoxon. Individual comparisons by ranking methods. In Breakthroughs in statistics: Methodology and distribution, pages 196\u2013202. Springer, 1992.   \nManuel Wiesenfarth, Annika Reinke, Bennett A Landman, Matthias Eisenmann, Laura Aguilera Saiz, M Jorge Cardoso, Lena Maier-Hein, and Annette Kopp-Schneider. Methods and open-source toolkit for analyzing and visualizing challenge results. Scientific reports, 11(1):2369, 2021.   \nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u20131059. PMLR, 2016.   \nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.   \nZiyin Liu, Zhikang Wang, Paul Pu Liang, Russ R Salakhutdinov, Louis-Philippe Morency, and Masahito Ueda. Deep Gamblers: Learning to Abstain with Portfolio Theory.   \nCharles Corbi\u00e8re, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick P\u00e9rez. Addressing failure prediction by learning model confidence. Advances in Neural Information Processing Systems, 32, 2019.   \nTerrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection in neural networks. arXiv preprint arXiv:1802.04865, 2018.   \nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 7. Granada, Spain, 2011.   \nAlex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. ", "page_idx": 11}, {"type": "text", "text": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-thewild distribution shifts. In International conference on machine learning, pages 5637\u20135664. PMLR, 2021. ", "page_idx": 12}, {"type": "text", "text": "Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020. ", "page_idx": 12}, {"type": "text", "text": "Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. ", "page_idx": 12}, {"type": "text", "text": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. ", "page_idx": 12}, {"type": "text", "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. ", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Technical Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1.1 AURC Failure Contribution ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $\\{\\tau_{t}\\}_{t=1}^{N}$ provide a total order on the predictions (relaxing the assumption to a partial order leads to a more complicated but very similar formulation). Then, for a rejection threshold $\\tau_{t}$ , a failure case at rank $t^{*}$ contributes to the Selective Risk (Equation 2) by $\\textstyle{\\frac{1}{t}}$ if $t^{\\ast}\\leq t$ . Averaging over all thresholds yields a contribution through the failure at rank $t^{*}$ to the AURC of tN=t\u2217 t1 . For the Generalized Risk, the contribution at $t^{\\ast}\\leq t$ is always $\\textstyle{\\frac{1}{N}}$ , resulting in an overall contribution of $\\frac{N-t^{*}-1}{N^{2}}$ to the AUGRC. The failure contribution curves are displayed in Figure 2. ", "page_idx": 12}, {"type": "text", "text": "A.1.2 Relationship between AUGRC and failure AUROC ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $Y_{\\mathrm{f}}=\\mathbb{I}(m(x)\\neq y)$ be a binary indicator variable for classification failures. Then, the Generalized Risk corresponds to the joint probability that a sample is accepted and wrongly classified. We can write the AUGRC as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{AUGRC}=\\displaystyle\\int P(Y_{\\mathsf{f}}=1,g(x)\\ge\\tau)\\,\\mathrm{d}P(g(x)\\ge\\tau)}\\\\ {=\\displaystyle\\int(1-P(Y_{\\mathsf{f}}=0|g(x)\\ge\\tau))\\cdot P(g(x)\\ge\\tau)\\,\\mathrm{d}P(g(x)\\ge\\tau)}\\\\ {=\\frac{1}{2}-\\displaystyle\\int P(g(x)\\ge\\tau|Y_{\\mathsf{f}}=0)P(Y_{\\mathsf{f}}=0)\\,\\mathrm{d}P(g(x)\\ge\\tau)}\\\\ {\\overset{(*)}{=}\\frac{1}{2}-\\mathrm{acc}\\left[\\frac{1}{2}\\mathrm{acc}+(1-\\mathrm{acc})\\cdot\\displaystyle\\int P(g(x)\\ge\\tau|Y_{\\mathsf{f}}=0)\\,\\mathrm{d}P(g(x)\\ge\\tau|Y_{\\mathsf{f}}=1)\\right]}\\\\ {=\\frac{1}{2}-\\mathrm{acc}\\left[\\frac{1}{2}\\mathrm{acc}+(1-\\mathrm{acc})\\cdot\\mathrm{AUROC}_{\\mathsf{f}}\\right]}\\\\ {=(1-\\mathrm{AUROC}_{\\mathsf{f}})\\cdot\\mathrm{acc}(1-\\mathrm{acc})+\\frac{1}{2}(1-\\mathrm{acc})^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "At $(*)$ , we use that $P(g(x)\\geq\\tau)=P(g(x)\\geq\\tau|Y_{\\mathrm{f}}=0)\\cdot\\operatorname{acc}+P(g(x)\\geq\\tau|Y_{\\mathrm{f}}=1)\\cdot(1-\\operatorname{acc})$ . On choosing two random samples, the second term in the final equation represents the probability that both are failures and the first term represents the probability that one is a failure, the other is not, and that the failure has higher confidence score than the other. ", "page_idx": 12}, {"type": "text", "text": "The AUGRC is monotonic in acc and $\\mathrm{AUROC_{f}}$ as both partial derivatives are negative $\\forall\\operatorname{acc}\\ \\in$ $(0,1),\\forall\\,\\mathrm{AUROC_{f}\\in[0,1]}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\mathrm{AUGRC}}{\\partial\\mathrm{AUROC}_{\\mathrm{f}}}=-\\mathrm{acc}\\cdot(1-\\mathrm{acc})\\qquad\\qquad}\\\\ {\\frac{\\partial\\mathrm{AUGRC}}{\\partial\\mathrm{acc}}=2\\cdot\\mathrm{AUROC}_{\\mathrm{f}}\\cdot\\mathrm{acc}-\\mathrm{acc}-\\mathrm{AUROC}_{\\mathrm{f}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "2TktDpGqNM/tmp/e23cdc1bd5c8fa01e512a810c4626d8e5e19f0871b86a2104a9dad17c7945ec5.jpg", "img_caption": ["Figure 5: Visualization of the relationship between AUGRC and $\\mathbf{AUROC_{f}}.$ . (a) The Selective Risk curve can be transformed into the Generalized Risk curve via multiplication by the respective coverages. The resulting curve is monotonically increasing and bounded by the diagonal; decreasing Selective Risk corresponds to a plateau in Generalized Risk. The AUGRC corresponds to the AUGRC of an optimal CSF (shaded red) plus the re-scaled $\\mathrm{{AUROC}_{f}}$ (shaded in green). The $\\mathrm{{AUROC}_{f}}$ corresponds to the fraction of the area (parallelogram) enclosed by the green dashed line that lies above the Generalized Risk curve. (b) AUGRC (color-coded) and its negative gradients (arrows) in the Accuracy-AUROCf space. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.1.3 F1-AUC does not fulfill R2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For the optimal CSF, all failure cases are attributed to lower confidence scores than correct predictions. Hence, the precision $P(Y_{\\mathrm{f}}=0|g(x)\\geq\\tau)$ is one for coverages up to $P(Y_{\\mathrm{f}}=0)$ and decreases as $P(Y_{\\mathrm{f}}\\,=\\,0)/\\bar{P(g(x)\\,\\geq\\,\\tau)}$ above. Following the task formulation as defined in Malinin et al. [2021], we can derive the following analytical expression for the optimal F1-AUC\u2217: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{F1-AUC}^{*}=\\displaystyle\\int_{0}^{1}\\mathrm{F1}(\\tau)\\,\\mathrm{d}P(g(x)\\ge\\tau)}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{0}^{P(Y_{t}=0)}\\frac{2\\cdot P(g(x)\\ge\\tau)}{P(Y_{t}=0)+P(g(x)\\ge\\tau)}\\,\\mathrm{d}P(g(x)\\ge\\tau)}\\\\ &{\\qquad\\qquad+\\displaystyle\\int_{P(Y_{t}=0)}^{1}\\frac{2\\cdot P(Y_{t}=0)}{P(Y_{t}=0)+P(g(x)\\ge\\tau)}\\,\\mathrm{d}P(g(x)\\ge\\tau)}\\\\ &{\\qquad=2\\cdot\\mathrm{acc}\\cdot\\left[1+\\ln\\left(\\frac{1}{4}\\cdot(1+\\frac{1}{\\mathrm{acc}})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "F1-AUC\u2217increases with increasing accuracy up to $P(Y_{\\mathrm{f}}=0)\\approx0.56$ , then it decreases, favoring models with lower classification performance. Thus, the F1-AUC does not fulfill the monotonicity requirement (R2). ", "page_idx": 13}, {"type": "text", "text": "A.2 Experiment Setup ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Datasets and Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We compare the following CSFs: From the classifier\u2019s logits we compute the Maximum Softmax Response (MSR), Maximum Logit Score (MLS), Predictive Entropy (PE), and the MLS based on temperature-scaled logits, for which a scalar temperature parameter is tuned based on the validation set [Guo et al., 2017]. Three predictive uncertainty measures are based on Monte Carlo Dropout [Gal and Ghahramani]: mean softmax (MCD-MSR), predictive entropy (MCD-PE), and mutual information (MCD-MI). We additionally include the DeepGamblers method [Liu et al.], which learns a confidence like reservation score (DG-Res), ConfidNet Corbi\u00e8re et al. [2019], which is trained as an extension to the classifier, and the work of DeVries and Taylor [2018]. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "We evaluate SC methods on the FD-Shifts benchmark [J\u00e4ger et al., 2023], which considers a broad range of datasets and failure sources through various distribution shifts: SVHN [Netzer et al., 2011], CIFAR-10, and CIFAR-100 [Krizhevsky] are evaluated on semantic and non-semantic new-class shifts in a rotating fashion including Tiny ImageNet [Le and Yang, 2015]. Additionally, we consider sub-class shifts on iWildCam [Koh et al., 2021], BREEDS-Enity-13 [Santurkar et al., 2020], CAMELYON-17-Wilds [Koh et al., 2021], and on CIFAR-100 (based on super-classes) as well as corruption shifts based on Hendrycks and Dietterich [2019] on CIFAR-100. The data preparation and splitting are done as described in J\u00e4ger et al. [2023]. For the corruption shifts, we reduce the test split size to 75000 (subsampled within each corruption type and intensity level). ", "page_idx": 14}, {"type": "text", "text": "The following classifier architecture are used in the benchmark: small convolutional network for SVHN, VGG-13 [Simonyan and Zisserman, 2014] on CIFAR-10/100, ResNet-50 [He et al., 2016] on the other datasets. ", "page_idx": 14}, {"type": "text", "text": "If the distribution shift is not mentioned explicitly, we evaluate on the respective i.i.d. test datasets. ", "page_idx": 14}, {"type": "text", "text": "Our method ranking study focuses on the evaluation of CSF performance based on the existing FD-Shifts benchmark, hence we required no GPU\u2019s for the analysis in Section. 4. As both AURC and AUGRC can be computed efficiently, (CPU) evaluation time for a single test set is less than a minute; evaluation on 500 bootstrap samples on a single CPU core take around 3 hours. ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Hyperparameters and Model Selection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The experiments are based on the same hyperparameters as reported in Table 4 in J\u00e4ger et al. [2023]. Based on the performance on the validation set, we choose the DeepGambler reward hyperparameter and whether to use dropout (for the non-MCD-based CSFs). For the former, we select from [2.2, 3, 6, 10] on Wilds-Camelyon-17, CIFAR-10, and SVHN, from [2.2, 3, 6, 10, 15] on iWildCam and BREEDS-Entity-13, and from [2.2, 3, 6, 10, 12, 15, 20] on CIFAR-100. When performing model selection based on the AURC metric, we obtain the same configurations as reported in J\u00e4ger et al. [2023]. When performing model selection based on the AUGRC metric, we obtain the parameters as reported in Tab. 1 and Tab. 2. For temperature scaling, we optimize the NLL on the validation set using the L-BFGS algorithm with $\\mathrm{lr}=0.01$ . ", "page_idx": 14}, {"type": "table", "img_path": "2TktDpGqNM/tmp/106865ed6e7d195552b3169600daa417fd6c54c332c2b3719a725e90ea35e896.jpg", "table_caption": ["Table 1: Selected DeepGambler reward hyperparameter based on the AUGRC on the validation set for all confidence scoring functions trained with the DeepGamblers objective. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 2: Whether or not dropout-training has been selected based on the AUGRC on the validation set. This selection is only done for deterministic confidence scoring methods (no MCD). \"1\" denotes dropout training and $\"0\"$ denotes training without dropout. ", "page_idx": 14}, {"type": "table", "img_path": "2TktDpGqNM/tmp/2a27f919e256eab18e92aacb88292eaf5cfaf6d2229cfe57f26fdc7b8715381a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 AUGRC Computation Time ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The main bottleneck of the AUGRC computation is the sort operation on the confidence scores. This is the same as for the AURC and there is no computational overhead compared to the AURC. For a small benchmark of the computational time, we evaluate the AURC, AUGRC, and $\\mathrm{{AUROC}_{f}}$ on 1000 random scores and failure labels. We obtain the following results (averaged over 5000 cases): $564\\mu s\\pm2\\mu s$ (AURC), $562\\mu s\\pm2\\mu s$ (AUGRC), $730\\mu s\\pm13\\mu s$ $\\mathrm{{AUROC}_{f})}$ ). ", "page_idx": 15}, {"type": "text", "text": "A.4 Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 3 shows the AUGRC results for the 13 compared CSFs across all datasets and distribution shifts. While the MSR baseline is not consistently outperformed across all experiments by any of the compared CSFs, MCD improves MSR scores in most settings. Temperature scaling does not exhibit consistent improvement of MLS scores. ", "page_idx": 15}, {"type": "text", "text": "Table 3: FD-Shifts benchmark results measured as AUGRC $\\times10^{3}$ (score range: [0, 500], lower is better \u2193). The color heatmap is normalized per column, whereby whiter colors depict better scores. \"cor\" is the average over 5 intensity levels of image corruption shifts. AUGRC scores are averaged over 10 runs on CAMELYON-17-Wilds, over 2 runs on BREEDS, and over 5 runs on all other datasets. Abbreviations: ncs: new-class shift (s for semantic, ns for non-semantic), iid: independent and identically distributed, sub: sub-class shift, cor: image corruptions, c10/100: CIFAR-10/100, ti: TinyImagenet. ", "page_idx": 15}, {"type": "image", "img_path": "2TktDpGqNM/tmp/5aa9377192f2622e4e3c007a871476acdb41101120bbdb8f9b557fc7a4a90f7c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 4: Comparing Rankings of $\\mathbf{AURC}\\to\\alpha$ versus $\\mathbf{AUGRC}\\to{\\boldsymbol{\\beta}}.$ . Differences in the method rankings between AURC and AUGRC demonstrate the relevance of the pitfalls of the AURC discussed in Section 2.4. Upper half: Model selection for dropout and DG hyperparameter was done for both metrics separately. Lower half:Model eletionwasdonebasedonAUGRCforboth $\\alpha$ and $\\beta$ The selctedyperparametrs arrepordnppndixA2.Tholrheatmapis normalizd peroluwreywhitclorspittecreoristaveraevrnnsityevelfmagecorrutionhiftoresareaveragedv 10 runs on CAMELYON-17-Wilds, over 2 runs on BREEDS, and over 5 runs on all other datasets. Abbreviations: ncs: new-class shift (s for semantic, ns for non-semantic), id: independent and identically distributed, sub: sub-class shift, cor: image corruptions, c10/100: CIFAR-10/100, ti: TinyImagenet. ", "page_idx": 16}, {"type": "image", "img_path": "2TktDpGqNM/tmp/934fdde879fa9aff30b0a2e0a6d94d1c0bd66aa342a97b48d141804a44107313.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "2TktDpGqNM/tmp/446c5603207111b2f7815854d83e9d77dfe39c2fa59709d9d0e4c113d6268e90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: Method ranking analysis for distribution shifts. This figure displays the results for the method ranking analysis for AURC and AUGRC for evaluation under distribution shifts, analogous to Figure 3. The subfigures are titled as follows: \"training dataset\" $\\rightarrow$ \"evaluation dataset\". Details on the distribution shifts are given in Section A.2.1. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset toensure ranking stability.Ranking change are reected ichangesn thecolor sequenceand highlighted byred arrows.We assess the stability of themethod rankings for each metric individuallyusing onesided Wilcoxon signed-rank testsbased on the bootstrap samples at $5\\%$ significance level with adjustment for multiple testing according to Holm.Adacent to each ranking,we present the resulting signifcancemaps for the pairwise CSF comparisons.Thesemaps canb interpreted as follows: At each grid position $(x,y)$ , flled entries indicate that metric values of CSF $y$ are ranked significantlybetter than those from CSF $x$ (across botstrap samples), crosmarks indicate no signifcant superiority. Abbreviations: ns: new-class shift s for smantic, ns for non-smanti). ", "page_idx": 17}, {"type": "image", "img_path": "2TktDpGqNM/tmp/f05e54eb2ea0e59f8de9f9086be9b6fbbeb727cb8b40f3121792a54939ec9044.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Method ranking analysis without correction for multiple testing. In contrast to the results shown in Figure 3, which include the Holm correction for multiple testing, this figure displays the results without correction. On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed AUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests, each with a $5\\%$ significance level, based on the bootstrap samples. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position $(x,y)$ , filled entries indicate that metric values of $\\mathrm{CSF\\,}y$ are ranked significantly better than those from CSF $x$ (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The described pitfalls of existing metrics affect Selective Classification evaluation in general. Hence, we thoroughly discuss all common metrics in the field and demonstrate the relevance on a state-of-the-art benchmark. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The empirical study is based on an extensive, state-of-the-art, yet naturally limited benchmark. However, the scope is clearly defined in the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: For the analytical relationships, we provide the assumptions and derivations. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The evaluation protocol as well as all experiment setups are described. Additionally, we provide the code for reproducing the results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The code for reproducing the experimental results is published on github. All datasets used are freely accessible. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The details of the experiments setups are described in the paper. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We validate the significance of our results via statistical tests based on bootstrapping. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Details are mentioned in the Appendix ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: There are no conflicts with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We don\u2019t see any potential negative societal impacts. We address enhanced reliability of real-world decision systems through the studied methods as a potential positive impact. However, we propose no new Selective Classification method in our work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our experiments do not involve any new datasets or classification models. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We added proper credit for all datasets and models used in our experiments. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the code for our proposed evaluation protocol including an implementation of our proposed metric. Documentation is added for reproducibility. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Not applicable. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]