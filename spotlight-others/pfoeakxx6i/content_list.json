[{"type": "text", "text": "Algebraic Positional Encodings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Konstantinos Kogkalidis1,2 Jean-Philippe Bernardy3,4 Vikas Garg1,5 kokos.kogkalidis@aalto.fi jean-philippe.bernardy@gu.se vgarg@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "1Aalto University 2University of Bologna 3University of Gothenburg 4Chalmers University of Technology 5YaiYai Ltd ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often ad hoc, approaches. Our framework provides a flexible mapping from the algebraic specification of a domain to an interpretation as orthogonal operators. This design preserves the algebraic characteristics of the source domain, ensuring that the model upholds its desired structural properties. Our scheme can accommodate various structures, including sequences, grids and trees, as well as their compositions. We conduct a series of experiments to demonstrate the practical applicability of our approach. Results suggest performance on par with or surpassing the current state-of-the-art, without hyper-parameter optimizations or \u201ctask search\u201d of any kind. Code is available through https://aalto-quml.github.io/ape/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Attention-based models inheriting from the Transformer [Vaswani et al., 2017] have become ubiquitous in neural computation, supplanting the go-to models of the last decade and driving a continuous stream of breakthroughs across diverse domains. Their success is perhaps at odds with the Transformer\u2019s structural lenience \u2013 its key building block, dot-product attention, is by default unable to perceive and utilize the structure and arrangement of the input/output tokens being processed. To address this limitation, a plethora of works have sought to endow Transformers with appropriate inductive biases. The most common strategy is to adjust token representations via so-called positional encodings; vector operations that hint at the structure being modeled. Nonetheless, most positional encoding schemes to date are either empirically motivated, or tailored to specific tasks. This renders their theoretical evaluation challenging, and hinders any prospects of a unifying framework. ", "page_idx": 0}, {"type": "text", "text": "In this study, we seek to fill this gap with a theory-first approach. Through the lens of group theory, we scrutinize some of the most commonly targeted data structures, and express them by means of inductive definitions that reveal and explicate their structural properties. Leveraging this analysis, our modeling strategy invokes a homomorphic interpretation that maps each domain into algebraic positional encodings (APE): attention-compatible vector operations parameterizing (subgroups of) the orthogonal group. In the sequential context, algebraic positional encodings streamline the widely adopted rotary encodings of Su et al. [2023], while also offering clear theoretical insights on their success. More importantly, algebraic positional encodings naturally extend to non-sequential domains, such as $\\kappa$ -ary trees and multidimensional regular grids, paving the way for a simple and elegant methodology for interpretable and domain-general structurally-refined Transformers. We carry out an experimental evaluation in settings that allow for reproducible and statistically sound conclusions. Across the tasks considered, algebraic positional encodings consistently and significantly outperform strong baselines at an aggregate level, providing initial but compelling evidence that they constitute not just a sensible meta-theory for positional encodings, but also an actionable alternative to the current state of the art. ", "page_idx": 0}, {"type": "text", "text": "$\\S2$ BACKGROUND We introduce the problem (\u00a72.1) and the vocabulary of the solution (\u00a72.2). ", "page_idx": 1}, {"type": "text", "text": "We provide an algebraic characterization of positions in the context of different ambient structures. We frame algebraic positional encodings as structure-preserving semantic interpretations, and present reference implementations. Concretely: ", "page_idx": 1}, {"type": "text", "text": "$\\S3.1$ Sequences are an isomorphism of the free group $\\langle\\mathbb{1}\\rangle$ (i.e., the integers, $\\mathbb{Z}$ ), and can be interpreted as a single generator subgroup of the orthogonal group $O(d)$ .   \n$\\S3.2$ Rotary Positional Encodings correspond to a (quite literally) special case of this interpretation: $S O(d)$ .   \n$\\S3.3\\,k$ -ary Trees are an isomorphism of the finitely generated group $\\langle\\mathbb{1},2\\dots\\kappa\\rangle$ , and can be interpreted as a finitely generated subgroup of $O(d)$ .   \n$\\S3.4$ Regular Grids are the group direct sum of multiple sequences. They can be interpreted as the matrix direct sum of their components\u2019 interpretations.   \n$\\S3.5$ Extensions can be obtained in multiple directions. ", "page_idx": 1}, {"type": "text", "text": "$\\S4$ PRACTICE ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We carry out fair and replicable experiments across all three structures analyzed, namely Sequence Transduction (\u00a74.1), Tree Transduction (\u00a74.2) and Image Recognition (\u00a74.3), and find that algebraic positional encodings consistently match or outperform alternative schemes in the tasks considered (\u00a74.4). ", "page_idx": 1}, {"type": "text", "text": "$\\S5$ RELATED WORK We position our work in the context of the broader literature. ", "page_idx": 1}, {"type": "text", "text": "$\\S6$ LIMITATIONS We close with a brief discussion of possible limitations and potential future improvements. ", "page_idx": 1}, {"type": "text", "text": "Table 1: A summary of this paper. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 The Problem with Dot-Product Attention ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "All transformer variants employ some variation of the multi-head scaled dot-product attention mechanism of Vaswani et al. [2017]. For each attention head, the dot-product attention between queries $\\pmb{X}\\in\\mathbb{R}^{m\\times d}$ and keys $Y\\in\\mathbb{R}^{n\\times d}$ is defined as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{atn}(X,Y):=\\mathrm{softmax}_{(n)}\\bigg(\\frac{(X\\Phi^{(q)})(Y\\Phi^{(k)})^{\\top}}{\\sqrt{d}}\\bigg)\\,Y\\Phi^{(v)}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In equation (1), matrices $\\Phi^{(q)},\\Phi^{(k)},\\Phi^{(v)}:\\mathbb{R}^{d\\times d}$ enact linear functions, applied point-wise (broadcasted) across all $m$ and $n$ entries of $\\mathbf{\\deltaX}$ and $\\mathbf{\\deltaY}$ . The dot-product term $(X\\Bar{\\Phi^{(q)}})(\\Bar{Y}\\Phi^{(k)})^{\\top}$ contains unnormalized attention scores in the Cartesian product of queries and keys. Unmodified, dot-product attention is permutation invariant with respect to its second argument; that is, for any arbitrary permutation $p_{n}\\in S_{n}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{atn}(X,Y)\\equiv\\mathrm{atn}(X,p_{n}(Y))\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Unless one is dealing with orderless structures like multisets or fully connected graphs, this property is generally undesirable. The lack of structural biases is typically counteracted by the component-wise addition of unidimensional periodic signals of varying frequencies. These, however, often prove inadequate in data-scarce domains, where extensive pretraining is impossible, and structure-rich domains, where a sequence-of-tokens projection is too radical of a simplification. ", "page_idx": 1}, {"type": "text", "text": "2.2 Recap on Group Theory ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To address this issue, we propose an algebraic treatment of positional encodings, based on principles lent from group theory. For the sake of convenience and accessibility, we provide a brief recap of the notions of interest here. A group $G$ consists of a set of elements and a binary operation $\\mathrm{(~\\underline{{~\\cdot~}}~})$ satisfying four fundamental laws: ", "page_idx": 1}, {"type": "text", "text": "\u2022 The group is closed under the the group operation. For all $a,b$ in $G$ , $a\\cdot b$ is also in $G$ .   \n\u2022 The group operation is associative. For all $a$ , $b$ , $c$ in $G,\\,(a$ $(a\\cdot b)\\cdot c=a\\cdot(b\\cdot c)$ .   \n\u2022 The group operation has an identity element $e$ , such that for all $a$ in $G$ , $a\\cdot e=e\\cdot a=a$ .   \n\u2022 Each group member has an inverse. For all $a$ in $G$ , there exists some element $\\overline{a}$ such that $a{\\overline{{a}}}={\\overline{{a}}}a=e$ , where $e$ is the identity element. ", "page_idx": 1}, {"type": "text", "text": "A group is characterized as finite or infinite depending on the number of elements it has. If all elements of a group $G$ can be expressed as a combination of a subset $S$ of the group elements (combined by means of the group operation, applied either on the elements themselves or on their inverses), we write $G=\\langle S\\rangle$ . We say that $G$ is generated by $S$ , and we call the elements of $S$ the generators of $G$ . A group with a single generator is called cyclic. ", "page_idx": 2}, {"type": "text", "text": "3 The Algebra(s) of Positions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our objective is to establish a framework that offers general and extensible semantics for positions across various structures \u2013 what we commonly encounter in the literature as positional encodings. Most existing proposals adopt a rather parochial stance, relying on maneuvers or heuristics tailored to specific applications and driven, predominantly, by extensive empirical investigations. As such, they fall short with respect to accommodating or reflecting the properties of the underlying structure. In this work, we follow a different approach. We adopt Montague\u2019s perspective, succinctly paraphrased as: ", "page_idx": 2}, {"type": "text", "text": "\u201csyntax is an algebra, semantics is an algebra, and meaning is a homomorphism between them\u201d [Janssen, 2014]. ", "page_idx": 2}, {"type": "text", "text": "We begin by noting that \u201cpositions\u201d do not exist in isolation, but only in the context of some underlying ambient structure. We contend that reasonable positional encodings (semantics) may only be reliably obtained by taking into account exactly this structure, its formation rules and properties (syntax), and then applying an appropriate interpretation (meaning). This is not just an academic exercise: a careful syntactic specification is a prerequisite if we aim for semantics that adhere to certain properties, which is arguably preferable to searching for these properties in the wild. ", "page_idx": 2}, {"type": "text", "text": "3.1 Sequences ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Syntax We start from the simplest structure, and incidentally also the most standard one: the sequence. The full range of positions a token can occupy within a sequence coincides exactly with the naturals, \u2115. Relative paths $\\mathbb{P}$ between any two positions can then be seen as the integers, $\\mathbb{Z}$ , with positive (resp. negative) numbers denoting forward (resp. backward) offsets. Using this insight, it is handy to inspect how the standard inductive definition of the integers provides the building blocks for path formation. We start with two constants: the empty path $(\\mathbb{O})$ , which relates any given point to itself, and the unit path (\ud835\udfd9), which relates any point to its immediate next. We may compose simple paths into complex ones with the aid of a binary operation $+\\mathbf{\\rho}_{\\mathbb{P}}$ . This already suffices to specify all forward offsets. In order to construct backward offsets, we need a unary operation $(-)_{\\mathbb{P}}$ , such that $-\\,\\rho$ denotes the inverse of $\\rho$ . We can summarize the above by the grammar: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}:=\\mathbb{0}\\mid\\mathbb{1}\\mid\\mathbb{P}+_{\\mathbb{P}}\\mathbb{P}\\mid-\\mathbb{P}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For this to make sense, the operations must be coherent; that is, all ways to start from point $\\rho_{1}$ and end up in point $\\rho_{2}$ should be equivalent, even if apparently distinct. The needed equivalences exactly correspond to the group laws, with closure internalized by the inductive definition of (3): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left(\\rho_{1}+\\lnot\\mathrm{p}\\;\\rho_{2}\\right)+\\l p\\;\\rho_{3}=\\rho_{1}+\\l p\\left(\\rho_{2}+\\lnot\\mathrm{p}\\;\\rho_{3}\\right)}}\\\\ {{\\rho+\\l p\\;\\mathbb{0}=\\rho=\\mathbb{0}+\\rho}}\\\\ {{\\rho+\\l p\\left(-\\,\\rho\\right)=\\mathbb{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The (unsurprising) insight here is that paths in a sequence form a free group, generated by a single generator (\ud835\udfd9) \u2013 the uniqueness of the generator exceptionally also makes the group abelian (i.e., commutative). For convenience, we adopt the notational shorthand $\\mathbb{1}^{p}$ , where: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{1}^{p}:=\\left\\{\\overbrace{\\underbrace{(-\\mathbb{1})+\\mathbb{P}\\cdot\\cdot\\cdot\\cdot+\\mathbb{P}\\left(-\\mathbb{1}\\right)}_{p}}^{\\mathbb{1}+\\mathbb{P}\\cdot\\mathbb{\\cdot\\cdot\\cdot}+\\mathbb{P}}\\right.\\quad p\\ge0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Semantics The syntactic specifications of the previous paragraph impose constraints on the candidate semantic targets. Among these candidates, we isolate and focus on $\\langle W\\rangle$ , the subgroup of the orthogonal group $O(d)$ that is generated by a single orthogonal matrix $W$ . This semantics is not only sound1 with respect to the structure under scrutiny, but also a familiar object in machine learning literature [Arjovsky et al., 2016, Bernardy and Lappin, 2022, inter alia]. Note that for $\\langle W\\rangle$ , the group axioms are obtained for free from the orthogonal group, and the additional requirement of commutativity is again satisfied by the uniqueness of the generator. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To illustrate the correspondence between the two structures (and at risk of being pedantic), we spell out the homomorphism \u2308.\u2309, which maps paths $\\mathbb{P}$ to elements of $\\langle W\\rangle$ , and path operations to operations on orthogonal matrices of size $d$ . For the primitives, we have $\\lceil\\emptyset\\rceil\\,:=\\,I_{d}$ and $\\lceil\\mathbb{1}\\rceil:=\\bar{W}$ . Path composition amounts to matrix multiplication, i.e., $\\left\\lceil\\rho_{1}+\\!\\mathbb{P}\\,\\rho_{2}\\right\\rceil:=\\left\\lceil\\rho_{1}\\right\\rceil\\left\\lceil\\rho_{2}\\right\\rceil$ , while path inversion corresponds to matrix transposition, i.e., $\\left\\lceil-\\,\\rho\\right\\rceil:=\\left\\lceil\\rho\\right\\rceil^{-1}\\equiv\\left\\lceil\\rho\\right\\rceil^{\\top}$ . The fact that orthogonal matrices form a group under multiplication is folklore; one can easily verify that the group laws hold also for the semantics.2 ", "page_idx": 3}, {"type": "text", "text": "Implementation In practice, we have $\\left\\lceil\\mathbb{T}^{p}\\right\\rceil\\ \\mapsto\\ W^{p}$ ; a norm-preserving bilinear form $\\mathbb{R}^{d}~\\times$ $\\mathbb{R}^{d}\\ \\rightarrow\\ \\mathbb{R}$ which can be used to mediate the dot-product between a query $q$ and a key $k$ offset by a relative distance of $p$ . The representation of all paths up to length $p$ can thus be implemented as a matrix collection $[\\boldsymbol{W}^{0},\\ldots,\\boldsymbol{W}^{p}]$ , which can asymptotically be obtained using ${\\mathcal{O}}(\\lceil\\log_{2}(p)\\rceil)$ matrix products (of exponentially larger matrices), and taking up the storage space equivalent of $(p d^{2})$ floats. Transposed, the same matrices also serve to represent backwards paths $[W^{-\\bar{p}},\\dots,W^{0}]$ . Storing the representations of all relative paths between queries and keys in a tensor $\\pmb{T}:\\mathbb{R}^{m\\times n\\times d\\times d}$ , we may then substitute the dot-product term of equation (1) for the tensor contraction: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{\\alpha,\\beta}X_{m\\alpha}\\pmb{\\Phi}_{\\alpha\\beta}^{(q)}T_{m n\\beta\\gamma}Y_{n\\delta}\\pmb{\\Phi}_{\\delta\\gamma}^{(k)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Albeit transparent, this reduction strategy is computationally unappealing due to the doubly quadratic nature of $\\textbf{\\emph{T}}$ . We can do better by noting that ${\\mathbf{}}T_{m n}$ is (definitionally) equal to: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal T}_{m n\\alpha\\beta}=\\sum_{\\gamma}{\\cal A}_{m\\gamma\\alpha}^{(X)}{\\cal A}_{n\\gamma\\beta}^{(Y)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A^{(X)}$ and $A^{(Y)}$ are the matrices containing representations for the absolute positions of the entries in $\\mathbf{\\deltaX}$ and $\\mathbf{\\deltaY}$ , respectively. Concretely, a single relative representation is built by composing the inverted representation of the source with the representation of the target. Intuitively, each query follows the path that takes it back to the origin, which then allows it to directly combine with each forward-offset key; see Figure 1a for a visual example. This insight allows us to keep the memory footprint of equation (1) unchanged, replacing expression (5) with: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{\\alpha,\\beta,\\gamma,\\delta,\\epsilon}X_{m\\alpha}\\Phi_{\\alpha\\beta}^{(q)}A_{m\\gamma\\beta}^{(X)}A_{n\\gamma\\delta}^{(Y)}Y_{n\\epsilon}\\Phi_{\\epsilon\\delta}^{(k)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This version decomposes the tensor contraction into two matrix multiplications, essentially transforming (rotating or reflecting) the entries of $\\mathbf{\\deltaX}$ and $\\mathbf{Y}$ independently according to their positions. ", "page_idx": 3}, {"type": "text", "text": "3.2 Intermezzo: Equivalence with RoPE", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The story so far should be reminiscent of the rotary positional encoding scheme of Su et al. [2023, RoPE]. Not unlike our approach, RoPE substitutes the vanilla dot-product for a position-dependent bilinear form. Underlying the form is a $d\\times d$ -dimensional matrix $\\boldsymbol{R}$ with a block-diagonal structure, where each $2\\times2$ -sized block corresponds to a rotation matrix that acts on a 2-dimensional subspace of $\\mathbb{R}^{d}$ . These independent rotations are parameterized by a (fixed) set of base angles $\\Theta:=[\\theta_{1},\\ldots,\\theta_{d/2}]$ To incorporate position-dependence, i.e., for a query/key pair at a relative distance of $p$ , the base angles are multiplied by $p$ , effectively altering the rotations applied. ", "page_idx": 3}, {"type": "text", "text": "At first glance, rotary encodings appear to be under-parameterized, and thus strictly weaker than orthogonal ones. However, any orthogonal matrix $W\\in O(d)$ admits a canonical form $\\dot{\\boldsymbol{W}}=\\boldsymbol{P}\\boldsymbol{Q}\\boldsymbol{P}^{\\intercal}$ , where $_{P}$ is an orthogonal change of basis, and $Q$ is block-diagonal, with the $2\\times2$ -sized blocks being, once again, $2-$ dimensional rotation matrices [Murnaghan and Wintner, $1931]^{3}$ . Owing to the orthogonality of $_{P}$ , raising $W$ to its $p$ th power is equal to $P Q^{p}P^{\\top}$ (i.e., it leaves the change of basis unaffected). In turn, raising $Q$ to its $p$ th power is equivalent to simply multiplying the rotation angles of its blocks by $p$ . Finally, given the linearity of the transformations $\\Phi^{\\bar{(q)}}$ and $\\Phi^{(k)}$ , their compositions with $_{P}$ are also linear. By identifying $Q$ with RoPE\u2019s $\\boldsymbol{R}$ , we can then see that, for any given collection of angles $\\Theta$ , APE and RoPE coincide under the substitutions: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi_{\\mathrm{RoPE}}^{(q)}=\\Phi^{(q)}Q\\quad\\mathrm{and}\\quad\\Phi_{\\mathrm{RoPE}}^{(k)}=\\Phi^{(k)}Q\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, APE is practically equivalent to a trainable version of RoPE, where the rotation angles $\\boldsymbol{\\Theta}$ may vary and be optimized during training4. ", "page_idx": 4}, {"type": "text", "text": "Which of the two parameterizations is preferable is up to debate. On the one hand, APE\u2019s formulation is FLOP-optimized (being just matrix multiplications), and obviates the need for backpropagating through trigonometric functions (which are periodic, non-monotonic, and prone to gradient instabilities). On the other hand, RoPE\u2019s diagonalized form gives access to a memory-efficient contraction that does away with the matrix multiplications of expression (7) altogether; we direct the interested reader to Su et al. [2023, Section 3.4.2] for a reference implementation5. ", "page_idx": 4}, {"type": "text", "text": "In either case, the equivalence between the two is confined to the sequential setup; we will now move on to generalize our strategy to other, previously inaccessible, structures. ", "page_idx": 4}, {"type": "text", "text": "3.3 Trees ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Syntax In the previous section, we characterized the structure of relative paths on a sequence as the free group with one generator, and uncovered a (practically) isomorphic interpretation in the subgroup of orthogonal matrices with a single generator. Upon closer inspection, we note that a sequence can be viewed as a special case of the more general structure of $\\kappa$ -ary branching trees, where the branching factor $\\kappa$ just so happens to be 1. Denoting the more general case as $\\mathbb{P}_{\\kappa}$ , we must first extend the set of primitives to include all branching options, $\\mathbb{1},2,\\dots\\kappa:\\mathbb{P}_{\\kappa}$ . Each primitive now denotes a choice of branch (except for $\\mathbb{0}$ , which is again the empty path). Paths now form a free group with $\\kappa$ distinct generators. The presence of multiple generators means that commutativity no longer holds; $\\mathbb{1}+\\mathbb{P}_{\\kappa}\\mathrm{~}^{2}$ is distinct from $2+\\mathbb{P}_{\\kappa}\\mathbb{\\Omega}$ (the former prescribes a descent down branch $\\mathbb{1}$ then branch \ud835\udfda, whereas the latter prescribes a descent down branch \ud835\udfdathen branch \ud835\udfd9). Inversion is as before: for every path from each local root to some descendant down the line, there is also an inverse path from that descendant up to its ancestor. Perhaps more interestingly, upwards and downwards paths can be joined, allowing the precise specification of relative paths between any two nodes, even when the two do not share a single line of descent (think nephews, aunts and all other sorts of distant relatives, see Figure 1b for an example). Adjusting grammar (3) accordingly, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\kappa}:=\\mathbb{0}\\mid\\mathbb{1}\\mid2\\mid\\dots\\mid\\kappa\\mid\\mathbb{P}_{\\kappa}+\\mathbb{P}_{\\kappa}\\mid-\\mathbb{P}_{\\kappa}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with laws L1, L2 and L3 still in effect. ", "page_idx": 4}, {"type": "text", "text": "Semantics The interpretation follows along the same lines as before. This time around, however, we cannot make do with a single orthogonal matrix $W-\\mathrm{we}$ need a collection of $\\kappa$ matrices, one for each branch option. As a consequence, the semantic target is now $\\left\\langle W_{1},W_{2},\\ldots W_{\\kappa}\\right\\rangle$ . Note that the target is no longer commutative (in alignment with the source). ", "page_idx": 4}, {"type": "text", "text": "Implementation For a tree structure of depth $\\delta$ and branching factor $\\kappa$ , let $\\nu$ denote the number of unique absolute positions occupied (upper bound by $\\kappa^{\\delta}$ in the case of a complete tree). Their representations can be computed in $\\delta\\kappa$ steps of parallel matrix-matrix multiplications and a memory cost of $\\nu d^{2}$ , as follows. First, we can build up a collection of all unique absolute paths, each represented as a (right-padded) word of length $\\delta$ from the vocabulary of primitives. Their corresponding representations constitute a tensor of size $\\nu\\times d\\times d$ , initialized as $\\nu$ identity matrices. We can then iterate across these words in parallel, one primitive per step (i.e., depth) $t$ , selecting all words that take the same branching direction at the current depth, and right-multiplying their representations by the corresponding orthogonal generator. Finally, absolute paths can be composed into relative ones using the modified dot-product attention of expression (7), just like before. ", "page_idx": 4}, {"type": "text", "text": "3.4 Grids ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The generalization from sequences to trees rests on the observation that a sequence is a tree with a deficit of choices. An altogether different axis of generalization can be obtained by recalling that composite groups can be constructed by joining together two or more elementary groups. Moreover, if it just so happens that the original groups were abelian, then so is their composition; in that case, we call the composite a group direct sum. This construction provides access to an extension from sequences to multidimensional regular grids. ", "page_idx": 5}, {"type": "text", "text": "For the sake of simplicity and without loss of generality, we consider a standard instance of a twodimensional grid: an image. An image is a collection of pixels (or pixel patches) that inhabit a coordinate system $(h,w)$ . Each of $h$ and $w$ is the product of grammar (3), inheriting all path-related notions discussed earlier. Since $\\mathbb{P}$ is an abelian group, the coordinate system also constitutes an abelian group $\\mathbb{P}^{2}:=\\mathbb{P}\\oplus\\mathbb{P}$ . The new group and inversion operations are $+\\mathbb{P}^{2}$ and $(-)_{\\mathbb{P}^{2}}$ , and denote the act of joining and inverting two-dimensional paths, respectively. Both are canonically defined component-wise, on the basis of their one-dimensional counterparts: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{(x,y)+_{\\mathbb{P}^{2}}(z,w):=(x+_{\\mathbb{P}}y,z+_{\\mathbb{P}}w)}\\\\ {-(x,y):=(-\\,x,-\\,y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with ${\\mathbb0}^{2}:=(0,0)$ as the new neutral element. Intuitively, $+\\mathbb{P}^{2}$ corresponds to vector addition, and $(-)_{\\mathbb{P}^{2}}$ to a reflection about the origin with respect to both axes. ", "page_idx": 5}, {"type": "text", "text": "Semantics The specifications above allow us to reuse the notions from Section 3.1 in order to interpret the components and operations of $\\mathbb{P}^{2}$ . What is left unspecified is the interpretation of the group elements themselves; that is, we have yet to explicate what an object of $\\lceil\\mathbb{P}\\oplus\\mathbb{P}\\rceil$ looks like. The quest is a short one; the notion of a direct sum carries over to matrices, where it is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A\\oplus B:={\\left[\\!\\!{\\begin{array}{l l}{A}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{B}\\end{array}}\\!\\!\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "From this, we get the (rather straightforward) interpretation $\\left\\lceil(\\rho_{1},\\rho_{2})\\right\\rceil\\mapsto\\left\\lceil\\rho_{1}\\right\\rceil\\oplus\\left\\lceil\\rho_{2}\\right\\rceil$ . ", "page_idx": 5}, {"type": "text", "text": "Implementation In practice, we now split the vector space in two independent parts. The first part is modulated by orthogonal matrices from $\\langle H\\rangle$ , and the second part by orthogonal matrices from $\\langle W\\rangle$ . For a query $q$ and a key $k$ that reside at a relative distance of $(h,w)$ , their attention score is computed as $q(H^{h}\\oplus W^{w})k-\\mathrm{set}$ e Figure 1c for an illustration. Each axis contributes an additive but separable factor to the attention score, forcing the model to learn contextual alignments between token pairs on the basis of their coordinate-wise distances. Not much else is different: we can still compute all matrices in parallel, temporally bound by a logarithmic complexity of $\\log_{2}(\\operatorname*{max}(h,w))$ and $\\operatorname*{max}(h,w)(\\frac{d}{2})^{2}$ storage space, given a grid of size $(h,w)$ . Subquadratic memory complexity can once more be achieved by virtue of diagonalization, just as in the sequential case. ", "page_idx": 5}, {"type": "text", "text": "3.5 Variants & Extensions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The structures that we have seen so far are not the only ones that our methodology can tackle \u2013 in fact, many other group-like structures are amenable to similar interpretations. We sketch out some enticing examples below. ", "page_idx": 5}, {"type": "text", "text": "Absolute Positions Our analysis has so far focused on paths relative to positions. Fixing the point of origin allows a straightforward simplification to absolute positions. The new structure is that of a monoid: there\u2019s no longer an inversion, and laws L1 and L2 only are now in effect. The framework remains largely unchanged: one can still use subgroups of matrices to represent positions, except this time applying them on either the queries or the keys (rather than both). ", "page_idx": 5}, {"type": "text", "text": "Periodic Domains Under addition, the integers form an infinite cyclic group. An interesting twist would be to consider the positional encodings of finite cyclic groups instead. Such structures are not uncommon; in chemistry, for instance, a benzene molecule comprises six carbon atoms arranged in a ring. The semantics of such a structure would need to be of a matching period; that is, we would need a generator $W$ such that $W^{6}=I$ . Such a parameterization is straightforward; we simply need to fix the orthogonal matrix so as to have it implement rotations at angle-multiples of $\\pi/3$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W^{-1}W^{4}=W^{3}}\\\\ &{\\qquad\\underbrace{\\cdots^{\\cdots^{\\cdots^{\\cdots\\cdots\\cdots\\cdots\\cdots}\\cdots}\\cdots}\\cdots}_{\\mathrm{\\normalfont~\\uparrow~\\!~\\!~I~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!~\\!\\!~\\!~\\!~\\!\\!~\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!~\\!\\!\\!\\!~\\!\\!\\!\\!~\\!\\!\\!\\!\\!~\\!\\!\\!\\!\\!\\!\\!~\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "PfOeAKxx6i/tmp/862765066196b0fd51e44d0118aa85b77d0cfb7173509c5acb59f1642984f83c.jpg", "img_caption": ["(a) The half-axis of absolute positions on a sequence, with a visualization of the two directions of relative paths between points 1 and 4. In either case, the interpretation is the matrix multiplication of the inverted source against the target. ", "(b) The space of paths on binary branching trees, with an illustration of the relative path from \ud835\udfda\ud835\udfd9to \ud835\udfd9\ud835\udfda. Same as before, the interpretation is the matrix multiplication of the inverted source against the target "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "PfOeAKxx6i/tmp/f55ab848a5e05d7744bd9cab05155ff5a2e42637a66eca8feac4cf3d1eb57b4d.jpg", "img_caption": ["(c) The quarter-plane of absolute positions on a 2-dimensional grid, with a visualization of the two directions of relative paths between points $(3,0)$ and $(1,3)$ . The interpretation is now a block-diagonal matrix consisting of the blocks interpreting the path over each coordinate. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: Example paths and their interpretations across the structures examined. ", "page_idx": 6}, {"type": "text", "text": "Time Series & Subsampling Our sequential case analysis assumed a dense sequence with a uniform sampling rate. However, our strategy also applies to any series, even if sparsely sampled, as long as the sampling rate is quantized (i.e., a multiple of some constant step). That is, positional indices (and their representations) do not need to match the placement of tokens in the sequence. ", "page_idx": 6}, {"type": "text", "text": "Composite Groups The direct sum interpretation of Section 3.4 is applicable for arbitrary groups that can be described as products, commutative or otherwise. This allows the representation of positional encodings for several other kinds of composite structures that can be concocted using the same principles, such as sequences of trees, trees of grids, etc. ", "page_idx": 6}, {"type": "text", "text": "Beyond Dot-Product Attention Throughout the previous sections, we have adopted a dot-product formulation for the attention weight function. Nonetheless, APE can be readily integrated into any other attention mechanism, such as linear [Katharopoulos et al., 2020], cluster [Vyas et al., 2020] and \u201csoftmax-free\u201d [Lu et al., 2021] variants, inter alia. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To assess the viability of our approach, we conduct a series of experiments across a range of tasks, in setups that allow for replicable and reliable comparisons with alternatives. When using APE, we follow $\\mathrm{Wu}$ et al. [2021] in scaling the dot-product score between two tokens at a distance of $p$ (i.e., $p$ steps away) by $p^{c}$ ; here, we set $c:=0.98$ . This serves to stabilize training by introducing a locality bias (or long-distance decay) factor. For the sake of parameter compression, we share the orthogonal matrices between the different encoder/decoder layers, but use a distinct matrix (or collection of matrices) per head. To isolate and quantify the effect of initialization, we report results on two different initialization strategies: one where the orthogonal operators are set to mimic RoPE rotations (default), and one where they are set to be close to the identity (no init). Similarly, to isolate and quantify the effect of trainability when comparing to RoPE, we report results over both fixed (frozen) and trainable (tuned) rotation angles. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We provide an extensive account of our experimental setups in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "4.1 Sequence Transduction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Machine Translation First, we follow Vaswani et al. [2017] in training a TransformerBASE model on machine translation over WMT14 EN $\\rightarrow$ DE [Bojar et al., 2014]. ", "page_idx": 7}, {"type": "text", "text": "To provide a comprehensive comparison, we pit our proposed methodology against standard positional encoding schemes from the literature: the vanilla Sinusoidal encodings of Vaswani et al. [2017], the Absolute encodings of Gehring et al. [2017], the Relative encodings of Shaw et al. [2018] and the Rotary encodings of Su et al. [2023]. To ensure a fair comparison, we allow all models the exact same budgets (both memory and time). ", "page_idx": 7}, {"type": "text", "text": "Synthetic Tasks We further examine three standard sequence transduction tasks: sequence copying, sequence reversal, and sequence repetition. These are meant to directly assess each model\u2019s capacity for algorithmic induction, in setups where explicit position-based addressing, both absolute and relative, is required. ", "page_idx": 7}, {"type": "text", "text": "4.2 Tree Transduction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we consider four algorithmic transduction tasks on binary branching trees: tree copying, recursive tree rotation up to a fixpoint, algebraic reduction of $C_{3}$ expressions, and self-referential tree manipulation; see Appendix B for details. ", "page_idx": 7}, {"type": "text", "text": "In addition to previous sequential baselines, we compare our model to the encodings of Shiv and Quirk [2019, Tree-SQ]. For all four tasks, we experiment with both breadth-first and depth-first decoding. ", "page_idx": 7}, {"type": "text", "text": "4.3 Image Recognition ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finaly, we train a Compact Convolutional Transformer [Hassani et al., 2021] on CIFAR-10 [Krizhevsky et al., 2009]. ", "page_idx": 7}, {"type": "text", "text": "Typically, attention-based architectures for vision rely on additive positional encoding schemes, applied on the image prior to it being sequentialized (row-by-row flattened). Here, we compare fixed [Wang and Liu, 2019, Sinusoidal $2D]$ and parametric [Gehring et al., 2017, Absolute] variants of the above against both the sequential and the grid-structured versions of our scheme. ", "page_idx": 7}, {"type": "text", "text": "4.4 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We repeat each experiment three times, varying the seeds used for weight initialization and optimization, but fixing the data across repetitions. We report means and $95\\%$ CIs in Table 2. We highlight each category\u2019s best (in red), and underline scores where the CI spans the mean of the respective best. ", "page_idx": 7}, {"type": "text", "text": "At the macro level and consistently across modalities, domain-appropriate algebraic interpretations match or surpass strong and specialized baselines \u2013 without any hyper-parameter tuning or search. Specifically, across the 13 setups considered, APE is the uncontested top performer in 8, ranks among the best in 3, and falls within the confidence margin of the top performer in one. Exceptionally, in the breadth-first version of the tree-copy task, tree algebraic encodings are surpassed by a handful of sequential alternatives; this is no surprise, since in this case the tree structure is practically a task-irrelevant syntactic confound. Perhaps more surprisingly, in the breadth-first version of the tree-manipulation task, tree algebraic encodings are surpassed only by their non-initialized, sequential version; this is likely a statistical anomaly, since one of the three repetitions resulted in an unusually low perplexity score. ", "page_idx": 7}, {"type": "table", "img_path": "PfOeAKxx6i/tmp/ffdd61238b204c3f774bb9eb4420b183a40bef7ce85c07000152028d4cf981bc.jpg", "table_caption": ["Scheme "], "table_footnote": ["(a) Performance results on neural machine translation and synthetic sequence transduction. "], "page_idx": 8}, {"type": "table", "img_path": "PfOeAKxx6i/tmp/df1312d7119eb1a7493d64582fbac460f83ac9a04c801a60be88ed44b4d1f243.jpg", "table_caption": ["Task/Regression "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "PfOeAKxx6i/tmp/46499f56a52e4693dd490a5c0e0c1690e976c137585bc8802a0315bce58cfee2.jpg", "table_caption": ["(b) Performance results on algorithmic tree manipulation tasks. ", "Table 2: Experimental results and baselines across the tasks considered. ", ""], "table_footnote": ["(c) Best-by-epoch top-1 accuracy scores on image recognition on CIFAR-10. "], "page_idx": 8}, {"type": "text", "text": "We also note three general trends. First, initializing APE to match RoPE frequency bands at the start of training consistently and significantly improves performance, possibly because RoPE rotary primitives have undergone empirical tuning for stability and performance. Second, given identical initialization, a sequential APE generally outperforms a trainable RoPE, despite their theoretical equivalence. This might be due to the difficulty of optimizing periodic signals (i.e., RoPE\u2019s trigonometric functions) compared to APE\u2019s (orthogonal) matrix multiplications. Third, a frozen RoPE performs comparably to a randomly initialized APE in most tasks considered, suggesting that adjusting rotoreflection angles during training is not necessarily better than adjusting rotation planes while keeping the angles fixed. Contrary to all the above, a frozen RoPE weakly outperforms both a tunable RoPE and an initialized APE in the neural machine translation task; likely an artifact of attention overfitting to specific positional patterns. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Dense attention is by now a foundational component of various problem- and domain-general architectures. Combined with its structural indifference, this underscores the pressing need for learning strategies capable of injecting structural biases directly at the representation level. As such, positional encodings have garnered significant community attention in recent years \u2013 too much, in fact, to permit an exhaustive enumeration here. An extensive survey and meta-review is provided by Dufter et al. [2022] who group and rank these works on the basis of several criteria. Our work presents a universal, intuitive and formally grounded recipe that meets all these criteria: it is trainable, amenable to problem-specific and data-driven tuning; reference-adjustable, allowing both absolute and relative positional specifications; unbounded, capable of representing enumerably infinite positions irrespective of model instantiation and/or the targeted data size; contextual, implementing a dynamic effect that varies depending on token content; effective, consistently matching or surpassing baselines in the tasks considered; and, finally, efficient, exhibiting generally favorable asymptotic complexities. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We must point out that the concept of positional encodings as sequence homomorphisms has already been hinted at, first by Wang et al. [2020] and later by Su et al. [2023], even if not explicitly formulated as such. Despite approaching the problem from different angles, both approaches interpret positions as multiplicative, norm-preserving (rotation-like) operations. Our proposal expands upon these two, first in providing a proper algebraic framing of the problem, and second in extending the interpretation from rotations around the axes to rotations and reflections about arbitrary planes. In the case of a single generator matrix (i.e., sequences), this difference turns to be non-essential, being practically neutralized by the Transformer\u2019s trainable weights. This no longer holds, however, in the case of multiple generator matrices (i.e., grids or trees), where each generator should be able to rotate and reflect different sets of planes. In that sense, algebraic positional encodings offer an appealing unifying perspective of a multidimensional generalization to the aforementioned rotation-based frameworks. This sentiment is shared by Lim et al. [2023] who, in parallel to our work, similarly advocate for positional encodings as group homomorphisms, there framed as irreducible group representations. Modulo presentation, the two approaches are variations on a common theme; theirs is technically concerned with post-hoc representation of symmetries and equivariances at a per-datum scale, whereas ours focuses on the interpretation of domain signatures at the dataset scale. ", "page_idx": 9}, {"type": "text", "text": "More generally, algebraic manipulations are not uncommon in modern machine learning literature. The recognition of abstract algebra as a practical tool for imposing structural well-behavedness has led to its increased adoption as a reliable recipe for structure-informed neural architectures, largely obsoleting the inefficient and ad-hoc augmentation routines of the past. This line of work can be traced back to the group equivariant convolutions of Cohen and Welling [2016], which have by now bloomed into a field of their own; see Weiler et al. [2023] for an up-to-date overview. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We recognize weaknesses and limitations across three fronts. On the theoretical front, we have limited our scope to simple inductive groups, consciously ignoring potential interpretations of more complex constructions. We defer this to future work. On the empirical front, having to recompute positional encodings once per batch increases a model\u2019s temporal complexity during training. While this is barely noticeable in sequential and grid constructions, which scale logarithmically, it becomes evident when dealing with complete trees, which scale linearly and require explicit for-loops. On the epistemic front, we conducted a limited set of experiments, focusing primarily on replicability and fairness. We leave more exhaustive empirical comparisons on practical downstream tasks to future work or interested parties. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented a theoretically motivated approach towards constructing positional encodings for a variety of structures. Without any significant modification or overhead, our methodology can capture sequences and their (multi-dimensional as well as multi-branching) generalizations. In doing so, it reconciles powerful but structurally oblivious models with their missing inductive biases, permitting structure-aware architectural refinements across a range of tasks and setups (see also Kogkalidis et al. [2024] for parallel work employing the methodology in a neurosymbolic representation learning setup). Beyond that, our approach grants full control over how these biases are to be implemented, while also being amenable to adjustments and extensions. Our work indicates that generality and extensibility are not in spite of, but rather due to structural discipline and abstraction. We perceive it as an important step towards data-efficient, general and transparent models of neural computation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "KK and VG were supported by Saab-WASP via the project \u201cNeurodynamic Programming and Reinforcement Learning\u201d (grant 411025). VG also acknowledges the support from Academy of Finland (grant 342077) for \u201cHuman-steered next-generation machine learning for reviving drug design\u201d, and the Jane and Aatos Erkko Foundation (grant 7001703) for \u201cBiodesign: Use of artificial intelligence in enzyme design for synthetic biology\u201d. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120\u20131128. PMLR, 2016.   \nJ.-P. Bernardy and S. Lappin. Unitary recurrent networks: Algebraic and linear structures for syntax. In Algebraic Structures in Natural Language, pages 243\u2013278. CRC Press, 2022.   \nO. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pages 12\u201358, 2014.   \nT. Cohen and M. Welling. Group equivariant convolutional networks. In International conference on machine learning, pages 2990\u20132999. PMLR, 2016.   \nP. Dufter, M. Schmitt, and H. Sch\u00fctze. Position information in transformers: An overview. Computational Linguistics, 48(3):733\u2013763, Sept. 2022. doi: 10.1162/coli_a_00445. URL https://aclanthology.org/2022.cl-3.7.   \nP. Gage. A new algorithm for data compression. The C Users Journal, 12(2):23\u201338, 1994.   \nJ. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243\u20131252. PMLR, 2017.   \nA. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi. Escaping the big data paradigm with compact transformers. arXiv preprint arXiv:2104.05704, 2021.   \nT. Janssen. Foundations and applications of Montague grammar. PhD thesis, University of Amsterdam, 2014. Originally published: April 1983 (UvA).   \nA. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u2013 5165. PMLR, 2020.   \nK. Kogkalidis, O. Melkonian, and J.-P. Bernardy. Learning structure-aware representations of dependent types. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.   \nA. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nY. Li, R. Zemel, M. Brockschmidt, and D. Tarlow. Gated graph sequence neural networks. In Proceedings of ICLR\u201916, 2016.   \nD. Lim, H. Lawrence, N. T. Huang, and E. H. Thiede. Positional encodings as group representations: A unified framework. 2023.   \nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \nJ. Lu, J. Yao, J. Zhang, X. Zhu, H. Xu, W. Gao, C. Xu, T. Xiang, and L. Zhang. Soft: Softmax-free transformer with linear complexity. Advances in Neural Information Processing Systems, 34: 21297\u201321309, 2021.   \nF. Murnaghan and A. Wintner. A canonical form for real matrices under orthogonal transformations. Proceedings of the National Academy of Sciences, 17(7):417\u2013420, 1931.   \nM. Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191, Belgium, Brussels, Oct. 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W18-6319.   \nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, 2016.   \nP. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. In Proceedings of NAACL-HLT, pages 464\u2013468, 2018.   \nV. Shiv and C. Quirk. Novel positional encodings to enable tree-based transformers. Advances in neural information processing systems, 32, 2019.   \nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, page 127063, 2023.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nA. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. Advances in Neural Information Processing Systems, 33:21665\u201321674, 2020.   \nB. Wang, Z. Donghao, L. Christina, Q. Li, Z. Peng, J. G. Simonsen, et al. Encoding word order in complex embeddings. In ICLR 2020-Proceedings of Eighth International Conference on Learning Representations, 2020.   \nZ. Wang and J.-C. Liu. Translating math formula images to latex sequences using deep neural networks with sequence-level training, 2019.   \nM. Weiler, P. Forr\u00e9, E. Verlinde, and M. Welling. Equivariant and coordinate independent convolutional networks. A Gauge Field Theory of Neural Networks, 2023.   \nC. Wu, F. Wu, and Y. Huang. DA-Transformer: Distance-aware transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059\u20132068, 2021.   \nY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Parameterizing APE ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Orthogonalization ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The orthogonal primitives underlying APE can be procured by matrix-exponentiating skew-symmetric bases. Concretely, for some cyclic group $\\langle C\\rangle$ : ", "page_idx": 12}, {"type": "text", "text": "1. Start with an upper triangular matrix $\\pmb{A}$ ; this matrix parameterizes the entire group.   \n2. Obtain the skew symmetric $B:=A-A^{\\top}$   \n3. Obtain the matrix exponent $C:=\\exp(B)$ ; the resulting matrix is orthogonal, and acts as the group\u2019s generator. ", "page_idx": 12}, {"type": "text", "text": "A.2 Switching between APE and RoPE ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In the commutative (direct sum of finitely many cyclic groups) case, it is possible to switch freely between APE and RoPE. Doing so might be useful, e.g., for initializing APE, for inspecting the learned rotoreflections post-training, or for making use of RoPE\u2019s memory-optimized vector-multiplication formula in a system originally trained with APE. Note that here we consider the purely real-valued version of RoPE (and APE). ", "page_idx": 12}, {"type": "text", "text": "$\\mathbf{RoPE}\\rightarrow\\mathbf{APE}$ To convert RoPE to APE for some collection of angles $\\Theta:=[\\theta_{1},\\ldots\\theta_{n}]$ : ", "page_idx": 12}, {"type": "text", "text": "1. Expand $\\Theta$ into a rotation matrix $_{C}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nC:=\\left[\\begin{array}{c c c c c}{{c o s}{\\theta_{1}}}&{{-s i n}{\\theta_{1}}}&{{0}}&{{0}}&{{\\ldots}}\\\\ {{s i n}{\\theta_{1}}}&{{c o s}{\\theta_{1}}}&{{0}}&{{0}}&{{\\ldots}}\\\\ {{0}}&{{0}}&{{c o s{\\theta_{2}}}}&{{-s i n{\\theta_{2}}}}&{{\\ldots}}\\\\ {{0}}&{{0}}&{{s i n{\\theta_{2}}}}&{{c o s{\\theta_{2}}}}&{{\\ldots}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\vdots}}&{{\\vdots}}&{{\\ddots}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note: Stop here if not interested in parameterizing $_{C}$ . ", "page_idx": 12}, {"type": "text", "text": "2. Use a solver to approximate the matrix logarithm of $_{C}$ , $B:=\\log(C)$ . 3. Find a matrix $\\pmb{A}$ such that $\\mathrm{mse}(B,A-A^{\\top})\\leq\\epsilon$ , e.g., using a numerical optimizer. Matrix $\\pmb{A}$ can be used to parameterize the group, $c f.$ . A.1. ", "page_idx": 12}, {"type": "text", "text": "$\\mathbf{APE}\\rightarrow\\mathbf{RoPE}$ To convert APE to RoPE for some cyclic group $\\langle W\\rangle$ :", "page_idx": 12}, {"type": "text", "text": "1. Find the normal form $W=P Q P^{\\top}$ .   \n2. Extract the angles in each block of $Q$ ; the resulting collection of angles is RoPE\u2019s $\\Theta$ .   \n3. For each attention head involved, right-compose the Transformer\u2019s $\\Phi^{(q)}$ and $\\Phi^{(k)}$ with $_{P}$ . ", "page_idx": 12}, {"type": "text", "text": "B Experimental Setups ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Machine Translation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For our machine translation experiments, we use the official dataset breakdown (including the extended evaluation set). We tokenize the training and evaluation sets with MOSES6, using the standard pipeline: punctuation normalization $\\rightarrow$ unicode normalization $\\rightarrow$ language-specific tokenization. We apply byte-pair encoding [Gage, 1994, Sennrich et al., 2016] using the subword-nmt package7. We apply 32k merges across the source and target training corpora, without truncating the resulting (shared) vocabulary (of size 35 533). Our loss term is given as the cross-entropy between the teacher-forced predictions and the ground-true labels, smoothed by $10\\%$ . We train in a distributed environment consisting of 4 GPUs, with a batch size of 3 072 target tokens per GPU. We average gradients and update parameters once every 2 GPU iterations (or: 8 batches). We optimize using Adam with a learning rate dictated by the schedule prescribed by Vaswani et al. [2017]. We stop optimizing after 150 000 parameter updates or 16 hours, whichever comes first. Throughout training, we circularly store the 10 best checkpoints, ranked on the basis of dev set loss (evaluated once every 500 updates). During inference, we average the 10 checkpoints into a single model, and select hypotheses from a beam of width 4 and a length penalty of 0.6 [Wu et al., 2016]. We report BLEU scores over the test set (newstest2014), comparing the BPE-merged and detokenized output against the raw references using sacrebleu [Post, 2018]8. ", "page_idx": 12}, {"type": "table", "img_path": "PfOeAKxx6i/tmp/d49d893137cdeca0eecc71c8a5f0f9f9458409e0d5cbef06df3f1c903c8205dd.jpg", "table_caption": ["Experiment/Value "], "table_footnote": ["Table 3: Hyperparameter setups, grouped by experiment. "], "page_idx": 13}, {"type": "text", "text": "B.2 Synthetic Transduction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Tree Task Descriptions The tree copy task is morally identical to its sequential version \u2013 the tree structure (and its positional specification) is practically a confound. ", "page_idx": 13}, {"type": "text", "text": "In the tree rotation\u22c6task, the output tree is the result of recursively right-rotating all subtrees of the input. The task is challenging but purely structural, in the sense that its resolution requires no real interaction between content and position. ", "page_idx": 13}, {"type": "text", "text": "For the algebraic expression reduction task, we consider input trees that specify a complex expression from the cyclic group $C_{3}$ , and task the model with producing the result of a single reduction step (i.e., reducing all subtrees of depth 1 into a leaf). This time around, the model has to identify reducible subtrees, match operators to their argument and collapse the three into a single node depending on their content. ", "page_idx": 13}, {"type": "text", "text": "The tree operations task, finally, combines the aspects of the other three, requiring content-based addressing, structure manipulation and dynamic semantics resolution. Concretely, we generate an input tree consisting of unique nodes, and randomly select one of its subtrees as well as one of four operators. We then construct a deeper tree, where the new root corresponds to the chosen operator, its left branch corresponds to the numerical index of the selected subtree, and the right branch is the original tree in its entirety. The model is then tasked with producing the correct output given this combination of an operator, a tree, and an index. We consider four operations: extraction (i.e., return the indexed subtree), flip-extraction (i.e., return the indexed subtree, rotated), truncation (i.e., return the full tree with the indexed subtree removed) and a no-op (i.e., return the full tree as-is, ignoring indexing). ", "page_idx": 13}, {"type": "text", "text": "Hyperparameters For all synthetic tasks, we generate disjoint train, dev and test sets of sizes 6 000, 2 000 and 2 000. We train a small Transformer model, optimizing with AdamW [Loshchilov and Hutter, 2017] for 400 epochs and a batch size of 64, using a linear warmup \u2013 cosine decay schedule. For the sequential tasks, we populate the datasets with words of random lengths from $\\mathcal{N}(100,10)$ and a vocabulary size of 20 (to ensure token repetition and diffuse the possibility for leaning on content-based addressing). For the tree tasks, we populate the datasets with non-uniform trees of random depths sampled from ${\\mathcal{N}}(7,1)$ . For the tree-ops task, exceptionally, we set the vocabulary size to 128 so as to have enough unique nodes to allow content-based addressing. ", "page_idx": 13}, {"type": "text", "text": "When using a positional encoding scheme that requires fixing the size of the structure being modeled (i.e., the Tree, Relative, and Absolute schemes), we fix it at approximately the maximum training size, practically ensuring the most stringent comparison. ", "page_idx": 13}, {"type": "text", "text": "In all experiments, we share source and target embedding weights between both the encoder-decoder embedding layers, and the decoder\u2019s classification head. ", "page_idx": 13}, {"type": "text", "text": "B.3 Image Recognition ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For our image recognition experiments, we largely rely on the setup of Hassani et al. [2021]. Concretely, we apply a small-step \u201ctokenizing\u201d convolution on the input image, downsample the result with max pooling and flatten the result into a sequence. After we pass the sequence through the encoder, we apply a global soft attention [Li et al., 2016, inter alia] (rediscovered by Hassani et al. [2021], there dubbed \u201csequence pooling\u201d) to aggregate into a single vector prior to applying the classifier. To attain competitive scores, we apply standard CIFAR-10 data augmentations and more aggressive regularization: a $10\\%$ attention weight dropout, a stochastic depth of $10\\%$ for each consecutive layer, and a weight decay of $3\\cdot10^{-2}$ . The above settings and the hyperparameter setup are taken without modification from Hassani et al. [2021]. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We carefully summarize our contributions and refrain from making any claims that we cannot theoretically or empirically support. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We have a dedicated limitations section (\u00a76), and openly and explicitly discuss algorithm complexity and experimental scope in the relevant sections. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 15}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our algebraic connections make no assumptions and are fully explicit in their presentation. The equivalence with RoPE clarifies all assumptions it makes. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide the reviewers with both an extensive appendix detailing our experimental setup, and the code used to implement our methodology and conduct our experiments. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Yes \u2013 see answer above. Our training scripts are provided virtually unchanged. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Yes, see above. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We take extra care to conduct our experiments openly and transparently so as to deliver statistically sound results and draw solid conclusions. We repeat all experiments multiple times, and report means and $95\\%$ confidence intervals. For each experiment, we visually mark all models that overlap with the best performer in the category. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: While we do report hardware infrastructure, we do not report memory consumption or clock times. With the exception of machine translation, our experiments are moderately cheap to run, requiring no specialized hardware other than GPU accelaration. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Checked and done. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do, albeit briefly. We do not see possible negative implications. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We perceive no risks that would require safeguards of any kind. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We cite all software libraries and datasets we use, and comply with their licenses. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: While we do provide reference implementations, we do not see them as assets per se, neither do we hand them out as ready-to-use integrations. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No human subjects were involved in this study. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: See above. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]