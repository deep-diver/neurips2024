[{"figure_path": "xojbzSYIVS/tables/tables_6_1.jpg", "caption": "Table 1: The overall results of competing baselines and our LLM-ESR. The boldface refers to the highest score and the underline indicates the next best result of the models. \u201c*\u201d indicates the statistically significant improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.", "description": "This table presents a comprehensive comparison of the proposed LLM-ESR model against various baseline models across three real-world datasets (Yelp, Fashion, Beauty).  The performance is evaluated using two metrics (H@10 and N@10) and broken down by four user/item groups (overall, tail item, head item, tail user, and head user) to show the effectiveness of the LLM-ESR model specifically on long-tail items and users.  Statistical significance is indicated using a two-sided t-test.", "section": "4.2 Overall Performance"}, {"figure_path": "xojbzSYIVS/tables/tables_7_1.jpg", "caption": "Table 2: The ablation study on the Yelp dataset with SASRec as the backbone SRS model. The boldface refers to the highest score and the underline indicates the next best result of the models.", "description": "This ablation study analyzes the impact of different components of the LLM-ESR model on its performance using the Yelp dataset and SASRec as the base model.  It shows the results with all components, and then systematically removes components (collaborative view, semantic view, self-distillation, shared encoder, cross-attention) to understand their individual contributions.  It also shows the impact of using a one-layer adapter versus random initialization.", "section": "4.3 Ablation Study"}, {"figure_path": "xojbzSYIVS/tables/tables_15_1.jpg", "caption": "Table 1: The overall results of competing baselines and our LLM-ESR. The boldface refers to the highest score and the underline indicates the next best result of the models. \u201c*\u201d indicates the statistically significant improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.", "description": "This table presents the overall performance comparison between LLM-ESR and various baselines across three datasets (Yelp, Fashion, Beauty) using three different sequential recommendation models (GRU4Rec, Bert4Rec, SASRec).  It shows the H@10 and NDCG@10 metrics for overall performance, long-tail items, long-tail users, head items, and head users.  The '*' indicates statistical significance.", "section": "4.2 Overall Performance"}, {"figure_path": "xojbzSYIVS/tables/tables_17_1.jpg", "caption": "Table 1: The overall results of competing baselines and our LLM-ESR. The boldface refers to the highest score and the underline indicates the next best result of the models. \u201c*\u201d indicates the statistically significant improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.", "description": "This table presents the overall performance comparison between LLM-ESR and several baselines on three datasets (Yelp, Fashion, Beauty).  The results are broken down by model (GRU4Rec, Bert4Rec, SASRec), and further categorized by overall performance, performance on long-tail items, performance on head items, performance on long-tail users, and performance on head users.  The H@10 and N@10 metrics are used to evaluate performance.  Asterisks (*) indicate statistically significant improvements compared to the best-performing baseline.", "section": "4.2 Overall Performance"}, {"figure_path": "xojbzSYIVS/tables/tables_17_2.jpg", "caption": "Table 1: The overall results of competing baselines and our LLM-ESR. The boldface refers to the highest score and the underline indicates the next best result of the models. \u201c*\u201d indicates the statistically significant improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.", "description": "This table presents a comparison of the proposed LLM-ESR model against several baseline models for sequential recommendation.  It shows the performance (H@10 and N@10) across three datasets (Yelp, Fashion, Beauty) and broken down by user and item categories (overall, head/tail items and users).  Statistical significance is indicated using a *.", "section": "4.2 Overall Performance"}, {"figure_path": "xojbzSYIVS/tables/tables_18_1.jpg", "caption": "Table 6: The experiments for limited text and the design of freezing semantic embedding. All the experiments are conducted on the Yelp dataset and for LLM-ESR. \u201cFull\u201d and \u201cCrop\u201d mean that we use the completed item prompt and attribute-cropped prompt to get the LLM embeddings, respectively. \u201cw/o F\u201d means that we train the LLM-ESR without freezing the semantic embedding layer.", "description": "This table presents an ablation study on the Yelp dataset using the LLM-ESR model.  It compares the performance of the model under different conditions: using full item prompts vs. cropped prompts, and with vs. without freezing the semantic embedding layer. The results show the impact of these factors on the overall performance and on performance for different user and item groups.", "section": "C.1 Ablation Study"}]