[{"figure_path": "Ib2iHIJRTh/tables/tables_6_1.jpg", "caption": "Table 1: Computational complexity of the different deep learning methods in terms of: 1) the number of neural function evaluations (NFEs) needed to predict h time steps, and 2) Total inference runtime (simulating 10 years), including the time needed to compute metrics (in hours:minutes). N refers to the number of diffusion steps which usually ranges between 20 to 1000.", "description": "This table compares the computational complexity of different deep learning models for climate prediction, measured by the number of neural network forward passes (NFEs) and the total inference time for a 10-year simulation. It shows that the proposed Spherical DYffusion method is significantly more computationally efficient than standard diffusion models and comparable to deterministic baselines, while still achieving high accuracy.", "section": "Runtime analysis"}, {"figure_path": "Ib2iHIJRTh/tables/tables_8_1.jpg", "caption": "Table 2: Global area-weighted mean of the spread of an ensemble of 10-yr time-mean's for surface pressure, total water path, air temperature, zonal wind, and meridional wind (the last three at the near-surface level). The climate variability of our method is consistent with the reference model.", "description": "This table presents a comparison of the spread (standard deviation) of 10-year time-mean climate variables across different models.  The \"spread\" represents the variability within each model's ensemble of simulations. The table shows that the spread generated by the proposed Spherical DYffusion model closely matches the spread observed in the reference FV3GFS model, indicating that the new model accurately captures climate variability.  The other models, DYffusion and ACE-STO, show larger spreads than the reference and the proposed model.", "section": "Climate variability"}, {"figure_path": "Ib2iHIJRTh/tables/tables_16_1.jpg", "caption": "Table 3: Input and output variables used in this work. The table was adapted based on Table 1 of [67]. The k subscript refers to a vertical layer index and ranges from 0 to 7 starting at the top of the atmosphere and increasing towards the surface. The two prognostic surface variables, T<sub>s</sub> and p<sub>s</sub>, do not have this additional vertical dimension. Each of their snapshots is a 2D latitude-longitude matrix. The Time column indicates whether a variable represents the value at a particular time step (\"Snapshot\"), the average across the 6-hour time step (\"Mean\"), or a quantity that does not depend on time (\"Invariant\"). \"TOA\" denotes \"Top Of Atmosphere\", the climate model's upper boundary.", "description": "This table lists the input, output, and forcing variables used in the study.  It details the description, units, time dependency (snapshot, mean, or invariant), and dimensionality (3D or not) of each variable.  The table is organized into sections for prognostic variables (both input and output), forcing variables (input-only), and additional input-only variables. A final section provides a list of derived variables used only for evaluation.", "section": "B Dataset"}, {"figure_path": "Ib2iHIJRTh/tables/tables_19_1.jpg", "caption": "Table 7: Table is directly taken from [67], and reports the SFNO hyperparameters used for ACE as well as the interpolator and forecasting networks of our method.", "description": "This table shows the hyperparameters used in the SFNO architecture for both the ACE model and the proposed Spherical DYffusion model.  It details the values for parameters such as embedding dimension, filter type, number of layers, operator type, scale factor, and number of spectral layers.  These settings are critical in determining the performance and characteristics of each model, highlighting the design choices made for optimizing the individual components within each model architecture.", "section": "5.2 Baselines"}, {"figure_path": "Ib2iHIJRTh/tables/tables_19_2.jpg", "caption": "Table 8: Optimization hyperparameters. The effective batch size is calculated as data loader batch size \u00d7 number of GPUs \u00d7 number of gradient accumulation steps, and is ensured to be the same for all our trained models regardless of the number of GPUs used.", "description": "This table presents the hyperparameters used during the optimization process for training the deep learning models.  It includes the optimizer used (AdamW), the initial learning rate, weight decay, learning rate schedule (cosine annealing), number of training epochs, effective batch size, exponential moving average decay rate, and gradient clipping value.  The effective batch size is dynamically calculated to remain constant across different hardware setups.", "section": "5 Experiments"}, {"figure_path": "Ib2iHIJRTh/tables/tables_22_1.jpg", "caption": "Table 4: Comprehensive evaluation of simulated 10-year time-means. Bias, RMSE, and MAE represent average member-wise scores. For Bias (Spread-skill ratio; SSR) closer to 0 (1) is better. For the other metrics, lower is better, with relative changes from the reference shown in parentheses. See Appendix D for mathematical formulations and Table 3 for variable descriptions and units.", "description": "This table presents a comprehensive comparison of different metrics (Bias, RMSE, MAE, RMSEens, SSR, CRPS) for various climate variables (TWP, Ps, T7, T5, To, U7, u7, WS7, WS5, WSo) across four different models: Reference, Ours, ACE, ACE-STO, and DYffusion.  It shows the performance of the proposed model (Ours) against the baseline models in terms of bias reduction, accuracy, and uncertainty quantification.  The relative changes from the reference model are provided in parentheses to show the improvement or degradation. The table highlights the effectiveness of the proposed method in achieving lower biases and improved accuracy compared to the baselines. Appendix D provides detailed information on the metrics, and Table 3 offers descriptions of the climate variables.", "section": "5.3 Climate Biases"}]