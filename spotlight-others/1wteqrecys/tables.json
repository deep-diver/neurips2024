[{"figure_path": "1WtEqReCyS/tables/tables_4_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table presents the results of experiments comparing different training methods on the DataComp benchmark. The methods involve training with filtered raw captions, filtered translated captions, and combinations of both.  The table shows that using translated captions leads to improved performance across multiple metrics, including ImageNet accuracy, ImageNet distribution shift accuracy, and retrieval performance. Combining both raw and translated captions yields the best results overall, indicating the benefits of incorporating multilingual data into the training process.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/tables/tables_8_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table presents the results of experiments conducted on the DataComp benchmark using different training data: filtered raw captions, filtered translated captions, and combinations thereof.  The table shows that training with translated captions generally outperforms using only raw captions across various metrics including ImageNet accuracy, ImageNet distribution shift accuracy, retrieval performance, and average performance across 38 tasks. The best performance is achieved by combining both filtered raw and translated captions. The table also includes results when training for a longer duration (10x longer).", "section": "Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/tables/tables_15_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table compares the performance of different training methods on the DataComp benchmark.  The methods vary in how they use raw and translated captions, as well as the size of the filtered training dataset.  The table shows improvements when using translated captions, particularly when combined with raw captions, across a range of metrics including ImageNet accuracy, ImageNet distribution shift, retrieval accuracy, and GeoDE (a geographically diverse task).  The results suggest that incorporating translated multilingual data improves model performance, especially when combined with English data.", "section": "Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/tables/tables_16_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table presents the results of several experiments on the DataComp benchmark, comparing different approaches to training a CLIP model.  The key comparison is between using only English captions, raw multilingual captions, translated multilingual captions, and combinations of these.  The table shows the dataset sizes after filtering, and the performance on ImageNet, ImageNet distribution shifts, image retrieval, and a geographically diverse task (GeoDE) and average across 38 tasks from the DataComp benchmark.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/tables/tables_18_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table presents the results of several experiments on the DataComp benchmark, comparing different methods of training a CLIP model.  The key comparison is between using only English captions, only translated non-English captions, and combining both. The table shows that using translated non-English captions, and particularly combining them with English captions, leads to significant improvements in various metrics, such as ImageNet accuracy, ImageNet distribution shift accuracy, retrieval performance, and average performance across 38 DataComp tasks.  The results are presented for different dataset sizes achieved through varying filtering thresholds.  The experiment uses the same training steps for all baselines, allowing for direct comparison based solely on data composition.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/tables/tables_19_1.jpg", "caption": "Table 6: When fixing the training images and replacing translated English captions with synthetic captions generated by BLIP2, we find that performance decreases in general. Since filtering from translated captions exposes CLIP to both new images and new text distributions, we seek to disentangle the impact of these two factors on model performance. Our results suggest that having access to more diverse images alone (without the corresponding translated multilingual captions) may be insufficient for achieving performance gains.", "description": "This table presents an ablation study to understand the impact of translated multilingual captions on model performance.  It compares performance when using translated captions against performance when replacing the translated captions with synthetic captions generated by BLIP2, while keeping the training images the same. The results show that simply having diverse images is not enough to replicate the performance gains from using translated multilingual captions, highlighting the importance of both diverse images and diverse text for improved model performance.", "section": "4.2 Ablations"}, {"figure_path": "1WtEqReCyS/tables/tables_20_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table presents the results of experiments on the DataComp benchmark, comparing the performance of different training methods on several metrics.  The methods involve training on filtered raw captions, filtered translated captions, and combinations of both.  The table shows that using translated captions improves performance on ImageNet, ImageNet distribution shifts, retrieval, and an average of 38 tasks in the DataComp benchmark, with the best performance achieved when combining both raw and translated captions.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}, {"figure_path": "1WtEqReCyS/tables/tables_21_1.jpg", "caption": "Table 1: On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains. We report the performance of select baselines on the DataComp benchmark [14]; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (\"Filtered translated captions\") is more effective than training on raw captions (\"Filtered raw captions\") as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.", "description": "This table presents the results of experiments on the DataComp benchmark, comparing the performance of different training methods on various metrics.  The methods involve using only raw captions, only translated captions, and combinations of both. The table demonstrates that using translated captions improves performance across ImageNet, ImageNet distribution shifts, retrieval, and GeoDE, especially when both raw and translated captions are combined. The filtering threshold is tuned for each method to maximize average accuracy across 38 tasks.", "section": "4 Impacts of using (translated) multilingual captions on standard vision tasks"}]