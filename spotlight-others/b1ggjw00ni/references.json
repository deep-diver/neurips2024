{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduced the Vision Transformer (ViT) architecture, which is the foundation upon which this paper's work on video transformers is built."}, {"fullname_first_author": "Anurag Arnab", "paper_title": "ViViT: A video vision transformer", "publication_date": "2021-10-12", "reason": "This paper is among the earliest to adapt the ViT architecture for video data, representing a significant early step in the field that directly informs this paper's approach."}, {"fullname_first_author": "Gedas Bertasius", "paper_title": "Is space-time attention all you need for video understanding?", "publication_date": "2021-07-01", "reason": "This paper explores the use of space-time attention in video transformers, a central theme that this paper also addresses with its run-length tokenization method."}, {"fullname_first_author": "Daniel Bolya", "paper_title": "Token merging: Your ViT but faster", "publication_date": "2023-05-01", "reason": "This paper proposes a method to accelerate ViT training and inference, a goal that this paper also aims to achieve but using a different approach."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces the VideoMAE model, which is used as the basis for pre-training the video transformers in the experiments conducted in this paper."}]}