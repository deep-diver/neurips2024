[{"figure_path": "Pf7kdIjHRf/tables/tables_4_1.jpg", "caption": "Table 1: Network Details of HPT. The width denotes the latent dimension size of the trunk transformer and the depth denotes the number of blocks. The default setup is the HPT-Small model.", "description": "This table presents the architectural details of the Heterogeneous Pre-trained Transformers (HPT) models used in the paper.  It shows the depth (number of transformer blocks), width (latent dimension size), number of attention heads, and total number of parameters for five different HPT variants: Small, Base, Large, XLarge, and Huge. The table highlights the scaling of model size across these variants, showing a significant increase in parameters from the smallest (HPT-Small) to the largest (HPT-Huge) model.", "section": "3 Heterogeneous Pre-trained Transformers (HPT)"}, {"figure_path": "Pf7kdIjHRf/tables/tables_4_2.jpg", "caption": "Table 2: Dataset Details of Pre-train Settings. The default setup is trained with 27 datasets from RT-X with 16k trajectories (maximum 1000 trajectories each) and scaled setup involves more data and compute.", "description": "This table shows the dataset configuration used for pre-training the Heterogeneous Pre-trained Transformers (HPT) model. It compares two settings: \"Default\" and \"Scaled\". The \"Default\" setting uses 27 datasets from the RT-X dataset with a total of 16,000 trajectories (with a maximum of 1000 trajectories per dataset). The \"Scaled\" setting uses a significantly larger dataset, comprising 52 datasets with 270,000 trajectories in total.  The table also provides the number of samples and the batch size used for training in each setting.", "section": "3.2 Training Objective"}, {"figure_path": "Pf7kdIjHRf/tables/tables_18_1.jpg", "caption": "Table 4: Detailed Dataset Mixture. We include the detailed number of trajectories and the number of dataset samples in the training mixture. These include 41 dataset from Open-X [14], 7 datasets from simulation, 3 datasets from human video, and 1 from in-the-wild deployed dataset.", "description": "This table provides a detailed breakdown of the datasets used in the HPT pre-training.  It lists each dataset, the number of trajectories and samples within each dataset, and the percentage each dataset contributes to the total number of trajectories and samples.  The datasets are categorized into four groups: real-world robot teleoperation datasets from Open-X, simulation datasets, human video datasets, and in-the-wild deployed robot datasets. This breakdown shows the heterogeneity of the data used for pre-training, highlighting the variety of sources and the diverse nature of the robotic data included.", "section": "A.1 Dataset Details"}, {"figure_path": "Pf7kdIjHRf/tables/tables_19_1.jpg", "caption": "Table 5: Experiment Statistics. By leveraging heterogeneous datasets, the embodiment diversity in data and training scales reaches across several orders. Note that the training flops and the number of tokens are approximated from a single iteration and the model size only counts the trunk parameters (stem and head only have a small active parameter count. HPT is also provided with multiple open-source implementations and extensive simulation evaluation tasks across 6 different benchmarks.", "description": "This table summarizes the key statistics and characteristics of the experiments conducted in the paper, comparing HPT's performance against other methods. It highlights the significant increase in data diversity and scale achieved by HPT, contrasting its use of 52 datasets with other methods utilizing far fewer.  The table also shows the disparity in model sizes employed, with HPT's 1.1B parameters exceeding those of its counterparts.  Finally, it notes the availability of multiple open-source implementations and the extensive evaluation performed across six benchmark tasks.", "section": "Experiments on Pre-training"}]