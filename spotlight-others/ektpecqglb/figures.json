[{"figure_path": "ektPEcqGLb/figures/figures_0_1.jpg", "caption": "Figure 1: Graphical abstract. Introducing the Poisson Variational Autoencoder (P-VAE), which draws on key concepts in neuroscience. When trained on natural image patches, P-VAE with a linear decoder develops Gabor-like feature selectivity, reminiscent of Sparse Coding [3]. In sharp contrast, the standard Gaussian VAE learns the principal components [4].", "description": "This figure is a graphical abstract that introduces the Poisson Variational Autoencoder (P-VAE).  It highlights the model's key features: encoding inputs as discrete spike counts, incorporating a metabolic cost term linking to sparse coding, and unifying major neuroscience theories under Bayesian inference. The figure contrasts the P-VAE's Gabor-like feature learning with the principal component analysis of standard Gaussian VAEs when trained on natural images.", "section": "1 Introduction"}, {"figure_path": "ektPEcqGLb/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Model architecture. Colored shapes indicate learnable model parameters, including the prior firing rates, r. We color code the model's inference and generative components using red and blue, respectively. The P-VAE encodes its inputs in discrete spike counts, z, significantly enhancing its biological realism. (b) \u201cAmortized Sparse Coding\u201d is a special case within the P-VAE model family: it\u2019s a P-VAE with a linear decoder and an overcomplete latent space.", "description": "Figure 2 illustrates the architecture of the Poisson Variational Autoencoder (P-VAE). Panel (a) shows the general structure, highlighting the encoder (red), decoder (blue), and the process of encoding inputs into discrete spike counts. Panel (b) focuses on a special case of the P-VAE, named \"Amortized Sparse Coding\", featuring a linear decoder and an overcomplete latent space.", "section": "3 Poisson Variational Autoencoder"}, {"figure_path": "ektPEcqGLb/figures/figures_4_1.jpg", "caption": "Figure 3: Relaxed Poisson distribution. Samples are drawn using Algorithm 1 for \u03bb = 1. At non-zero temperatures, samples are non-integer, but approach the true Poisson distribution as T \u2192 0.", "description": "This figure shows the effect of temperature parameter in Algorithm 1 on the generated Poisson distribution.  Algorithm 1 uses a reparameterization trick to sample from a Poisson distribution.  The temperature parameter controls the sharpness of the thresholding function within the algorithm.  As the temperature approaches zero, the resulting distribution more closely resembles a true Poisson distribution, with non-integer values present at non-zero temperatures.  The figure contains four plots, one each for T = 1.0, T = 0.1, T = 0.01, and T = 0.0.", "section": "3 Poisson Variational Autoencoder"}, {"figure_path": "ektPEcqGLb/figures/figures_6_1.jpg", "caption": "Figure 4: Learned basis elements, K = 512 total, each made of 16 \u00d7 16 = 256 pixels (i.e., \u03a6 \u2208 R<sup>256\u00d7512</sup>). These results are from fully linear VAEs (both the encoder and decoder were linear). Features are ordered from top-left to bottom-right, in ascending order of their associated KL divergence (P-VAE, G-VAE, L-VAE), or the magnitude of posterior logits (C-VAE). The sparse coding results (LCA and ISTA) are ordered randomly.", "description": "This figure compares the learned basis elements (dictionary) from different VAE models and sparse coding algorithms. Each image represents a basis element. The ordering of the VAE basis elements are determined by their KL divergence value, while the sparse coding results are ordered randomly. The figure visually demonstrates that P-VAE learns basis elements that closely resemble the Gabor-like receptive fields found in the visual cortex, similar to sparse coding.", "section": "Experiments"}, {"figure_path": "ektPEcqGLb/figures/figures_7_1.jpg", "caption": "Figure 4: Learned basis elements, K = 512 total, each made of 16 \u00d7 16 = 256 pixels (i.e., \u03a6 \u2208 R256\u00d7512). These results are from fully linear VAEs (both the encoder and decoder were linear). Features are ordered from top-left to bottom-right, in ascending order of their associated KL divergence (P-VAE, G-VAE, L-VAE), or the magnitude of posterior logits (C-VAE). The sparse coding results (LCA and ISTA) are ordered randomly.", "description": "This figure shows the learned basis elements (dictionary) for several VAE models, including the Poisson VAE, compared to sparse coding methods.  It highlights that the Poisson VAE with a linear decoder learns Gabor-like filters, similar to sparse coding algorithms, while other VAEs (Gaussian, Laplace, Categorical) show more noise and less organized structure. The arrangement of the basis elements reflects the order of their KL divergence or logit magnitude.", "section": "Experiments"}, {"figure_path": "ektPEcqGLb/figures/figures_18_1.jpg", "caption": "Figure 6: Left, residual term f(\u03b4r) from eq. (14). Right, quadratic approximation of f from eq. (15).", "description": "The figure shows two plots. The left plot shows the residual term f(\u03b4r) = 1 - \u03b4r + \u03b4r log \u03b4r as a function of \u03b4r. The right plot shows a quadratic approximation of f(\u03b4r), which is 0.5 * (1 - \u03b4r)^2, along with the actual f(\u03b4r) function for comparison.  These plots illustrate the behavior of the KL term in the Poisson VAE loss function, particularly how it penalizes deviations from the prior firing rate.", "section": "A.2 The KL term"}, {"figure_path": "ektPEcqGLb/figures/figures_22_1.jpg", "caption": "Figure 4: Learned basis elements, K = 512 total, each made of 16 \u00d7 16 = 256 pixels (i.e., \u03a6 \u2208 R256\u00d7512). These results are from fully linear VAEs (both the encoder and decoder were linear). Features are ordered from top-left to bottom-right, in ascending order of their associated KL divergence (P-VAE, G-VAE, L-VAE), or the magnitude of posterior logits (C-VAE). The sparse coding results (LCA and ISTA) are ordered randomly.", "description": "This figure compares the learned basis elements (dictionary) from different VAE models (P-VAE, G-VAE, L-VAE, C-VAE) and sparse coding methods (LCA, ISTA) trained on natural image patches. Each basis element is a 16x16 pixel image.  The ordering of the elements is based on either the KL divergence (for continuous VAEs) or the magnitude of posterior logits (for C-VAE).  The comparison highlights the differences in the learned representations: P-VAE learns Gabor-like features similar to sparse coding, while other VAEs show less interpretable, more noisy features.  This suggests P-VAE's ability to learn biologically plausible representations.", "section": "Experiments"}, {"figure_path": "ektPEcqGLb/figures/figures_23_1.jpg", "caption": "Figure 4: Learned basis elements, K = 512 total, each made of 16 \u00d7 16 = 256 pixels (i.e., \u03a6 \u2208 R256\u00d7512). These results are from fully linear VAEs (both the encoder and decoder were linear). Features are ordered from top-left to bottom-right, in ascending order of their associated KL divergence (P-VAE, G-VAE, L-VAE), or the magnitude of posterior logits (C-VAE). The sparse coding results (LCA and ISTA) are ordered randomly.", "description": "This figure compares the learned basis elements (dictionary) from different VAE models with those obtained from sparse coding algorithms. The P-VAE learns Gabor-like features, similar to those observed in the visual cortex and obtained by sparse coding methods. In contrast, the Gaussian VAE learns principal components, and the Laplace VAE learns a mixture of Gabor-like and noisy features. The categorical VAE also learns Gabor-like features, but with more noise.", "section": "Experiments"}, {"figure_path": "ektPEcqGLb/figures/figures_24_1.jpg", "caption": "Figure 4: Learned basis elements, K = 512 total, each made of 16 \u00d7 16 = 256 pixels (i.e., \u03a6 \u2208 R256\u00d7512). These results are from fully linear VAEs (both the encoder and decoder were linear). Features are ordered from top-left to bottom-right, in ascending order of their associated KL divergence (P-VAE, G-VAE, L-VAE), or the magnitude of posterior logits (C-VAE). The sparse coding results (LCA and ISTA) are ordered randomly.", "description": "This figure compares the learned basis elements (filters) from different VAE models (Poisson VAE, Gaussian VAE, Laplace VAE, Categorical VAE) and sparse coding methods (LCA, ISTA).  The filters from linear decoders, which are ordered based on their KL divergence or logit magnitudes, show the ability of the Poisson VAE to learn Gabor-like features, similar to sparse coding, unlike the others which learn noisy elements or principal components. The image clearly demonstrates the P-VAE's capacity for learning biologically plausible features compared to other VAE models.", "section": "Experiments"}]