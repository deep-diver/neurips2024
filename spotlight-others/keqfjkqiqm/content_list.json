[{"type": "text", "text": "Distributed-Order Fractional Graph Operating Network ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kai Zhao1\u2217, Xuhao $\\mathbf{L}\\mathbf{i}^{2*}$ , Qiyu Kang1\u2020, Feng Ji1, Qinxu Ding3, Yanan Zhao1, Wenfei Liang1, Wee Peng Tay1 1Nanyang Technological University, 2Anhui University, 3 Singapore University of Social Sciences ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce the Distributed-order fRActional Graph Operating Network (DRAGON), a novel continuous Graph Neural Network (GNN) framework that incorporates distributed-order fractional calculus. Unlike traditional continuous GNNs that utilize integer-order or single fractional-order differential equations, DRAGON uses a learnable probability distribution over a range of real numbers for the derivative orders. By allowing a flexible and learnable superposition of multiple derivative orders, our framework captures complex graph feature updating dynamics beyond the reach of conventional models. We provide a comprehensive interpretation of our framework\u2019s capability to capture intricate dynamics through the lens of a non-Markovian graph random walk with node feature updating driven by an anomalous diffusion process over the graph. Furthermore, to highlight the versatility of the DRAGON framework, we conduct empirical evaluations across a range of graph learning tasks. The results consistently demonstrate superior performance when compared to traditional continuous GNN models. The implementation code is available at https://github.com/zknus/NeurIPS-2024-DRAGON. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have been developed to handle graph-structured data, which is prevalent in domains such as social networks [1], traffic networks [2], and molecular structures [3]. The fundamental principle of GNNs is to learn representations of nodes or entire graphs that encompass both the attributes of individual nodes and the topology of their connections. This objective is accomplished through a method known as message passing or information propagation, whereby each node aggregates information from its neighbors and possibly itself, over multiple iterations or layers [4]. Recent developments in the GNN landscape have increasingly embraced the principles of continuous dynamical systems for information propagation, as discussed in [5]. This trend is exemplified in works such as CGNN [6], GRAND [7], GRAND $^{++}$ [8], GraphCON [9], Beltrami [10], GREAD [11], CDE [12], and HANG [13], which employ ordinary or partial differential equations (ODEs/PDEs) on graphs for feature aggregation. Within these continuous GNN models, the differential operator ${\\frac{\\mathrm{d}^{\\alpha}}{\\mathrm{d}t^{\\alpha}}}$ is typically constrained to integer values of $\\alpha$ , primarily 1 or 2. ", "page_idx": 0}, {"type": "text", "text": "Two directions have been proposed recently based on the aforementioned continuous GNN models to enhance their capabilities. One approach is TDE-GNN [14], which proposes to learn higher integerorder temporal dependencies for continuous GNN models. The other approach is FROND [15], which incorporates graph neural Fractional-order Differential Equations (FDEs), extending the conventional integer-order derivative ddt\u03b1\u03b1 to encompass a positive real number $\\alpha$ . This adaptation not only bolsters the model\u2019s efficacy but also enhances its adversarial robustness by varying the value of $\\alpha$ [16]. ", "page_idx": 0}, {"type": "text", "text": "TDE-GNN, however, is limited to utilizing integer-order ODEs and does not account for the nonlocal memory effects inherent in fractional-order differential operators. These operators [17] have been developed to overcome the limitations of their traditional integer-order counterparts when modeling complex real-world dynamics. The key difference between fractional and integer operators can be grasped from a microscopic random walk perspective as shown in [15, 18]. For instance, traditional integer-order diffusion PDEs, which model diffusive transport in homogeneous porous media, typically ignore the waiting times between particle movements. However, these models struggle when applied to solute diffusion in heterogeneous porous media, prompting the introduction of fractional-order operators to better handle these complexities [19, 20]. In fractional scenarios, particles may remain at their current position, delaying jumps to subsequent locations with fading waiting times and leading to a non-Markovian process. In contrast, traditional integer-order differential equations are typically used to model Markovian movement of particles, as the derivative $\\frac{\\mathrm{d}f(t)}{\\mathrm{d}t}\\,=$ $\\begin{array}{r}{\\operatorname*{lim}_{\\Delta t\\to0}{\\frac{f(t+\\Delta t)-f(t)}{\\Delta t}}}\\end{array}$ f(t+\u2206\u2206t)t\u2212f(t) captures the local rate of function changes. On the other hand, although FROND utilizes a fractional-order $\\alpha$ and demonstrates performance improvement, its capacity for feature updating dynamics remains constrained by limited temporal dependencies with a single $\\alpha$ . Moreover, the optimized performance of FROND is achieved through extensive fine-tuning of $\\alpha$ across various graph datasets. Observations from Fig. 2 indicate that performance can fluctuate significantly as the value of the fractional order varies from 0 to 1. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The distributed-order fractional differential operator has gained recognition in fractional calculus for its capacity to model complex dynamics that traditional differential equations with integer or single fractional orders cannot sufficiently capture [21]. Inspired by this advancement, we introduce a novel continuous GNN framework named the Distributed-order fRActional Graph Operating Network (DRAGON), which extends beyond existing frameworks like TDE-GNN and FROND. Rather than designating a single, constant $\\alpha$ with extensive fine-tuning, DRAGON employs a learnable measure $\\mu$ over a range $[a,b]$ for $\\alpha$ . The foundation of our framework is the distributed-order fractional differential operator [22]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\int_{a}^{b}D^{\\alpha}f(t)\\,\\mathrm{d}\\mu(\\alpha),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which can be perceived as the limiting case of $\\textstyle\\sum_{i}w(\\alpha_{i})D^{\\alpha_{i}}f(t)$ , a weighted summation over derivatives of multiple orders with weight $w(\\cdot)$ (we employ this more common notation $D^{\\alpha}$ instead of $\\mathrm{d}^{\\alpha}/\\mathrm{d}t^{\\alpha}$ henceforth). Notably, unlike TDE-GNN, which restricts $\\alpha_{i}$ to integer values, DRAGON allows for a continuous range of values, significantly broadening its application scope and flexibility in modeling. This operator also addresses the limitations of the single fractional-order operator $D^{\\alpha}$ employed in FROND, which still has a restricted capacity to model the intricacies of feature updating dynamics. From the perspective of a random walk in a diffusion process, a single $D^{\\alpha}$ dictates that the waiting time between particle jumps follows a fixed power-law distribution $\\propto t^{-\\alpha-1}$ for $0\\textless\\alpha\\textless1$ . In contrast, DRAGON adopts a more flexible approach, enabling a broader range of waiting times across multiple temporal scales. In this paper, we demonstrate the efficacy of the DRAGON framework in modeling more intricate non-Markovian node feature updating dynamics in graph-based data. We provide evidence that DRAGON can approximate any given waiting time probability distribution pertinent to graph random walks, thus showcasing its advanced capability in capturing complex feature dynamics. ", "page_idx": 1}, {"type": "text", "text": "Main contributions. Our objective is to develop a general continuous GNN framework that enhances flexibility in graph feature updating dynamics. Our key contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a generalized continuous GNN framework that incorporates distributed-order fractional derivatives, extending previous continuous GNN models into a unified approach. Specifically, our framework treats these models as special cases with $\\mu(\\alpha)$ taking a single positive real value for [7,8,11,13,15] or multiple integer values [9,14]. Our approach facilitates flexible and learnable node feature updating dynamics stemming from the superposition of dynamics across various derivative orders. \u2022 From a theoretical standpoint, we present the non-Markovian graph random walk with flexible waiting time for DRAGON, presuming that the feature updating dynamics adhere to a diffusion principle. This exposition elucidates the rationale behind the flexible feature updating dynamics. \u2022 Through empirical assessments, we test the DRAGON-enhanced versions of several prominent continuous GNN models. Our findings consistently demonstrate their outperformance. This underscores the DRAGON framework\u2019s potential as an augmentation to amplify the effectiveness of a range of continuous GNN models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper focuses on developing a new GNN framework centered around distributed-order fractional dynamic processes. In this section, we provide a concise introduction to the key concepts in fractional calculus. Throughout the paper, we adopt certain standard assumptions to ensure problem wellposedness. For instance, the well-definedness of integrations, the existence and uniqueness of the differential equation solution [23,24], and the allowance for interchange between summation and limit via the monotone or dominated convergence theorem [25] are all assumed. ", "page_idx": 2}, {"type": "text", "text": "2.1 Fractional Derivative ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The single fractional-order operator $D^{\\alpha}$ in the distributed-order fractional operator in (1) can assume various definitions. In this study, we start off with the Marchaud\u2013Weyl fractional derivative $\\mathrm{~\\boldmath~\\cal~M~}^{D^{\\alpha}}$ , recognized for its efficacy in elucidating the fading memory phenomena [26\u201328], which we will discuss further in Sections 2.1.1 and 3.2. ", "page_idx": 2}, {"type": "text", "text": "Remark 1. However, in practical engineering implementations, the Caputo fractional derivative $\\mathrm{c}D^{\\alpha}$ is more commonly utilized [15,17]. Due to space limitations, the introduction of Caputo\u2019s derivative is deferred to the Appendix $B$ and will be subsequently employed in Section 3.3 to solve DRAGON. The Marchaud\u2013Weyl and Caputo definitions are equivalent under certain constraints $[I7,29]$ . ", "page_idx": 2}, {"type": "text", "text": "For any $\\alpha\\in(0,1)$ , the Marchaud\u2013Weyl $\\alpha$ -order derivative of a function $f$ , defined over the real line, at a specified point $t$ is defined as [29]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{~\\boldmath~\\lambda~}_{\\mathrm{M}}D^{\\alpha}f(t)=\\frac{\\alpha}{\\Gamma(1-\\alpha)}\\int_{0}^{\\infty}\\frac{f(t)-f(t-\\tau)}{\\tau^{1+\\alpha}}\\,\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Gamma(\\cdot)$ is the Gamma function. For sufficiently smooth functions, according to [29], we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\alpha\\to1^{-}}\\ \\mathrm{M}^{D^{\\alpha}}f(t)=\\frac{\\mathrm{d}f(t)}{\\mathrm{d}t}=\\operatorname*{lim}_{\\Delta t\\to0}\\frac{f(t+\\Delta t)-f(t)}{\\Delta t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is evident from (2) that the Marchaud\u2013Weyl fractional derivative is a nonlocal operator and accounts for the past values of $f$ within the $(-\\infty,t)$ range, indicative of its memory effect. In terms of probability, the related non-Markovian processes for fractional systems are characterized by state evolution that depends not just on the current state, but also on historical states [18]. As $\\alpha\\rightarrow1^{-}$ in (3), the operator reverts to the traditional first-order derivative, representing the local change rate of the function with respect to time. ", "page_idx": 2}, {"type": "text", "text": "2.1.1 Non-Markovian Random Walk Interpretation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We elucidate fractional-order derivatives by linking them to one-dimensional heat diffusion and memory-decaying non-Markovian random walks [28]. Assuming a random walker moves along an axis with infinitesimal intervals of space $\\Delta x>0$ and time $\\Delta\\tau>0$ , the walker moves a distance of $\\Delta x$ from the current point $x$ in either direction with equal probability and waits at each location for a random period of time, a positive integer multiple of $\\Delta\\tau$ . This introduces randomness in the waiting times between steps. We aim to compute $u(x,t)$ , the probability of the walker arriving at position $x$ at time $t$ . The waiting time distribution, $\\psi_{\\alpha}(n)$ , is given by a power-law function $d_{\\alpha}n^{-(1+\\alpha)}$ with $d_{\\alpha}>0$ chosen to ensure $\\begin{array}{r}{\\sum_{n=1}^{\\infty}\\psi_{\\alpha}(n)=1}\\end{array}$ . The law of total probability is expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nu(x,t)=\\sum_{n=1}^{\\infty}\\left[\\frac{1}{2}u(x-\\Delta x,t-n\\Delta\\tau)\\,+\\,\\frac{1}{2}u(x+\\Delta x,t-n\\Delta\\tau)\\right]\\,\\psi_{\\alpha}(n).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, the terms within brackets denote the probability of arriving at $x$ from either neighboring points, $x-\\Delta x$ or $x+\\Delta x$ , each with probability $1/2$ . The sum over $n$ accounts for the possibility that the walker could have remained stationary for an extended period $n\\Delta\\tau$ with a waiting time probability $\\psi_{\\alpha}(n)$ . After subtracting $\\begin{array}{r}{\\sum_{n=1}^{\\infty}\\psi_{\\alpha}(\\boldsymbol{\\dot{n}})u(\\boldsymbol{x},t-n\\Delta\\tau)}\\end{array}$ from both sides and rearranging, we obtain: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{\\infty}\\frac{u(x,t)-u(x,t-n\\Delta\\tau)}{(n\\Delta\\tau)^{1+\\alpha}}(\\Delta\\tau)=\\frac{(\\Delta x)^{2}}{2d_{\\alpha}(\\Delta\\tau)^{\\alpha}}\\sum_{n=1}^{\\infty}\\delta_{2}u(x,t-n\\Delta\\tau)\\psi_{\\alpha}(n).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the second-order incremental quotient is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta_{2}u(x,t)=\\frac{u(x-\\Delta x,t)+u(x+\\Delta x,t)-2u(x,t)}{(\\Delta x)^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the limit as $\\Delta x,\\Delta\\tau\\to0$ and assuming that $\\begin{array}{r}{\\frac{(\\Delta x)^{2}}{d_{\\alpha}(\\Delta\\tau)^{\\alpha}}\\rightarrow k_{\\alpha}|\\Gamma(-\\alpha)|}\\end{array}$ for a positive $k_{\\alpha}$ [28], we obtain the time-fractional diffusion equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{~\\boldmath~\\nabla~}_{\\!\\mathrm{M}}D^{\\alpha}u=\\frac{k_{\\alpha}}{2}u_{x x},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the summations on the left-hand side of (4) converge to the integration (2). As $\\alpha\\rightarrow1^{-}$ , (5) reverts to the standard heat diffusion equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial u(x,t)}{\\partial t}=\\frac{k_{1}}{2}u_{x x}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consequently, the aforementioned non-Markovian random walk with fading memory simplifies to the Markovian random walk, thereby eliminating the memory effects. ", "page_idx": 3}, {"type": "text", "text": "2.2 Integer-Order Continuous GNN Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We denote an undirected graph as $\\mathcal{G}=(\\mathcal{V},\\mathbf{W})$ , where $\\mathcal{V}$ is the set of $|\\gamma|\\,=\\,N$ nodes and $\\mathbf{X}=$ $\\left(\\left[\\mathbf{x}_{1}\\right]^{\\mathsf{T}},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\left[\\mathbf{x}_{N}\\right]^{\\mathsf{T}}\\right)^{\\mathsf{T}}\\,\\in\\,\\mathbb{R}^{N\\times d}$ consists of rows $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{1\\times d}$ as node feature vectors. The $N\\times N$ adjacency matrix $\\mathbf{W}\\ :=\\ (W_{i j})$ has elements $W_{i j}$ indicating the edge weight between the $i$ -th and $j$ -th nodes with $W_{i j}\\,=\\,W_{j i}$ . In the subsequent GNNs inspired by dynamic processes, we let $\\mathbf{X}(t)=\\left(\\left[\\mathbf{x}_{1}(t)\\right]^{\\intercal},\\ldots,\\left[\\mathbf{x}_{N}(t)\\right]^{\\intercal}\\right)^{\\intercal}\\in\\mathbb{R}^{N\\times d}$ be the features at time $t$ with $\\mathbf{X}(0)=\\mathbf{X}$ serving as the initial condition. The time $t$ here acts as an analog to the layer index [7,30]. Typically, these dynamics can be described by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}=\\mathcal{F}(\\mathbf{W},\\mathbf{X}(t)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The function $\\mathcal{F}$ is specifically tailored for graph dynamics as illustrated in Appendix F. For instance, in the GRAND model, $\\mathcal{F}$ is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "GRAND [7]: Drawing from the standard heat diffusion equation, GRAND formulates the following feature updating dynamics: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}}=(\\mathbf{A}(\\mathbf{X}(t))-\\mathbf{I})\\mathbf{X}(t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{A}(\\mathbf{X}(t))$ is a learnable attention or fixed normalized matrix, and $\\mathbf{I}$ is an identity matrix. ", "page_idx": 3}, {"type": "text", "text": "2.3 Fractional-Order Continuous GNN Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recently, the paper [15] introduces FROND, extending traditional integer-order graph neural differential equations such as (8), (40) and (42) to fractional-order equations. The framework is formalized as ", "page_idx": 3}, {"type": "equation", "text": "$$\nD^{\\alpha}\\mathbf{X}(t)=\\mathcal{F}(\\mathbf{W},\\mathbf{X}(t)),\\quad\\alpha>0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{F}$ represents the graph dynamics. Further, the study in [16] explores the robustness of FROND, demonstrating its ability to enhance the resilience of integer-order continuous GNNs under perturbations. This underscores the potential applications of FROND in various domains. ", "page_idx": 3}, {"type": "text", "text": "2.4 Motivation: Advanced Dynamics Modeling Capability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To intuitively understand the versatility and efficacy of the DRAGON framework in learning dynamics, we consider three classical stress-strain constitutive models for viscoelastic solids: the single-order Maxwell model [31], the multi-order Zener model [32], and the distributed-order Kelvin-Voigt model [33]. Using the FROND and DRAGON frameworks, we develop Neural Network(NN) methods to predict future states based on current observations. ", "page_idx": 3}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/3c2f4a5376784c48ec3625705505218e74d0e61090d6d724a5d78104d492a41b.jpg", "table_caption": ["Table 1: Comparison of MSE for the Maxwell, Zener, and Kelvin-Voigt models using FROND-NN and DRAGON-NN frameworks. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "The detailed descriptions and implementation specifics can be found in Appendix G.1. The results presented in Table 1 demonstrate that the DRAGON framework excels in fitting not only the multiorder model but also in capturing the dynamics of single-order and distributed-order models. We observe that the DRAGON framework achieves a Mean Squared Error (MSE) that is ten times smaller than that of the FROND method across all three models. This highlights the DRAGON framework\u2019s exceptional ability to effectively learn and adapt to a diverse range of dynamics, surpassing the capabilities of FROND. ", "page_idx": 4}, {"type": "text", "text": "3 DRAGON Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce our general DRAGON framework for GNNs, with a random walk interpretation that elucidates the underlying mechanics when a specific diffusion-inspired system is utilized. Subsequently, we discuss numerical techniques for solving DRAGON. The versatility of our framework is highlighted by its capacity to encapsulate a broad spectrum of existing continuous GNN architectures, while simultaneously nurturing the development of more flexible continuous GNN designs within the research community in the future. ", "page_idx": 4}, {"type": "text", "text": "3.1 Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "DRAGON generalizes the current integer-order and fractional-order continuous GNNs as it uses a learnable probability distribution over a range of real numbers for the fractional derivative orders. Consider a graph $\\dot{\\boldsymbol{\\mathcal{G}}}=(\\boldsymbol{\\mathcal{V}},\\mathbf{W})$ composed of $|\\gamma|\\,=N$ nodes with W being adjacency matrix as defined in Section 2.2. Similar to the approach used in integer-order continuous GNN models [5,15] as presented in Section 2.2, we apply a preliminary learnable encoder function $\\varphi:\\mathcal{V}\\,\\to\\,\\mathbb{R}^{d}$ that maps each node to a feature vector. After stacking all these feature vectors, we obtain $\\textbf{X}\\in$ $\\mathbb{R}^{N\\times d}$ . Employing the distributed-order fractional derivative outlined in (1), the feature dynamics in DRAGON are characterized by the following graph dynamic equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{a}^{b}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=\\mathcal{F}(\\mathbf{W},\\mathbf{X}(t)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $[a,b]$ denotes the range of the order $\\alpha,\\,\\mu$ is a learnable measure of $\\alpha$ , and $\\mathcal{F}$ is a dynamic operator on the graph as illustrated in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "Remark 2. In practical engineering settings, the Caputo fractional derivative, represented by $\\mathrm{c}D^{\\alpha}$ , is commonly used [15, 17]. When leveraging the Caputo definition for the fractional derivative, as detailed in Section 3.3, the initial condition for (10) is given by $\\mathbf{\\dot{X}}^{[n]}(0)\\mathbf{\\dot{\\Sigma}}=\\mathbf{X}$ , where $\\mathbf{X}^{[n]}(0)$ denotes the $n$ -th order derivative at $t=0$ , encompassing the initial node features for all integers $n\\in\\mathbb{N}\\cap[0,\\lceil b\\rceil]\\,i$ [23]. Here, $\\lceil\\cdot\\rceil$ is the ceiling function, and this setup ensures a unique solution $[23]$ . For instance, when $[a,b]=[0,1].$ , we define the initial condition as ${\\bf\\bar{X}}(0)={\\bf X}$ . ", "page_idx": 4}, {"type": "text", "text": "This framework generalizes prior continuous GNN models, encompassing them as special instances. Specifically, with $\\mu(\\alpha)=\\delta(\\alpha-1)$ , where $\\delta$ is the Dirac delta function, (10) simplifies to a local first-order differential equation like [7,8,10\u201313]. When $[a,b]=[0,2]$ , we may obtain a distributedorder fractional wave propagation GNN model [21], which generalizes the second-order GraphCON model (40). When $\\mu(\\alpha)\\,=\\,\\delta(\\alpha-\\alpha_{o})$ for $\\alpha_{o}\\in\\mathbb{R}^{+}$ , (10) reduces to the FROND framework (9). Additionally, when $\\mu$ adopts a discrete distribution over multiple integers, the model corresponds to TDE-GNN [14]. ", "page_idx": 4}, {"type": "text", "text": "Following previous works, we set an integration time parameter $T$ to obtain ${\\mathbf X}(T)$ . The final node embeddings, employed for subsequent downstream tasks, can be decoded as $\\zeta(\\mathbf{X}(T))$ , where $\\zeta$ symbolizes a learnable decoder function. ", "page_idx": 4}, {"type": "text", "text": "3.2 Non-Markovian Graph Random Walk with Flexible Memory ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we provide a non-Markovian graph random walk interpretation for DRAGON under a specific anomalous diffusion setting, where the dynamic operator $\\mathcal{F}(\\mathbf{W},\\mathbf{X}(t))$ in (10) is set as $(\\mathbf{A}(\\mathbf{X}(t))-\\mathbf{I})\\mathbf{X}(t)$ in (8) with a fixed constant matrix A. More specifically, we obtain the following linear distributed-order FDE: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{0}^{1}{}_{\\mathrm{M}}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=\\mathbf{L}\\mathbf{X}(t),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we set $\\mathbf{A}=\\mathbf{W}\\mathbf{D}^{-1}$ and ${\\bf L}:={\\bf W}{\\bf D}^{-1}-{\\bf I}$ is the random walk Laplacian. Here, $\\mathbf{D}$ is a diagonal matrix with $D_{i i}\\,=\\,d_{i}$ , the degree of node $i$ . For clarity, without loss of generality, similar to the approach in Section 2.1.1, we interpret $\\mathbf X(t)$ as a $N$ -dimensional probability or concentration vector $\\mathbb{P}(t)$ over the graph nodes $\\mathcal{V}$ at time $t$ . The Marchaud\u2013Weyl $\\mathrm{M}^{D^{\\alpha}}$ employed in (11) helps expedite the exposition of the subsequent random walk, drawing an analogy from the one-dimensional random walk discussed in Section 2.1.1. ", "page_idx": 5}, {"type": "text", "text": "For every individual value $\\alpha_{o}\\in(0,1)$ , we consider a random walker navigating over graph $\\mathcal{G}$ with an infinitesimal interval of time $\\Delta\\tau>0$ . We assume that there is no self-loop in the graph topology. The dynamics of the random walk are characterized as follows: ", "page_idx": 5}, {"type": "text", "text": "1. The walker is expected to wait at the current location for a random period of time. The distribution of waiting times, $\\psi_{\\alpha_{o}}(n)$ , is given by a power-law function $d_{\\alpha_{o}}n^{-\\bar{(1+\\alpha_{o})}}$ with $d_{\\alpha_{o}}>0$ chosen to ensure $\\begin{array}{r}{\\sum_{n=1}^{\\infty}\\psi_{\\alpha_{o}}(n)=1}\\end{array}$ .   \n2. Upon deciding to make a jump, the walker can either move from the current node $i$ to a neighboring node $j$ with a probability of $\\begin{array}{r}{(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}|\\Gamma(-\\alpha_{o})|\\frac{W_{i j}}{d_{i}}}\\end{array}$ if $i\\neq j$ . Alternatively, with a probability of $1-(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}|\\Gamma(-\\alpha_{o})|$ , it will remain at the current node $i$ . ", "page_idx": 5}, {"type": "image", "img_path": "kEQFjKqiqM/tmp/901ac5801082013cd88b1eb7b2ef1e2c86307e0251f6ca98fd21d745b4716087.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: Visualization of the Non-Markovian Graph Random Walk. The diagram illustrates the walker\u2019s decision-making process during the walk. After waiting for a random duration $n\\Delta$ , the walker may either remain on the current node or proceed to a neighborhood node. This reflects the flexible, memory-influenced dynamics of the walker\u2019s movement. ", "page_idx": 5}, {"type": "text", "text": "We denote $\\mathbb{P}_{j}(t;\\boldsymbol{\\alpha}_{o})$ , the probability of the walker being at node $j$ at time $t$ with a specific order $\\alpha_{o}$ and $\\mu(\\alpha)=\\stackrel{\\cdot}{\\delta}(\\alpha-\\alpha_{o})$ . The law of total probability is expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}_{j}(t;\\alpha_{o})=\\sum_{n=1}^{\\infty}\\bigg[\\sum_{i\\in\\mathcal{V}}\\mathbb{P}_{i}(t-n\\Delta\\tau;\\alpha_{o})(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}\\lvert\\Gamma(-\\alpha_{o})\\rvert\\frac{W_{i j}}{d_{i}}}}\\\\ &{}&{+\\,\\mathbb{P}_{j}(t-n\\Delta\\tau;\\alpha_{o})\\big(1-(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}\\lvert\\Gamma(-\\alpha_{o})\\rvert\\big)\\bigg]\\psi_{\\alpha_{o}}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this equation, the summation over $n$ accounts for the possibility that the walker may have remained stationary for a period of $n\\Delta\\tau$ , with a waiting time probability of $\\psi_{\\alpha_{o}}(n)$ . Fig. 1 provides a visualization of the non-Markovian graph random walk. For more explanation of the non-Markovian random walker on graphs, please refer to Appendix E. From (12), we can derive Theorem 1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Given $\\mu(\\alpha)=\\delta(\\alpha-\\alpha_{o})$ where $\\alpha_{o}\\in(0,1)$ and $\\Delta\\tau\\to0$ , we establish that $\\mathbb{P}(t;\\boldsymbol{\\alpha_{o}})$ , the probability vector whose $j$ -th element is $\\mathbb{P}_{j}(t;\\boldsymbol{\\alpha}_{o})$ , solves (11). That is to say, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int_{0}^{1}\\mathrm{\\mathrm{\\scriptscriptstyleM}}D^{\\alpha}\\mathbb{P}(t;\\alpha_{o})\\,\\mathrm{d}\\mu(\\alpha)=\\mathbf{L}\\mathbb{P}(t;\\alpha_{o}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 3. In Theorem 1, we present the graph random walk interpretation for the fractional anomalous diffusion equation (11) under the condition that $\\mu=\\delta(\\alpha-\\alpha_{o})$ . This condition represents a single-term fractional scenario similar to FROND. At its core, this type of random walk is nonMarkovian, underscoring the importance of the entire walk history. ", "page_idx": 6}, {"type": "text", "text": "From the discussion above, for a specific $\\alpha_{o}$ , the waiting time is steered by the power-law distribution $\\propto n^{-(\\alpha_{o}+1)}$ . Moreover, the distributed-order fractional operator can be interpreted as a flexible superposition of the dynamics behaviors embodied by individual fractional-order operators. This generalization reframes the interpretation of graph random walk and enables more nuanced dynamics that accommodate diverse waiting times. Although it is feasible to formulate a random walk interpretation where the waiting time is linked to the measure $\\mu$ and converges to the solution of (11), this approach relies on the intricate stopping time technique [34][Sec 7.5] and may sacrifice flexibility in waiting time insights. Instead, we propose a more modest conclusion, demonstrating that a weighted sum of $\\psi_{\\alpha_{i}}(n)$ can approximate any waiting time, highlighting the capability of our framework in comparison to FROND. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $C_{0}(\\mathbb{N})$ be the space of functions on the natural numbers $\\mathbb{N}$ vanishing at $\\infty$ , i.e., $f\\in C_{0}(\\mathbb{N})$ if and only $\\textstyle\\operatorname*{lim}_{n\\to\\infty}f(n)=0$ . Assume the sequence $(\\alpha_{m})_{m=1}^{\\infty}$ is strict increasing in $[0,1]$ , then the span of $\\{\\psi_{\\alpha_{m}},m\\ge1\\}$ is dense in $C_{0}(\\mathbb{N})$ in the sense of uniform convergence. ", "page_idx": 6}, {"type": "text", "text": "Remark 4. Theorem 2 demonstrates the DRAGON framework\u2019s ability to approximate any waiting time distribution for graph random walkers, offering flexibility in modeling feature updating dynamics with varying extents of memory incorporation. This highlights the advantage of using DRAGON for deploying learnable and flexible feature updating dynamics. In contrast, FROND is confined to a fixed waiting time distribution, limiting its adaptability in modeling feature updating over time. ", "page_idx": 6}, {"type": "text", "text": "3.3 Solving DRAGON ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Previous continuous GNNs have leveraged neural ODE solvers [30] when $\\mu=\\delta(\\alpha-1)$ . For example, in the explicit Euler scheme, neural ODEs are effectively reduced to residual networks with shared hidden layers [30]. Addressing the challenge of solving the distributed-order FDE (10) given by DRAGON, the standard approach involves discretizing it into a multi-term FDE. This is achieved by using a quadrature formula to approximate the integral term [21,23]. As articulated in Sections 2.1 and 3.1, we follow the convention in the fractional calculus literature for real-world applications and employ the Caputo definition $\\mathrm{c}D^{\\alpha}$ in this section. This choice is intuitive, as it seamlessly incorporates initial conditions into the problem as previously discussed under (10). The initial step is to approximate (10) as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{n}w_{j}\\mathrm{C}{D^{\\alpha_{j}}}\\mathbf{X}(t)=\\mathcal{F}(\\mathbf{W},\\mathbf{X}(t))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha_{j}\\in[a,b]$ , $j=0,1,\\dots,n$ , are distinct interpolation points and $w_{j}$ are weights associated with the measure $\\mu$ . Reflecting the learnable nature of $\\mu,\\,w_{j}$ is directly set to be a learnable parameter in our implementation. ", "page_idx": 6}, {"type": "text", "text": "The next step is to solve the multi-term FDE presented in (14). According to the approach outlined in [17, Theorem 8.1], the multi-term FDE can be transformed into a system of single-order equations $\\mathrm{c}D^{\\gamma}$ , where $\\gamma:=1/M$ and $M$ is the least common multiple of the denominators of $\\alpha_{0},\\alpha_{1},\\ldots,\\alpha_{n}$ when these coefficients are rational numbers. The classical fractional Adams\u2013Bashforth\u2013Moulton method can then be applied to solve the resulting system of single-order equations [15, 35]. This method is a generalization of the Euler scheme for ODEs to fractional scenarios (see Appendix C.3 for a detailed explanation). ", "page_idx": 6}, {"type": "text", "text": "An alternative approach involves directly approximating the fractional derivative operators as demonstrated in [36]. This discretization method can then be used to derive iterative methods for solving the multi-term FDE given in (14). Detailed procedures for this method are provided in Appendix C.4. Additionally, the approximation error analysis of the numerical solvers is discussed in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "3.4 DRAGON GNNs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Section 2.2 and Appendix F, several continuous GNNs, such as (8), (40) and (42), which employ integer-order derivatives, are introduced. We now extend these dynamical systems to operate under ", "page_idx": 6}, {"type": "text", "text": "our proposed DRAGON framework, which generalizes the scenarios to involve distributed-order fractional derivatives. More specifically, we present the following GNNs, which will be utilized in Section 4 to show the advantages of our framework over various graph benchmarks. ", "page_idx": 7}, {"type": "text", "text": "1. D-GRAND: By extending (8), we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\int_{0}^{1}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=(\\mathbf{A}(\\mathbf{X}(t))-\\mathbf{I})\\mathbf{X}(t).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2. D-GraphCON: By extending (40), we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\int_{0}^{2}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=\\sigma(\\mathbf{F}_{\\theta}(\\mathbf{X}(t),t))-\\gamma\\mathbf{X}(t).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "3. D-CDE: By extending (42), we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\int_{0}^{1}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=(\\mathbf{A}(\\mathbf{X}(t))-\\mathbf{I})\\mathbf{X}(t)+\\mathrm{div}(\\mathbf{V}(t)\\circ\\mathbf{X}(t)),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\operatorname{div}(\\mathbf{V}(t)\\circ\\mathbf{X}(t))$ is given in (43) and (44). ", "page_idx": 7}, {"type": "text", "text": "Depending on the method used to compute the matrix $\\mathbf{A}$ in (15), the D-GRAND model can be categorized into two versions: linear (D-GRAND-l) and non-linear (D-GRAND-nl). Similarly, based on the computation of $\\mathbf{F}_{\\theta}$ in (16), the D-GraphCON model also has two versions: linear (DGraphCON-l) and non-linear (D-GraphCON-nl). Detailed explanations are provided in Appendix F.1. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our approach aims to enhance the capabilities of continuous GNN models by flexibly combining graph dynamics across different derivative orders. To achieve this, we have integrated DRAGON into several existing continuous GNN models and assessed their performance. Specifically, we conduct experiments on our proposed D-GRAND (15), D-GraphCON (16), and D-CDE (17) in this section, as well as D-GREAD and D-GRAND $^{++}$ in Appendix I.3 and Appendix I.4. ", "page_idx": 7}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our approach, we employ a fully connected (FC) layer as the encoder, $\\varphi:\\dot{\\mathcal{V}}\\dot{\\to}\\mathbb{R}^{d}$ , to determine the initial values for DRAGON. Subsequently, another FC layer $\\zeta$ serves as the decoder, transforming the output of DRAGON for downstream tasks. Most existing continuous GNNs are first-order or can be transformed into first-order representations of certain dynamic processes across graphs [9]. The FROND framework also restricts the fractional order to the range [0, 1], maintaining identical initial conditions to those utilized in the original models. Given these considerations, we mainly restrict $\\alpha_{j}$ values between [0,1] in our implementation, while also balancing computational costs. The parameter $\\alpha_{j}$ is selected to evenly divide the entire range, aiming to comprehensively cover values between [0, 1]. Typically, we set the number of $\\alpha_{j}$ in (14) to 10. We also explore the empirical results when $\\alpha_{j}$ exceeds 1, as shown in Appendix I.5. For a sensitivity analysis of the number and value of $\\alpha_{j}$ , we refer the readers to Appendix I.6. Details on the datasets used can be found in Appendix G.2. ", "page_idx": 7}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/2b32c5991b8581804b9bdef86e9e612ffe02c2a812503cd5e0ba7585cf966e6b.jpg", "table_caption": ["Table 2: Numerical results for various methods on LRGB tests. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Long Range Graph Benchmark ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As illustrated in Remark 4, the DRAGON framework exhibits a distinctive intrinsic property: its ability to capture flexible memory effects, which is crucial for modeling long-range dependencies in graph data [44]. To empirically validate this capability, we conduct experiments using the LongRange Graph Benchmark (LRGB) [47]. Specifically, we focus on the Peptides molecular graphs dataset, performing graph classification on the Peptides-func dataset and graph regression based on the 3D structure of peptides in the Peptides-struct dataset. The performance metrics used are Average Precision (AP) for classification and Mean Absolute Error (MAE) for regression tasks. From Table 2, it is evident that the DRAGON framework outperforms the other methods on these two long-range graph datasets, even when compared to state-of-the-art (SOTA) techniques. Notably, DRAGON achieves an improvement of approximately $4{\\sim}6\\%$ over traditional continuous GNNs like GRAND-l and F-GRAND-l. This demonstrates DRAGON\u2019s capability to effectively capture long-range dependencies in graph data. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Node Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.3.1 Homophilic Graph Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In our evaluation on homophilic datasets, we leverage a diverse set of datasets including citation networks (Cora [48], Citeseer [49], Pubmed [50]), tree-structured datasets (Disease and Airport [51]), as well as coauthor and co-purchasing graphs (CoauthorCS [52], Computer and Photo [53]). For the Disease and Airport datasets, we follow the data partitioning and preprocessing procedures as described in [51]. For all other datasets, we adopt random splits for the largest connected component (LCC), in line with the approach detailed in [7]. ", "page_idx": 8}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/abb61d391cab9c135edcd28fe113efa7fd782ac2708422a9e82751dbbb52564b.jpg", "table_caption": ["Table 3: Node classification results $(\\%)$ for random train-val-test splits. The best result of each continuous GNN family is highlighted in red. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/e319f05648dd8568ba1d88d61a5c2ce2b3cb69697b15505283251f88aac0c40a.jpg", "table_caption": ["Table 4: Node classification results $(\\%)$ . The best and the second-best result for each criterion are highlighted in red and blue, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.2 Heterophilic Graph Datasets ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For evaluating performance on heterophilic datasets, we utilize six datasets introduced in [66], with details provided in Appendix G.2. As highlighted in [66], these datasets are characterized by lower adjusted homophily $h_{\\mathrm{adj}}$ , indicating a higher degree of heterophily. In our experimental setup with these heterophilic datasets, we follow the data splitting strategy described in [66], dividing the data into $50\\%$ for training, $25\\%$ for validation, and $25\\%$ for testing. ", "page_idx": 9}, {"type": "text", "text": "4.3.3 Performance of DRAGON framework ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As shown in Table 3, for homophilic datasets such as citation networks, coauthor networks, and copurchasing networks, our DRAGON framework enhances the performance of continuous backbones like GRAND and GraphCON. This demonstrates the ability of our DRAGON framework to seamlessly integrate with existing continuous GNNs and improve their performance. Notably, on tree-structured datasets, our DRAGON framework significantly boosts the performance of both GRAND and GraphCON. In particular, on the Airport dataset, our DRAGON framework excels, achieving a $7\\%$ performance increase compared to the GIL model specifically designed for this type of tree-like dataset. Compared to FROND, our DRAGON framework shows improvements on most datasets. The results of the graph node classification on heterophilic datasets are presented in Table 4. As indicated in Table 4, the proposed D-CDE model with our DRAGON framework improves the performance of the original CDE and F-CDE models on five out of the six datasets. This underscores the ability of DRAGON to capture flexible memory effects as proved in Theorem 2, highlighting its enhanced capability in modeling complex feature updating dynamics. ", "page_idx": 9}, {"type": "text", "text": "4.4 Model Complexity ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For the Adams-Bashforth-Moulton method (25), the numerical solution is computed iteratively for $E:=T/h$ time steps, where $h$ represents the discretization size and $T$ the integration time. This process involves repeated computation of $\\mathcal{F}(\\mathbf{W},\\mathbf{X}_{j})$ for each iteration. By storing intermediate function evaluation values $\\{\\mathcal{F}({\\bf W},{\\bf X}_{j})\\}_{j}$ , we can express the total computational time complexity across the process as $\\scriptstyle\\sum_{k=0}^{E}(C\\,+\\,O(k))$ , where $O(k)$ indicates the computational overhead from summing and weighting the $k$ terms at each step. Here, represents the complexity of computing $\\mathcal{F}$ . This yields a total cost of $O\\left(E C+E^{2}\\right)$ . If a fast algorithm for the convolution computations is available, we typically require $O(E\\log E)$ for the convolution [67], resulting in $O(E C+E\\log E)$ . If the cost of weighted summing is minimal, the complexity is reduced to $O(E C)$ . For the Gr\u00fcnwaldLetnikov method (32), the computational complexity is the same as that of the method (25). ", "page_idx": 9}, {"type": "text", "text": "The term $C$ denotes the computational complexity of the function $\\mathcal{F}$ . For instance, setting $\\mathcal{F}$ to the GRAND model results in $C=|\\mathcal{E}|d_{:}$ , where $\\vert\\mathcal{E}\\vert$ represents the edge set size and $d$ the dimensionality of the features [7]. Alternatively, using the GREAD model results in $C=O((|\\mathcal{E}|+|\\mathcal{E}_{2}|)d+|\\mathcal{E}|d_{\\operatorname*{max}})$ , where $|\\mathcal{E}_{2}|$ accounts for the number of two-hop edges, and $d_{\\mathrm{max}}$ is the maximum degree among nodes [11]. More details of the computation cost can be found in Appendix $_\\mathrm{H}$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce the DRAGON framework, which incorporates distributed-order fractional derivatives into continuous GNNs. DRAGON advances the field by employing a learnable distribution of fractional derivative orders, surpassing the constraints of existing continuous GNN models. This approach eliminates the need for fine-tuning the fractional order, as required in FROND, and enriches the dynamics and representational capacity of existing continuous GNN models. We also provide a flexible random walk interpretation. Through rigorous empirical testing, DRAGON has demonstrated not only its adaptability but also its consistent outperformance compared to other continuous GNN models. Consequently, DRAGON establishes itself as a powerful framework for advancing graphrelated tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research is supported by the National Research Foundation, Singapore and Infocomm Media Development Authority under its Future Communications Research and Development Programme. Xuhao Li is supported by National Natural Science Foundation of China (Grant No. 12301491). The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg). To improve the readability, parts of this paper have been grammatically revised using ChatGPT [68]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. Wu, F. Sun, W. Zhang, X. Xie, and B. Cui, \u201cGraph neural networks in recommender systems: a survey,\u201d ACM Computing Surveys, vol. 55, no. 5, pp. 1\u201337, 2022. [2] W. Jiang and J. Luo, \u201cGraph neural network for traffic forecasting: A survey,\u201d Expert Systems with Applications, vol. 207, p. 117921, 2022.   \n[3] Y. Wang, J. Wang, Z. Cao, and A. Barati Farimani, \u201cMolecular contrastive learning of representations via graph neural networks,\u201d Nature Machine Intelligence, vol. 4, no. 3, pp. 279\u2013287, 2022.   \n[4] J. Feng, Y. Chen, F. Li, A. Sarkar, and M. Zhang, \u201cHow powerful are k-hop message passing graph neural networks,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 4776\u20134790, 2022.   \n[5] A. Han, D. Shi, L. Lin, and J. Gao, \u201cFrom continuous dynamics to graph neural networks: Neural diffusion and beyond,\u201d arXiv preprint arXiv:2310.10121, 2023.   \n[6] L.-P. Xhonneux, M. Qu, and J. Tang, \u201cContinuous graph neural networks,\u201d in Proc. International Conference Machine Learning, 2020, pp. 10 432\u201310 441.   \n[7] B. Chamberlain, J. Rowbottom, M. I. Gorinova, M. Bronstein, S. Webb, and E. Rossi, \u201cGrand: Graph neural diffusion,\u201d in Proc. Int. Conf. Mach. Learn., 2021, pp. 1407\u20131418.   \n[8] M. Thorpe, H. Xia, T. Nguyen, T. Strohmer, A. Bertozzi, S. Osher, and B. Wang, \u201cGrand $^{++}$ : Graph neural diffusion with a source term,\u201d in Proc. International Conference Learning Representations, 2022.   \n[9] T. K. Rusch, B. Chamberlain, J. Rowbottom, S. Mishra, and M. Bronstein, \u201cGraph-coupled oscillator networks,\u201d in Proc. International Conference Machine Learning, 2022.   \n[10] Y. Song, Q. Kang, S. Wang, K. Zhao, and W. P. Tay, \u201cOn the robustness of graph neural diffusion to topology perturbations,\u201d in Advances Neural Information Processing Systems, 2022.   \n[11] J. Choi, S. Hong, N. Park, and S.-B. Cho, \u201cGread: Graph neural reaction-diffusion equations,\u201d in Proc. International Conference Machine Learning, 2023.   \n[12] K. Zhao, Q. Kang, Y. Song, R. She, S. Wang, and W. P. Tay, \u201cGraph neural convection-diffusion with heterophily,\u201d in Proc. International Joint Conference on Artificial Intelligence, 2023.   \n[13] K. Zhao, Q. Kang, Y. Song, R. She, S. Wang, and W. Tay, \u201cAdversarial robustness in graph neural networks: A hamiltonian approach,\u201d in Advances Neural Information Processing Systems, 2023.   \n[14] M. Eliasof, E. Haber, E. Treister, and C.-B. B. Sch\u00f6nlieb, \u201cOn the temporal domain of differential equation inspired graph neural networks,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2024, pp. 1792\u20131800.   \n[15] Q. Kang, K. Zhao, Q. Ding, F. Ji, X. Li, W. Liang, Y. Song, and W. P. Tay, \u201cUnleashing the potential of fractional calculus in graph neural networks with FROND,\u201d in Proc. International Conference on Learning Representations, Vienna, Austria, 2024.   \n[16] Q. Kang, K. Zhao, Y. Song, Y. Xie, Y. Zhao, S. Wang, R. She, and W. P. Tay, \u201cCoupling graph neural networks with fractional order continuous dynamics: A robustness study,\u201d in Proc. AAAI Conference on Artificial Intelligence, Vancouver, Canada, Feb. 2024.   \n[17] K. Diethelm, The analysis of fractional differential equations: an application-oriented exposition using differential operators of Caputo type. Lect. Notes Math., 2010, vol. 2004.   \n[18] R. Gorenflo and F. Mainardi, \u201cFractional diffusion processes: probability distributions and continuous time random walk,\u201d in Process. Long-Range Correlations: Theory Appl., 2003, pp. 148\u2013166.   \n[19] C. Ionescu, A. Lopes, D. Copot, J. T. Machado, and J. H. Bates, \u201cThe role of fractional calculus in modeling biological phenomena: A review,\u201d Communications in Nonlinear Science and Numerical Simulation, vol. 51, pp. 141\u2013159, 2017.   \n[20] D. Krapf, \u201cMechanisms underlying anomalous diffusion in the plasma membrane,\u201d Current Topics Membranes, vol. 75, pp. 167\u2013207, 2015.   \n[21] W. Ding, S. Patnaik, S. Sidhardh, and F. Semperlotti, \u201cApplications of distributed-order fractional operators: A review,\u201d Entropy, vol. 23, no. 1, p. 110, 2021.   \n[22] M. Caputo, \u201cMean fractional-order-derivatives differential equations and fliters,\u201d Annals of the University of Ferrara, vol. 41, no. 1, pp. 73\u201384, 1995.   \n[23] K. Diethelm and N. J. Ford, \u201cNumerical analysis for distributed-order differential equations,\u201d $J.$ Comput. Appl. Math., vol. 225, no. 1, pp. 96\u2013104, 2009.   \n[24] K. Diethelm and N. Ford, \u201cAnalysis of fractional differential equations,\u201d Journal of Mathematical Analysis and Applications, vol. 265, no. 2, pp. 229\u2013248, 2002.   \n[25] P. Billingsley, Convergence of probability measures. John Wiley & Sons, 2013.   \n[26] S. G. Samko, \u201cFractional integrals and derivatives,\u201d Theory Appl., 1993.   \n[27] A. Bernardis, F. J. Mart\u00edn-Reyes, P. R. Stinga, and J. L. Torrea, \u201cMaximum principles, extension problem and inversion for nonlocal one-sided equations,\u201d J. Differ. Equ., vol. 260, no. 7, pp. 6333\u20136362, 2016.   \n[28] P. R. Stinga, \u201cFractional derivatives: Fourier, elephants, memory effects, viscoelastic materials and anomalous diffusions,\u201d arXiv preprint arXiv:2212.02279, 2022.   \n[29] F. Ferrari, \u201cWeyl and marchaud derivatives: A forgotten history,\u201d Mathematics, vol. 6, no. 1, p. 6, 2018.   \n[30] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, \u201cNeural ordinary differential equations,\u201d arXiv preprint arXiv:1806.07366, 2018.   \n[31] Y. Rossikhin and M. Shitikova, \u201cA new method for solving dynamic problems of fractional derivative viscoelasticity,\u201d Int. J. Eng. Sci., vol. 39, pp. 149\u2013176, 2001.   \n[32] T. Atanackovic, S. Konjik, L. Oparnica, and D. Zorica, \u201cThermodynamical restrictions and wave propagation for a class of fractional order viscoelastic rods,\u201d Abstr. Appl. Anal., vol. 2011, 2011.   \n[33] B. Stankovic and T. Atanackovic, \u201cDynamics of a rod made of generalized kelvin\u2013voigt viscoelastic material,\u201d J. Math. Anal. Appl., vol. 268, pp. 550\u2013563, 2002.   \n[34] M. M. Meerschaert and A. Sikorskii, Stochastic models for fractional calculus. Walter de Gruyter GmbH & Co KG, 2019, vol. 43.   \n[35] K. Diethelm, N. J. Ford, and A. D. Freed, \u201cDetailed error analysis for a fractional adams method,\u201d Numer. Algorithms, vol. 36, pp. 31\u201352, 2004.   \n[36] D. Baleanu, K. Diethelm, E. Scalas, and J. J. Trujillo, Fractional calculus: models and numerical methods. World Scientific, 2012, vol. 3.   \n[37] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in Proc. Int. Conf. Learn. Representations, 2017.   \n[38] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, \u201cSimple and deep graph convolutional networks,\u201d in Proc. Int. Conf. Mach. Learn., 2020, pp. 1725\u20131735.   \n[39] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec, \u201cStrategies for pre-training graph neural networks,\u201d 2020.   \n[40] X. Bresson and T. Laurent, \u201cResidual gated graph convnets,\u201d 2018.   \n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[42] D. Kreuzer, D. Beaini, W. Hamilton, V. L\u00e9tourneau, and P. Tossou, \u201cRethinking graph transformers with spectral attention,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 21 618\u201321 629, 2021.   \n[43] V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson, \u201cGraph neural networks with learnable structural and positional representations,\u201d 2022.   \n[44] B. Gutteridge, X. Dong, M. M. Bronstein, and F. Di Giovanni, \u201cDrew: Dynamically rewired message passing with delay,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 12 252\u201312 267.   \n[45] G. Michel, G. Nikolentzos, J. F. Lutzeyer, and M. Vazirgiannis, \u201cPath neural networks: Expressive and accurate graph neural networks,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 24 737\u201324 755.   \n[46] J. M. Baker, Q. Wang, M. Berzins, T. Strohmer, and B. Wang, \u201cMonotone operator theoryinspired message passing for learning long-range interaction on graphs,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2024, pp. 2233\u20132241.   \n[47] V. P. Dwivedi, L. Ramp\u00e1\u0161ek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini, \u201cLong range graph benchmark,\u201d 2023.   \n[48] A. McCallum, K. Nigam, J. D. M. Rennie, and K. Seymore, \u201cAutomating the construction of internet portals with machine learning,\u201d Inf. Retrieval, vol. 3, pp. 127\u2013163, 2004.   \n[49] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, \u201cCollective classification in network data,\u201d AI Magazine, vol. 29, no. 3, p. 93, Sep. 2008.   \n[50] G. M. Namata, B. London, L. Getoor, and B. Huang, \u201cQuery-driven active surveying for collective classification,\u201d in Workshop Min. Learn. Graphs, 2012.   \n[51] I. Chami, Z. Ying, C. R\u00e9, and J. Leskovec, \u201cHyperbolic graph convolutional neural networks,\u201d in Advances Neural Inf. Process. Syst., 2019.   \n[52] O. Shchur, M. Mumme, A. Bojchevski, and S. G\u00fcnnemann, \u201cPitfalls of graph neural network evaluation,\u201d Relational Representation Learn. Workshop, Advances Neural Inf. Process. Syst.,, 2018.   \n[53] J. McAuley, C. Targett, Q. Shi, and A. van den Hengel, \u201cImage-based recommendations on styles and substitutes,\u201d in Proc. Int. ACM SIGIR Conf. Res. Develop. Inform. Retrieval, 2015, p. 43\u201352.   \n[54] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio, \u201cGraph attention networks,\u201d in Proc. Int. Conf. Learn. Representations, 2018, pp. 1\u201312.   \n[55] S. Zhu, S. Pan, C. Zhou, J. Wu, Y. Cao, and B. Wang, \u201cGraph geometry interaction learning,\u201d in Advances Neural Information Processing Systems, 2020.   \n[56] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. Conf. Comput. Vision Pattern Recognition, 2016.   \n[57] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra, \u201cBeyond homophily in graph neural networks: Current limitations and effective designs,\u201d Advances in Neural Inf. Process. Syst., vol. 33, pp. 7793\u20137804, 2020.   \n[58] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra, \u201cGraph neural networks with heterophily,\u201d in Proc. AAAI Conf. Artif. Intell., 2021, pp. 11 168\u201311 176.   \n[59] E. Chien, J. Peng, P. Li, and O. Milenkovic, \u201cAdaptive universal generalized pagerank graph neural network,\u201d in Int. Conf. Learn. Representations, 2021.   \n[60] X. Li, R. Zhu, Y. Cheng, C. Shan, S. Luo, D. Li, and W. Qian, \u201cFinding global homophily in graph neural networks when meeting heterophily,\u201d arXiv preprint arXiv:2205.07308, 2022.   \n[61] D. Bo, X. Wang, C. Shi, and H. Shen, \u201cBeyond low-frequency information in graph convolutional networks,\u201d in Proc. AAAI Conf. Artif. Intell., 2021, pp. 3950\u20133957.   \n[62] L. Du, X. Shi, Q. Fu, X. Ma, H. Liu, S. Han, and D. Zhang, \u201cGbk-gnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily,\u201d in Proc. ACM Web Conf. 2022, 2022, pp. 1550\u20131558.   \n[63] S. Luan, C. Hua, Q. Lu, J. Zhu, M. Zhao, S. Zhang, X.-W. Chang, and D. Precup, \u201cRevisiting heterophily for graph neural networks,\u201d arXiv preprint arXiv:2210.07606, 2022.   \n[64] C. Bodnar, F. D. Giovanni, B. P. Chamberlain, P. Li\u00f2, and M. M. Bronstein, \u201cNeural sheaf diffusion: A topological perspective on heterophily and oversmoothing in GNNs,\u201d in Advances Neural Inf. Process. Syst., 2022.   \n[65] Y. Wang, K. Yi, X. Liu, Y. G. Wang, and S. Jin, \u201cAcmp: Allen-cahn message passing with attractive and repulsive forces for graph neural networks,\u201d in Proc. Int. Conf. Learn. Representations, 2023.   \n[66] O. Platonov, D. Kuznedelev, A. Babenko, and L. Prokhorenkova, \u201cCharacterizing graph datasets for node classification: Beyond homophily-heterophily dichotomy,\u201d arXiv preprint arXiv:2209.06177, 2022.   \n[67] M. Mathieu, M. Henaff, and Y. LeCun, \u201cFast training of convolutional networks through ffts,\u201d arXiv preprint arXiv:1312.5851, 2013.   \n[68] OpenAI, \u201cChatgpt-4,\u201d 2022, available at: https://www.openai.com (Accessed: 10 April 2024).   \n[69] A. M. Cohen, Inversion Formulae and Practical Results. Boston, MA: Springer US, 2007, pp. 23\u201344. [Online]. Available: https://doi.org/10.1007/978-0-387-68855-8_2   \n[70] K. Diethelm, N. J. Ford, and A. D. Freed, \u201cDetailed error analysis for a fractional adams method,\u201d Numer. Algorithms, vol. 36, pp. 31\u201352, 2004.   \n[71] G.-h. Gao and Z.-z. Sun, \u201cTwo alternating direction implicit difference schemes for twodimensional distributed-order fractional diffusion equations,\u201d Journal of Scientific Computing, vol. 66, pp. 1281\u20131312, 2016.   \n[72] A. Quarteroni, R. Sacco, and F. Saleri, Numerical mathematics. Springer Science & Business Media, 2010, vol. 37.   \n[73] I. Podlubny, Fractional Differential Equations. Academic Press, 1999.   \n[74] B. Jin, B. Li, and Z. Zhou, \u201cCorrection of high-order bdf convolution quadrature for fractional evolution equations,\u201d SIAM Journal on Scientific Computing, vol. 39, no. 6, pp. A3129\u2013A3152, 2017.   \n[75] Y. Dou, K. Shu, C. Xia, P. S. Yu, and L. Sun, \u201cUser preference-aware fake news detection,\u201d in Proc. International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021.   \n[76] H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang, \u201cGeom-GCN: Geometric graph convolutional networks,\u201d in Proc. International Conference Learning Representations, 2020.   \n[77] Y. Yan, M. Hashemi, K. Swersky, Y. Yang, and D. Koutra, \u201cTwo sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks,\u201d in Proc. IEEE International Conference on Data Mining. IEEE, 2022, pp. 1287\u20131292.   \n[78] D. Lim, F. Hohne, X. Li, S. L. Huang, V. Gupta, O. Bhalerao, and S. N. Lim, \u201cLarge scale learning on non-homophilous graphs: New benchmarks and strong simple methods,\u201d pp. 20 887\u2013 20 902, 2021.   \n[79] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, \u201cSimple and deep graph convolutional networks,\u201d in Proc. International Conference Machine Learning, 2020.   \n[80] B. Chamberlain, J. Rowbottom, D. Eynard, F. Di Giovanni, X. Dong, and M. Bronstein, \u201cBeltrami flow and neural diffusion on graphs,\u201d in Advances Neural Inf. Process. Syst., 2021, pp. 1594\u20131609.   \n[81] F. Di Giovanni, J. Rowbottom, B. P. Chamberlain, T. Markovich, and M. M. Bronstein, \u201cGraph neural networks as gradient flows,\u201d arXiv preprint arXiv:2206.10991, 2022.   \n[82] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, \u201cOpen graph benchmark: Datasets for machine learning on graphs,\u201d 2021.   \n[83] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, \u201cGraphsaint: Graph sampling based inductive learning method,\u201d 2020.   \n[84] J. Almira, \u201cM\u00fcntz type theorems I,\u201d Surveys in Approximation Theory, vol. 3, 2007.   \n[85] K. Kong, J. Chen, J. Kirchenbauer, R. Ni, C. B. Bruss, and T. Goldstein, \u201cGoat: A global transformer on large-scale graphs,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 17 375\u201317 390. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Introduction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This supplementary material complements the main body of our paper by providing additional details and supporting evidence for the assertions made therein. The structure of this document is organized as follows: ", "page_idx": 14}, {"type": "text", "text": "1. A comprehensive background on fractional calculus is detailed in Appendix B.   \n2. Details of the FDE solvers used in our paper are outlined in Appendix C, along with the corresponding approximation error analysis for the solvers in Appendix D.   \n3. Additional explanations for the non-Markovian random walk interpretation are provided in Appendix E.   \n4. An extended introduction to traditional integer-order continuous GNNs from the literature is presented in Appendix F.   \n5. Additional implementation details, dataset specifics, and model complexity are elaborated in Appendices G and H.   \n6. More experimental results are available in Appendix I.   \n7. Theoretical results from the main paper are rigorously proven in Appendix J.   \n8. Limitations and broader impacts are discussed in Appendix K. ", "page_idx": 14}, {"type": "image", "img_path": "kEQFjKqiqM/tmp/383a43d6866a5c911bfe221da8a6569ab4a44dae8d92b5a9702d7f95fadff6a8.jpg", "img_caption": ["Figure 2: Variation of test accuracy with fractional order $\\alpha$ in the FROND model "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Caputo Fractional Derivative ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our study, we introduce two definitions for fractional derivatives. While the elegance and interpretability of the Marchaud\u2013Weyl derivative, especially its connection to random walks, is thoroughly discussed in the main paper, the practical realm of engineering often gravitates towards the Caputo fractional derivative, denoted as $\\mathrm{c}D^{\\alpha}$ [17]. Our alignment with the fractional calculus literature leads us to adopt the Caputo definition in Section 3.3. This preference stems from the inherent advantage of the Caputo derivative: it naturally integrates initial conditions, as elaborated in (10). The two definitions are equivalent under certain constraints [17,29]. ", "page_idx": 14}, {"type": "text", "text": "Below, we explore further the details of the Caputo fractional derivative to provide readers with a deeper understanding. For notational simplicity in this supplementary material, except in Appendix J, we use $D^{\\alpha}$ interchangeably with $\\mathrm{c}D^{\\alpha}$ , as we solely focus on the Caputo definition in this context. ", "page_idx": 14}, {"type": "text", "text": "The Caputo fractional derivative of a function $f(t)$ over an interval $[0,T]$ , of a general positive order $\\alpha\\in(0,\\infty)$ , is defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD^{\\alpha}f(t)=\\frac{1}{\\Gamma(\\lceil\\alpha\\rceil-\\alpha)}\\int_{0}^{t}(t-\\tau)^{\\lceil\\alpha\\rceil-\\alpha-1}f^{[\\lceil\\alpha\\rceil]}(\\tau)\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $\\lceil\\alpha\\rceil$ is the smallest integer greater than or equal to $\\alpha,\\Gamma(\\cdot)$ symbolizes the gamma function, and $f^{[\\lceil\\alpha\\rceil]}(\\tau)$ signifies the $\\lceil\\alpha\\rceil$ -order derivative of $f$ . Within this definition, it is presumed that $f^{[\\lceil\\alpha\\rceil]}\\in L^{1}[0,T]$ , i.e., $f^{[\\lceil\\alpha\\rceil]}$ is Lebesgue integrable, to ensure the well-defined nature of $D^{\\alpha}f(t)$ as per (18) [17]. When addressing a vector-valued function, the Caputo fractional derivative is defined on a component-by-component basis for each dimension, similar to the integer-order derivative. For ease of exposition, we explicitly handle the scalar case here, although all following results can be generalized to vector-valued functions. The Laplace transform for a general order $\\alpha\\in(0,\\infty)$ is presented in [17, Theorem 7.1] as: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}D^{\\alpha}f(s)=s^{\\alpha}\\mathcal{L}f(s)-\\sum_{k=1}^{\\lceil\\alpha\\rceil}s^{\\alpha-k}f^{[k-1]}(0).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we assume that $\\mathcal{L}f$ exists on $[s_{0},\\infty)$ for some $s_{0}\\,\\in\\,\\mathbb{R}$ . In contrast, for the integer-order derivative $f^{[\\alpha]}$ when $\\alpha$ is a positive integer, we also have the formulation (19), with the only difference being the range of $\\alpha$ . Therefore, as $\\alpha$ approaches some integer, the Laplace transform of the Caputo fractional derivative converges to the Laplace transform of the traditional integer-order derivative. As a result, we can conclude that the Caputo fractional derivative operator generalizes the traditional integer-order derivative since their Laplace transforms coincide when $\\alpha$ takes an integer value. Furthermore, the inverse Laplace transform indicates the uniquely determined $D^{\\alpha}f=f^{[\\alpha]}$ (in the sense of almost everywhere [69]). ", "page_idx": 15}, {"type": "text", "text": "Under specific reasonable conditions, we can directly present this generalization as follows. We suppose $f^{[{\\lceil\\alpha\\rceil}]}(t)$ (18) is continuously differentiable. In this context, integration by parts can be utilized to demonstrate that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D^{\\alpha}f(t)=\\frac{1}{\\Gamma(\\lceil\\alpha\\rceil-\\alpha)}\\Bigg(-\\left[f^{[\\alpha]}|(\\tau)\\frac{(t-\\tau)^{[\\alpha]-\\alpha}}{\\lceil\\alpha\\rceil-\\alpha}\\right]\\Bigg|_{0}^{t}+\\int_{0}^{t}{f^{[[\\alpha]+1]}(\\tau)\\frac{(t-\\tau)^{[\\alpha]-\\alpha}}{\\lceil\\alpha\\rceil-\\alpha}\\mathrm d\\tau}\\Bigg)}\\\\ &{\\qquad\\quad=\\frac{t^{[\\alpha]-\\alpha}f^{[[\\alpha]]}(0)}{\\Gamma(\\lceil\\alpha\\rceil-\\alpha+1)}+\\frac{1}{\\Gamma(\\lceil\\alpha\\rceil-\\alpha+1)}\\times\\int_{0}^{t}(t-\\tau)^{[\\alpha]-\\alpha}f^{[[\\alpha]+1]}(\\tau)\\mathrm d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\alpha\\rightarrow\\lceil\\alpha\\rceil$ , we get the following ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{\\alpha\\to\\lceil\\alpha\\rceil}D^{\\alpha}f(t)=f^{[\\lceil\\alpha\\rceil]}(0)+\\int_{0}^{t}f^{[\\lceil\\alpha\\rceil+1]}(\\tau)\\mathrm{d}\\tau}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=f^{[\\lceil\\alpha\\rceil]}(0)+f^{[\\lceil\\alpha\\rceil]}(t)-f^{[\\lceil\\alpha\\rceil]}(0)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=f^{[\\lceil\\alpha\\rceil]}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In parallel to the integer-order derivative, given certain conditions ( [17, Lemma 3.13]), the Caputo fractional derivative possesses the semigroup property: ", "page_idx": 15}, {"type": "equation", "text": "$$\nD^{\\varepsilon}D^{n}f=D^{n+\\varepsilon}f.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note, however, that in general, the Caputo fractional derivative does not possess semigroup property [17, Lemma 3.12]. The Caputo fractional derivative also exhibits linearity, but does not adhere to the same Leibniz and chain rules as its integer counterpart. As such properties are not utilized in our work, we refer interested readers to [17, Theorem 3.17 and Remark 3.5.]. We believe the above explanation facilitates understanding the relation between the Caputo derivative and its generalization of the integer-order derivative. ", "page_idx": 15}, {"type": "text", "text": "C Numerical Solvers for FDEs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we introduce basic single-term FDEs along with techniques for solving them. We also discuss multi-term FDEs and describe methods to convert them into single-term FDEs. In our paper, we approximate the distributed-order FDE (10) using the multi-term FDE (14). We present two techniques to solve the multi-term FDE (14): one technique directly uses the single-term FDE solver, while the other approximates each fractional differential operator. For conditions necessary for the existence and uniqueness of solutions for single- and multi-term FDEs, we direct interested readers to [17, Chapter 6 and 8] and [15]. ", "page_idx": 15}, {"type": "text", "text": "C.1 Single-Term Solver ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A single-term FDE is represented as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nD^{\\alpha}y(t)=f(t,y(t))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the initial conditions take the form: ", "page_idx": 16}, {"type": "equation", "text": "$$\nD^{k}y(0)=y_{0}^{[k]},\\quad k=0,1,\\ldots,\\lceil{\\alpha}\\rceil-1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $y_{0}^{[k]}$ representing the $k$ -order derivative at point 0. ", "page_idx": 16}, {"type": "text", "text": "Our approach to solving (23) is based on the fractional Adams\u2013Bashforth\u2013Moulton method described in [70]. The basic predictor $y k{+}1$ is expressed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{k+1}=\\sum_{j=0}^{\\left\\lceil{\\alpha}\\right\\rceil-1}\\frac{t_{k+1}^{j}}{j!}y_{0}^{\\left\\lfloor{j}\\right\\rfloor}+\\frac{1}{\\Gamma(\\alpha)}\\sum_{j=0}^{k}b_{j,k+1}f(t_{j},y_{j}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $k$ denotes the current iteration or time step index in the discretization process, $h$ is the step size or time interval between successive approximations with $t_{j}\\,=\\,h j$ , and $y_{j}$ is the numerical approximation of $y(t_{j})$ . \u2308\u00b7\u2309represents the ceiling function, and when $0\\,<\\,\\alpha\\,\\leq\\,1$ , $\\lceil\\alpha\\rceil=1$ . The coefficients $b_{j,k+1}$ are defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nb_{j,k+1}=\\frac{h^{\\alpha}}{\\alpha}\\left((k+1-j)^{\\alpha}-(k-j)^{\\alpha}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using this predictor, it is possible to derive a corrector term to improve the accuracy of the solver. Nonetheless, we omit this corrector term in this work and leave its detailed exploration and implications for DRAGON to subsequent studies. ", "page_idx": 16}, {"type": "text", "text": "C.2 Convert Multi-Term to Single-Term ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We reference a theorem from [17] which provides a method to transform multi-term FDEs into their single-term counterparts, specifically when dealing with rational numbers. ", "page_idx": 16}, {"type": "text", "text": "Theorem 3. $I I7_{:}$ , Theorem 8.1.] Consider the equation ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{t}^{n_{k}}y(x)=f\\left(x,y(x),D_{t}^{n_{1}}y(x),D_{t}^{n_{2}}y(x),\\ldots,D_{t}^{n_{k-1}}y(x)\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "subject to the initial conditions ", "page_idx": 16}, {"type": "equation", "text": "$$\ny^{[j]}(0)=y_{0}^{[j]},\\quad j=0,1,\\ldots,\\lceil n_{k}\\rceil-1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $n_{k}>n_{k-1}>...>n_{1}>0,n_{j}-n_{j-1}\\leq1$ for all $j=2,3,\\ldots,k$ and $0<n_{1}\\leq1.$ . Assume that $n_{j}\\in\\mathbb{Q}$ for all $j=1,2,\\dots,k_{!}$ , define $M$ to be the least common multiple of the denominators of $n_{1},n_{2},\\ldots,n_{k}$ and set ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\gamma:=1/M\\,a n d\\,N:=M n_{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then this initial value problem is equivalent to the system of equations ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{D_{t}^{\\gamma}y_{0}(x)=y_{1}(x),}}\\\\ {{D_{t}^{\\gamma}y_{1}(x)=y_{2}(x),}}\\\\ {{\\vdots}}\\\\ {{D_{t}^{\\gamma}y_{N-2}(x)=y_{N-1}(x),}}\\\\ {{D_{t}^{\\gamma}y_{N-1}(x)=f\\left(x,y_{0}(x),y_{n_{1}/\\gamma}(x),\\ldots,y_{n_{k-1}/\\gamma}(x)\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "together with the initial conditions ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{j}(0)=\\left\\{y_{0}^{[j/M]},\\quad i f j/M\\in\\mathbb{N}_{0},\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "in the following sense: ", "page_idx": 16}, {"type": "text", "text": "(a) Whenever $Y:=\\left(y_{0},\\ldots,y_{N-1}\\right)^{\\mathrm{T}}$ with $y_{0}\\in C^{\\lceil n_{k}\\rceil}[0,T]$ for some $c>0$ is the solution of the system (28), the function $y:=y_{0}$ solves the multi-term equation initial value problem (27). Here, the notation $C^{m}[0,T]$ denotes the space of functions that have a continuous $m$ -th derivative. ", "page_idx": 17}, {"type": "text", "text": "(b) Whenever $y\\in C^{[n_{k}]}[0,T]$ is a solution of the multi-term initial value problem (27), the vector function $\\begin{array}{r}{Y\\;:=\\;\\left(y_{0},\\ldots y_{N-1}\\right)^{\\intercal}\\;:=\\;\\left(y,D_{t}^{\\gamma}y,D_{t}^{2\\gamma}y,\\ldots,D_{t}^{(N-1)\\gamma}y\\right)^{\\intercal}}\\end{array}$ solves the multidimensional initial value problem (28). ", "page_idx": 17}, {"type": "text", "text": "C.3 Solution Strategy I for (14) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Utilizing the theorem mentioned earlier from [17], we can address the solution of (14) as presented in the main manuscript. Specifically, we can express (14) as ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{n}D^{\\alpha_{n}}\\mathbf{X}(t)=\\mathcal{F}(\\mathbf{W},\\mathbf{X}(t))-\\sum_{j=0}^{n-1}w_{j}D^{\\alpha_{j}}\\mathbf{X}(t).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Subsequently, the single-term solver (25) and Theorem 3 can be employed to solve this equation. ", "page_idx": 17}, {"type": "text", "text": "C.4 Solution Strategy II for (14) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Consider the general multi-term (or more precisely, $n$ -term) fractional differential equation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{n}w_{j}D^{\\alpha_{j}}y(t)=f(t,y(t)),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with initial condition $y(0)=y_{0}$ , where $w_{j}$ are coefficients, $\\alpha_{j}\\in(0,1)$ are fractional orders, and $f(t)$ is a given function. ", "page_idx": 17}, {"type": "text", "text": "Divide the interval $[0,T]$ into $E$ equally spaced points with step size $h$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nt_{i}=i h,\\quad i=0,1,2,\\dots,E,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{h\\,=\\,\\frac{T}{E}}\\end{array}$ . The Gr\u00fcnwald-Letnikov approximation for the fractional derivative $D^{\\alpha}y(t)$ with $\\alpha\\in(0,1)$ is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{\\alpha}y(t_{i})\\approx\\frac{1}{h^{\\alpha}}\\sum_{k=0}^{i}(-1)^{k}{\\binom{\\alpha}{k}}[y(t_{i-k})-y_{0}],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\binom{\\alpha}{k}$ is the binomial coefficient for non-integer $\\alpha$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\binom{\\alpha}{k}}=\\frac{\\Gamma(\\alpha+1)}{\\Gamma(k+1)\\Gamma(\\alpha-k+1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The fractional derivative $D^{\\alpha_{j}}y(t_{i})$ for each $\\alpha_{j}$ can be approximated as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{\\alpha_{j}}y(t_{i})\\approx\\frac{1}{h^{\\alpha_{j}}}\\sum_{k=0}^{i}(-1)^{k}{\\binom{\\alpha_{j}}{k}}[y(t_{i-k})-y_{0}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We then combine the terms for the multi-term FDE: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{n}w_{j}{\\frac{1}{h^{\\alpha_{j}}}}\\sum_{k=0}^{i}(-1)^{k}{\\binom{\\alpha_{j}}{k}}[y(t_{i-k})-y_{0}]=f(t_{i-1},y(t_{i-1}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": ", or equivalently, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{j=0}^{n}w_{j}\\frac{1}{h^{\\alpha_{j}}}\\sum_{k=1}^{i}(-1)^{k}\\binom{\\alpha_{j}}{k}[y(t_{i-k})-y_{0}]+\\displaystyle\\sum_{j=0}^{n}w_{j}\\frac{1}{h^{\\alpha_{j}}}y(t_{i})-\\displaystyle\\sum_{j=0}^{n}w_{j}\\frac{1}{h^{\\alpha_{j}}}y_{0}}\\\\ {=f(t_{i-1},y(t_{i-1}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, denoting the approximation of $y(t_{i})$ as $y_{i}$ at each iteration, for each $i$ from 1 to $E:=T/h$ , we update the numerical solution $y_{i}$ using: ", "page_idx": 18}, {"type": "equation", "text": "$$\ny_{i}=\\frac{f(t_{i-1},y_{i-1})+\\sum_{j=0}^{n}w_{j}\\frac{1}{h^{\\alpha_{j}}}y_{0}-\\sum_{j=0}^{n}w_{j}\\frac{1}{h^{\\alpha_{j}}}\\sum_{k=1}^{i}(-1)^{k}\\binom{\\alpha_{j}}{k}[y_{i-k}-y_{0}]}{\\sum_{j=0}^{n}w_{j}\\frac{1}{h^{\\alpha_{j}}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This provides a step-by-step approach to iteratively update the solution of the $n$ -term FDE using the Gr\u00fcnwald-Letnikov approximation for fractional derivatives. ", "page_idx": 18}, {"type": "text", "text": "Substituting the (32) into (14), we obtain the numerical solution: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{X}_{i}=\\frac{\\mathcal{F}(\\mathbf{W},\\mathbf{X}_{i-1})+\\sum_{j=0}^{n}w_{j}\\,\\frac{1}{h^{\\alpha_{j}}}\\mathbf{X}_{0}-\\sum_{j=0}^{n}w_{j}\\,\\frac{1}{h^{\\alpha_{j}}}\\sum_{k=1}^{i}(-1)^{k}\\binom{\\alpha_{j}}{k}[\\mathbf{X}_{i-k}-\\mathbf{X}_{0}]}{\\sum_{j=0}^{n}w_{j}\\,\\frac{1}{h^{\\alpha_{j}}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{X}_{i}$ is numerical approximation of ${\\bf X}(t_{i})$ . ", "page_idx": 18}, {"type": "text", "text": "D Approximation Error ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As discussed in Section 3.3, solving the distributed-order FDE as specified in (10) involves two primary steps: ", "page_idx": 18}, {"type": "text", "text": "1. Discretizing the distributed-order derivative using a classical quadrature rule. For instance, assuming $w(\\alpha)=\\mu^{\\prime}\\bar{(}\\alpha)$ , the application of the composite Trapezoid rule [71,72] yields: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{a}^{b}D^{\\alpha}{\\bf X}(t)\\,\\mathrm{d}\\mu(\\alpha)=\\frac{\\Delta\\alpha}{2}\\left[w(\\alpha_{0})D^{\\alpha_{0}}{\\bf X}(t)+2\\sum_{j=1}^{n-1}w(\\alpha_{j})D^{\\alpha_{j}}{\\bf X}(t)+w(\\alpha_{n})D^{\\alpha_{n}}{\\bf X}(t)\\right]}\\\\ {\\phantom{D^{\\alpha{0}}{\\bf X}(t)+2\\alpha}+O((\\Delta\\alpha)^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Delta\\alpha=(b-a)/n$ and $\\alpha_{j}=a+j\\Delta\\alpha$ . After omitting smaller terms, this approximation leads to the multi-term FDE presented in (14). ", "page_idx": 18}, {"type": "text", "text": "2. Solving (14) using the fractional Adams\u2013Bashforth\u2013Moulton method as described in (25) or the Gr\u00fcnwald-Letnikov method as specified in (32). ", "page_idx": 18}, {"type": "text", "text": "Therefore, the approximation error of the true solution comprises the numerical quadrature error in Step 1 and the numerical solver error in Step 2. The quadrature error is directly evidenced by (34). To address the solver error, we consider the general $n$ -term FDE as detailed in (30). ", "page_idx": 18}, {"type": "text", "text": "For the fractional Adams\u2013Bashforth\u2013Moulton method described in (25), the multi-term FDEs are transformed into a system of single-term equations. This system is then solved using the method specified in (25). The approximation error for this solver is quantified as follows [35]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j=0,1,\\dots,E}|y(t_{j})-y_{j}|=O(h^{1+\\operatorname*{min}\\{\\alpha_{j}\\}}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $y_{j}$ denotes the value of the solution at time $t_{j}$ as computed by the numerical method, and $y(t_{j})$ represents the exact solution at time $t_{j}$ , $h$ is the step size. ", "page_idx": 18}, {"type": "text", "text": "For the Gr\u00fcnwald-Letnikov method detailed in (32), we apply the Gr\u00fcnwald-Letnikov approximation [73] to each fractional derivative $D^{\\alpha_{j}}{}y(t)$ , which is computed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nD^{\\alpha_{j}}y(t_{i})={\\frac{1}{h^{\\alpha_{j}}}}\\sum_{k=0}^{i}(-1)^{k}{\\binom{\\alpha_{j}}{k}}[y(t_{i-k})-y_{0}]+O(h).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Utilizing correction techniques detailed in [74], the approximation error is calculated as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j=0,1,\\ldots,E}|y(t_{j})-y_{j}|=O(h),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the total error is a cumulative measure of the approximation errors from both Step 1 and Step 2. ", "page_idx": 18}, {"type": "text", "text": "E Non-Markovian Graph Random Walk Interpretation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Section 3.2 details the dynamics of the random walk. For enhanced clarity, here we include the corresponding transition probability representation for the non-Markovian random walker at time $t$ , which explicitly accounts for node positions throughout the entire path history $(\\ldots,q(t-n\\Delta\\tau),\\ldots,q(t-$ $\\Delta\\tau)$ ). Here, $q(t)$ represents the walker\u2019s position on the graph nodes $\\{1,2,\\ldots,|\\nu|\\}$ at time $t$ . This model ensures that all historical states influence transitions, emphasizing the model\u2019s non-Markovian nature. We consider a random walker navigating over graph $\\mathcal{G}$ with an infinitesimal interval of time $\\Delta\\tau>0$ . We assume that there is no self-loop in the graph topology. ", "page_idx": 19}, {"type": "text", "text": "For every individual value $\\alpha_{o}\\in(0,1)$ , the transition probability of the random walk dynamics as described above Fig. 1 is characterized as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(q(t)=j_{t}\\ \\middle|\\ \\dots,q(t-n\\Delta\\tau)=j_{t-n\\Delta\\tau},\\ldots,q(t-\\Delta\\tau)=j_{t-\\Delta\\tau}\\right)}\\\\ &{\\ \\ \\ =\\left\\{\\begin{array}{l l}{(1-K)\\,\\psi_{\\alpha_{o}}(n)}&{\\mathrm{if~revisiting~historical~positions~}q(t-n\\Delta\\tau)\\mathrm{~with~}j_{t}=j_{t-n\\Delta\\tau},\\mathrm{i.e.,}}\\\\ &{\\mathrm{walker`s~wait~time~is~}n\\Delta\\tau\\mathrm{~and~stays~at~the~same~node,}}\\\\ {\\left(K\\frac{W_{j_{t-\\Delta\\tau}j_{t}}}{d_{j_{t-\\Delta\\tau}}}\\right)\\psi_{\\alpha_{o}}(n)}&{\\mathrm{if~jumping~from~historical~positions~}j_{t-n\\Delta\\tau}\\mathrm{~to~}j_{t},\\mathrm{i.e.,}\\,\\mathrm{the~walker`s~varighbout~}}\\\\ &{\\mathrm{time~is~}n\\Delta\\tau\\mathrm{~and~jumps~to~}j_{t-n\\Delta\\tau}\\mathrm{~sneighbout~}j_{t}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K:=(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}|\\Gamma(-\\alpha_{o})|$ is a normalization coefficient, $j_{t-n\\Delta\\tau}$ is the node index visited at time $t-n\\Delta\\tau$ , and $\\psi_{\\alpha_{o}}(n)$ is the probability that the walker\u2019s waiting time is $n\\Delta\\tau$ . For a specific $\\alpha_{o}$ , the waiting time $\\psi_{\\alpha_{o}}(n)$ follows a power-law distribution $\\propto n^{-(\\alpha_{o}+1)}$ . Additionally, our distributedorder fractional operator $\\begin{array}{r}{\\int D^{\\alpha}\\mathbf{X}(\\bar{t})\\mathrm{d}\\mu(\\alpha)}\\end{array}$ acts as a flexible superposition of the dynamics driven by individual fractional-order operators $D^{\\alpha}$ . This approach allows for nuanced dynamics that adapt to diverse waiting times. Theorem 2 demonstrates its capability to approximate any waiting time distribution $f(n)$ for graph-based random walkers, thereby providing versatility in modeling feature updating dynamics with varied memory incorporation levels. ", "page_idx": 19}, {"type": "text", "text": "F Integer-Order Continuous GNNs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 GRAND and GraphCON ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the general GRAND model, the governing equation is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}}=(\\mathbf{A}(\\mathbf{X}(t))-\\mathbf{I})\\mathbf{X}(t).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the case of GRAND-l, the adjacency matrix $\\mathbf{A}(\\mathbf{X}(t))$ remains constant throughout the integration process, i.e., ${\\bf A}({\\bf X}(t))={\\bf A}({\\bf X}(0))$ . ", "page_idx": 19}, {"type": "text", "text": "For GRAND-nl, the adjacency matrix $\\mathbf{A}(\\mathbf{X}(t))$ is time-varying and is calculated using $\\mathbf X(t)$ with the attention mechanism. The entries of $\\mathbf{A}(\\mathbf{X}(t))$ are given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\na(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\mathrm{softmax}\\left(\\frac{(\\mathbf{W}_{K}\\mathbf{x}_{i})^{\\top}\\mathbf{W}_{Q}\\mathbf{x}_{j}}{\\bar{d}_{k}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{W}_{K}$ and $\\mathbf{W}_{Q}$ are learned matrices, and $\\bar{d}_{k}$ is a hyperparameter determining the dimension of Wk. ", "page_idx": 19}, {"type": "text", "text": "GraphCON [9]: Influenced by oscillator dynamical systems, GraphCON is given by the following second-order differential equation ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}^{2}\\mathbf{X}(t)}{\\mathrm{d}t^{2}}}=\\sigma(\\mathbf{F}_{\\theta}(\\mathbf{X}(t),t))-\\gamma\\mathbf{X}(t)-\\beta{\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{F}_{\\theta}(\\cdot)$ represents a learnable 1-neighborhood coupling function, $\\sigma$ is an activation function, and $\\gamma$ and $\\beta$ are adjustable parameters. Equivalently, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{r l}&{\\frac{\\mathrm{d}\\mathbf{Y}(t)}{\\mathrm{d}t}=\\sigma(\\mathbf{F}_{\\theta}(\\mathbf{X}(t),t))-\\gamma\\mathbf{X}(t)-\\beta\\mathbf{Y}(t),}\\\\ &{\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}=\\mathbf{Y}(t),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the case of GraphCON-l, similar to GRAND-l, $\\mathbf{F}_{\\theta}(\\mathbf{X}(t),t)\\ =\\ \\mathbf{A}(\\mathbf{X}(t))\\ =\\ \\mathbf{A}(\\mathbf{X}(0))$ . For GraphCON-nl, similar to GRAND-nl, $\\mathbf{F}_{\\theta}(\\mathbf{X}(t),t)\\,=\\,\\dot{\\mathbf{A}}(\\dot{\\mathbf{X}(t)})$ , where $\\mathbf{A}(\\mathbf{X}(t))$ is still obtained from (39). ", "page_idx": 20}, {"type": "text", "text": "F.2 Other Continuous GNNs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Heterophilic CDE [12]: Based on the convection-diffusion equation, Heterophilic CDE includes both a diffusion and convection term to address information propagation from heterophilic neighbors: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}=(\\mathbf{A}(\\mathbf{X}(t))-\\mathbf{I})\\mathbf{X}(t)+\\mathrm{div}(\\mathbf{V}(t)\\circ\\mathbf{X}(t)),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{V}_{i j}(t)\\;\\in\\;\\mathbb{R}^{d}$ is the velocity vector associated with each edge $(i,j)$ at time $t$ , ${\\bf V}(t)\\;=\\;$ $\\{\\mathbf{V}_{i j}(t)\\}_{(i,j)\\in\\mathcal{E}}$ , $\\mathcal{E}$ is the edge set containing all the pairs $(i,j)$ s.t. $W_{i j}\\neq0$ ) and ", "page_idx": 20}, {"type": "equation", "text": "$$\ni{\\mathrm{-th~row~of~}}(\\operatorname{div}(\\mathbf{V}(t)\\circ\\mathbf{X}(t)))=\\sum_{j:(i,j)\\in\\varepsilon}\\mathbf{V}_{i j}(t)\\odot\\mathbf{x}_{j}(t)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for each node $i\\in\\mathcal{V}$ . The velocity $\\mathbf{V}_{i j}(t)$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{V}_{i j}(t)=\\sigma\\left(\\mathbf{M}(\\mathbf{x}_{j}(t)-\\mathbf{x}_{i}(t)\\right))\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with M is a learnable matrix and $\\sigma$ denotes an activation function. ", "page_idx": 20}, {"type": "text", "text": "GREAD: To tackle the challenges associated with heterophilic graphs, the paper [11] introduces the GREAD model. This model extends the GRAND framework by incorporating a reaction term, thereby establishing a diffusion-reaction equation for GNNs. The governing equation for this model is expressed as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}=-\\alpha\\mathbf{L}(\\mathbf{X}(t))+\\beta r(\\mathbf{X}(t)),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $r(\\mathbf{X}(t))$ is a reaction term, $\\alpha$ and $\\beta$ are trainable parameters designed to balance each term. ", "page_idx": 20}, {"type": "text", "text": "GRAND $^{1++}$ : Building upon the GRAND model, the paper [8] presents the GRAND $^{++}$ model. This enhancement adds a source term to the original GRAND framework, aimed at addressing challenges associated with training on limited labeled data. The differential equation used in $\\scriptstyle\\mathrm{GRAND++}$ is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{X}(t)}{\\mathrm{d}t}=-\\mathbf{L}(\\mathbf{X}(t))+\\mathbf{C}(0)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{L}(\\mathbf{X}(t))$ denotes the graph Laplacian matrix, and ${\\bf C}(0)$ represents a subset of $\\mathbf{X}(0)$ , consisting only of nodes identified as \"trustworthy\". ", "page_idx": 20}, {"type": "text", "text": "G Implementation Specifics and Dataset Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Example Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The distributed-order fractional model is a natural generalization of single-term as well as multiterm fractional order models. It is more powerful and practical in applications. Due to multiscale characteristics in some physics problems, single-term fractional order model fails to capture this feature. Though multi-term fractional order models can capture multiscale properties, they are unsuitable in applications where the number of terms and corresponding fractional orders are unknown. However, the distributed-order fractional model is capable of dealing with multiscale characteristics and does not require knowing the number of terms and corresponding fractional orders a priori. Graph data has a complex nature as it is from the real world. Therefore, it is natural to use a distributed-order fractional model. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Kelvin-Voigt model [33]: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sigma(t)=\\mathrm{E}\\tau^{\\gamma}\\int_{0}^{1}D^{\\alpha}\\epsilon(t)d\\alpha.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Maxwell model [31]: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma(t)=\\mathrm{E}_{\\infty}\\tau^{\\alpha}D^{\\alpha}\\epsilon(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Zener model [32]: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(1+\\frac{a_{o}}{b_{o}})\\sigma(t)=a_{o}D^{\\alpha}\\epsilon(t)+c_{o}(1+\\frac{a_{o}}{b_{o}})D^{\\beta}\\epsilon(t).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For simplicity, take $\\mathrm{~E~=~E_{\\infty}~}=\\ 1,\\tau\\ =\\ 1,a_{o}\\ =\\ b_{o}\\ =\\ 0.1,c_{o}\\ =\\ 1/4$ and $\\alpha~=~0.3$ in Maxwell model, $\\alpha\\;=\\;0.2,\\beta\\;=\\;0.6$ in Zener model. The toy data is generated for a common $\\sigma(t)=\\cos(t)$ and $\\epsilon(0)=0.5$ . We generate the data through an open source package FractionalDiffEq.jl (https://scifracx.org/FractionalDiffEq.jl/stable/) that is totally driven by Julia and licensed with MIT License. We follow standard setups and apply built-in algorithms. Specifically, we choose PIEX algorithm which is an explicit method for Maxwell and Zener models, and DOMatrixDiscrete algorithm that is a strip matrix method for Kelvin-Voigt model. ", "page_idx": 21}, {"type": "text", "text": "For the implementation of FROND-NN and DRAGON-NN, we split the data into $80\\%$ training and $20\\%$ testing sets. We construct identical two-layer neural networks with activation functions for both FROND and DRAGON. Using the current observations, we predict the next 10 points in the trajectory on the test data and calculate the MSE. ", "page_idx": 21}, {"type": "text", "text": "G.2 Dataset Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we detail the statistics of the datasets utilized in this paper, as illustrated in Tables 5 to 7. The datasets span various domains and scales, providing a comprehensive evaluation of DRAGON\u2019s performance. ", "page_idx": 21}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/34e882f18274e69ce2cf139ba981f2e44524aaf13faca25db39070ef234978cc.jpg", "table_caption": ["Table 5: Dataset statistics used in Table 3 "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/0c39262b4fb4ea2c92630083562aea58b5e2b67b3b85d518fab315aed52dba90.jpg", "table_caption": ["Table 6: Dataset statistics of used in Table 4 "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/141f6713d413bce0ec82e364e86c7969f11171db203dc1e6373050bc09978379.jpg", "table_caption": ["Table 7: Dataset and graph statistics used in Table 10 "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "H Time Complexity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we discuss the time complexity of the model, as detailed in Table 8 and Table 9. It is observed that the DRAGON framework exhibits computational costs comparable to those of ", "page_idx": 21}, {"type": "text", "text": "traditional continuous GNN models. All experiments are conducted on NVIDIA GeForce RTX 3090 or A5000 GPUs with 24GB of memory. ", "page_idx": 22}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/b52010b0c2d5836cbcc1aa7f934a9a7ec5501d57c9cd3433a1d23f37f360e6df.jpg", "table_caption": ["Table 8: Inference time of models on the Cora dataset: integral time $T=10$ and step size of 1 "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/3b5a703748e58b266468308891f66a0619575a6e18ef88d22872dcf97a25af8b.jpg", "table_caption": ["Table 9: Training time per epoch on the Cora dataset: integral time $T=10$ and step size of 1 "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "I More Experiment Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "I.1 Graph Classification ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Following the experiments of FROND [15], we perform graph classification tasks on the FakeNewsNet datasets [75]. The dataset features a diverse array of node features, including BERT embeddings, features derived from spaCy\u2019s pre-trained models, and proflie-specific features from Twitter accounts. The performance outcomes, as detailed in Table 10, reveal that the DRAGON-based model outperforms its counterparts, showcasing the significant enhancements brought about by the DRAGON framework. This is because DRAGON enables feature updating dynamics with flexible memory effects stemming from the coexistence of multiple orders of derivatives. ", "page_idx": 22}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/22f3eda811d2380b1b3340c636c2c54f27af57a5c18ae8b0978cd4abc6e5e51f.jpg", "table_caption": ["Table 10: Graph classification results "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "I.2 Oversmoothing Mitigation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The FROND framework has demonstrated strong performance in mitigating the oversmoothing issue in GNNs [15]. As shown in Theorem 2, DRAGON can approximate any waiting time distribution, suggesting its potential to address the oversmoothing problem as well. To verify this, we conduct node classification experiments under different integration times, which can be viewed as the number of layers when the step size is set to 1. From Table 11, we observe that the DRAGON framework maintains comparable performance across various depths, demonstrating consistent mitigation of the oversmoothing issue. Furthermore, we find that DRAGON obviously outperforms FROND on the Pubmed dataset. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/24a88819721c82105c1f3b219c8c29fc30e0a6f20d2a14ef58c660fb64421664.jpg", "table_caption": ["Table 11: Oversmoothing mitigation under fixed data splitting without using largest connected component (LCC). \u2018-\u2019 indicates the numerical solvers failed. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "I.3 D-GREAD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Building upon the GREAD model [11], we introduce D-GREAD with the following formulation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{0}^{1}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=-\\alpha\\mathbf{L}(\\mathbf{X}(t))+\\alpha r(\\mathbf{X}(t))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Following the experimental setting in [11], we conduct a node classification task on three heterophilic graph datasets, adhering to the data split method described in [76]. The baseline results are directly reported from [11]. As shown in Table 12, the DRAGON framework significantly improves upon the corresponding continuous GNNs, achieving the best performance across all three datasets. Notably, even the GRAND model, which traditionally underperforms on heterophilic graph datasets, performs exceptionally well when integrated with the DRAGON framework. This demonstrates the DRAGON framework\u2019s capability to learn a wide range of temporal dynamics and seamlessly integrate with continuous GNNs. ", "page_idx": 23}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/8e4d0b144a6db7dd743cf2fdd07662bd213109510053362de6fe697ec9f8b689.jpg", "table_caption": ["Table 12: Node classification results $(\\%)$ of heterophilic graph under fixed data splits [76] "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "I.4 D-GRAND++ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Expanding on the GRAND $^{++}$ model [8], we introduce D-GRAND $^{++}$ with the following formulation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{0}^{1}D^{\\alpha}\\mathbf{X}(t)\\,\\mathrm{d}\\mu(\\alpha)=-\\mathbf{L}(\\mathbf{X}(t))+\\mathbf{C}(0)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We adhere to the experimental framework outlined in the GRAND $^{++}$ study, focusing specifically on the model\u2019s efficacy in limited-label scenarios. The key difference in our approach is the integration of DRAGON framework. Our results in Table 13 clearly show that D-GRAND $^{++}$ not only consistently outperforms the baseline $\\scriptstyle\\mathrm{GRAND++}$ across various tests but also shows competitive performance with F-GRAND $^{1++}$ . ", "page_idx": 24}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/d7dba6a07f02537715ab4018a359d8ab53a37e818db39e120af59b4b2d422dda.jpg", "table_caption": ["Table 13: Node classification results $(\\%)$ under limited-label scenarios "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "I.5 D(oscillation)-GRAND ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our framework accommodates any floating value for $\\alpha$ . Nonetheless, for the experiments presented in our main paper, we have specified $\\alpha\\in[0,1]$ to ensure a fair comparison by maintaining identical initial conditions to those utilized in the original models. ", "page_idx": 24}, {"type": "text", "text": "For instance, we can let $\\alpha$ range between 0 and 2, leading to the D(oscillation)-GRAND model: ", "page_idx": 24}, {"type": "text", "text": "In contrast to GRAND and D-GRAND, which employ the initial condition $\\mathbf{X}(0)=\\mathbf{X}$ , D(oscillation)- GRAND is characterized as an oscillation-type differential equation and adopts the initial condition $\\mathbf{X}^{\\prime}(0)=\\mathbf{X}(0)=\\mathbf{X}$ . However, comparing this model to GRAND or F-GRAND makes it challenging to ascertain whether performance differences arise from the varied initial conditions or the incorporation of distributed fractional derivatives. To preserve the D-GRAND as a diffusion-type equation with the same initial condition as its counterparts, GRAND and F-GRAND, we limit $\\alpha$ to the range $0<\\alpha\\le1$ . ", "page_idx": 24}, {"type": "text", "text": "We showcase preliminary results for D(oscillation)-GRAND in Table 14. The findings reveal that D(oscillation)-GRAND does not outperform GRAND or D-GRAND on these datasets, suggesting that increasing the value of $\\alpha$ does not contribute positively to these tasks and instead elevates the model\u2019s complexity. ", "page_idx": 24}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/bb97a6c2f0c9ae015968babd1928576b6bb73153d8bc6fdbaebdeecd1dcbe60e.jpg", "table_caption": ["Table 14: Comparison between GRAND, D-GRAND, and D(oscillation)-GRAND "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "I.6 Sensitivity Analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "As demonstrated in our main paper, a significant advantage of the DRAGON framework is its ability to learn the optimal $\\alpha$ through the adjustment of weights $w_{j}$ in (14). We analyze the impact of varying the number $n$ in (14) on the final accuracy. The findings, illustrated in Table 15 and Table 16, reveal that test accuracy remains stable despite changes in $n$ , underscoring DRAGON\u2019s robustness against parameter selection. This stability highlights the framework\u2019s considerable improvements, as compared with the FROND framework results depicted in Fig. 2. ", "page_idx": 25}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/bd741063217474b3520e00a2ab224af694d88f8fac18c9761b42880807b13498.jpg", "table_caption": ["Table 15: Learned $w_{j}$ of Airport dataset "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/9680242d16c51aa0dd6f2cea8c2b3b8f88536208afe0eb6bfb29691c09285d69.jpg", "table_caption": ["Table 16: Learned $w_{j}$ of Roman-empire dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "I.7 Large Scale Ogb-Products dataset ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To showcase the scalability of the DRAGON framework to large-scale datasets, we expand our evaluation to include the $\\mathrm{Ogb}$ -products dataset, following the experimental protocols detailed in [82]. To manage this extensive dataset effectively, we adopted a mini-batch training strategy that involves sampling nodes and constructing subgraphs, as introduced by GraphSAINT [83]. The outcomes presented in Table 17 demonstrate that the DRAGON-based model outperforms others, highlighting DRAGON\u2019s efficiency and scalability. ", "page_idx": 25}, {"type": "text", "text": "I.8 Hyperparameters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The hyperparameters employed in Table 4 are detailed in Table 18. For the hyperparameters pertaining to all other experiments, they will be disclosed alongside the code release. ", "page_idx": 25}, {"type": "text", "text": "J Proofs of Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide detailed proofs of the results stated in the main paper. ", "page_idx": 25}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/b93db8f4ea38ff0e491e0277ecd1ace8d7435559baebce859a10bbccf3c51134.jpg", "table_caption": ["Table 17: Node classification accuracy $(\\%)$ on Ogb-products dataset "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "kEQFjKqiqM/tmp/30da97baba8be1685e11de4eb180299e1f45765722298954037118a82f9d8bf0.jpg", "table_caption": ["Table 18: Hyper-parameters used in Table 4 "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "J.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. Noting that $\\begin{array}{r}{\\sum_{n=1}^{\\infty}\\psi_{\\alpha_{o}}(n)=1}\\end{array}$ , we subtract $\\begin{array}{r}{\\sum_{n=1}^{\\infty}\\psi_{\\alpha_{o}}(n)\\mathbb{P}_{j}(t-n\\Delta\\tau;\\alpha_{o})}\\end{array}$ from both sides of (12) to yield the  following: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\sum_{n=1}^{\\infty}(\\mathbb{P}_{j}(t;\\alpha_{o})-\\mathbb{P}_{j}(t-n\\Delta\\tau;\\alpha_{o}))\\,\\psi_{\\alpha_{0}}(n)}}}\\\\ {{{\\displaystyle=(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}\\big|\\Gamma(-\\alpha_{o})\\big|\\sum_{n=1}^{\\infty}\\Bigg[\\sum_{i\\ne j}\\mathbb{P}_{i}(t-n\\Delta\\tau;\\alpha_{o})\\frac{W_{i j}}{d_{i}}-\\mathbb{P}_{j}(t-n\\Delta\\tau;\\alpha_{o})\\Bigg]\\psi_{\\alpha_{o}}(n)}}}\\\\ {{{\\displaystyle=(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}\\big|\\Gamma(-\\alpha_{o})\\big|\\sum_{n=1}^{\\infty}[\\mathbb{L}\\mathbb{P}(t-n\\Delta\\tau;\\alpha_{o})]_{j}\\,\\psi_{\\alpha_{o}}(n).}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Divide both sides by $(\\Delta\\tau)^{\\alpha_{o}}d_{\\alpha_{o}}|\\Gamma(-\\alpha_{o})|$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{|\\Gamma(-\\alpha_{o})|}\\sum_{n=1}^{\\infty}\\frac{\\mathbb{P}_{j}(t;\\alpha_{o})-\\mathbb{P}_{j}(t-n\\Delta\\tau;\\alpha_{o})}{(n\\Delta\\tau)^{1+\\alpha_{o}}}\\Delta\\tau}\\\\ {\\displaystyle=\\sum_{n=1}^{\\infty}\\left[\\mathbf{L}\\mathbb{P}(t-n\\Delta\\tau;\\alpha_{o})\\right]_{j}\\psi_{\\alpha_{o}}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\Delta\\tau\\to0$ and switch the limit and the summation according to dominated convergence theorem (we assume the conditions are satisfied), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{|\\Gamma(-\\alpha_{o})|}\\int_{0}^{\\infty}\\!\\frac{\\mathbb{P}_{j}(t;\\alpha_{o})-\\mathbb{P}_{j}(t-\\tau;\\alpha_{o})}{\\tau^{1+\\alpha_{o}}}\\,\\mathrm{d}\\tau}&{}\\\\ &{=\\left[\\mathbf{L}\\mathbb{P}(t;\\alpha_{o})\\right]_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\Gamma(1-\\alpha_{o})=\\alpha_{o}\\Gamma(-\\alpha_{o})$ , according to (2), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\boldmath~\\lambda~}_{\\mathrm{M}}D^{\\alpha_{o}}\\mathbb{P}(t;\\alpha_{o})={\\bf L}\\mathbb{P}(t;\\alpha_{o}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The proof is now complete. ", "page_idx": 26}, {"type": "text", "text": "J.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. Let $r_{i}=\\alpha_{i}+1$ . It is obvious that $\\begin{array}{r}{\\sum_{i\\geq1}1/r_{i}=\\infty.}\\end{array}$ . Let $C[0,1]$ be the space of continuous function on the interval $[0,1]$ with the $\\infty$ -norm. By the M\u00fcntz\u2013Sz\u00e1sz theorem [84], the span of $\\{x^{r_{i}},r_{i}\\in\\mathbb{R}\\}$ is dense in $C[0,1]$ . ", "page_idx": 26}, {"type": "text", "text": "Consider any $f\\in C_{0}(\\mathbb{N})$ . We define a function ${\\overline{{f}}}\\in C[0,1]$ associated with $f$ as follows. We set $\\overline{{f}}(0)=0,\\overline{{f}}(1/n)=f(n),n\\in\\mathbb{N}.$ . We then linearly interpolate between $1/n+1$ and $1/n$ for any $n\\geq1$ to obtain $\\overline{{f}}$ on the remaining points of $[0,1]$ . Apart from 0, the function $\\overline{{f}}$ is piecewise linear and hence continuous. It is also continuous at 0 as $f$ is vanishing at $\\infty$ . ", "page_idx": 26}, {"type": "text", "text": "According to the first paragraph, for any $\\epsilon>0$ , we can find a $N$ and coefficients $\\{w_{i},0\\le i\\le N\\}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|{\\overline{{f}}}(x)-\\sum_{i=0}^{N}w_{i}x^{r_{i}}\\right|<\\epsilon,{\\mathrm{~for~any~}}x\\in[0,1].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Letting $x=0$ , we see that $|w_{0}|<\\epsilon$ . Therefore, for any $n\\in\\mathbb N$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\left\\lvert f(n)-\\sum_{i=1}^{N}w_{i}^{\\prime}\\cdot\\psi_{\\alpha_{i}}(n)\\right\\rvert}\\\\ {\\displaystyle=\\left\\lvert\\overline{{f}}(\\frac{1}{n})-\\sum_{i=1}^{N}w_{i}\\cdot\\frac{1}{n^{r_{i}}}\\right\\rvert}\\\\ {\\displaystyle\\leq\\left\\lvert\\overline{{f}}(\\frac{1}{n})-\\sum_{i=0}^{N}w_{i}\\cdot\\frac{1}{n^{r_{i}}}\\right\\rvert+\\epsilon}\\\\ {\\displaystyle<2\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $w_{i}^{\\prime}$ is defined s.t. $w_{i}=w_{i}^{\\prime}d_{\\alpha_{i}}$ . The proof is now complete3. ", "page_idx": 27}, {"type": "text", "text": "K Limitations and Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This paper proposes a generalized framework, DRAGON, that enhances existing continuous GNNs. However, its application is currently limited to continuous GNNs. For other types of GNNs, such as graph transformers [85], they need to be transformed into the formulation of differential equations before being combined with DRAGON. A future direction to address this limitation is to develop a more general DRAGON framework that does not rely on numerical solvers. Regarding broader impacts, the future societal impact of this work depends on a commitment to ethical standards and responsible use. It is crucial to ensure that advancements lead to positive outcomes without compromising individual rights or contributing to inequality. ", "page_idx": 27}, {"type": "text", "text": "L Contribution Statement ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The concept of DRAGON was initially proposed by Feng Ji and the framework is fully developed by Qiyu Kang and Kai Zhao. The manuscript was written collaboratively by Kai Zhao, Xuhao Li, and Qiyu Kang. Theoretical support for FDE was provided by Feng Ji, Qiyu Kang, Xuhao Li, and Qinxu Ding. Kai Zhao was responsible for writing the implementation code and organizing the experiments. Wenfei Liang and Yanan Zhao provided experimental support. Guidance throughout the process was provided by Wee Peng Tay. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper\u2019s contributions and scope are detailed in the abstract and introduction Section 1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have discussed the limitations in Appendix K. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The theorems proposed in Section 3.2 are supported by detailed proofs provided in Appendix J. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have included the implementation details in Appendix G and the hyperparameters in Appendix I.8. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have provided the source code in the supplementary material. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have included the implementation details in Appendix G and the hyperparameters in Appendix I.8. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We report mean and standard deviation values in our main experiments. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We report the computing cost in Appendix H. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have discussed broader impacts in Appendix K. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package or dataset. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]