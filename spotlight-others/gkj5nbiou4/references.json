{"references": [{"fullname_first_author": "Alistarh", "paper_title": "QSGD: Communication-efficient SGD via gradient quantization and encoding", "publication_date": "2017-12-03", "reason": "This paper introduces a communication-efficient stochastic gradient descent method using gradient quantization and encoding, which is a fundamental concept in the field of distributed optimization and directly relevant to the current paper's focus on improving communication complexity."}, {"fullname_first_author": "Gorbunov", "paper_title": "MARINA: Faster non-convex distributed learning with compression", "publication_date": "2021-07-01", "reason": "MARINA is a state-of-the-art algorithm for non-convex distributed optimization with compression, providing a basis and comparison point for the proposed MARINA-P method."}, {"fullname_first_author": "Szlendak", "paper_title": "Permutation compressors for provably faster distributed nonconvex optimization", "publication_date": "2021-07-01", "reason": "This paper introduces permutation compressors, a key component of the proposed MARINA-P algorithm, and provides theoretical analysis that improves the algorithm's performance."}, {"fullname_first_author": "Carmon", "paper_title": "Lower bounds for finding stationary points I", "publication_date": "2020-07-01", "reason": "This paper establishes lower bounds for finding stationary points in non-convex optimization, providing a theoretical foundation for analyzing the efficiency of the proposed algorithms."}, {"fullname_first_author": "Kone\u010dn\u00fd", "paper_title": "Federated learning: Strategies for improving communication efficiency", "publication_date": "2016-10-26", "reason": "This paper introduces federated learning, a distributed optimization setting that motivates the research in the current paper, which focuses on reducing communication complexity in a similar distributed framework."}]}