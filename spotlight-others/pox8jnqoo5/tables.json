[{"figure_path": "Pox8jNQOo5/tables/tables_17_1.jpg", "caption": "Table 1: Values of LDS parameters and global distributions for the sampling of LDS parameters in learning an adaptive Kalman filter task (Section 4.2) p(x) denotes the distribution that x is drawn from and U(a, b) denotes the uniform distribution between a and b.", "description": "This table shows the parameters used to generate the data for the adaptive Kalman filter task in Section 4.2 of the paper.  It specifies the values for the time horizon (T), the fixed duration before context switches (Tfix), the survival time of each context (d), and the probability distributions for the parameters of the linear dynamical systems (LDSs) used in the task. Specifically, it details the uniform distributions from which the mean (b), the survival time (\u03c4), the observation noise variance (\u03c3\u03b2), and the process noise variance (\u03c3) are sampled.", "section": "E.2 Details on Figure 2"}, {"figure_path": "Pox8jNQOo5/tables/tables_20_1.jpg", "caption": "Table 2: Hyperparameters used for Adam. \u03b7 is to the learning rate.", "description": "This table shows the hyperparameter settings used for the Adam optimizer across different tasks in the paper.  It lists the batch size and learning rate (\u03b7) used for each task. The learning rate is a crucial hyperparameter that controls the step size during the optimization process.", "section": "F Hyperparameter values"}, {"figure_path": "Pox8jNQOo5/tables/tables_20_2.jpg", "caption": "Table 3: Ranges of hyperparameters explored for Adam", "description": "This table shows the ranges of hyperparameter values explored for the Adam optimizer across various tasks in the paper.  The hyperparameters include the batch size and the learning rate (\u03b7). The table provides the ranges tested for each task, allowing for a better understanding of the parameter search space explored during the experiments.", "section": "F Hyperparameter values"}, {"figure_path": "Pox8jNQOo5/tables/tables_20_3.jpg", "caption": "Table 4: Hyperparameters used for SOFO. K is the number of tangents used, P is the total number of network parameters, \u03b7 is the learning rate and \u03bb is the relative damping applied on SOFO. Note that for experiments where a range of K is used the corresponding entry in the table is left as empty.", "description": "This table presents the hyperparameters used for the SOFO optimizer in various experiments presented in the paper.  It shows the batch size, the number of tangents (K), the ratio of K to the total number of parameters (K/P), the learning rate (\u03b7), and the relative damping parameter (\u03bb).  Note that for some experiments, a range of K values were tested, hence the empty entries.", "section": "F Hyperparameter values"}, {"figure_path": "Pox8jNQOo5/tables/tables_20_4.jpg", "caption": "Table 5: Ranges of hyperparameters explored for SOFO", "description": "This table shows the ranges of hyperparameters used in the experiments for the SOFO optimizer.  The hyperparameters include batch size, the number of tangents (K), the learning rate (\u03b7), and the relative damping parameter (\u03bb).  Each row represents a different task used in the experiments, showing the range of values explored for each hyperparameter in that specific task.  The table is crucial for understanding the range of parameter settings used to find the optimal configurations for SOFO across various experiments.", "section": "F Hyperparameter values"}, {"figure_path": "Pox8jNQOo5/tables/tables_21_1.jpg", "caption": "Table 6: Range of the learning rate \u03b7 explored and all final hyperparameters used for FGD [4], first-order SOFO [28] for the Lorenz task (Section 4.1) and batched FORCE (Appendix B) for the 3-bit flip-flop task (Section 4.3). Range of \u03b1 refers to the range of the parameter \u03b1 explored.", "description": "This table shows the hyperparameters used for three different optimizers: FGD, first-order SOFO, and batched FORCE.  It indicates the batch size, the number of tangents (K, relevant only to SOFO), the learning rate (\u03b7), the range of learning rates explored during hyperparameter search, and the range of \u03b1 values (only for batched FORCE).  The optimizers are applied to two different tasks: the Lorenz task and the 3-bit flip-flop task. The table helps clarify the settings used in the experiments for each optimizer and task.", "section": "F Hyperparameter values"}]