[{"figure_path": "eKHQbgvL3G/tables/tables_5_1.jpg", "caption": "Table 1: The evaluation of point tracking performance for dynamic objects. We benchmark the quality of point tracking in DAVIS [12] videos with the point annotations provided by TAP-Net [11]. We note that TrackIME is incorporated with TAPIR point tracker [6].", "description": "This table presents a comparison of point tracking performance on the DAVIS dataset for dynamic objects.  Several state-of-the-art methods are evaluated using two different query scenarios: 'First Query' (single query at the start) and 'Strided Query' (queries at intervals).  The metrics used to evaluate performance are Jaccard Index (J1 and AJ), average displacement error (\u03b4\u03b5 and \u03b4avg), and occlusion accuracy (OA). TrackIME, the proposed method, is compared against other methods (TAPNet, PIPS2, TAPIR, CoTracker, OmniMotion) to show its performance gains.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_6_1.jpg", "caption": "Table 2: Universality of TrackIME with different point tracking models. We incorporate recent point tracking model baselines [6, 7, 8, 9, 11] with our method, and benchmark its performance on DAVIS [12], RGBStacking [33], and Kinetics [34]. \u2020: the underlined results are obtained with subsets of RGBStacking and Kinetics datasets due to a large optimization cost for the OmniMotion [8].", "description": "This table demonstrates the universality of TrackIME by incorporating it with five different point tracking models (TAPNet, PIPS2, CoTracker, OmniMotion, TAPIR) and evaluating their performance on three benchmark datasets (DAVIS, RGBStacking, Kinetics).  It shows consistent performance improvements across all baselines and datasets when TrackIME is incorporated. Note that some results for OmniMotion are obtained using subsets of RGBStacking and Kinetics datasets because of high computational costs.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study of the components in our model. We ablate the effect of search space pruning (Pruning), trajectory aggregation (Aggregation), and the progressive inference (Progressive) modules for point tracking. We evaluate the tracking benchmark in DAVIS scenes [11, 12].", "description": "This table presents an ablation study evaluating the individual and combined effects of three key components of the TrackIME model on point tracking performance.  The components are search space pruning, trajectory aggregation, and progressive inference. The performance is measured using the Jaccard index (J1), average Jaccard index (AJ), and average displacement error (\u03b4x) at 1-pixel and average pixel thresholds. The evaluation was conducted on the DAVIS benchmark dataset, which is widely used for dynamic object tracking.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_9_1.jpg", "caption": "Table 4: Zero-shot video object segmentation performance in DAVIS benchmark. We consider two set of zero-shot baselines, those utilizing the set of classes [23, 24, 25, 26, 27, 28, 29] and the baseline utilizing a set of query points [30] in a similar manner to our TrackIME. \u2020: we produced the results for TrackIME and SAM-PT [30] under the common set-up, such as the number of tracking points, segmentation function (HQ-SAM [35]), and the same mask formatting for the benchmark.", "description": "This table presents the performance comparison of different zero-shot video object segmentation methods on the DAVIS benchmark.  It contrasts methods using class labels as input with those using point trajectories (like TrackIME). The results are given in terms of mean Jaccard (Jm), mean F-measure (Fm), and their average (J&F)m for both the validation and test-dev sets.", "section": "4.2 Video Object Segmentation"}, {"figure_path": "eKHQbgvL3G/tables/tables_13_1.jpg", "caption": "Table 5: The pruned resolutions in our method for each baseline point tracking model. We report the specific values for  \ud835\udc3b\u2080, \ud835\udc3b\u2081, \ud835\udc4a\u2080, \ud835\udc4a\u2081 when TAPNet[11], PIPS2[9], CoTracker[7], OmniMotion[8], or TAPIR[6] is used as the baseline.", "description": "This table shows the input size used for each baseline model in the TrackIME framework.  The baseline models are TAPNet, PIPS2, CoTracker, OmniMotion, and TAPIR. The table lists the height and width of the input frames (\ud835\udc3b\u2080 and \ud835\udc4a\u2080) and the height and width of the pruned input frames (\ud835\udc3b\u2081 and \ud835\udc4a\u2081).  The pruned input frames are used to reduce computation time. This table is helpful to understand how the input sizes are adapted for different models within the TrackIME framework.", "section": "A Experimental details for point tracking"}, {"figure_path": "eKHQbgvL3G/tables/tables_14_1.jpg", "caption": "Table 1: The evaluation of point tracking performance for dynamic objects. We benchmark the quality of point tracking in DAVIS [12] videos with the point annotations provided by TAP-Net [11]. We note that TrackIME is incorporated with TAPIR point tracker [6].", "description": "This table presents a comparison of different point tracking methods on the DAVIS dataset, focusing on dynamic objects.  The metrics used to evaluate performance include Jaccard Index (J1 and AJ), average delta errors (\u03b4\u03b5 and \u03b4\u03b1\u03bdg), and occlusion accuracy (OA). The results show that TrackIME, when combined with the TAPIR tracker, consistently outperforms other state-of-the-art methods in various metrics.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_15_1.jpg", "caption": "Table 7: The comparison of the FLOP counts of the TAPIR [6] models and TrackIME. We report the FLOP counts to process 64 video frames by TAPIR with the input dimensions (256 \u00d7 256) (default), (512 \u00d7 512), and (768 \u00d7 768), TAPIR Hi-Res (a fine-tuned model for (1080 \u00d7 1080)) and TrackIME (ours). For each model, we further report the benchmark results in terms of AJ (Average Jaccard), \u03b4avg, and OA (Occlusion Accuracy), evaluated under DAVIS-F and DAVIS-S. For TAPIR Hi-Res, numbers are excerpted from [6], where results for DAVIS-F are not available.", "description": "This table compares the computational cost (measured in FLOPs) and performance of different versions of the TAPIR model and the TrackIME model.  It shows how FLOPs increase with higher input resolution for TAPIR, and also shows the performance of TrackIME, which uses a smaller input size while outperforming the other higher resolution TAPIR models.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_15_2.jpg", "caption": "Table 2: Universality of TrackIME with different point tracking models. We incorporate recent point tracking model baselines [6, 7, 8, 9, 11] with our method, and benchmark its performance on DAVIS [12], RGBStacking [33], and Kinetics [34]. \u2020: the underlined results are obtained with subsets of RGBStacking and Kinetics datasets due to a large optimization cost for the OmniMotion [8].", "description": "This table demonstrates the universality of TrackIME by incorporating it with different state-of-the-art point tracking models (TAPNet, PIPS2, CoTracker, OmniMotion, TAPIR).  It shows the average Jaccard index (AJ) and average accuracy (\u03b4\u03b1avg) for each model, both with and without TrackIME, across three benchmark datasets (DAVIS, RGBStacking, Kinetics). The results highlight consistent improvements achieved by TrackIME across various models and datasets.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_16_1.jpg", "caption": "Table 9: Ablation study of the pruning size in our framework. We ablate the pruning size considered in TrackIME. For the evaluation, we calculate both pixel-scale and average-scale metrics under the DAVIS-F dataset [11].", "description": "This table shows the ablation study of different pruning sizes used in the TrackIME framework.  It compares the performance using various pruning sizes (1080, 960, 768, 512, 384 pixels) with a single progressive step (K=1), and a configuration with two progressive steps (K=2) using sizes 960 and 384. The results are evaluated using various metrics, including pixel-scale metrics (J1, \u03b41, J2, \u03b42) and average-scale metrics (AJ, \u03b4avg). This analysis demonstrates how different pruning strategies impact performance in terms of accuracy and efficiency.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_16_2.jpg", "caption": "Table 10: Ablation study of the effect of the number of semantic neighbors in our method. We ablate the number of semantic neighbors considered in our method. For the evaluation, we calculate both pixel-scale and average-scale metrics under the DAVIS-F dataset [11].", "description": "This table presents an ablation study on the effect of varying the number of semantic neighbors (S+1) used in the TrackIME framework.  The study evaluates the impact on the performance of the point tracking task, measured by both pixel-level (J1, J2) and average-scale (AJ, \u03b4avg) metrics. The results are obtained using the DAVIS-F dataset.", "section": "4 Experiments"}, {"figure_path": "eKHQbgvL3G/tables/tables_17_1.jpg", "caption": "Table 11: Forcing equal weights in the aggregation. We ablate the use of aggregation weights in our method. For the evaluation, we calculate both pixel-scale and average-scale metrics under DAVIS-F dataset [11].", "description": "This table presents an ablation study on the impact of using equal weights versus default weights in the aggregation process within the TrackIME framework.  The evaluation metrics used are J1 (Jaccard-1), AJ (Average Jaccard), \u03b4\u03b11 (d-average accuracy at 1-pixel threshold), \u03b4\u03b1avg (average d-average accuracy), and OA (Occlusion Accuracy).  The results show that using the default weights yields slightly better performance than using equal weights for all metrics. This highlights the importance of the weighted aggregation strategy employed in TrackIME for improved accuracy.", "section": "4.1 Point Tracking"}, {"figure_path": "eKHQbgvL3G/tables/tables_17_2.jpg", "caption": "Table 1: The evaluation of point tracking performance for dynamic objects. We benchmark the quality of point tracking in DAVIS [12] videos with the point annotations provided by TAP-Net [11]. We note that TrackIME is incorporated with TAPIR point tracker [6].", "description": "This table presents a comparison of point tracking performance on dynamic objects using the DAVIS dataset.  It compares several methods, including TAPNet, PIPS2, TAPIR, and CoTracker, and shows how TrackIME improves upon the baseline performance of TAPIR. The metrics used for evaluation include Jaccard Index (J1, AJ), average displacement error (\u03b4\u03b5), and occlusion accuracy (OA).  Two query scenarios are considered: First Query (single query at the start of the video) and Strided Query (queries at every 5 frames).", "section": "4 Experiments"}]