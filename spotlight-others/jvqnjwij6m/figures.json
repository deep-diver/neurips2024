[{"figure_path": "JvQnJWIj6m/figures/figures_0_1.jpg", "caption": "Figure 1: Our C-JEPA achieves faster and better convergence than I-JEPA.", "description": "This figure shows a comparison of the convergence speed and performance of the proposed C-JEPA model against the baseline I-JEPA model.  The y-axis represents the linear probing performance (percentage) achieved on a ViT-B model, and the x-axis represents the number of pre-training epochs.  The line graph clearly demonstrates that C-JEPA surpasses I-JEPA in both convergence rate (reaching higher performance with fewer epochs) and final performance (achieving a higher percentage). This indicates that the improvements introduced by C-JEPA, such as the integration of VICReg, significantly enhance the stability and quality of visual representation learning.", "section": "1 Introduction"}, {"figure_path": "JvQnJWIj6m/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of I-JEPA (a) and SimSiam (b).", "description": "This figure illustrates the architectures of I-JEPA and SimSiam, two self-supervised learning methods.  Panel (a) shows I-JEPA, which uses a masking strategy to predict masked patches of an image. A context encoder processes the entire image, and a target encoder processes the masked patches. The predictor aims to reconstruct the masked patches based on the context encoder's output. Panel (b) shows SimSiam, which uses two differently augmented views of the same image. These views are processed by the encoder, and a predictor network is used to create a similarity between the two outputs.  Both methods aim to learn robust image representations in an unsupervised manner, but they use different strategies for achieving this.", "section": "3 Methodology"}, {"figure_path": "JvQnJWIj6m/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA [2], attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.", "description": "This figure presents a qualitative comparison of attention maps generated by the I-JEPA and C-JEPA models on a set of images. Each image is accompanied by three attention maps: one from I-JEPA's target encoder, one from C-JEPA's target encoder, and one from C-JEPA's context encoder. The figure visually demonstrates that C-JEPA produces more focused and contextually relevant attention maps than I-JEPA, highlighting its improved ability to capture important features and relationships within images.", "section": "4.3 Experimental analysis"}, {"figure_path": "JvQnJWIj6m/figures/figures_21_1.jpg", "caption": "Figure 3: Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA [2], attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.", "description": "This figure compares attention maps from the I-JEPA and C-JEPA models for several images.  It shows attention maps from both the target and context encoders for each model. The caption states that the C-JEPA model produces better attention maps. This visualization supports the claim that C-JEPA improves the quality and stability of visual representation learning.", "section": "4.3 Experimental analysis"}, {"figure_path": "JvQnJWIj6m/figures/figures_22_1.jpg", "caption": "Figure 3: Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA [2], attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.", "description": "This figure shows a qualitative comparison of attention maps generated by the I-JEPA and C-JEPA models on the ViT-B/16 architecture.  For each image, it displays the original image along with three attention maps: one from the target encoder of the original I-JEPA, one from the target encoder of the improved C-JEPA, and one from the context encoder of the C-JEPA. The comparison highlights the improved quality and focus of attention maps produced by the C-JEPA model, indicating its ability to identify and focus on relevant features more effectively. The improved attention maps from C-JEPA suggest a superior understanding of the image context and are consistent with the quantitative performance improvements reported in the paper.", "section": "E More Visualizations"}, {"figure_path": "JvQnJWIj6m/figures/figures_22_2.jpg", "caption": "Figure 3: Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA [2], attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.", "description": "This figure displays a qualitative comparison of attention maps generated by the original I-JEPA and the improved C-JEPA models.  For each image, three sets of attention maps are shown: I-JEPA's target encoder, C-JEPA's target encoder, and C-JEPA's context encoder. The visualization demonstrates that C-JEPA produces more focused and contextually relevant attention maps compared to I-JEPA, highlighting the efficacy of the proposed modifications. The improved attention maps suggest that C-JEPA learns more meaningful and informative representations from the image data.", "section": "4.3 Experimental analysis"}, {"figure_path": "JvQnJWIj6m/figures/figures_23_1.jpg", "caption": "Figure 3: Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA [2], attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.", "description": "This figure shows a qualitative comparison of attention maps generated by I-JEPA and C-JEPA models. Each row represents a different image, and the columns show the original image along with attention maps from the target and context encoders of both I-JEPA and C-JEPA. The C-JEPA attention maps are more focused and contextually relevant compared to I-JEPA. This demonstrates that C-JEPA learns more focused and meaningful representations.", "section": "More Visualizations"}, {"figure_path": "JvQnJWIj6m/figures/figures_23_2.jpg", "caption": "Figure 3: Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA [2], attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.", "description": "This figure displays a qualitative comparison of attention maps generated by the I-JEPA and C-JEPA models.  For each image, three columns show the original image, attention maps from the target encoder of the I-JEPA model, attention maps from the target encoder of the C-JEPA model, and finally, attention maps from the context encoder of the C-JEPA model. The visualization demonstrates that C-JEPA produces more focused and contextually relevant attention maps than I-JEPA, highlighting the improvement in feature extraction and contextual understanding.", "section": "4.3 Experimental analysis"}]