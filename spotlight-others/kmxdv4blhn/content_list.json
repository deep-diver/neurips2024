[{"type": "text", "text": "Rethinking 3D Convolution in $\\ell_{p}$ -norm Space ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Hefei Institute of Physical Science, Chinese Academy of Sciences 2 University of Science and Technology of China, Hefei, China   \n3 School of Mathematical Sciences, Peking University. Beijing, China 4 Astribot, Shenzhen, China 5 Shandong University, Jinan, China 6 Hefei University of Technology, Hefei, China   \nzanly@mail.ustc.edu.cn, zhongyan@stu.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Convolution is a fundamental operation in the 3D backbone. However, under certain conditions, the feature extraction ability of traditional convolution methods may be weakened. In this paper, we introduce a new convolution method based on $\\ell_{p}$ -norm. For theoretical support, we prove the universal approximation theorem for $\\ell_{p}$ -norm based convolution, and analyze the robustness and feasibility of $\\ell_{p}$ - norms in 3D point cloud tasks. Concretely, $\\ell_{\\infty}$ -norm based convolution is prone to feature loss. $\\ell_{2}$ -norm based convolution is essentially a linear transformation of the traditional convolution. $\\ell_{1}$ -norm based convolution is an economical and effective feature extractor. We propose customized optimization strategies to accelerate the training process of $\\ell_{1}$ -norm based Nets and enhance the performance. Besides, a theoretical guarantee is given for the convergence by regret argument. We apply our methods to classic networks and conduct related experiments. Experimental results indicate that our approach exhibits competitive performance with traditional CNNs, with lower energy consumption and instruction latency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The convolution-based 3D backbone networks have demonstrated substantial success in foundational tasks such as classification [1], object tracking [2], scene segmentation [3], etc. Some downstream tasks also heavily rely on these networks, such as interactive perception [4], object manipulation [5], imitation learning [6], and human-machine collaboration [7]. In the traditional 3D convolution, suppose $K\\in\\mathbb{R}^{m\\times n}$ is the fliter, and $P_{t}\\in\\mathbb{R}^{m\\times n}$ is the sampled matrix from the $t$ -th sliding window on input data, $1\\leq t\\leq T$ . $T$ is the total sliding counts. For any $t\\geq1$ , the $t$ -th convolution is calculated as: ", "page_idx": 0}, {"type": "equation", "text": "$$\nP_{t}\\odot K=\\sum_{1\\leq i\\leq m}\\sum_{1\\leq j\\leq n}P_{t}(i,j)\\cdot K(i,j)\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "which is the same as inner product between vectors. To distinguish it from our new convolution framework, we refer to it as inner product based convolution in the following discussion. A geometric consideration arises when $P_{t}$ follows a certain symmetric distribution, such as a Gaussian or uniform distribution. By symmetry, there exist some of $\\{P_{t}\\}_{t=1}^{T}$ situated close to the subspace perpendicular to $K$ , which means $K\\odot P_{t}\\approx0$ . This inevitably leads to explicit feature loss, diminishing the model\u2019s ability on information extraction. ", "page_idx": 0}, {"type": "text", "text": "In previous works, $\\ell_{p}$ -norms $(p=1,2,3,\\cdots\\,,\\infty)$ demonstrated strong performance across various domains [8, 9, 10]. These norms exhibit remarkable capabilities in expressing spatial structures and local relationships within sets of points. To address the limitations of inner product-based convolution in certain extreme cases and to explore the potential of $\\ell_{p}$ -norms in feature extraction, we propose $\\ell_{p}$ -norm-based convolution, i.e., for any kernel $K$ and sampled matrix $P_{t}$ , it can be formulated as Eq. 2: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\|P_{t}-K\\|_{p}\\triangleq\\big(\\sum_{1\\leq i\\leq m}\\sum_{1\\leq j\\leq n}(P_{t}(i,j)-K(i,j))^{p}\\big)^{1/p}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "More precisely, the goal of this paper is to leverage the power of $\\ell_{p}$ -norm measurement (Fig. 1 (a)) and devise efficient and robust optimization methods for it. Our solutions are as follows: ", "page_idx": 1}, {"type": "text", "text": "From the theoretical standpoint, we prove the universal approximation theorem of $\\ell_{p}$ -norm Nets (for $p=1,2,3,\\cdots,\\infty)$ . Besides, we show that $\\ell_{p}$ -norm based convolutions are more robust than the traditional ones via variance analysis under random noise. ", "page_idx": 1}, {"type": "text", "text": "From the practical standpoint, we first discuss the performance of different $\\ell_{p}$ -norms in actual execution. 3D convolution in $\\ell_{\\infty}$ -norm space tends to lose multiple useful pieces of information since only the maximum absolute value is reserved. The $\\ell_{2}$ -norm measure is inherently a linear transformation of the traditional convolution (details can be found in Sec. A). In contrast, the $\\ell_{1}$ -norm has ", "page_idx": 1}, {"type": "image", "img_path": "kMxdV4Blhn/tmp/d7148006fbe28196b29b1dc8b6249f68685126fa09a77bb1821d5ec66b3f3866.jpg", "img_caption": ["Figure 1: (a) Visualizing the circles of $\\ell_{p}$ -norms. $({\\bf b})$ Manhattan distance based $\\ell_{1}$ -norm measure. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "unique potential for 3D point cloud tasks. However, directly replacing traditional convolution with an $\\ell_{1}$ -norm-based one is not feasible in practice due to the difficult convergence and local optima. To enhance network performance, we propose customized optimization strategies. The first strategy is a mixed gradient strategy (MGS), and the second is a dynamic learning rate controller (DLC). These strategies are applied in the training process (Algorithm 1) to accelerate network convergence and avoid local optima. We also provide a convergence guarantee for our optimization strategies from the perspective of regret. ", "page_idx": 1}, {"type": "text", "text": "We evaluate our method on several benchmarks, ranging from global, semi-dense, and dense prediction tasks. The experimental results show that $\\ell_{1}$ -norm Net has the same competitive performance as traditional convolution. Moreover, the proposed $\\ell_{1}$ -norm Net has three advantages: 1) $\\ell_{1}$ -norm (inherently addition operation) has lower computational complexity compared to multiplication; 2) addition significantly reduces energy consumption [11]; 3) $\\ell_{1}$ -norm operations (addition) has lower instruction latencies [12] than inner product process (multiplication). These properties facilitate the 3D point cloud tasks especially online tasks such as 3D real-time object detection, pose tracking, etc. ", "page_idx": 1}, {"type": "text", "text": "Contributions. 1) We prove the universal approximation for $\\ell_{p}$ -norm Nets. And we show that $\\ell_{p}$ -norm Nets are robust under random noise. 2) We compare different $\\ell_{p}$ -norm based convolutions, and further propose a reliable and efficient $\\ell_{1}$ -norm Net for 3D point cloud tasks with customized optimization strategies. We also give a theoretical guarantee for convergence by regret argument. 3) Experimental results demonstrate the effectiveness of our methods in 3D point cloud tasks, showing lower energy consumption and faster instruction execution. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Different Convolution Methods. Convolutions have seen significant success, leading to various convolution methods aimed at improving performance and efficiency. Traditional convolutions, introduced by [13], use fixed-size kernels to extract features but are computationally intensive and may not capture diverse patterns effectively. To overcome these limitations, several alternatives have been proposed: 1) depthwise separable convolutions [14, 15]. Popularized by MobileNets, these decompose standard convolutions into depthwise and pointwise operations. 2) dilated convolutions [16, 17, 18]. These introduce spaces between kernel elements, expanding the receptive field without increasing parameters. 3) deformable convolutions [19, 20]. These adapt the sampling loca", "page_idx": 1}, {"type": "text", "text": "tions of the convolutional kernel, enhancing the network\u2019s ability to model geometric transformations.   \nHowever, due to their unique strengths, they only excel at some specific tasks. ", "page_idx": 2}, {"type": "text", "text": "$\\ell_{p}$ -norm Measure in Different Tasks. Using the $\\ell_{p}$ -norm as a feature measurement function for convolutional kernels offers several advantages: 1) Flexibility: The $\\ell_{p}$ -norm allows adjusting the parameter $p$ according to specific needs [21, 22, 23]. 2) Sparsity: It encourages most elements in the convolutional kernel to approach zero, reducing computational complexity and storage requirements [21, 24]. Overall, in diverse settings, employ distinctive approaches. The $\\ell_{p}$ -norm is widely used across various fields. For example, in image processing, the $\\ell_{1}$ -norm is used for sparse representation in image compression [25], enabling efficient storage and transmission. In machine learning and optimization, optimization problems also use $\\ell_{p}$ -norm constraints to impose sparsity or specific patterns in solutions [26, 27]. Despite progress, directly migrating these methods into 3D point cloud tasks causes a domain gap. In this work, we aim to explore $\\ell_{p}$ -norm measure for 3D point cloud tasks in depth. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. For the sake of simplicity, in what follows, we take the classic PointNet+ $^+$ [28] as the basis model to estimate the efficiency of $\\ell_{p}$ -norm based Nets with the proposed optimization strategies. Note that, we directly replace the inner product based convolution by $\\ell_{p}$ -norms $(p=1,2,3,\\ldots,\\infty)$ based one, and denote the corresponding network by $\\ell_{p}$ -PointNet++ or $\\ell_{p}$ -norm Net. Moreover, the proposed $\\ell_{p}$ -norm based convolution can also be called $\\ell_{p}$ -norm neuron. ", "page_idx": 2}, {"type": "text", "text": "3.1 Universal Approximation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The universal approximation ability of a neural network is crucial. Firstly, it establishes a solid theoretical foundation for the network\u2019s capabilities [29], which asserts that certain architectures and activation functions enable neural networks to approximate any continuous function. There is a series of works on the approximation capacity, such as theories for feedforward networks [30], RNNs [31], Transformer [32]. However, the universal approximation property of $\\ell_{p}$ -PointNet+ $^{-+}$ has not been studied thoroughly up to now. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. Assume $S=\\{x_{1},\\cdots,x_{N}\\}\\subset\\mathbb{R}^{k}$ is an arbitrary point cloud. $J\\subset\\mathbb{R}^{k}$ is any compact set and $S\\subset J$ . For any continuous function $f$ defined on $2^{J}$ with respect to Hausdroff distance $d_{H}(\\cdot,\\cdot)$ , there exists an $\\ell_{p}$ -PointNet $++\\mathcal{P}$ satisfying for any $\\epsilon>0$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n|f(S)-{\\mathcal{P}}(S)|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, for any $\\ell_{1}$ -integrable function $g$ defined on $J$ , there exists an $\\ell_{p}\u2013P o i n t N e t{+}+\\mathcal{P}^{\\prime}$ , for any $\\epsilon^{\\prime}>0$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\int_{x\\in J}|g(x)-\\mathcal{P}^{\\prime}(x)|d x<\\epsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Briefly speaking, $f$ could be approximated by an MLP consisting of $\\ell_{p}$ -norm convolution layers and a max pooling layer. And $g$ could be approximated by a network composed of an $\\ell_{p}$ -norm convolution layer and a fully connected layer. The detailed proof can be found in Sec. A from the appendix. ", "page_idx": 2}, {"type": "text", "text": "3.2 Robustness Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the following, we show that under Gaussian random noise on input data, $\\ell_{p}$ -norm based convolutions are more robust than that based on inner product. Suppose $G\\in\\mathbb{R}^{m\\times\\dot{n}}$ is a Gaussian matrix. Each $G(i,j)\\sim N(0,\\sigma^{2})$ where $\\sigma\\,>\\,0$ is a constant. Let $P_{t}\\,\\in\\,\\mathbb{R}^{m\\times n}$ be the data at time $t$ and $K\\in\\mathbb{R}^{m\\times n}$ be the kernel function. ", "page_idx": 2}, {"type": "text", "text": "For inner product, ", "page_idx": 2}, {"type": "equation", "text": "$$\nV a r\\big[(G+P_{t})\\odot K\\big]=\\mathbb{E}_{G}\\Big[\\big(G\\odot K-\\mathbb{E}_{G}[G\\odot K]\\big)^{2}\\Big]=V a r\\big[G\\odot K\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and ", "page_idx": 2}, {"type": "equation", "text": "$$\nG\\odot K=\\sum_{i=1}^{m}\\sum_{j=1}^{n}G(i,j)K(i,j)\\sim N\\bigg(0,\\sigma^{2}\\cdot\\sum_{i=1}^{m}\\sum_{j=1}^{n}K(i,j)^{2}\\bigg).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Suppose $\\forall i\\in[m]$ and $\\forall j\\,\\in\\,[n],\\,K(i,j)$ is a constant, we have $V a r\\big[(G+P_{t})\\odot K\\big]\\,=$ $\\Theta(m n)$ . ", "page_idx": 3}, {"type": "text", "text": "For $\\ell_{p}$ -norm, first we could prove that when $p=2$ $\\dot{2},V a r\\big[||G+X-K||_{2}\\big]=O(1)$ , which is significantly smaller than $V a r\\big[(G+P_{t})\\odot$ $K]$ . The details of calculation could be found in Sec. A from the appendix. For the more ", "page_idx": 3}, {"type": "text", "text": "general cases $(p=1,2,3,\\cdots\\,,\\infty)$ , we show that $\\ell_{p}$ -norm has a small variance through numerical computation in the Tab. 1, where we take $\\sigma=1$ . ", "page_idx": 3}, {"type": "text", "text": "Table 1: Variance of the $\\ell_{p}$ -norm of Gaussian random vector when $m n=9$ . ", "text_level": 1, "page_idx": 3}, {"type": "table", "img_path": "kMxdV4Blhn/tmp/26a6c135330309e1d910cca92523012d0096d1523085cc328c4ab2b7d720df60.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Implementation of $\\ell_{p}$ -norm Nets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Note that although Theorem 1 guarantees a universal approximation capability, it does not mean that all the $\\ell_{p}$ -norm Nets are efficient and feasible in practice. Therefore, we further discuss the characteristics of each specific $\\ell_{p}$ -norm Nets $(p=1,2,3,\\cdots,\\infty)$ in detail. ", "page_idx": 3}, {"type": "text", "text": "Assume the input data follows Gaussian distribution, saying $G$ is the standard Gaussian matrix. For $\\ell_{p}$ -norm based convolution, when $p$ is greater than or equal to 3, the distribution of the output data is very close. We present the simulation results in Fig. 2. It\u2019s clear that when $p$ is getting larger, the distribution of $\\|G\\|_{p}$ gradually overlaps with the distribution of $\\|G\\|_{\\infty}$ . There", "page_idx": 3}, {"type": "image", "img_path": "kMxdV4Blhn/tmp/d190330f77eb5cce5a232f6dabc14b114a8e253ad0eb81eafdb763f4009b3441.jpg", "img_caption": ["Figure 2: (Left) The distribution of $\\|G\\|_{p}$ ,where $G$ is the standard Gaussian vector, $p=1,2,3,\\infty$ and $d i m(G)\\stackrel{=}{=}9$ . (Right) The distribution of $\\|G\\|_{p}$ $|{\\boldsymbol{p}},{\\boldsymbol{p}}=3,4,5,6,7,8,9,\\infty$ and $d i m(G)=9$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "fore, we take $p=\\infty$ as the representative case for $p\\geq3$ . ", "page_idx": 3}, {"type": "text", "text": "Actually, $l_{\\infty}$ -norm exhibits weaknesses due to its overly simplistic emphasis on the largest element. Namely this approach tends to oversimplify the feature space by disproportionately emphasizing only one dimension, potentially discarding valuable information present in other dimensions. Also, this concept is supported by experimental results in Sec. 5. Besides, $\\ell_{2}$ -norm inherently is calculated by taking the square root of the sum of the squares of its elements. And $\\ell_{2}$ -norm based convolution $\\mathcal{C}_{\\ell_{2}}$ can be regarded as an equivalence transformation of the traditional convolution $\\mathcal{C}$ . Briefly speaking, we could show that $\\mathcal{C}_{\\ell_{2}}^{2}=\\alpha+\\beta\\times{\\mathcal{C}}$ , where $\\alpha$ and $\\beta$ are constants. ", "page_idx": 3}, {"type": "text", "text": "$\\ell_{1}$ -norm can synthesize each element of the feature vector. And the $\\ell_{1}$ -norm Net is not equivalent to a translation transform, which we believe holds potential as a 3D convolutional similarity metric function according to the Theorem. 1. To this end, our method focuses on rationalizing the $\\ell_{1}$ -norm measure to maximize its potential in feature extraction. Mathematically, if the similarity measurement function between the input data and kernel function is replaced with the $\\ell_{1}$ -norm, the convolution can be re-formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nY(P_{t},K)=-\\sum_{t\\geq1}\\sum_{i,j}\\vert P_{t}(i,j)-K(i,j)\\vert\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The underlying operation of $\\ell_{1}$ -norm kernel function is addition, which has more development potential and application value in real scenarios. Specifically, 1) It contains almost no multiplication but addition, resulting in lower computational complexity of the model. 2) $\\ell_{1}$ -norm operation (addition) is proved to have lower energy consumption compared to the inner product (multiplication) calculation [33]. Take the operation of floating-point addition and multiplication as an example, which has energy costs of $0.9\\,p J$ and $3.7\\;p J$ , respectively. 3) Low latency is also a consideration in practical application scenarios. [12] tells us that multiplication (inner product process) has longer theoretical instruction wait times than addition operations. Table 1 of this study lists the instruction latency, throughput, and micromanipulation faults for Intel, AMD, and VIA CPUs. For instance, the latency of float multiplication and addition is 4 and 2 in the VIA Nano 2000 series. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.4 Regret ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "It\u2019s a good way [34, 35, 36] to demonstrate the convergence of an optimization process by analyzing the regret. Performance measurement [37], optimization guidance [38], and feedback mechanisms [39] can be summarized as its advantages. We employ it the construct the convergence theorem for our optimization strategies in Sec. 4. ", "page_idx": 4}, {"type": "text", "text": "Consider a general online optimization model between a player and an adversary. A subset ${\\mathcal{F}}\\in\\mathbb{R}^{m}$ is non-empty, bounded and closed. For each iteration $k\\in[T^{*}]$ , the player choose a point $\\mathbf{x}_{k}\\in\\mathcal{F}$ $T^{*}$ is not known for player). After committing to this choice, a convex function $h_{k}$ will be revealed by the adversary. And we note the cost of this game by regret: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{T^{*}}=\\sum_{k=1}^{T^{*}}h_{k}(\\mathbf x_{k})-\\operatorname*{min}_{\\mathbf x\\in\\mathscr F}\\sum_{k=1}^{T^{*}}h_{k}(\\mathbf x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The player aims to carefully select $\\mathbf{x}_{k}$ to minimize regret as much as possible, while conversely the adversary aims to specifically choose $h_{k}$ to hinder the player. Intuitively, if an algorithm(the player) could bound regret by a sub-linear function of $T^{*}$ , i.e., $R_{T^{*}}=o(T^{*})$ , we could conclude that \u201con the average\u201d the algorithm performs as well as the best fixed strategy in hindsight [40]. ", "page_idx": 4}, {"type": "text", "text": "4 Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By the argument above, we are motivated to devise a new convolution based on $\\ell_{1}$ -norm. However, direct training of $\\ell_{1}$ -norm Nets can easily lead to unsatisfactory results. Thus, two customized optimization strategies are proposed for training. Before introducing these optimization strategies, we clarify the notations in the following. ", "page_idx": 4}, {"type": "text", "text": "Notations Recall that $K\\in\\mathbb{R}^{m\\times n}$ is the kernel and $P_{t}\\in\\mathbb{R}^{m\\times n}$ is the sliding window on the input data, $1\\leq t\\leq T$ . $Y(P_{t},K)$ is the convolution of $K$ and $P_{t}$ . $L$ denotes the loss function in training process. We use the $m\\times n$ matrix $\\frac{\\partial L}{\\partial K}$ to denote the gradient on of $L$ on $K$ , where $\\begin{array}{r}{(\\frac{\\partial L}{\\partial K})_{i,j}=\\frac{\\partial L}{\\partial K(i,j)}}\\end{array}$ Besides, define the vectors ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{\\partial L}{\\partial Y}}\\triangleq\\left({\\frac{\\partial L}{\\partial Y(P_{1},K)}},{\\frac{\\partial L}{\\partial Y(P_{2},K)}},\\cdot\\cdot\\cdot,{\\frac{\\partial L}{\\partial Y(P_{T},K)}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial Y}{\\partial K(i,j)}\\triangleq\\Big(\\frac{\\partial Y(P_{1},K)}{\\partial K(i,j)},\\frac{\\partial Y(P_{2},K)}{\\partial K(i,j)},...,\\frac{\\partial Y(P_{T},K)}{\\partial K(i,j)}\\Big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.1 MGS: Mixed Gradient Strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we focus on the gradient descent in training process, especially the partial derivative of loss function $L$ on the kernel $K$ . It should be pointed out that $L$ is a function on $(Y(P_{1},K),Y(P_{2},K),\\ldots,Y(P_{T},K))$ . By chain rule of derivation we have for any given $(i,j)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{\\partial L}{\\partial K(i,j)}}=\\sum_{t=1}^{T}{\\frac{\\partial L}{\\partial Y(P_{t},K)}}\\cdot{\\frac{\\partial Y(P_{t},K)}{\\partial K(i,j)}}=\\langle{\\frac{\\partial L}{\\partial Y}},{\\frac{\\partial Y}{\\partial K(i,j)}}\\rangle\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that when loss function $L$ is fixed, $\\frac{\\partial L}{\\partial Y}$ is regardless of the choice of $Y(P_{t},K)$ (inner product or $\\ell_{p}$ -norm). And we should only focus on the vecto r\u2202K\u2202(Yi,j). In the context of \u21131-PointNet++: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial Y(P_{t},K)}{\\partial K(i,j)}=\\mathrm{sgn}\\big(P_{t}(i,j)-K(i,j)\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\operatorname{sgn}({\\mathord{\\cdot}})$ represents the sign function. ", "page_idx": 4}, {"type": "text", "text": "There are two unavoidable problems: 1) the use of Eq. 10 results in a signSGD update. As discussed in [41], the direction of signSGD is not aligned with the steepest descent, and this misalignment exacerbates with increasing dimensionality. 2) The gradient of $\\ell_{1}$ -norm Net is significantly smaller than that of inner product convolution in the experiment. Namely, $\\|\\frac{\\partial L}{\\partial K}\\|_{2}$ is extremely small when we choose the convolution $Y$ as $\\ell_{1}$ -norm. Taking PointNet++ on S3DIS as an example, we report the $\\ell_{2}$ norm of gradient of $\\ell_{1}$ -PointNet+ $^{-+}$ in Fig. 3. The gradient from $\\ell_{1}$ -PointNet++ is much smaller than that in PointNet $^{++}$ (e.g., $\\ell_{1}$ -PointNet $^{++}$ : 0.0002, PointNet $^{++}$ : 0.3162 in layer I). Hence, this small gradient $\\textstyle{\\frac{\\partial L}{\\partial K}}$ in $\\ell_{1}$ -norm Net would significantly slow down the training process. ", "page_idx": 5}, {"type": "text", "text": "Based on the above observations, we introduce a novel Mixed Gradient Strategy (MGS) tailored for $\\ell_{1}$ -PointNet $^{++}$ training. This approach strategically combines the gradients of the $\\ell_{1}$ -PointNet $^{++}$ and that of $\\ell_{2}$ - PointNe $^{++}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial Y(P_{t},K)}{\\partial K(i,j)}=\\frac{P_{t}(i,j)-K(i,j)}{||K-P_{t}||_{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Actually, as we discussed above, $\\ell_{2}$ -norm based convolution is a linear transform of inner product convolution. So gradient of $\\ell_{2}$ - norm Net has a proper scale. The mixed strategy involves dynamically adjusting \u2202\u2202Y K(P(it,,jK)) during training, guided by a parameter $0<$ $\\lambda<1$ and the training step $k$ . The mixed gradient strategy is expressed as: ", "page_idx": 5}, {"type": "image", "img_path": "kMxdV4Blhn/tmp/cc4d60c8f5bf9c9f219e826916e9a9c90b961550f4596e81607ecfd5285d9d7a.jpg", "img_caption": ["Figure 3: The gradient of weight in each layer using two different networks at 1st iteration. Layer I to III represent 3 SetAbstractions modules in $\\ell_{1}$ -PointNet++ and layer IV to $\\mathrm{v}$ represent fully connected layers. Note that the y-axis is on a logarithmic scale to reflect the magnitude of the values. "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial Y(P_{t},K)}{\\partial K(i,j)}=(1-\\lambda^{k})\\mathrm{sgn}(P_{t}(i,j)-K(i,j))+\\lambda^{k}(P_{t}(i,j)-K(i,j)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This dynamic adjustment introduces a controlled transition in the gradient computation as training progresses. Taking $\\lambda~=~0.99$ for example, when $k$ is small, the term $\\lambda^{\\hat{k}}$ dominates and \u2202\u2202Y K(P(it,,jK)) approximates to Pt(i, j) \u2212K(i, j). This initial configuration aligns with the more efficient $\\ell_{2}$ -like update, providing stability and aiding in faster convergence. As training progresses ( $k$ gets larger), the term $\\lambda^{k}$ becomes more prominent, shifting the gradient computation towards $\\operatorname{sgn}(P_{t}(i,j)\\,-\\,K(i,j))$ . This transition allows the model to leverage the advantages of the $\\ell_{1}$ - PointNet $^{++}$ structure, facilitating sparsity in the learned features. By dynamically adapting the gradient computation based on the training step, the mixed strategy offers a flexible and adaptive approach to overcome the challenges associated with fixed gradient schemes. This dynamic adjustment provides a thoughtful compromise, combining the efficiency of $\\ell_{2}$ -like updates in the initial stages with the sparsity-inducing benefits of $\\ell_{1}$ -PointNet++ in later stages. ", "page_idx": 5}, {"type": "text", "text": "In fact, there is quite a bit of literature supporting the effectiveness of the signSGD update scheme, and in particular, it has been shown that it has some advantages in avoiding saddle points [42]. However, when certain random rotations of the objective appear, signSGD may become trapped in a periodic behavior that hinders convergence in such cases. To address this unexpected behavior, we additionally explored the introduction of momentum into the update rule. Our experimental results prove that this modification effectively breaks the symmetry induced by random rotations, preventing the model from getting stuck and fostering smoother convergence. ", "page_idx": 5}, {"type": "text", "text": "4.2 DLC: Dynamic Learning rate Controller ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Considering the uniqueness of the mixed gradient strategy, we focus on achieving larger update magnitudes and faster convergence rates during the initial stages of training. However, in the later stages, we aim to revert to signSGD, implementing a more cautious update strategy to enhance the model\u2019s precision. Therefore, we propose a learning rate update strategy that adapts to this characteristic: Dynamic Learning rate Controller (DLC), maintaining a higher rate in the early training phase, and returning to a lower rate in the later phase. ", "page_idx": 5}, {"type": "text", "text": "To this end, we design two bound functions to control the learning rate: the lower bound ", "page_idx": 6}, {"type": "text", "text": "and the upper bound ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\alpha_{1}(k)=p_{1}\\cdot(1+\\displaystyle\\frac{p_{2}}{e^{k}})}\\\\ {\\alpha_{2}(k)=p_{1}\\cdot(1+\\displaystyle\\frac{p_{3}}{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p_{1},p_{2}$ and $p_{3}$ are hyper-parameters to be determined and $k$ denotes the training step. And we use simple comparison operations to make learning rate $\\alpha(k)$ locate in $[\\alpha_{1}(k),\\alpha_{2}(k)]$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\alpha}(k)\\gets\\operatorname*{min}\\{\\operatorname*{max}\\{\\alpha_{1}(k),A[\\alpha(k)]\\},\\alpha_{2}(k)\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To enhance the universality of this dynamic control framework, $\\boldsymbol{\\mathcal{A}}$ could be another learning rate optimization algorithm like the adaptive learning rate strategy of [43], which can be specifically switched according to the task at hand. However, regardless of $\\boldsymbol{\\mathcal{A}}$ , we will later demonstrate that dynamic control alone is sufficient to provide theoretical convergence guarantees by the regret argument of Theorem 2, and it also performs well in experiments. ", "page_idx": 6}, {"type": "text", "text": "4.3 Training Framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "It has been noted from previous discussions that the momentum method can help signSGD avoid getting trapped in cycles, thereby improving training stability. Combining the methods above, we present the global optimization algorithm (Optimizer with Mixed gradient strategy and Dynamic learning rate controller, OMD) for $\\ell_{1}$ -PointNet $^{++}$ training. Details are shown in Algorithm. 1 ", "page_idx": 6}, {"type": "text", "text": "Here we give a convergence guarantee for OMD under an online optimization framework, which is harder than offline optimization. We could show that regret $R_{T^{*}}$ of OMD is bounded by $O(\\sqrt{T^{*}})$ . Low regret means the algorithm ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 OMD   \nInput: Initial learning rate $\\alpha$ , hyper-parameters $p_{1},p_{2},p_{3}$ , re  \nferred by Eq. 13 and Eq. 14. $q_{0}$ and $q$ in $(0,1)$ .   \n1: $\\mathbf{m}_{0}=0$ , $\\alpha(0)=\\alpha$ , $\\mathbf{x}_{1}=\\vec{0}$ .   \n2: Set the functions $\\alpha_{1}(k)$ and $\\alpha_{2}(k)$ by hyper-parameters $p_{1},p_{2},p_{3}.$ . 3: for $k=1$ to $T^{*}$ do   \n4: $\\mathbf{g}_{k}\\leftarrow\\,\\frac{\\partial L}{\\partial K}\\big(\\mathbf{x}_{k}\\big)$ # Consider the gradient $\\frac{\\partial L}{\\partial K}$ as an vector here. $\\begin{array}{r}{\\overbrace{\\partial K(i,j)}^{\\partial L}=\\langle{\\frac{\\partial L}{\\partial Y}},{\\frac{\\partial Y}{\\partial K(i,j)}}\\rangle.\\ {\\frac{\\partial L}{\\partial Y}}}\\end{array}$ only depends on the choice of loss function. See Eq. 12 for\u2202 $\\frac{\\partial Y}{\\partial K(i,j)}$ 5: $\\begin{array}{r l}&{q_{k}=q_{0}\\cdot q^{k}.}\\\\ &{\\mathbf{m}_{k}=q_{k}\\cdot\\mathbf{m}_{k-1}+(1-q_{k})\\cdot\\mathbf{g}_{k}}\\\\ &{\\hat{\\alpha}(k)\\gets\\operatorname*{min}\\left\\{\\operatorname*{max}\\{\\alpha_{1}(k),A[\\alpha(k)]\\},\\alpha_{2}(k)\\right\\}}\\\\ &{\\alpha(k)\\gets\\hat{\\alpha}(k)/\\sqrt{k}}\\\\ &{\\mathbf{x}_{k+1}=\\Pi_{\\mathcal{F},\\alpha(k)}\\mathrm{-}1/2\\left(\\mathbf{x}_{k}-\\alpha(k)\\cdot\\mathbf{m}_{k}\\right)}\\end{array}$   \n6: 7:   \n8:   \n9:   \n10: end for ", "page_idx": 6}, {"type": "text", "text": "progressively gets closer to the optimal solution over time. This shows that OMD has reliable convergence properties, making it a dependable optimization method. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Continue with the settings and notations of Algorithm 1. Suppose ${\\mathcal{F}}\\subset\\mathbb{R}^{n}$ is bounded, saying $m a x_{\\mathbf{x},\\mathbf{y}\\in\\mathcal{F}}\\|\\mathbf{x}-\\mathbf{y}\\|_{\\infty}\\leq B_{\\infty}$ Besides, suppose $\\forall k\\in[T^{*}],$ , $\\|\\mathbf{g}_{k}\\|_{2}\\leq B_{2}$ . we could show that for any convex functions {hk}tT =1, ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T^{*}}=\\sum_{k=1}^{T^{*}}h_{k}(\\mathbf{x}_{k})-\\sum_{t=1}^{T^{*}}h_{k}(\\mathbf{x}^{\\star})\\leq C_{1}\\cdot\\sqrt{T^{*}}+C_{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C_{1}$ and $C_{2}$ are constants that rely on $p_{1}$ , $p_{2}$ , $p_{3}$ , $B_{\\infty}$ , $B_{2}$ , $q_{0}$ and $q$ . And $\\mathbf{x}^{\\star}$ \u225c $\\begin{array}{r}{a r g\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{F}}\\sum_{k=1}^{T^{*}}h_{k}(\\mathbf{x})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "The proof could be found in the appendix, Sec. A. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate the generalizability and robustness of the method and thus ensure its effectiveness and broad applicability, we verify the performance of our method in several tasks, ranging from Global Tasks (i.e., Parts Segmentation), Semi-dense Prediction (i.e., scenario semantic segmentation), and Dense Prediction (i.e., pose estimation) tasks. Shapenet, S3DIS, and GarmentNets Simulation are used as the datasets. ", "page_idx": 6}, {"type": "text", "text": "5.1 Dataset and Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset. 1) ShapeNet. In ShapeNet, there are 16,881 shapes from 16 categories, which are annotated with 50 parts in total. Note that most object categories are labeled with two to five parts and Ground Truth annotations are labeled on sampled points on the shapes. This task can be regarded as a point-wise classification task. 2) S3DIS. The Stanford Large-Scale 3D Indoor Spaces Dataset, which encompasses 3D scans obtained from Matterport scanners across 6 distinct areas, comprising a total of 271 rooms. Within the S3DIS dataset, every point within the scans is labeled with a semantic category from a set of 13 distinct classes. These classes encompass various elements such as chairs, tables, floors, walls, among others, in addition to a category for clutter. 3) GarmentNets Simulation. GarmentNets Simulation is a large-scale dataset proposed by [44]. This dataset has six garment categories with a total data volume of 1.72TB. Dress, Jump, Skirt, Top, Pants and Shirt are included. ", "page_idx": 7}, {"type": "text", "text": "Experimental Settings. We train our frameworks using CrossEntropy loss and the AdamW optimizer [45], with an initial learning rate of 0.001, a weight decay of $10^{\\bar{-4}}$ , Cosine Decay, and a batch size of 32. The total training consists of 200 epochs. All tasks use the same settings unless otherwise specified. All experiments are conducted on a computer workstation with three GeForce GTX 3090 GPUs using the PyTorch deep learning framework. The best model on the validation set is selected for testing. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments on Global Task ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Parts Segmentation. As a classic global task, 3D object parts segmentation is an important predecessor for articulated objects from the embodied intelligence community, such as pose estimation [46, 47], manipulation [48, 49], etc. In this section, we conduct experiments on ShapeNet part dataset [50]. ", "page_idx": 7}, {"type": "table", "img_path": "kMxdV4Blhn/tmp/2510181fcfc9cbae1a608965de81df3a57537eb8641310bdaab31d7f4bac2f36.jpg", "table_caption": ["Table 2: Quantitative segmentation results on ShapeNet part dataset. Note that a 3D fully convolutional network is proposed as the 3DCNN, mIoU $\\%)$ is reported as the metric on points. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "From Tab. 2, see them all: we find that our method has almost equivalent performance to the conventional method when being equipped with PointNet, and achieves superior performance on 3DCNN and PointNet++. Treat them equally: we see that our method can often perform better in some categories (e.g., car, motor, rocket, etc.), these categories usually have a larger volume (i.e., a more sparsified point cloud) compared to other objects. We propose that the inner product within convolutional networks has a tendency to highlight local context among points, yet it is greatly affected by the overall translation and scaling of the dataset. Our method focuses on the points drawn from $\\ell_{1}$ -norm space and addresses this problem by integrating the inherent distance measure into our architecture. Specifically speaking, The Manhattan distance based $\\ell_{1}$ -norm Nets tend to avoid this problem, which notices point cloud features at a longer distance. ", "page_idx": 7}, {"type": "text", "text": "5.3 Experiments on Semi-dense Prediction Task ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Scenario Semantic Segmentation. As a semi-dense prediction task, this task aims to segment distinct regions within a 3D scene based on their semantic meaning using point cloud data. Semantic scene segmentation is crucial for understanding and interpreting the spatial arrangement and relationships between objects in 3D scenes. For our study, we utilize the S3DIS dataset. The metrics and experimental settings follow those outlined in [28]. ", "page_idx": 7}, {"type": "text", "text": "Following the training and test strategies used in [51], we first divide the point cloud using the room as the basic unit and then sample the room at a size of $1\\mathrm{m}*1\\mathrm{m}$ (randomly sampling up to 4096 ", "page_idx": 7}, {"type": "text", "text": "Table 3: (Left) Results on Semantic Segmentation in Scenes. Metric is average IoU $(\\%)$ over 13 classes (structural and furniture elements plus clutter) and classification accuracy is calculated on points. Our methods achieved competitive performance with significant energy reductions $(\\mathbf{61\\%})$ . ", "page_idx": 8}, {"type": "table", "img_path": "kMxdV4Blhn/tmp/39ff11bd16afa663132acd14f40a2f6e1be2e15e438df2aa0e90984384043bf7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "points during training, and all points are involved in the computation during testing), which in turn predicts the class of each point in each block. Note that we use a 9-dimensional vector to represent each point, representing XYZ, RGB, and normalized room location (ranging from 0 to 1). K-fold strategy is also used for training and testing. ", "page_idx": 8}, {"type": "text", "text": "The quantitative results are reported in Tab. 3. Experimental results show that although our approach achieves almost equivalent performance to inner product based networks, we maximize the potential of $\\ell_{1}$ -norm measure by relying on our proposed optimization strategy, which allows us to achieve similar performance but with less computational complexity and lower energy consumption (Almost ${\\bf61\\%}$ energy reductions). Also, we provide qualitative segmentation results for visualization in Fig. 4. Overall, our model generates consistent object predictions and is resilient to the presence of absent points and obstructions. ", "page_idx": 8}, {"type": "text", "text": "5.4 Experiments of Dense Prediction Task ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Garment Pose Estimation Garments, vital in daily life, present unique challenges for machine perception and interaction due to properties like infinite degrees of freedom and thin structure. Garment pose estimation and tracking systems hold potential for applications in mixed reality [52, 53], augmented real", "page_idx": 8}, {"type": "text", "text": "Table 4: Quantitative Results on Garment Pose Estimation. The metric is measured using Chamfer distance (cm) under the canonical pose. The lower is the better result. ", "page_idx": 8}, {"type": "table", "img_path": "kMxdV4Blhn/tmp/ab1476711c1bf2467a7d780a547c89464c8914635bfdf5476a91d26d5e138fb2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ity [54, 52], and robotic manipulation [55, 49]. Addressing these challenges, mainstream methods typically employ Normalized Object Coordinate Space (NOCS) [56] for dense prediction tasks. In this section, we introduce GarmentNets [44], a baseline focusing on garment pose estimation using partial point clouds as input and generating complete point clouds as output. Our approach utilizes the GarmentNets Simulation Dataset to evaluate this task. The total epoch number is 200, and the batch size is 16. ", "page_idx": 8}, {"type": "text", "text": "Quantitative results are in Tab. 4. Note that we use Symmetric Chamfer Distance as the metric, This metric measures accuracy and completeness for surface reconstruction. The accuracy metric is defined as the mean L2 distance of points on the output mesh to their nearest neighbors on the GT mesh. From the table, it can be seen that our method performs comparably to the original method. ", "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Replacing Means. The most critical structure of PointNet $^{++}$ is the 3 separate SetAbstractions modules (SA). Hence, to explore the effect of using the $\\ell_{1}$ -PointNet $^{++}$ at different places and in different ratios, we remove the modules at different ratios and places on S3DIS. The experimental result is shown in Tab. 5. In many aspects, we can infer that the average mean IOU and accuracy are higher under the $66.7\\%$ ratio than those reported under the $33.3\\%$ ratio. This result tells the conclusion that our $\\ell_{1}$ -norm measure can exact more useful features from sparse point clouds. we hope these results can prompt further study on replacing means, such as different replacing ratios in each inner module, creditable ways to combine hybrid convolutional blocks. etc. We leave this for more passionate researchers in the future. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Table 5: Comparisons of Results on S3DIS with Different Replacing Ratio and Places. We estimate the energy costs according to [11], i.e., one operation of floating-point addition and multiplication have energy costs of $0.9\\,p J$ and $3.7\\;p J$ , respectively. SA: SetAbstractions module. $\\star$ means that $33.3\\%$ replacing ratio of $\\mathrm{PointNet++}$ has #Add- $0.492\\,\\mathrm{M}$ , #Mul-0.984 M, Energy- $4.0836\\ \\mu J$ , while $\\clubsuit$ means that $66.7\\%$ replacing ratio of PointNet $^{++}$ has #Add-0.984 M, #Mul0.492 M, Energy- $.2.7060\\;\\mu J$ . ", "page_idx": 9}, {"type": "table", "img_path": "kMxdV4Blhn/tmp/960c800d4f54d27cd0f23d5f9daadac0dae0d68d277b084c63f4cf11aebfd591.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 6: Ablation Results on S3DIS Dataset Using Different Variants of $\\ell_{1}$ -Nets. Mean IOU and overall Accuracy $(\\%)$ are reported. Note that the results of $\\ell_{1}$ -PointNet are reported from I to IV, and $\\ell_{1}$ -PointNet++ are reported from $\\mathrm{v}$ to VIII. Besides, vanilla Net represents the model without our customized optimization strategy while training. ", "page_idx": 9}, {"type": "table", "img_path": "kMxdV4Blhn/tmp/f764057b0dbd5df2fd7ce54e961ebbfba8b8b03bcc8e732640b8907da9d4967c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Optimization Strategy. As demonstrated in Sec. 4, we propose mixed gradient strategy (MGS) to accelerate network convergence, while dynamic learning rate controller (DLC) helps our network move away from local optima. To evaluate the effectiveness of MGS and DLC, we remove them separately from $\\ell_{1}$ -PointNet $^{++}$ and evaluate the scenario semantic segmentation performance on S3DIS. Tab. 6 presents the quantitative results. The baselines (I and V) indicate that we only use $\\ell_{1}$ -norm as the similarity measurement but without any optimization. It can be observed that they both resulted in huge performance degradation. Besides, we can see that both our MGS and DLC contribute to network convergence and optimization results. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Firstly, some of the other convolutions (e.g., sparse convolution, group convolution, dilated convolution) and additional computer vision tasks remain unexplored. Secondly, the inference speed of the $\\ell_{1}$ -norm Net is marginally slower than that of traditional one. This is attributed to the lack of CUDA and cuDNN optimized operations for Manhattan distance metrics. It\u2019s noteworthy that, beyond introducing a novel convolution based on the $\\ell_{p}$ -norm and proving the universal approximation theorem for theoretical support, this paper also presents customized optimization strategies. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we are motivated to explore $\\ell_{p}$ -norm measure to replace the classic inner product convolution. we first prove the universal approximation of $\\ell_{p}$ -norm Nets. And then we compare different $\\ell_{p}$ -norm measures and propose the $\\ell_{1}$ -norm Net for 3D point cloud tasks. Furthermore, we design the customized optimization strategies (i.e., mixed gradient strategy and dynamic control on learning rate) for $\\ell_{1}$ -norm Net. When introducing our method to classical 3D networks, they achieve competitive performances at a lower energy cost. In summary, our $\\ell_{1}$ -norm Net can achieve similar performance to traditional convolution network, but with less computational cost and lower instruction latency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by National Natural Science Foundation of China under Grant 62302143 and Anhui Provincial Natural Science Foundation under Grant 2308085QF207. Thanks for the help of Xinyuan Song. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Min Zhang, Yifan Wang, Pranav Kadam, Shan Liu, and C-C Jay Kuo. Pointhop $^{++}$ : A lightweight learning model on point sets for 3d classification. In 2020 IEEE International Conference on Image Processing (ICIP), pages 3319\u20133323. IEEE, 2020.   \n[2] Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, and Yang Xiao. P2b: Point-to-box network for 3d object tracking in point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6329\u20136338, 2020. [3] Angela Dai and Matthias Nie\u00dfner. 3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 452\u2013468, 2018.   \n[4] Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, and Cewu Lu. Gamma: Generalizable articulation modeling and manipulation for articulated objects. arXiv preprint arXiv:2309.16264, 2023.   \n[5] Haoyu Xiong, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, and Cewu Lu. Robotube: Learning household manipulation from human videos with simulated twin environments. In Conference on Robot Learning, pages 1\u201310. PMLR, 2023.   \n[6] Fan Zhang and Yiannis Demiris. Learning garment manipulation policies toward robot-assisted dressing. Science robotics, 7(65):eabm6010, 2022.   \n[7] Kailin Li, Lixin Yang, Haoyu Zhen, Zenan Lin, Xinyu Zhan, Licheng Zhong, Jian Xu, Kejian Wu, and Cewu Lu. Chord: Category-level hand-held object reconstruction via shape deformation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9444\u2013 9454, 2023.   \n[8] Marius Kloft, Ulf Brefeld, S\u00f6ren Sonnenburg, and Alexander Zien. Lp-norm multiple kernel learning. The Journal of Machine Learning Research, 12:953\u2013997, 2011.   \n[9] Qingsong Gu and Po-Lam Yung. A new formula for the lp norm. Journal of Functional Analysis, 281(4):109075, 2021.   \n[10] James A Cadzow. Minimum $\\ell_{1}$ , $\\ell_{2}$ , and $\\ell_{\\infty}$ norm approximate solutions to an overdetermined system of linear equations. Digital Signal Processing, 12(4):524\u2013560, 2002.   \n[11] William Dally. High-performance hardware for machine learning. Nips Tutorial, 2:3, 2015.   \n[12] Instruction latencies of different operations. www.agner.org/optimize/ instruction_tables.pdf.   \n[13] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.   \n[15] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251\u20131258, 2017.   \n[16] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015.   \n[17] Dren Gashi, Mike Pereira, and Valeriia Vterkovska. Multi-scale context aggregation by dilated convolutions machine learning-project. 2017.   \n[18] Xin Wang, Rongrong Lv, Yang Zhao, Tangwen Yang, and Qiuqi Ruan. Multi-scale context aggregation network with attention-guided for crowd counting. In 2020 15th IEEE International Conference on Signal Processing (ICSP), volume 1, pages 240\u2013245. IEEE, 2020.   \n[19] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 764\u2013773, 2017.   \n[20] Feng Chen, Fei Wu, Jing Xu, Guangwei Gao, Qi Ge, and Xiao-Yuan Jing. Adaptive deformable convolutional network. Neurocomputing, 453:853\u2013864, 2021.   \n[21] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301\u2013320, 2005.   \n[22] Julianne Chung and Silvia Gazzola. Flexible krylov methods for \\ ell_p regularization. SIAM Journal on Scientific Computing, 41(5):S149\u2013S171, 2019.   \n[23] Martine Labb\u00e9, Justo Puerto, and Mois\u00e9s Rodr\u00edguez-Madrena. Shortest paths and location problems in a continuous framework with different \\ ell $\\boldsymbol{p}$ -norms on different regions. arXiv preprint arXiv:2110.07866, 2021.   \n[24] Jinglai Shen and Seyedahmad Mousavi. Least sparsity of p-norm based optimization problems with p>1. SIAM Journal on Optimization, 28(3):2721\u20132751, 2018.   \n[25] CS Sastry and Ashish Mishra. Application of l1-norm minimization technique to image retrieval. World Academy of Science, Engineering and Technology, 56(145):801\u2013804, 2009.   \n[26] Twan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.   \n[27] Stephen Boyd and Venkataramanan Balakrishnan. A regularity result for the singular values of a transfer matrix and a quadratically convergent algorithm for computing its $\\ell_{\\infty}$ -norm. Systems & Control Letters, 15(1):1\u20137, 1990.   \n[28] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet $^{++}$ : Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.   \n[29] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359\u2013366, 1989.   \n[30] Non-Polynomial Activation Functions Can. Multilayer feedforward networks with nonpolynomial activation functions can approximate any function\u2013_-. 1991.   \n[31] Anton Maximilian Sch\u00e4fer and Hans Georg Zimmermann. Recurrent neural networks are universal approximators. In Artificial Neural Networks\u2013ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16, pages 632\u2013640. Springer, 2006.   \n[32] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.   \n[33] Mark Horowitz. 1.1 computing\u2019s energy problem (and what we can do about it). In 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC), pages 10\u201314. IEEE, 2014.   \n[34] Noam Brown and Tuomas Sandholm. Regret transfer and parameter optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.   \n[35] Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization in non-stationary environments. Advances in Neural Information Processing Systems, 33:6743\u20136754, 2020.   \n[36] Zi Wang, Beomjoon Kim, and Leslie P Kaelbling. Regret bounds for meta bayesian optimization with an unknown gaussian process prior. Advances in Neural Information Processing Systems, 31, 2018.   \n[37] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[38] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2):107\u2013194, 2012.   \n[39] S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends\u00ae in Machine Learning, 5(1):1\u2013122, 2012.   \n[40] Elad Hazan. 10 the convex optimization approach to regret minimization. Optimization for machine learning, page 287, 2012.   \n[41] Jeremy Bernstein, Kamyar Azizzadenesheli, Yu-Xiang Wang, and Anima Anandkumar. Convergence rate of sign stochastic gradient descent for non-convex functions. 2018.   \n[42] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. Advances in Neural Information Processing Systems, 35:9955\u20139968, 2022.   \n[43] Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. Addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1468\u20131477, 2020.   \n[44] Cheng Chi and Shuran Song. Garmentnets: Category-level pose estimation for garments via canonical space shape completion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3324\u20133333, 2021.   \n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. 2019.   \n[46] Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu Lu. Toward real-world category-level articulation pose estimation. IEEE Transactions on Image Processing, 31:1072\u20131083, 2022.   \n[47] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, and Shuran Song. Categorylevel articulated object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3706\u20133715, 2020.   \n[48] Jianren Wang, Sudeep Dasari, Mohan Kumar Srirama, Shubham Tulsiani, and Abhinav Gupta. Manipulate by seeing: Creating manipulation controllers from pre-trained representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3859\u20133868, 2023.   \n[49] Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis Aloimonos. Robot learning manipulation action plans by\" watching\" unconstrained videos from the world wide web. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015.   \n[50] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):1\u201312, 2016.   \n[51] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652\u2013660, 2017.   \n[52] Julie Carmigniani and Borko Furht. Augmented reality: an overview. Handbook of augmented reality, pages 3\u201346, 2011.   \n[53] Dhiraj Amin and Sharvari Govilkar. Comparative study of augmented reality sdks. International Journal on Computational Science & Applications, 5(1):11\u201326, 2015.   \n[54] Ronald T Azuma. A survey of augmented reality. Presence: teleoperators & virtual environments, 6(4):355\u2013385, 1997.   \n[55] Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, and Shuran Song. Cloth funnels: Canonicalized-alignment for multi-purpose garment manipulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 5872\u20135879. IEEE, 2023.   \n[56] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642\u20132651, 2019.   \n[57] Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks. Neural computation, 3(2):246\u2013257, 1991.   \n[58] H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. arXiv preprint arXiv:1002.4908, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "\u2013 Appendix \u2013 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the Appendix, we present additional information on our methods. Concretely, we provides a detailed theoretical analysis of the theorems from the main paper, including the variance analysis, the proposed universal approximation, the regret argument, and the equivalence of $\\ell_{2}$ -norm Measure. ", "page_idx": 14}, {"type": "text", "text": "A Additional Theoretical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Omitted proof of variance analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Since adding a constant does not significantly affect variance, for ease of demonstration, we could assume $V a\\overline{{r}}\\big[\\|G+P_{t}-K\\|_{2}\\big]\\approx\\bar{V}a r\\big[\\|G\\|_{2}\\big]$ . Notice that ", "page_idx": 14}, {"type": "equation", "text": "$$\nV a r\\big[\\|G\\|_{2}\\big]=\\mathbb{E}_{G\\sim N(0,\\mathbb{I}_{m})}\\big[\\|G\\|_{2}^{2}\\big]-\\bigg(\\mathbb{E}_{G\\sim N(0,\\mathbb{I}_{m})}\\big[\\|G\\|_{2}\\big]\\bigg)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It\u2019s easy to verify that for any $u\\geq0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{u}\\geq(1+u-(u-1)^{2})/2.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let u = \u2225G\u222522 and calculate the expectation of $G$ on both sides of the inequality, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[\\|G\\|_{2}]}{\\sqrt{m}}\\geq\\frac{1}{2}\\cdot\\big(2-\\mathbb{E}\\Big[\\big(\\frac{\\|G\\|_{2}^{2}}{m}-1)^{2}\\Big]\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Because $\\begin{array}{r}{\\mathbb{E}\\big[(\\frac{\\|G\\|_{2}^{2}}{m}-1)^{2}\\big]=\\frac{1}{m^{2}}\\cdot\\mathbb{E}\\big[\\sum_{i=1}^{m}(G(i)^{2}-1)^{2}+\\sum_{i\\neq j}(G(i)^{2}-1)(G(j)^{2}-1)\\big]}\\end{array}$ and $\\forall i$ , $\\mathbb{E}[G(i)^{2}-1]=0$ , we can conclude that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[(\\frac{\\|G\\|_{2}^{2}}{m}-1)^{2}\\big]=\\frac{1}{m}\\cdot\\mathbb{E}[G(1)^{4}+1-2\\cdot G(1)^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{2}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first equation holds for all the $G(i)\\mathbf{s}$ are i.i.d. The second equation holds for $\\mathbb{E}[G(1)^{4}]=3$ , $\\mathbb{E}[G(1)^{2}]=1$ . Combining inequality (17) and Equation (18b), $\\begin{array}{r}{\\mathbb{E}_{G\\sim\\mathcal{N}(0,\\mathbb{I}_{m})}\\left[\\|G\\|_{2}\\right]\\geq\\frac{\\sqrt{m}}{2}\\cdot\\left(2-\\frac{2}{m}\\right)}\\end{array}$ . Therefore, by Equation (16) we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nV a r[\\|G\\|_{2}]<2-\\frac{1}{m}=O(1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we have shown that $V a r\\big[\\|(G+P_{t})-K\\|_{2}\\big]=O(1).$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Scaling S and J by diam1(J) where $d i a m(J)\\,=\\,\\mathrm{max}_{x,y\\in J}\\{\\|x\\mathrm{~-~}y\\|_{\\infty}\\}$ , we could assume $S\\,=$ $\\{x_{1},\\cdot\\cdot\\cdot,x_{N}\\}$ and $\\forall i\\in[N]\\;x_{i}\\in J\\subset[0,1]^{k}$ . For convenience, first we show the case $k=1$ . Here we refer the construction of soft occupancy function in [51]. Because $f$ is continuous function, for any $\\epsilon\\,>\\,0,\\,\\exists\\;\\sigma\\,>\\,0$ so that $|f(S_{1})-f(S_{2})|\\,<\\,\\epsilon$ for any $S_{1}$ and $S_{2}$ with $d_{H}(S_{1},S_{2})\\,<\\,\\delta$ . Let $\\begin{array}{r}{M=\\left\\lceil\\frac{1}{\\delta}\\right\\rceil}\\end{array}$ and $h_{m}(x)_{.}=e x p(-d_{H}(x,[\\frac{m-1}{M},\\frac{m}{M}]))$ ) be the soft occupancy function, for all $m\\in[M]$ . Next, for all $m\\in[M]$ define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{v}_{m}(S)=\\operatorname*{max}_{x\\in S}\\{h_{m}(x)\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and, ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{m}(S)=\\left\\{\\begin{array}{l l}{1,}&{\\hat{v}_{m}(S)\\geq1}\\\\ {0,}&{\\hat{v}_{m}(S)<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$v_{m}(S)$ indicates the occupancy of the $m$ -th interval by points in $S$ . Define $\\mathbf{v}:2^{J}\\,\\rightarrow\\,\\{0,1\\}^{M}$ $S\\,\\in\\,2^{J}$ $\\mathbf{v}({\\bar{S}})=\\mathbf{\\sigma}^{\\prime}(v_{1}(S),v_{2}(S),\\cdots,v_{M}({\\bar{S}}))$ r.u cAtinodn , $\\eta:\\{0,1\\}^{M^{\\star}}\\to2^{J}$ , $\\begin{array}{r}{\\eta(\\mathbf{v}(S))=\\{\\frac{m-1}{M}\\mid v_{m}(\\dot{S})\\geq1\\}}\\end{array}$ $\\begin{array}{r}{d_{H}\\left(\\eta(\\mathbf{v}(S)),S\\right)<\\frac{\\mathrm{~\\hat{1}~}}{M}\\leq\\delta}\\end{array}$ let $\\omega:\\{0,1\\}^{M}\\to\\mathbb{R}$ and $\\omega({\\mathbf v})=f(\\eta({\\mathbf v}))$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\omega(\\mathbf{v}(S))-f(S)|=|f(\\eta(\\mathbf{v}(S)))-f(S)|<\\epsilon\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last inequality holds for the definition of Hausdorff distance and continuity of $f$ . Here $\\omega$ and $\\lbrace h_{m}\\rbrace_{m=1}^{M}$ could be made up of a multi-layer perceptron network [51]. $\\{\\hat{v}_{m}\\}_{m=1}^{M}$ consist of a $\\{\\hat{v}_{m}\\}_{m=1}^{M}$ ,n gw hliacyhe rc oonm $\\hat{v}_{m}(S)$ n da hcea ng ebnee rcaol mcpasoesse $k\\geq1$ ,s iitm spulfe fpiceersc teop tgreotn  tlhaey sera mone conclusion by simply extending the 1 dimensional functions , $\\hat{v}_{m}$ , $v_{m}$ to dimension. So there is a $\\ell_{p}$ -PointNet $-+\\mathcal{P}$ that can approximate any continuous function $f$ on $2^{J}$ . ", "page_idx": 15}, {"type": "text", "text": "We employ the RBF theory of [57] to give the second conclusion. For completeness, we restate it here. ", "page_idx": 15}, {"type": "text", "text": "Theorem 3 ([57]). The radio basis networks consist of a family of functions $^{/}R B F)$ noted by $S_{K}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{H}a_{i}\\cdot K({\\frac{x-z_{i}}{\\sigma}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\in\\mathbb{R}^{d},\\,z_{i}\\in\\mathbb{R}^{d},\\sigma\\in\\mathbb{R},\\,H\\in\\mathcal{N}.\\ S_{K}$ is dense in $\\ell_{1}(\\mathbb{R}^{d})$ , if $K$ satisfies: 1.integrable bounded, $2.K$ is continuous almost everywhere, $3.\\textstyle\\int K(x)d x\\neq0$ . ", "page_idx": 15}, {"type": "text", "text": "It\u2019s clear that the $\\ell_{p}$ -norm $\\parallel\\cdot\\parallel_{p}:\\mathbb{R}^{d}\\to\\mathbb{R}$ satisfies all the three conditions on $K$ . Besides, a large enough $\\ell_{p}$ based convolution layer with a full connected layer could represent all the functions $\\textstyle\\sum_{i=1}^{H}a_{i}\\cdot\\|{\\big(}{\\frac{x-z_{i}}{\\sigma}}{\\big)}\\|_{p}$ . So for any $\\ell_{1}$ -integrable function $g$ , there exists an $\\ell_{p}{\\mathrm{-PointNet}}+\\mathcal{P}^{\\prime}$ such that for any $\\epsilon>0$ $0,\\int|g(x)-\\mathcal{P}^{\\prime}(x)|d x<\\epsilon.$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before proving, we restate an important result in online learning and we will use it in the following. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1 ([58]). For any $\\textit{Q}\\in\\textit{S}_{+}^{d}$ and convex feasible set $\\mathcal{F}\\subset\\mathbb{\\mathbb{R}}^{d}$ , suppose $\\begin{array}{r l}{u_{1}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathcal{F}}\\|Q^{1/2}(x-z_{1})\\|}\\end{array}$ and $\\begin{array}{r}{u_{2}\\,=\\,\\operatorname*{min}_{x\\in\\mathcal{F}}\\|Q^{1/2}(x-z_{2})\\|}\\end{array}$ then we have $\\|Q^{1/2}(u_{1}\\mathrm{~-~}u_{2})\\|\\leq$ $\\|Q^{1/2}(z_{1}-z_{2})\\|$ . ", "page_idx": 15}, {"type": "text", "text": "Our proof framework is similar to that of [58]. Here is a standard argument in momnet method. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Suppose $m_{t}=\\gamma m_{t-1}+(1-\\gamma)g_{t}$ with $m_{0}={\\bf0}$ and $0<\\gamma<1$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T^{*}}\\|m_{t}\\|^{2}\\leq\\sum_{t=1}^{T^{*}}\\|g_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By Cauchy-Schwarz and Young\u2019s inequality, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|m_{t}\\|^{2}\\leq\\gamma\\|m_{t-1}\\|^{2}+(1-\\gamma)\\|g_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that $m_{0}=\\mathbf{0}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\lVert m_{t}\\rVert^{2}}{\\gamma^{t}}\\leq\\left(1-\\gamma\\right)\\sum_{i=1}^{t}\\lVert g_{i}\\rVert^{2}\\gamma^{-i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|m_{t}\\|^{2}\\leq(1-\\gamma)\\sum_{i=1}^{t}\\|g_{i}\\|^{2}\\gamma^{t-i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Take the summation on $t$ for both sides of the inequality, we have the conclusion. ", "page_idx": 15}, {"type": "text", "text": "So we could begin to prove Theorem 2. Suppose $\\{x_{t}\\}\\,\\subset\\,\\mathbb{R}^{n}$ . As the notation before, $x^{\\star}=$ $\\begin{array}{r}{a r g\\operatorname*{min}_{x\\in\\mathcal{F}}\\sum_{t=1}^{\\bar{T^{*}}}h_{t}(\\bar{x})}\\end{array}$ and $x_{t+1}=\\Pi_{\\mathcal{F},\\alpha_{\\star}^{-1/2}}\\big(x_{t}-\\alpha(t)\\cdot m_{t}\\big)=\\operatorname*{min}_{x\\in\\mathcal{F}}\\|\\alpha(t)^{-1/2}\\cdot(x-\\big(x_{t}-x_{t}\\big))\\|_{L^{2}}.$ $\\alpha(t)\\cdot m_{t})\\vert|$ . By Lemma 1, $\\|\\alpha(t)^{-1/2}\\cdot(x_{t+1}-x^{\\star})\\|^{2}\\leq\\|\\alpha(t)^{-1/2}$ \u00b7 (xt \u2212\u03b1(t) \u00b7 mt \u2212x\u22c6)\u22252 = $\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}+\\|\\alpha(t)^{1/2}\\cdot m_{t}\\|^{2}-2\\langle q_{t}m_{t-1}+(1-q_{t})g_{t},x_{t}-x^{\\star}\\rangle$ . Rearrange the inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{t},x_{t}-x^{\\star}\\rangle\\leq\\displaystyle\\frac{1}{2(1-q_{t})}\\bigg[\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}-\\|\\alpha(t)^{-1/2}\\cdot(x_{t+1}-x^{\\star})\\|^{2}+\\|\\alpha(t)^{1/2}\\cdot m_{t}\\|^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{q_{t}}{2(1-q_{t})}\\cdot\\bigg(\\|\\alpha(t)^{1/2}\\cdot m_{t-1}\\|^{2}+\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}\\bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second inequality holds for Cauchy-Schwarz inequality and for any a, b \u2208R, ab \u2264a22+b2. ", "page_idx": 16}, {"type": "text", "text": "Because $\\{h_{t}\\}_{t=1}^{T^{*}}$ are convex functions: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{T^{*}}=\\sum_{t=1}^{T^{*}}h_{t}\\left(x_{t}\\right)-h_{t}\\left(x^{\\star}\\right)\\leq\\sum_{t=1}^{T^{*}}\\left\\langle g_{t},x_{t}-x^{\\star}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sqrt{\\displaystyle\\left[\\frac{1}{2(1-q_{t})}\\biggl[\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}-\\|\\alpha(t)^{-1/2}\\cdot(x_{t+1}-x^{\\star})\\|^{2}\\biggr]+\\frac{q_{t}}{2(1-q_{t})}\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}\\right]}+\\frac{q_{t}}{2(1-q_{t})}\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}}}{A}}\\\\ &{\\frac{\\sqrt{\\displaystyle\\left[\\frac{1}{2(1-q_{t})}\\|\\alpha(t)^{1/2}\\cdot m_{t}\\|^{2}+\\frac{q_{t}}{2(1-q_{t})}\\|\\alpha(t)^{1/2}\\cdot m_{t-1}\\|^{2}\\right]}}{B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "First we bound the part A: ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{T^{*}}\\left[\\frac{1}{2(1-q_{t})}\\left[\\|\\alpha(t)^{-1/2}\\cdot(x_{t}-x^{\\star})\\|^{2}-\\|\\alpha(t)^{-1/2}\\cdot(x_{t+1}-x^{\\star})\\|^{2}\\right]+\\frac{q_{t}}{2(1-q_{t})}\\|\\alpha(t)^{-1/2}\\cdot(x_{t+1}-x^{\\star})\\|^{2}\\right]}\\\\ {\\displaystyle\\le\\frac{1}{2(1-q_{1})}\\left[\\sum_{i=1}^{n}\\alpha_{1}^{-1}(x_{1}(i)-x^{\\star}(i))^{2}+\\displaystyle\\sum_{t=2}^{T^{*}}\\sum_{i=1}^{n}(\\alpha_{t}^{-1}-\\alpha(t-1)^{-1})(x_{t}(i)-x^{\\star}(i))^{2}+\\displaystyle\\sum_{t=1}^{T^{*}}\\sum_{i=1}^{n}q_{t+1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next we bound the part B. By definition of $\\alpha(t),\\alpha_{1}(1)/\\sqrt{t}\\leq\\alpha(t)\\leq\\alpha_{2}(1)/\\sqrt{t}$ . So we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{r}\\left[\\cfrac{1}{2(1-q)}\\cfrac{\\|\\alpha(t)^{1/2}\\cdot m_{t}\\|^{2}}{r}+\\cfrac{q_{t}}{2(1-q)}\\cfrac{q_{t}}{|\\alpha(t)^{1/2}\\cdot m_{t-1}|^{2}}\\right]}\\\\ &{\\leq\\frac{\\alpha_{2}(1)}{2(1-q)}\\left[\\cfrac{\\Gamma_{\\infty}^{\\prime}}{\\Gamma_{\\infty}}\\;\\|\\frac{m_{t}}{\\sqrt{t}}\\right]^{2}+\\frac{\\Gamma_{\\infty}^{\\prime}}{\\cfrac{1}{m}}\\frac{\\|m_{t-1}\\|^{2}}{\\sqrt{t}}}\\\\ &{\\leq\\frac{\\alpha_{2}(1)}{2(1-q)}\\left[\\frac{1}{\\mathcal{T}}\\left[\\cfrac{\\Gamma_{\\infty}^{\\prime}}{\\Gamma_{\\infty}}\\|m_{t}\\|^{2}\\right]^{2}+\\frac{1}{T}\\left[\\cfrac{\\Gamma_{\\infty}^{\\prime}}{\\Gamma_{\\infty}}\\|m_{t-1}\\|^{2-1}\\right]^{2}\\right]}\\\\ &{\\leq\\frac{\\alpha_{2}(1)}{2(1-q)}\\left[\\cfrac{1}{T}\\cfrac{T^{\\prime}}{\\Gamma_{\\infty}}\\|m_{t}\\|^{2}\\cdot\\sum_{t=1}^{r}\\|^{2}+\\cfrac{1}{T}\\sum_{t=1}^{r}\\|m_{t-1}\\|^{2}\\cdot\\sum_{t=1}^{r}\\|^{2-1}\\right]^{2}}\\\\ &{\\leq\\frac{\\alpha_{2}(1)B_{2}^{2}}{(1-q)}\\sum_{t=1}^{r}t^{-1/2}}\\\\ &{\\leq(2\\sqrt{T^{*}-1})\\frac{\\alpha_{2}(1)B_{2}^{2}}{(1-q)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second inequality holds for Jensen inequality and the third inequality follows from CauchySchwarz inequality. The forth inequalityholds for Lemma 2. ", "page_idx": 17}, {"type": "text", "text": "Combine the argument above and notice that $\\alpha(t)^{-1}\\leq p_{1}^{-1}\\cdot\\sqrt{T^{*}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{T^{*}}\\leq\\frac{B_{\\infty}^{2}}{2(1-q_{1})}\\left[n\\cdot\\alpha_{1}^{-1}+\\sum_{t=2}^{T^{*}}n\\cdot(\\alpha_{t}^{-1}-\\alpha_{t-1}^{-1})+\\sum_{t=1}^{T^{*}}n\\cdot q_{t}\\cdot\\alpha_{t}^{-1}\\right]+(2\\sqrt{T^{*}}-1)\\frac{\\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}}}\\\\ {{\\displaystyle\\leq\\sqrt{T^{*}}\\cdot\\left(\\frac{B_{\\infty}^{2}}{2(1-q_{1})}\\cdot n\\cdot\\hat{\\alpha}_{T^{*}}^{-1}+\\frac{2\\cdot\\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}\\right)-\\frac{\\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}+\\frac{B_{\\infty}^{2}}{2(1-q_{1})}\\sum_{t=1}^{T^{*}}n\\cdot q_{t}\\cdot\\alpha_{t}^{-1}}}\\\\ {{\\displaystyle\\leq\\sqrt{T^{*}}\\cdot\\left(\\frac{B_{\\infty}^{2}\\cdot n\\cdot p_{1}^{-1}}{2(1-q_{1})}\\cdot(1+2q_{0}q)+\\frac{2\\cdot\\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}\\right)-\\frac{\\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Equivalence of $\\ell_{2}$ -norm Measure and Classic Convolution in Convergence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We find that $\\ell_{2}$ -norm Net is the linear transformation to the inner product convolution network, here we give the detailed calculation. ", "page_idx": 17}, {"type": "text", "text": "The output of the $\\ell_{2}$ -norm Net in Eq. 25. ", "page_idx": 17}, {"type": "equation", "text": "$$\nY_{\\ell_{2}}(P_{t},K)=\\sqrt{\\sum_{t\\geq1}\\sum_{i,j}|P_{t}(i,j)-K(i,j)|^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we can express it as the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{Y_{\\ell_{2}}^{2}(P_{t},K)=\\sum_{t\\ge1}\\sum_{i,j}(P_{t}(i,j)^{2}+K(i,j)^{2}-2P_{t}(i,j)K(i,j))}}\\\\ &{=\\displaystyle\\sum_{t\\ge1}\\sum_{i,j}(P_{t}(i,j)^{2}+K(i,j)^{2})-\\displaystyle\\sum_{t\\ge1}\\sum_{i,j}P_{t}(i,j)K(i,j)}\\\\ &{=\\displaystyle\\sum_{t\\ge1}\\sum_{i,j}(P_{t}(i,j)^{2}+K(i,j)^{2})-2Y_{C N N}(P_{t},K).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notably, the term $\\textstyle\\sum_{i,j}K(i,j)^{2}$ remains constant for each channel, and $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{i,j}P_{t}(i,j)^{2}}\\end{array}$ represents the square of $\\ell_{2}$ -norm of each input patch. If this term is invariant across patches, the $\\ell_{2}$ -norm Net\u2019s output can be regarded as a linear transformation of the CNNs\u2019 output. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: see the abstract and introduction part. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: see Sec. 6. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: see Sec.A. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Instructions about experimental settings are in Sec. 5, and the URL of the project will be released after the paper is accepted. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: see the zip files of codes in the supplemental material. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: see the main manuscript. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: the effect of random seed could almost be negligible since we set the same initiation seed during experiments. Reproducibility can be guaranteed. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: see the main manuscript. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: we have conformed with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: see Sec. 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This work poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: see the main manuscript. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]