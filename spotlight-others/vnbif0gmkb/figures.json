[{"figure_path": "VNBIF0gmkb/figures/figures_1_1.jpg", "caption": "Figure 1: Diffusion Loss. Given a continuous-valued token x to be predicted, the autoregressive model produces a vector z, which serves as the condition of a denoising diffusion network (a small MLP). This offers a way to model the probability distribution p(x|z) of this token. This network is trained jointly with the autoregressive model by backpropagation. At inference time, with a predicted z, running the reverse diffusion procedure can sample a token following the distribution: x ~ p(x|z). This method eliminates the need for discrete-valued tokenizers.", "description": "This figure illustrates the Diffusion Loss function. The autoregressive model generates a vector *z* conditioned on previous tokens.  This vector *z* is then used as input to a small Multilayer Perceptron (MLP) along with a noisy version of the current token *x<sub>t</sub>*. The MLP predicts the noise *\u03b5* added to *x<sub>t</sub>*, and the loss function measures the difference between the predicted noise and the actual noise. At inference time, this process is reversed to sample a new token from the learned distribution p(x|z). The key advantage is the elimination of the need for vector quantization.", "section": "Method"}, {"figure_path": "VNBIF0gmkb/figures/figures_4_1.jpg", "caption": "Figure 2: Bidirectional attention can do autoregression. In contrast to conventional wisdom, the broad concept of \u201cautoregression\u201d (next token prediction) can be done by either causal or bidirectional attention. (a) Causal attention restricts each token to attend only to current/previous tokens. With input shifted by one start token [s], it is valid to compute loss on all tokens at training time. (b) Bidirectional attention allows each token to see all tokens in the sequence. Following MAE [21], mask tokens [m] are applied in a middle layer, with positional embedding added. This setup only computes loss on unknown tokens, but it allows for full attention capabilities across the sequence, enabling better communication across tokens. This setup can generate tokens one by one at inference time, which is a form of autoregression. It also allows us to predict multiple tokens simultaneously.", "description": "This figure compares causal and bidirectional self-attention mechanisms in autoregressive models for image generation.  Causal attention limits each token's attention to previous tokens, while bidirectional attention allows tokens to attend to all other tokens. This is exemplified in the context of standard, raster-ordered autoregressive models and masked autoregressive (MAR) models, demonstrating how bidirectional attention facilitates both standard and masked autoregressive generation.", "section": "3.4 Unifying Autoregressive and Masked Generative Models"}, {"figure_path": "VNBIF0gmkb/figures/figures_7_1.jpg", "caption": "Figure 1: Diffusion Loss. Given a continuous-valued token x to be predicted, the autoregressive model produces a vector z, which serves as the condition of a denoising diffusion network (a small MLP). This offers a way to model the probability distribution p(x|z) of this token. This network is trained jointly with the autoregressive model by backpropagation. At inference time, with a predicted z, running the reverse diffusion procedure can sample a token following the distribution: x ~ p(x|z). This method eliminates the need for discrete-valued tokenizers.", "description": "This figure illustrates the core concept of Diffusion Loss, a novel loss function proposed in the paper.  It shows how an autoregressive model generates a conditioning vector (z) for a continuous-valued token (x). This vector then serves as input to a small Multi-Layer Perceptron (MLP) which models the probability distribution p(x|z). The MLP is trained together with the autoregressive model. During inference, this allows for sampling a token from p(x|z) without using vector quantization.", "section": "Method"}, {"figure_path": "VNBIF0gmkb/figures/figures_8_1.jpg", "caption": "Figure 6: Speed/accuracy trade-off of the generation process. For MAR, a curve is obtained by different autoregressive steps (8 to 128). For DiT, a curve is obtained by different diffusion steps (50, 75, 150, 250) using its official code. We compare our implementation of AR and MAR. AR is with kv-cache for fast inference. AR/MAR model size is L and DiT model size is DiT-XL. The star marker denotes our default MAR setting used in other ablations. We benchmark FID and speed on ImageNet 256\u00d7256 using one A100 GPU with a batch size of 256.", "description": "This figure shows the trade-off between speed and accuracy (FID score) for different image generation models.  The x-axis represents inference time per image, and the y-axis represents the FID score, a measure of image quality.  Three models are compared: a standard autoregressive model (AR), a masked autoregressive model (MAR) with cross-entropy loss, and the same MAR model but using the proposed Diffusion Loss.  Each model's performance is shown as a curve representing different generation steps or diffusion steps to demonstrate the trade-off. The star marks the default setting used for the MAR model with Diffusion Loss in other experiments, highlighting its superior speed/accuracy balance.", "section": "5.2 Properties of Generalized Autoregressive Models"}, {"figure_path": "VNBIF0gmkb/figures/figures_9_1.jpg", "caption": "Figure 7: Qualitative Results. We show selected examples of class-conditional generation on ImageNet 256x256 using MAR-H with Diffusion Loss.", "description": "This figure displays a diverse set of images generated by the MAR-H model (masked autoregressive model with the largest architecture) utilizing the Diffusion Loss function.  The images demonstrate the model's ability to generate high-quality, class-conditional images across various categories from ImageNet.", "section": "5 Experiments"}, {"figure_path": "VNBIF0gmkb/figures/figures_13_1.jpg", "caption": "Figure 8: Failure cases. Similar to existing methods, our system can produce results with noticeable artifacts. For each pair, we show MAR-H and DiT-XL's results of the same class. The leftmost example of DiT is taken from their paper [37]; the others are obtained from their official code.", "description": "This figure shows a comparison of the image generation results between the proposed MAR-H model and the DiT-XL model.  Each pair of images shows results for the same class from both models, illustrating that both models can generate images with artifacts, despite their differences in approach.", "section": "A Limitations and Broader Impacts"}, {"figure_path": "VNBIF0gmkb/figures/figures_14_1.jpg", "caption": "Figure 2: Bidirectional attention can do autoregression. In contrast to conventional wisdom, the broad concept of \"autoregression\" (next token prediction) can be done by either causal or bidirectional attention. (a) Causal attention restricts each token to attend only to current/previous tokens. With input shifted by one start token [s], it is valid to compute loss on all tokens at training time. (b) Bidirectional attention allows each token to see all tokens in the sequence. Following MAE [21], mask tokens [m] are applied in a middle layer, with positional embedding added. This setup only computes loss on unknown tokens, but it allows for full attention capabilities across the sequence, enabling better communication across tokens. This setup can generate tokens one by one at inference time, which is a form of autoregression. It also allows us to predict multiple tokens simultaneously.", "description": "This figure compares causal and bidirectional attention mechanisms in autoregressive models.  Causal attention, used in standard autoregressive models, processes tokens sequentially, with each token only attending to preceding tokens. Bidirectional attention, however, allows tokens to attend to all other tokens, potentially enabling better information flow and faster generation. The figure highlights how bidirectional attention can still function as autoregressive (predicting the next token), and further allows simultaneous prediction of multiple tokens.", "section": "3.4 Unifying Autoregressive and Masked Generative Models"}]