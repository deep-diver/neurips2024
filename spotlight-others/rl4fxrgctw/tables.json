[{"figure_path": "RL4FXrGcTw/tables/tables_1_1.jpg", "caption": "Table 1: Some applications for functions of matrices. Log-determinants apply by combining log det(A) = trace (log(A)) with stochastic trace estimation, which is why most vectors in this table are Rademacher samples. \u201cPDE\u201d / \u201cODE\u201d = \u201cPartial/Ordinary differential equation\u201d.", "description": "This table presents several machine learning applications that involve evaluating functions of matrices. Each row represents an application, specifying the function applied to the matrix, the type of matrix, the vector used in matrix-vector products, and the parameters that are optimized.  The applications span diverse areas such as solving partial differential equations (PDEs), Gaussian processes, invertible residual networks, Gaussian sampling, and Neural ODEs.", "section": "1 Introduction"}, {"figure_path": "RL4FXrGcTw/tables/tables_5_1.jpg", "caption": "Table 2: Accuracy loss when differentiating the Arnoldi iteration on a Hilbert matrix in double precision ($ : decompose with a full-rank Arnoldi iteration, then reconstruct the original matrix; measure ||8 - I||; details in Appendix F).", "description": "This table shows the loss of accuracy when differentiating the Arnoldi iteration on Hilbert matrices of various sizes.  It compares three methods: adjoint without projection, adjoint with projection, and backpropagation. The results demonstrate that the adjoint method, especially with projection, maintains high accuracy, while backpropagation suffers from significant loss of accuracy as matrix size increases.", "section": "4.2 Matrix-free implementation"}, {"figure_path": "RL4FXrGcTw/tables/tables_6_1.jpg", "caption": "Table 3: Our method yields the same root-mean-square errors (RMSEs) as GPyTorch. It reaches lower training losses but is \u2248 20\u00d7 slower per epoch due to different matrix-vector-product backends (see Appendix G). Three runs, significant improvements in bold. We use an 80/20 train/test split.", "description": "This table compares the performance of the proposed method against GPyTorch on five different datasets for Gaussian process model selection.  The comparison includes RMSE, final training loss, and runtime per epoch.  The results show that the proposed method achieves similar RMSE and lower training loss but with significantly longer runtime, attributed to the differences in matrix-vector product backends.", "section": "5 Case study: Exact Gaussian processes"}, {"figure_path": "RL4FXrGcTw/tables/tables_8_1.jpg", "caption": "Table 4: All three methods reconstruct the parameter well (std.-deviations exceed differences for test-loss and RMSE), but Arnoldi and Dopri5 are faster than Tsit5. Dopri5 uses the BacksolveAdjoint, and Tsit5 the RecursiveCheckpointAdjoint in Diffrax [54]. We contribute Arnoldi's adjoints.", "description": "This table compares three different methods (Arnoldi, Dopri5, and Tsit5) for solving a physics-informed machine learning problem involving partial differential equations.  The methods are evaluated based on their test loss, parameter RMSE, and runtime per epoch.  The results show that while all three methods achieve comparable accuracy (as indicated by similar loss and RMSE values), the Arnoldi method using the authors' newly developed adjoint is significantly faster than the other two methods.", "section": "6 Case study: Physics-informed machine learning with PDEs"}, {"figure_path": "RL4FXrGcTw/tables/tables_9_1.jpg", "caption": "Table 3: Our method yields the same root-mean-square errors (RMSEs) as GPyTorch. It reaches lower training losses but is \u2248 20\u00d7 slower per epoch due to different matrix-vector-product backends (see Appendix G). Three runs, significant improvements in bold. We use an 80/20 train/test split.", "description": "This table compares the performance of the proposed method and GPyTorch on several datasets for Gaussian process model selection.  The RMSE, final training loss, and runtime per epoch are reported for both methods.  The results show comparable RMSEs, but the proposed method achieves lower training losses, although it is slower due to different matrix-vector product implementations.", "section": "Case study: Exact Gaussian processes"}, {"figure_path": "RL4FXrGcTw/tables/tables_22_1.jpg", "caption": "Table 3: Our method yields the same root-mean-square errors (RMSEs) as GPyTorch. It reaches lower training losses but is \u2248 20\u00d7 slower per epoch due to different matrix-vector-product backends (see Appendix G). Three runs, significant improvements in bold. We use an 80/20 train/test split.", "description": "This table compares the performance of the proposed method and GPyTorch on several datasets for Gaussian process regression.  The metrics compared are RMSE, final training loss, and runtime per epoch. The table shows that both methods achieve similar RMSE, but the proposed method achieves lower training loss. The significant difference in runtime is attributed to the use of different matrix-vector product backends. ", "section": "5 Case study: Exact Gaussian processes"}, {"figure_path": "RL4FXrGcTw/tables/tables_25_1.jpg", "caption": "Table 3: Our method yields the same root-mean-square errors (RMSEs) as GPyTorch. It reaches lower training losses but is \u2248 20\u00d7 slower per epoch due to different matrix-vector-product backends (see Appendix G). Three runs, significant improvements in bold. We use an 80/20 train/test split.", "description": "This table compares the performance of the proposed method against GPyTorch on five datasets for Gaussian process hyperparameter optimization.  The metrics considered are RMSE, final training loss, and runtime per epoch. The table shows that the proposed method achieves similar RMSE to GPyTorch, but with lower training losses, albeit at a significantly higher computational cost. This difference in speed is attributed to different matrix-vector product backends.", "section": "Case study: Exact Gaussian processes"}]