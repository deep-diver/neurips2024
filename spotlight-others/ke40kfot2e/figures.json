[{"figure_path": "Ke40kfOT2E/figures/figures_2_1.jpg", "caption": "Figure 1: PGM (a) \u2192 tree PIC (b)", "description": "This figure illustrates the transformation of a Probabilistic Graphical Model (PGM) into a tree-shaped Probabilistic Integral Circuit (PIC).  Panel (a) shows a simple PGM with continuous latent variables (Z1, Z2, Z3) and observed variables (X1, X2, X3, X4). Panel (b) displays the equivalent PIC, where the latent variables in (a) become integral units in (b), and the conditional distributions from (a) become the functions associated with the input and integral units in (b). The structure of the PGM directly informs the structure of the PIC. This conversion highlights the method used to translate between PGM representation and PIC representation, showcasing the hierarchical structure of continuous latent variables that PICs can capture.", "section": "2 Probabilistic integral circuits"}, {"figure_path": "Ke40kfOT2E/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline presented in this paper: RG \u2192 PIC \u2192 QPC \u2192 folded QPC. Starting from a (fragment of) a DAG-shaped region graph (a), we build a DAG-like PIC via Algorithm 1 using Tucker-merge (b). Then, we materialize a tensorized QPC encoding a hierarchical quadrature process via Algorithm 3, using K = 2 quadrature points, which we fold to allow faster inference (d).", "description": "This figure illustrates the four-step pipeline for building and training Probabilistic Integral Circuits (PICs). It starts with an arbitrary Directed Acyclic Graph-shaped Region Graph (RG) which represents a hierarchical variable decomposition. This RG is then converted into a DAG-shaped PIC using Algorithm 1 and the Tucker-merge technique.  The intractable PIC is then approximated by a tensorized Quadrature Probabilistic Circuit (QPC) using Algorithm 3 and a hierarchical quadrature process. Finally, the QPC is folded to improve inference speed.", "section": "3 Building, learning and scaling PICs"}, {"figure_path": "Ke40kfOT2E/figures/figures_4_1.jpg", "caption": "Figure 3: From functions to sum-product layers via multivariate numerical quadrature (Section 3.2). We illustrate how the 3-variate function f({Z}, {Y1, Y2}) (a) can be seen as an infinite (quasi) tensor that we first materialize w.r.t. integration points \u017e as a finite tensor W of size K \u00d7 K \u00d7 K (b, Equation (2)), then flatten as a matrix accounting for integration weights w (c, Equation (3)), and finally use to parameterize a Tucker layer (d, Equation (Tucker-layer)).", "description": "This figure illustrates the process of converting a 3-variate function into a sum-product layer using multivariate numerical quadrature.  It shows how an infinite quasi-tensor representation (a) is first approximated as a finite tensor (b) using integration points and weights, then flattened into a matrix (c) and finally used to parameterize a Tucker layer (d), a common architecture in probabilistic circuits.", "section": "3.2 Learning PICs via tensorized QPCs"}, {"figure_path": "Ke40kfOT2E/figures/figures_6_1.jpg", "caption": "Figure 4: From neural C-sharing to folded CP-layer (Section 3.3). We sketch a 4-headed MLP with Fourier-Features (a) which we use to parameterize a group of 4 integral units (at the same depth) of a PIC (b), whose materialization leads to a folded CP-layer parameterized by a tensor W of size 2 \u00d7 2 \u00d7 K \u00d7 K (c), with K being the number of integration point. Note that, during materialization, the FF-MLP block in (a) will be only evaluated K2 times, and not 4K2.", "description": "This figure illustrates the concept of neural functional sharing in the context of Probabilistic Integral Circuits (PICs). It shows how a multi-headed Multi-Layer Perceptron (MLP) with Fourier Features can parameterize a group of integral units within a PIC. The process of materializing the PIC into a Quadrature Probabilistic Circuit (QPC) leads to a folded CP-layer, which is a more efficient representation. The key idea is that the MLP is only evaluated K^2 times (K being the number of quadrature points), instead of 4K^2 times, resulting in computational savings.", "section": "3.3 Scaling PICs with neural functional sharing"}, {"figure_path": "Ke40kfOT2E/figures/figures_7_1.jpg", "caption": "Figure 5: Learning PICs using functional sharing requires (i) comparable resources as PCs and (ii) up to 99% less trainable parameters. We compare the GPU memory (top-left) and time (bottom-left) required to perform an optimization step with PCs (\u25cf), PICs with functional sharing (\u25a0), and without (\u25b2), while considering three different architectures (QT-CP, QG-CP, QG-TK). To the right, we report the number of trainable parameters for (i) PCs (\u25cf) at different K, and (ii) for PICs (\u25a0, \u25b2) at different MLP sizes M. The isolated \u25b2 nodes refer to refer to PIC (F, N) with QG-TK which we could only run at K = 16. The benchmark is conducted using a batch of 128 RGB images of size 64x64 and Adam [23]. Extra details in Appendix D.1.", "description": "This figure compares the GPU memory and time required for training Probabilistic Integral Circuits (PICs) with and without functional sharing against standard Probabilistic Circuits (PCs).  It demonstrates that functional sharing allows PICs to scale similarly to PCs, while requiring significantly fewer parameters (up to 99% less). The figure also shows the number of trainable parameters for PCs and PICs with varying parameters (K and M).", "section": "4 Experiments"}, {"figure_path": "Ke40kfOT2E/figures/figures_8_1.jpg", "caption": "Figure 5: Learning PICs using functional sharing requires (i) comparable resources as PCs and (ii) up to 99% less trainable parameters. We compare the GPU memory (top-left) and time (bottom-left) required to perform an optimization step with PCs (\u25cf), PICs with functional sharing (\u25a0), and without (\u25b2), while considering three different architectures (QT-CP, QG-CP, QG-TK). To the right, we report the number of trainable parameters for (i) PCs (\u25cf) at different K, and (ii) for PICs (\u25a0, \u25b2) at different MLP sizes M. The isolated \u25b2 nodes refer to refer to PIC (F, N) with QG-TK which we could only run at K = 16. The benchmark is conducted using a batch of 128 RGB images of size 64x64 and Adam [23]. Extra details in Appendix D.1.", "description": "This figure compares the GPU memory and time required for an optimization step using PCs, PICs with functional sharing, and PICs without functional sharing. It also shows the number of trainable parameters for PCs and PICs with different architectures and hyperparameters. The results demonstrate that using functional sharing in PICs reduces the resources required for training compared to PCs and PICs without functional sharing.", "section": "4 Experiments"}, {"figure_path": "Ke40kfOT2E/figures/figures_17_1.jpg", "caption": "Figure 2: The pipeline presented in this paper: RG \u2192 PIC \u2192 QPC \u2192 folded QPC. Starting from a (fragment of) a DAG-shaped region graph (a), we build a DAG-like PIC via Algorithm 1 using Tucker-merge (b). Then, we materialize a tensorized QPC encoding a hierarchical quadrature process via Algorithm 3, using K = 2 quadrature points, which we fold to allow faster inference (d).", "description": "This figure illustrates the four stages of the proposed pipeline for building and training probabilistic integral circuits (PICs).  It starts with a region graph (RG), a DAG representing a hierarchical decomposition of variables. This RG is then converted into a DAG-shaped PIC using Algorithm 1 and a merging strategy (Tucker-merge shown here, but CP-merge is another option). The resulting PIC, if intractable, is then approximated by a tensorized quadrature probabilistic circuit (QPC) via Algorithm 3, which encodes the hierarchical quadrature process. Finally, to speed up inference, the QPC is folded, reducing the number of layers while maintaining the expressiveness.", "section": "3 Building, learning and scaling PICs"}, {"figure_path": "Ke40kfOT2E/figures/figures_17_2.jpg", "caption": "Figure 5: Learning PICs using functional sharing requires (i) comparable resources as PCs and (ii) up to 99% less trainable parameters. We compare the GPU memory (top-left) and time (bottom-left) required to perform an optimization step with PCs (\u25cf), PICs with functional sharing (\u25a0), and without (\u25b2), while considering three different architectures (QT-CP, QG-CP, QG-TK). To the right, we report the number of trainable parameters for (i) PCs (\u25cf) at different K, and (ii) for PICs (\u25a0, \u25b2) at different MLP sizes M. The isolated \u25b2 nodes refer to refer to PIC (F, N) with QG-TK which we could only run at K = 16. The benchmark is conducted using a batch of 128 RGB images of size 64x64 and Adam [23]. Extra details in Appendix D.1.", "description": "This figure compares the GPU memory and time required for an optimization step for PCs and PICs with and without functional sharing.  It shows that PICs with functional sharing use comparable resources to PCs, while those without functional sharing require significantly more resources.  The figure also displays the number of trainable parameters for both PCs and PICs, demonstrating that PICs with functional sharing have up to 99% fewer parameters. The experiment uses a batch of 128 64x64 RGB images and the Adam optimizer.", "section": "Experiments"}]