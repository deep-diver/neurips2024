[{"figure_path": "T5UfIfmDbq/figures/figures_4_1.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two-stage process of MCTS-transfer. The pre-learning stage uses a Monte Carlo Tree Search (MCTS) to iteratively partition the search space based on data from source tasks.  This creates a tree structure where each node represents a subspace. The optimization stage then uses this pre-learned tree to guide the optimization of the target task.  Starting from the root node, the algorithm selects a leaf node with the highest Upper Confidence Bound (UCB) value and performs optimization within the corresponding subspace.  New data from the target task is then used to update the tree structure and guide further optimization.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of MCTS-transfer and other algorithms on Sphere2D", "description": "The figure compares the performance of MCTS-transfer with other search space transfer algorithms (Box-GP, Ellipsoid-GP, and Supervised-GP) on the Sphere2D problem.  It shows the best value achieved by each algorithm over a certain number of evaluations in both mixed and dissimilar transfer settings. The right panel displays the weight change curves for the three source tasks, demonstrating MCTS-transfer's ability to assign higher weights to more similar tasks.", "section": "4.1 Motivating Cases: Sphere2D"}, {"figure_path": "T5UfIfmDbq/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of MCTS-transfer and other algorithms on BBOB and real-world problems", "description": "This figure compares the performance of MCTS-transfer against other algorithms (GP, LA-MCTS, Box-GP, Ellipsoid-GP, Supervised-GP, PFN) on both synthetic benchmark functions from BBOB and real-world problems.  The graphs illustrate the rank of each algorithm over a certain number of evaluations, demonstrating MCTS-transfer's superior performance, particularly in mixed transfer scenarios (where a combination of similar and dissimilar source tasks are used for pre-training).  The results highlight the ability of MCTS-transfer to handle complex situations with varying degrees of task similarity.", "section": "4 Experiments"}, {"figure_path": "T5UfIfmDbq/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison of MCTS-transfer and other algorithms on Design-Bench and HPOB", "description": "This figure compares the performance of MCTS-transfer against other algorithms (GP, LA-MCTS, Box-GP, Ellipsoid-GP, Supervised-GP, PFN) on Design-Bench and HPOB benchmarks, under both similar and mixed transfer learning settings.  The plots show the mean rank of the algorithms over a series of evaluations.  MCTS-transfer demonstrates competitive performance, particularly in the mixed transfer setting where it needs to handle less similar source tasks.", "section": "4 Experiments"}, {"figure_path": "T5UfIfmDbq/figures/figures_9_2.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two-stage process of the MCTS-transfer algorithm.  The pre-learning stage uses MCTS to build a tree structure by recursively dividing the search space based on source task data. Each node in the tree represents a subspace. The optimization stage then uses this pre-learned tree to initialize the search for the target task, iteratively selecting subspaces with high potential based on UCB values and refining the tree structure with new target task data.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_17_1.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two-stage process of the MCTS-transfer algorithm.  The pre-learning stage uses MCTS to build a tree structure by recursively dividing the search space based on source task data.  Each node in the tree represents a subspace. The optimization stage then uses this pre-learned tree to initialize the search for the target task.  It iteratively selects subspaces based on an upper confidence bound (UCB) and then optimizes within the selected subspace before updating the tree based on new observations.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_18_1.jpg", "caption": "Figure 7: Sensitivity Analysis of Local and Global Modeling Approaches", "description": "This figure presents a sensitivity analysis comparing the performance of using a global Gaussian Process (GP) model versus a local GP model within the MCTS-transfer algorithm.  The analysis is performed on three real-world problems (LunarLander-Mixed, RobotPush-Mixed, and Rover-Mixed). The plots show the average objective value against the number of evaluations performed, with error bars indicating variability.  The results show how the choice of global versus local modeling affects the optimization performance across different tasks.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_18_2.jpg", "caption": "Figure 8: Sensitivity Analysis of Similarity Measures", "description": "This figure displays the sensitivity analysis of five different similarity measures used in the MCTS-transfer algorithm.  The measures are: Optimal solutions distance, Best 5 solutions distance, Best 30% solutions distance, KL divergence, and Kendall coefficient. The graph shows how the performance of the algorithm varies across three different real-world problems (LunarLander-Mixed, RobotPush-Mixed, and Rover-Mixed) depending on which similarity measure is selected.  Each line represents a different measure, and the y-axis represents the objective function value obtained by the algorithm.  The x-axis represents the number of evaluations performed. The comparison demonstrates the impact of the chosen similarity measure on the algorithm's ability to effectively transfer knowledge from source tasks to a target task and optimize its performance.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_19_1.jpg", "caption": "Figure 9: Sensitivity Analysis of Weight Change Strategy", "description": "The figure compares the performance of three different weight change strategies (linear, exponential, and all-one) across three real-world problems (LunarLander-Mixed, RobotPush-Mixed, and Rover-Mixed) using MCTS-transfer. The linear strategy shows relatively stable performance, while the exponential strategy leads to faster convergence initially but might under-utilize source task data. The all-one strategy demonstrates the ability to fully utilize information but is more sensitive to dissimilar data. The graph illustrates the best value obtained during optimization against the number of evaluations performed.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_19_2.jpg", "caption": "Figure 10: Sensitivity Analysis of Decay Factor \u03b3", "description": "This figure shows the sensitivity analysis of the decay factor (\u03b3) in MCTS-transfer on three real-world problems (LunarLander, RobotPush, and Rover) under mixed transfer settings.  The decay factor controls the influence of source tasks on the node potential calculations.  The results indicate that a decay factor of 0.99 and 1.0 leads to better performance compared to a factor of 0.1, suggesting that a moderate decay rate allows effective combination of information from source and target tasks to accelerate optimization.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_20_1.jpg", "caption": "Figure 11: Sensitivity Analysis of Important Source Task Ratio \u03b1", "description": "The figure displays the sensitivity analysis of the important source task ratio \u03b1 in the linear-change strategy. It shows how the performance of MCTS-transfer varies with different values of \u03b1 on three real-world problems (LunarLander, RobotPush, and Rover) under mixed transfer settings. The results indicate that a balanced \u03b1 value effectively utilizes information from both similar and dissimilar source tasks, achieving better overall performance.", "section": "4. Experiments"}, {"figure_path": "T5UfIfmDbq/figures/figures_20_2.jpg", "caption": "Figure 12: Sensitivity Analysis of Exploration Factor Cp", "description": "The figure shows the sensitivity analysis of the exploration factor Cp in MCTS-transfer on three real-world problems (LunarLander, RobotPush, and Rover) under mixed transfer settings.  Different values of Cp (0.1, 1.0, 10.0, and 100.0) were tested, and the resulting performance (value) is plotted against the number of evaluations. Error bars represent variability in the results.  The plot helps to understand how the exploration-exploitation balance (controlled by Cp) affects the optimization performance in different scenarios.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_20_3.jpg", "caption": "Figure 13: Sensitivity Analysis of The Splitting Threshold", "description": "This figure shows the sensitivity analysis of the splitting threshold (\u03b8) in MCTS-transfer on three real-world problems: LunarLander, RobotPush, and Rover.  Three different values of \u03b8 (10, 3, and 100) are tested under a mixed transfer setting. The plots illustrate how the choice of \u03b8 impacts the performance of the algorithm in terms of the objective function value over the number of evaluations. A smaller \u03b8 leads to a deeper tree, potentially increasing computational cost but also allowing for a more refined search space partition. The results show the impact of this parameter on the algorithm's convergence and exploration-exploitation balance.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_21_1.jpg", "caption": "Figure 14: Sensitivity Analysis of Binary Classifier", "description": "This figure shows the sensitivity analysis of different binary classifiers used in the MCTS-transfer algorithm for dividing the search space.  The three real-world problems, LunarLander, RobotPush, and Rover, are evaluated under mixed transfer settings.  Different classifiers (Logistic Regression, SVM with rbf, linear, and polynomial kernels) are compared to determine their effectiveness in dividing the search space into \"good\" and \"bad\" clusters. The results indicate that Logistic Regression and SVM with a linear kernel generally provide superior search space partitioning.", "section": "D Sensitivity Analysis of Hyper-parameters of MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_22_1.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the workflow of the MCTS-transfer algorithm.  The pre-learning stage uses MCTS to iteratively divide the search space into subspaces based on source task data, resulting in a tree structure. The optimization stage uses this pre-learned tree to iteratively select and optimize promising subspaces for the target task, adapting the tree structure dynamically during the optimization process based on newly sampled target task data.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_23_1.jpg", "caption": "Figure 16: Evaluations of MCTS-transfer and other algorithms on Real-world Problems", "description": "The figure presents the performance comparison of MCTS-transfer against other baseline algorithms (GP, LA-MCTS, Box-GP, Ellipsoid-GP, Supervised-GP, and PFN) on three real-world problems: LunarLander, RobotPush, and Rover.  The results are shown for both similar and mixed transfer settings. Each subfigure shows the optimization progress of the algorithms over a certain number of evaluations.  It visualizes the objective function values achieved over time for each algorithm in each scenario, illustrating the relative performance of MCTS-transfer compared to the baselines in different transfer settings and the problem's characteristics.", "section": "4.3 Real-world Problems"}, {"figure_path": "T5UfIfmDbq/figures/figures_23_2.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two main stages of the MCTS-transfer algorithm: pre-learning and optimization.  The pre-learning stage uses a Monte Carlo tree search (MCTS) to iteratively divide the search space based on source task data, creating a tree structure where each node represents a subspace. This tree serves as a warm start for the optimization stage. In the optimization stage, MCTS is used again to select a promising subspace for optimization, using the UCB (Upper Confidence Bound) value to guide the selection. The algorithm adaptively adjusts the search space partition based on newly generated target task data throughout the optimization process. ", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_24_1.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two-stage process of the MCTS-transfer algorithm. The pre-learning stage uses source task data to build a tree structure that partitions the search space.  Each node in the tree represents a subspace. The optimization stage then uses this pre-learned tree to guide the search for the target task, iteratively selecting subspaces with high potential and refining the tree structure based on new observations.  The UCB (Upper Confidence Bound) is used to balance exploration and exploitation during subspace selection.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_25_1.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two-stage process of the MCTS-transfer algorithm. The pre-learning stage uses MCTS to iteratively divide the search space into subspaces based on source task data.  The resulting tree structure guides the optimization process in the second stage. In the optimization stage, MCTS is again used to select a promising subspace from the pre-learned tree.  A Bayesian optimization algorithm is then applied to refine the search within this subspace. The process iteratively refines the search space and adapts it to the target task.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_26_1.jpg", "caption": "Figure 1: The workflow of MCTS-transfer. In pre-learning stage, MCTS-transfer builds the tree by clustering and classifying the samples apart recursively, until all nodes are not splitable. In optimization stage, the initial search space is based on the pre-learned tree. We will trace child node with greater UCB from ROOT and find the target leaf node to do optimization.", "description": "This figure illustrates the two main stages of the MCTS-transfer algorithm: pre-learning and optimization.  The pre-learning stage uses source task data to build a tree structure by recursively clustering and classifying samples, resulting in a hierarchical representation of the search space. The optimization stage leverages this pre-learned tree to efficiently guide the search for the target task.  The algorithm iteratively selects a leaf node based on its UCB value, optimizes within the corresponding subspace, and updates the tree structure dynamically based on new observations.", "section": "3 MCTS-transfer"}, {"figure_path": "T5UfIfmDbq/figures/figures_27_1.jpg", "caption": "Figure 21: Weight Changes: MCTS-transfer", "description": "The figure displays the weight changes of three mixed real-world problems over 100 evaluations.  The weights are assigned to source tasks (real-world problems, similar sphere, and dissimilar sphere) dynamically during the optimization process, reflecting the similarity between source tasks and the target task. The results show that the weights of real-world problems and similar sphere problems are higher than those of dissimilar sphere problems in most cases, even with inconsistencies in initialization. This demonstrates the algorithm's ability to prioritize similar source task data, leading to more accurate node potential evaluation and search space partition.", "section": "G Visualization of Weight Change Curve of Source Tasks"}]