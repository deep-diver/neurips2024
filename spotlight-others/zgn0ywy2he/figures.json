[{"figure_path": "zGN0YWy2he/figures/figures_1_1.jpg", "caption": "Figure 1: Failure cases generated by (a) text-to-image (T2I) (DALL-E 3 [3]), (b) layout-to-image (L2I) (LayoutDiffusion [10]), and (c) semantics-based scene-graph-to-image (SG2I) (R3CD [14]) methods. (d) Generalizable object Attribute Control (AC) under consistency achieved by our DisCo.", "description": "This figure showcases the limitations of existing text-to-image (T2I), layout-to-image (L2I), and semantics-based scene graph to image (SG2I) methods in generating complex scenes.  Specifically, (a) demonstrates the inability of DALL-E 3 to accurately represent relationships and quantities in a complex scene. (b) shows LayoutDiffusion's struggles with non-spatial interactions. (c) highlights the missing independent nodes from R3CD.  Finally, (d) demonstrates the improved, consistent object attribute control achieved by the authors' proposed DisCo method.", "section": "1 Introduction"}, {"figure_path": "zGN0YWy2he/figures/figures_1_2.jpg", "caption": "Figure 2: Comparison between the previous SG2I architectures and ours. (a) Layout-based SG2I model [15] generate a spatial arrangement with an object layout; (b) Semantic-based SG2I models [14, 16] build interactive semantic embedding between objects; (c) Our method leverages scene graph representation by jointly deriving the disentangled layout and semantics with the proposed SL-VAE.", "description": "This figure compares three different scene graph to image generation (SG2I) approaches.  (a) shows a layout-based approach where a layout predictor first generates the spatial arrangement of objects before image generation. (b) illustrates a semantic-based method that directly encodes graph relationships into node embeddings for generation. Finally, (c) presents the authors' proposed method (DisCo), which uses a variational autoencoder to disentangle spatial layout and interactive semantics from the scene graph before feeding them into a diffusion model for image generation.", "section": "1 Introduction"}, {"figure_path": "zGN0YWy2he/figures/figures_2_1.jpg", "caption": "Figure 3: Framework overview. (I) We parameterize the node embeddings into the Gaussian distribution with the Graph Union Encoder, which jointly models the spatial relationships and non-spatial interactions in scene graphs; (II) The Semantic and Layout Decoders generate spatial layouts and interactive semantics sampled from Gaussian distribution, respectively; (III) A diffusion model with the proposed Compositional Masked Attention (CMA) incorporates object-level conditions to generate visual images following the scene graph description; (IV) Detailed structure of CMA Layer.", "description": "This figure illustrates the framework of DisCo, which consists of four main parts: Graph Encoding for Semantics-Layout VAE, Graph Decoding for Semantics-Layout VAE, Visual Diffusion Process with Semantic and Layout Condition, and Details of CMA Mechanism.  The Graph Union Encoder jointly models spatial and non-spatial interactions in scene graphs. The Semantic and Layout Decoders generate spatial layouts and interactive semantics. A diffusion model with CMA incorporates object-level conditions to generate images from the scene graph.  The detailed structure of the CMA layer is also shown.", "section": "3 Methodology"}, {"figure_path": "zGN0YWy2he/figures/figures_4_1.jpg", "caption": "Figure 4: Toy example of (a) compositional masked attention, and (b) its corresponding attention mask. We use visual tokens and object embeddings of objects A and B for demonstration. A and B have 1 and 2 visual tokens, respectively, whose attribution is determined by bounding boxes.", "description": "This figure shows a toy example to illustrate the compositional masked attention mechanism. The figure consists of two parts: (a) shows the compositional masked attention mechanism that connects visual tokens and object embeddings based on bounding boxes, and (b) shows the corresponding attention mask that indicates the attention weights between visual tokens and object embeddings. ", "section": "3.2 Diffusion with Compositional Masked Attention"}, {"figure_path": "zGN0YWy2he/figures/figures_7_1.jpg", "caption": "Figure 1: Failure cases generated by (a) text-to-image (T2I) (DALL-E 3 [3]), (b) layout-to-image (L2I) (LayoutDiffusion [10]), and (c) semantics-based scene-graph-to-image (SG2I) (R3CD [14]) methods. (d) Generalizable object Attribute Control (AC) under consistency achieved by our DisCo.", "description": "This figure showcases failure cases from state-of-the-art Text-to-Image (T2I), Layout-to-Image (L2I), and Semantics-based Scene Graph-to-Image (SG2I) methods.  It highlights common issues like misinterpretations of relationships and quantities (T2I), difficulty handling non-spatial interactions (L2I), and missing objects or attributes (SG2I).  In contrast, the authors' method (DisCo) is shown to achieve greater accuracy and controllability, as evidenced by the example of consistent attribute control in (d).", "section": "Introduction"}, {"figure_path": "zGN0YWy2he/figures/figures_8_1.jpg", "caption": "Figure 6: Illustration of object-level Node Addition (NA) and Attribute Control (AC) in the scene. From left to right: (a) the \"boat\" and \"grass\" in the first line, image generated by the unmodified scene graph; (b) the chair addition; (c) the blue-colored wall; and (d) the red-colored wall.", "description": "This figure demonstrates the model's ability to perform object-level manipulation, specifically node addition and attribute control.  The image shows a scene of a room. (a) is the unmodified scene. (b) shows the addition of a chair (node addition). (c) and (d) show changes in the wall color (attribute control). The dashed boxes highlight the changes made via graph manipulation.", "section": "3.3 Multi-Layered Sampler"}, {"figure_path": "zGN0YWy2he/figures/figures_9_1.jpg", "caption": "Figure 6: Illustration of object-level Node Addition (NA) and Attribute Control (AC) in the scene. From left to right: (a) the \"boat\" and \"grass\" in the first line, image generated by the unmodified scene graph; (b) the chair addition; (c) the blue-colored wall; and (d) the red-colored wall.", "description": "This figure demonstrates the model's ability to perform object-level manipulation.  The image shows how the model can add new objects (node addition) or change attributes of existing objects (attribute control) while maintaining consistency in the scene. The four images showcase the effects of using an unmodified scene graph versus scene graphs modified to include a new chair, blue wall, and red wall, respectively.", "section": "3.3 Multi-Layered Sampler"}, {"figure_path": "zGN0YWy2he/figures/figures_16_1.jpg", "caption": "Figure 8: Generalizable Generation Samples under Consistency for Graph Manipulation.", "description": "This figure visualizes the results of applying graph manipulation (node addition and attribute control) to the scene graph.  It demonstrates the model's ability to generate images consistent with the manipulated graph, showing added objects and changed attributes while maintaining scene coherence.  Multiple examples are given for various scene types (ocean, room, house).", "section": "3.3 Multi-Layered Sampler"}, {"figure_path": "zGN0YWy2he/figures/figures_17_1.jpg", "caption": "Figure 1: Failure cases generated by (a) text-to-image (T2I) (DALL-E 3 [3]), (b) layout-to-image (L2I) (LayoutDiffusion [10]), and (c) semantics-based scene-graph-to-image (SG2I) (R3CD [14]) methods. (d) Generalizable object Attribute Control (AC) under consistency achieved by our DisCo.", "description": "This figure showcases the failure cases of existing state-of-the-art methods in generating complex images from text, layout, or scene graph.  (a) shows DALL-E 3 struggling with relationships and object counts from a complex text prompt.  (b) shows LayoutDiffusion's limitations in handling non-spatial relationships (e.g., \"playing\"). (c) demonstrates the issues of R3CD with independent node generation in scene graphs. Finally, (d) highlights the improved results of the proposed DisCo method, demonstrating its ability to maintain visual consistency while manipulating the scene graph (adding and changing attributes).", "section": "1 Introduction"}, {"figure_path": "zGN0YWy2he/figures/figures_18_1.jpg", "caption": "Figure 10: Qualitative Comparison with Layout-to-Image methods.", "description": "This figure compares the image generation results of four different methods: GLIGEN, LayoutDiffusion, MIGC, and the authors' proposed method (Ours).  Each row shows a different text prompt used to generate the images.  The figure highlights the differences in the generated images' adherence to the specified spatial relationships and object attributes, demonstrating the strengths and weaknesses of each method.", "section": "4 Experiments"}, {"figure_path": "zGN0YWy2he/figures/figures_19_1.jpg", "caption": "Figure 11: Qualitative Comparison with Scene-Graph-to-Image methods.", "description": "This figure presents a qualitative comparison of the proposed DisCo model against other state-of-the-art Scene-Graph-to-Image (SG2I) methods, including SGDiff, R3CD, SG2Im, and SceneGenie.  For each method, three example scenes are shown, along with their corresponding scene graphs. The scene graphs represent the objects and relationships in each scene.  The generated images demonstrate the ability of each model to produce images consistent with the scene graph. The differences highlight the advantages of DisCo, particularly its ability to handle complex relationships and object interactions. The superior visual quality and coherence achieved by DisCo compared to the other methods are easily discernible.  Notice how DisCo better handles objects that might otherwise be missed or misplaced in other methods.", "section": "Experiments"}]