[{"figure_path": "V0oJaLqY4E/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Overview of DxMI. The diffusion model \u03c0(x) is trained using the energy of q(x) as a reward. The EBM q(x) is trained using samples from \u03c0(x) as negative samples. (Right) ImageNet 64 generation examples from a 10-step diffusion model before DxMI fine-tuning (up) and after fine-tuning (down). Only the last six steps out of ten are shown.", "description": "This figure illustrates the DxMI framework. The left panel shows a schematic diagram of how the diffusion model (\u03c0(x)) and the energy-based model (EBM, q(x)) interact during training.  The diffusion model generates samples, and these samples are used to train the EBM, which in turn provides a reward signal for the diffusion model. This creates a feedback loop to refine the model. The right panel displays ImageNet 64 generation examples.  The top row shows images generated by a 10-step diffusion model *before* fine-tuning with DxMI; the bottom row shows images generated *after* fine-tuning, highlighting the improvement in image quality achieved by DxMI.", "section": "1 Introduction"}, {"figure_path": "V0oJaLqY4E/figures/figures_5_1.jpg", "caption": "Figure 2: 2D density estimation on 8 Gaussians. Red shades indicate the energy (white is low), and the dots are generated samples.", "description": "This figure visualizes the results of a 2D density estimation experiment using 8 Gaussian distributions.  The left two panels show the true energy function (E(x)) for the data, with white representing low energy and dark red representing high energy. The dots represent the generated samples from the model at different temperature settings (\u03c4=0 and \u03c4=1). The right two panels illustrate the estimated energy function (E\u03b8(x)) learned by the DxMI model at the same temperature settings (\u03c4=0 and \u03c4=1), also with white indicating low energy and dark red representing high energy. By comparing the left and right panels, one can assess the accuracy of the DxMI model in estimating the true energy function, and the impact of temperature (\u03c4) on the quality of density estimation, with the samples providing a visualization of the energy function's effect on sample distribution.", "section": "5.1 2D Synthetic Data"}, {"figure_path": "V0oJaLqY4E/figures/figures_17_1.jpg", "caption": "Figure 3: Value functions at each time step (\u03c4 = 0.1 case). Blue indicates a low value.", "description": "This figure visualizes the learned value functions V(x, t) at different time steps (t=0 to t=5) during the training process of the Diffusion by Maximum Entropy Inverse Reinforcement Learning (DxMI) model.  Each image represents a 2D heatmap where color intensity corresponds to the value function's output for a given input x.  Darker blue indicates lower values, and lighter blue indicates higher values. The final time step (t=5) represents the energy function E(x) learned by the energy-based model (EBM). The figure shows how the value function changes over time as it learns to approximate the data distribution.", "section": "4.2 Dynamic Programming"}, {"figure_path": "V0oJaLqY4E/figures/figures_20_1.jpg", "caption": "Figure 4: Randomly selected samples from CIFAR-10 training data, SFT-PG (T = 10, FID: 4.32), and DxMI (T = 10, FID: 3.19).", "description": "This figure compares image samples generated by three different methods: real CIFAR-10 images, samples generated using SFT-PG (a baseline method), and samples generated using DxMI (the proposed method).  Both SFT-PG and DxMI used 10 generation steps (T=10).  The FID (Fr\u00e9chet Inception Distance) scores are provided to quantify the quality of the generated images, with lower scores indicating better quality. DxMI achieves a lower FID score (3.19) compared to SFT-PG (4.32), suggesting that DxMI produces higher-quality samples.", "section": "5.2 Image Generation: Training Diffusion Models with Small T"}, {"figure_path": "V0oJaLqY4E/figures/figures_20_2.jpg", "caption": "Figure 5: Randomly selected samples from ImageNet 64\u00d764 training data, Consistency Model (T = 1, FID: 6.20), DxMI (T = 4, FID: 3.21), and DxMI (T = 10, FID: 2.68). Note that the Consistency Model samples distort human faces, while the DxMI samples depict them in correct proportions.", "description": "This figure compares image samples generated by different models: the original ImageNet data, a Consistency Model (a baseline model for generating images using a diffusion process in one step), and the proposed DxMI model trained with 4 and 10 steps.  The visual comparison highlights that DxMI produces higher-quality images, particularly when it comes to accurately representing human faces, which are often distorted by the baseline model.", "section": "5.2 Image Generation: Training Diffusion Models with Small T"}]