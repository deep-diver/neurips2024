[{"type": "text", "text": "Learning Formal Mathematics From Intrinsic Motivation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gabriel Poesia1 David Broman4 Nick Haber1,3 Noah D. Goodman1,2 {poesia,nhaber,ngoodman}@stanford.edu dbro@kth.se Departments of Computer Science1, Psychology2 and Education3, Stanford University, USA EECS and Digital Futures4, KTH Royal Institute of Technology, Sweden ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "How did humanity coax mathematics from the \u00e6ther? We explore the Platonic view that mathematics can be discovered from its axioms\u2014a game of conjecture and proof. We describe MINIMO (Mathematics from Intrinsic Motivation): an agent that jointly learns to pose challenging problems for itself (conjecturing) and solve them (theorem proving). Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model. Our method guarantees well-formed conjectures by construction, even as we start with a randomly initialized model. We use the same model to represent a policy and value function for guiding proof search. Our agent targets generating hard but provable conjectures\u2014a moving target, since its own theorem proving ability also improves as it trains. We propose novel methods for hindsight relabeling on proof search trees to significantly improve the agent\u2019s sample efficiency in both tasks. Experiments on 3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate that our agent can bootstrap from only the axioms, self-improving in generating true and challenging conjectures and in finding proofs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mathematical reasoning stands as a grand challenge for Artificial Intelligence (AI) research since the birth of the field [25]. Artificial agents capable of general mathematical reasoning have the potential to drastically impact both mathematics itself and areas where mathematical proof plays a key role, such as program and hardware verification [4]. While this goal has received significant attention from the AI community [22, 46, 30, 42], it still remains far from the breakthroughs that areas such as general game playing [36], image [32] generation or protein folding [35] have witnessed. ", "page_idx": 0}, {"type": "text", "text": "Prior work has reflected two main visions of how AI might achieve general mathematical reasoning abilities. One such strategy is to leverage all of the available human-produced mathematical knowledge as a starting point [31]. This knowledge is encoded in source as varied as textbooks, online forums, academic papers, as well as formal proofs written in computer languages such as Lean, Isabelle, Coq or Metamath [2]. Large Language Models (LLMs) can ingest all such sources of knowledge in a unified manner, and provide a foundation for tasks in both formal and informal mathematics. Benchmarks of mathematical problem solving in natural language, such as GSM8k [10] and MATH [15], have measured rapid progress over the years, but they remain limited to problems where the final answer is a number, due to the challenge of assessing the correctness of mathematical arguments written in natural language. This difficulty is not a challenge in formal theorem proving, where we can automatically verify the correctness of proofs in arbitrarily high-level mathematics. But benchmarks of formal theorem proving (such as minif2f [46] and LeanDojo [44]), even with rapid advances in LLMs, have not yet witnessed the same breakthroughs. In fact, these benchmarks remain far from solved even though all of the theorems and problems in them are known mathematical facts, often already presented informally in publicly available training data. ", "page_idx": 0}, {"type": "image", "img_path": "uNKlTQ8mBD/tmp/2e13857a57fe8af08e09fd180a1be9ba7b2a6bef2bd10505c982eabcdb76565c.jpg", "img_caption": ["Figure 1: We train mathematical reasoning agents starting only from the axioms of a given formal domain, where they jointly learn to pose challenging but provable conjectures and to find their proofs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "An alternative vision of how AI might master mathematical reasoning sees mathematics through the lens of game playing, observing that the rules of the \u201cgame of mathematics\u201d can be encoded in a variety of formal systems, such as dependent type theory, using a small set of axioms [24]. In principle, this can allows us to potentially borrow the successes of general game-playing agents, such as AlphaZero [36], that have achieved remarkable success in mastering complex games entirely from experience. Notably, AlphaZero achieves super-human performance in a range of games without leveraging any human examples. Instead, it learns entirely from experience given only an environment where it can play the game by itself. If this approach could be transported to mathematics, it would bypass the dependency on human examples, and allow us to explore mathematical domains \u2014 both known and new, without distinction \u2014 by utilizing large scale compute and the potential of axioms to produce infinite data. ", "page_idx": 1}, {"type": "text", "text": "However, there is a crucial and often neglected difference between mathematics and traditional board games where game-playing AI has succeeded: mathematics is a game with intrinsic rewards [8]. Board games, such as Go or Chess, have a fixed starting configuration, and their rules determine the outcome of the game unequivocally. Mastering the game then amounts to learning a policy that optimizes for the extrinsic signal of winning. In theorem proving, a starting configuration is given by a theorem statement, and the correctness of a proof can be assessed objectively in a formal system. But the choice to work on a particular statement \u2014 a conjecture, before it is proved \u2014 is not given a priori by the rules of the game of mathematics. Instead, these goals come from the intrinsically motivated agents [8, 34] who are playing the game. Thus, a key skill developed by human mathematicians is to decide which conjectures are worth considering. In stark contrast, current benchmarks of mathematical reasoning abilities, both formal [46, 44] and informal [10, 15], all measure performance on an extrinsically defined set of problems, without space for further discovery. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we make the first step towards creating intrinsically motivated mathematical agents by proposing MINIMO \u2014 Mathematics from Intrinsic Motivation \u2014, a method to jointly learn conjecturing and theorem proving, starting from nothing but the axioms of a given mathematical domain, represented in dependent type theory [12]. We borrow inspiration from the literature of intrinsic motivation in Reinforcement Learning (RL) [8, 7, 33, 26], where agents can learn from interacting with an environment even when no specific goals are given. Intrinsic motivation objectives have been instrumental for RL in hard exploration environments, where rewards are too sparse to seek directly [5]. The sparsity of rewards is also a major challenge in theorem proving, making this connection especially attractive. We thus define the objective of conjecturing as generating new problems that are challenging for the current agent but still provable within its given search budget. Since the agent also learns from the solutions it finds, the conjecturer has to continuously increase the difficulty of the problems it generates. ", "page_idx": 1}, {"type": "text", "text": "MINIMO performs both conjecturing and proof search with a Transformer [40] language model (LM), which starts randomly initialized. To sample conjectures even from a model that starts with no prior knowledge, we combine methods from type-directed program synthesis and constrained generation from language models, enabling us to get valid conjectures by construction \u2014 concretely, conjectures are simply terms of type prop, the type of mathematical propositions in our type theory. Then, we perform proof search in the Peano environment [28], which provides a finite action space for search in a dependent type theory, guiding search using the LM as a policy and value function. When a proof is found, proof search generates training data for improving the policy and values; it also provides data to improve conjecturing, since we then know how hard the problem was. We use this to alternate between conjecturing and theorem proving, in a self-improving loop. However, successful proofs are sparse. We thus adapt the idea of hindsight relabeling [1] to reinterpret failed trajectories as successful ones by rewriting their goals a posteriori. This significantly accelerates learning, allowing us to extract hundreds of new (true) conjectures, and their proofs, even from failed proof searches. In this way, even unprovable conjectures can be highly useful for the agent\u2019s learning. We evaluate our system on three axiomatic mathematical domains\u2014propositional logic, natural number arithmetic, and group theory\u2014showing both that agents self-improve successfully in proving theorems in all domains. We also find that they improve at an extrinsic evaluation of theorems (from a classical textbook on logic [20], and the Lean Natural Number Game [6]), not given in training. In summary, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce a method for conjecturing using LMs that generates valid conjectures by construction in an arbitrary theory, using constrained decoding and type-directed synthesis.   \n\u2022 We define a hindsight relabeling method that simultaneously generates training data for conjecturing and theorem proving.   \n\u2022 We combine these methods in a learning loop where a mathematical agent can self-improve in a given formal domain given only the axioms.   \n\u2022 We evaluate agents trained on axioms for propositional logic, group theory and arithmetic, showing that they improve in both intrinsic and extrinsic evaluations. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work is primarily related to prior work on mathematical conjecturing, learning to prove theorems, and on intrinsic motivation in Reinforcement Learning. To the best of our knowledge, our work is the first attempt to unify insights from these areas for training mathematical reasoning agents. ", "page_idx": 2}, {"type": "text", "text": "Mathematical conjecturing. The task of discovering mathematical facts was the subject of the influential Automated Mathematician (AM), developed by Lenat in the 1970s [23]. AM was able to conjecture several known mathematical facts and concepts (such as the definition of prime numbers, and the unique factorization theorem). Unlike our system, AM did not aim at proving the conjectures it formulated \u2014 instead, it proposed and judged them based on a set of principles and on empirical evidence collected by AM itself. More recently, other works have revisited the idea of generating conjectures by training language models on human-written theorem statements [39, 41]. Unlike our approach, this relies on pre-training data, and does not readily extend to conjecturing in new domains. ", "page_idx": 2}, {"type": "text", "text": "Learning to prove theorems from human data. A large body of recent work has used Large Language Models to guide formal theorem proving in a number of proof assistants, such as Lean [22], Isabelle [22, 18] and Metamath [31, 42]. Typically, these systems pre-train an LLM on large-scale Internet corpora and fine-tune on human-written formal mathematical proofs. Work on scaling laws for LLMs has shown that they generally improve when their parameter count and dataset size both increase in similar proportion. But the scarcity of formal proofs for training creates a challenge for this approach for learning formal theorem proving: even the largest datasets to date, such as the ProofPile, which aggregates libraries from 5 proof assistants, form relatively small datasets (e.g., 500MB of formal proofs on the ProofPile, contrasting to terabytes of Python code on Github). ", "page_idx": 2}, {"type": "text", "text": "Learning to prove theorems from synthetic data. One recent success in automated mathematical reasoning was AlphaGeometry [38], which was highly effective in solving olympiad-level geometry problems. AlphaGeometry, like our method, was trained entirely on synthetic data. Crucial to its approach is a method for generating both problems and solutions using only the axioms of geometry and a domain-specific deductive closure solver. This allows AlphaGeometry to synthesize and train on hundreds of millions of problems: many orders of magnitude more than existing human-created datasets of mathematical problems. Our approach shares the goal of AlphaGeometry of only using data derived from the axioms of the domain, with the difference that our method (a) is agnostic to the underlying axiomatic system and (b) does not rely on a domain-specific solver. Another line of work, including TacticZero [43] and rlCoP [19], has explored learning to prove theorems from reinforcement learning only, in a tabula rasa fashion, but still using a human-written set of problems for training (and manually-engineered features, in the case of rlCoP). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Intrinsic motivation We leverage inspiration from the literature on training reinforcement learning agents with intrinsic motivation objectives, allowing an agent to learn without pre-specified goals [26, 34, 33, 13, 27, 7, 3, 37]. Our setup is conceptually close to AMIGO [7], where agents attempt to generate challenging but achievable next goals. While AMIGO was demonstrated in a simple grid-world environment with a simple goal structure (any point in the grid gives a valid goal), we operate on a much richer setting, where the space of goals is unbounded \u2014 all conjectures in a formal mathematical theory. To sample conjectures, we use Synchromesh\u2019s Constrained Semantic Decoding algorithm [29], and guide it with type constraints. ", "page_idx": 3}, {"type": "text", "text": "Self-improvement of language models A significant line of recent work has explored the idea that LLMs can \u201cself-improve\u201d: increase performance on a given task by fine-tuning on their own generated reasoning traces, which are selected by some criterion that ensures their quality. STaR [45] fine-tuned on reasoning traces that reached the correct answer on mathematical and multiple choice questions; LMSI [16] was able to obtain self-improvement on a question-only dataset, sampling multiple rationales and training on those that agree with the majority answer. More related to our work, but less explored, is the direction of having LLMs also generate their own problems for training: this has been explored for self-improving on solving programming problems [14, 37], where code execution provides signal about correctness. In MINIMO, since our problems are formal conjectures and our solutions are formal proofs, we can guarantee correctness in a much stronger form than self-generated test cases. ", "page_idx": 3}, {"type": "text", "text": "3 MINIMO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Most recent work on AI for mathematical reasoning assumes a target set of problems to be solved. We deviate from this paradigm by having the agent itself propose problems for it to try to solve and learn from. Our goal is to target increasingly harder problems in a given mathematical domain, where the domain is specified as a set of axioms given in dependent type theory. ", "page_idx": 3}, {"type": "text", "text": "Our agent is represented by a language model, which we will use to encode (a) a proof search policy $\\pi_{\\theta}(a|s)$ , (b) a value function $V_{\\pi}(s)$ , and (c) a difficulty-conditioned conjecturer $P_{\\theta}(c\\mid d)$ , where $d$ is a discretized measure of difficulty and $c$ is a mathematical statement (a string). MINIMO consists of training both components in a loop that alternates between generating conjectures, trying to target hard but provable ones, and doing proof search, as we depict in Figure 1. As we describe in this section, proof search generates training data both for the conjecturer and the prover components \u2014 training on that data thus yields a self-improvement loop as the agent interacts with the environment. We now describe the first step in this loop: generating conjectures. ", "page_idx": 3}, {"type": "text", "text": "3.1 Conjecturing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim to sample conjectures from a language model, conditioned on a target difficulty. By construction, an autoregressive LM gives a distribution over all strings. But if the LM does not have prior knowledge about the domain, it is unlikely to put non-negligible probability mass on valid mathematical statements. We now address the challenge of sampling valid conjectures. ", "page_idx": 3}, {"type": "text", "text": "To that end, our main insight is to leverage constrained decoding to obtain valid conjectures by construction. Our method will turn any language model \u2014 including a randomly initialized one, which is what we start with \u2014 into a probability distribution over strings that represent well-formed conjectures in dependent type theory over a given set of axioms. Ultimately, we will also train the LM to generate conjectures given a target difficulty. We use this ability to attempt to generate increasingly difficult problems for training, according to the agent\u2019s current ability to prove theorems. ", "page_idx": 3}, {"type": "text", "text": "To reason about constraining the LM\u2019s outputs, we leverage the abstraction of a completion engine, first introduced in the context of code generation with language models [29]. Assuming $\\mathcal{C}$ is the set of all valid conjectures, a completion engine will allow us to use any LM to sample strings from $\\mathcal{C}$ in an autoregressive fashion. Mathematically, $\\mathcal{C}$ is a function $f_{\\mathcal{C}}:\\Sigma^{*}\\to\\mathcal{P}(\\Sigma^{*})$ , taking a string $s\\in\\Sigma^{*}$ and returning a set of strings $f_{\\mathcal{C}}(s)^{1}$ . Intuitively, we will sample conjectures from our LM by constraining it to strings that can be generated with iterated calls to $f_{\\mathcal{C}}$ . Concretely, we will start with $s=\\\"\"$ , and query $f_{\\mathcal{C}}\\bar{(}s)$ to obtain the set of valid ways to begin to state a conjecture. After we choose one of those $s_{1}\\in f_{\\mathcal{C}}(s)$ , we can then query $f_{\\mathcal{C}}(s_{1})$ to obtain the valid ways to proceed, and repeat until we have a complete conjecture. Our main challenge here is to construct a suitable $f_{\\mathcal{C}}$ that it is sound (all conjectures obtained by this procedure are valid) and complete (all valid conjectures can be obtained by this procedure). After we define $f_{\\mathcal{C}}$ , we can sample from any LM while guaranteeing that the output will belong to $\\mathcal{C}$ by using the Constrained Semantic Decoding (CSD) algorithm [29]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To construct $f_{\\mathcal{C}}$ , we analyze the minimal formulation of dependent type theory backing Peano [28] \u2013 essentially the classical Calculus of Constructions (CoC) [12]. In the CoC, mathematical propositions are constructions of type prop. Since generating a proposition might involve generating objects of arbitrary other types, depending on the axioms of the given domain, we will define a more general family of completion engines, $f_{t}$ , which will constrain the LM to sample an object of an arbitrary type $t$ . At the end, we obtain $f_{\\mathcal{C}}=f_{p r o p}$ . ", "page_idx": 4}, {"type": "text", "text": "To sample an object of type $t$ , we leverage the simple grammar of terms in Peano [28] to guide a recursive type-directed synthesis algorithm. Syntactically, terms in Peano are defined by the grammar $e:={\\mathrm{type~}}{\\bar{|}}{\\mathrm{~prop~}}|\\ x\\mid({\\bar{e}}\\ e)\\mid$ (lambda $x:e,e)$ $\\mid[(x:e)\\to e]$ . The first two production rules give the names of two built-in base types, type and prop. We then have variables, function application, lambda functions, and function types (denoted in square brackets). As conventional in type theory, let $\\Gamma$ be our typing context: a sequence of pairs of names and their types. For example, we might have $\\Gamma=[n a t:t y p e,z:n a t,s u c c:[n a t\\rightarrow n a t]]$ , a context with three objects: a type nat, an object $_\\textrm{z}$ having that type, and a function succ from nat to nat. Given a context, to obtain an object of type $t$ , it suffices to consider the formation rules in $\\mathrm{CoC}$ to guide generation: ", "page_idx": 4}, {"type": "text", "text": "\u2022 If our target type is $t={\\sf t y p e}$ , we can generate either one of the names in $\\Gamma$ having type $t={\\sf t y p e}$ (e.g., nat, for the example context above), or a function type.   \n\u2022 If our target type is $t={\\tt p r o p}$ , we can generate either one of the objects in $\\Gamma$ having type prop, or a function type where the output type has type prop.   \n\u2022 If our target type is a function type, we must start by generating a square bracket; then, we (recursively) iteratively generate a type for the next parameter, or, if we already have at least one parameter, we can also generate the final output type.   \n\u2022 An object of any type $t$ can be formed by function application of a function $f$ chosen from $\\Gamma$ , provided that the output type of $f$ can be unified with $t$ . ", "page_idx": 4}, {"type": "text", "text": "These rules allow us to determine the possible valid tokens at any point during generation. Besides the base types type and prop, objects of all other types are either in $\\Gamma$ or the result of a function application. We use a recursive descent parser to parse the (incomplete) term we have so far (as originally done in [29]), and compute the target type $t$ at the current point in generation. Then, we enumerate the possible next tokens for the LM by using the rules above, return the union of the sets of choices allowed by each rules as the output of $f_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Note that this is a general procedure for searching for objects of any given type $t$ (i.e., inhabitants of that type). This is undecidable in general (theorem proving is a special case of type inhabitation), so this procedure might not terminate. Therefore, we set a maximum number of tokens $K$ (150, in our experiments), and ignore samples where the LM fails to generate a conjecture after $K$ tokens. In practice, we find the rejection rate for generating propositions to be low, $<10\\%$ of the generations. ", "page_idx": 4}, {"type": "text", "text": "The above procedure forces the LM to generate a valid conjecture, but the LM still assigns a distribution over those. During training, we also aim to generate conjectures that are provable, but hard to prove. The signal on both success and difficulty is generated by running proof search (as we describe next) and, in cases where a proof is found, measuring it\u2019s log-likelihood under the current policy, which correlates with how many iterations MCTS takes to find the proof (see Appendix). ", "page_idx": 4}, {"type": "text", "text": "3.2 Proof Search ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having a conjecture represented by a target type $t$ , we then perform proof search using Monte Carlo Tree Search (MCTS; [21], [42]), guided by a learned policy $\\pi_{\\theta}$ and value function $V_{\\theta}$ . We represent both $\\pi_{\\theta}$ and $V_{\\theta}$ using the same underlying language model that we use for conjecturing. We use Peano [28] as the environment for proof search. Peano exposes a finite action space, so we don\u2019t need to generate actions using $\\pi_{\\theta}$ \u2014 it suffices to be able to evaluate them. More precisely, at a given state $s$ where we have actions $a_{i}^{(s)}$ available, we compute the distribution $\\pi_{\\theta}(a_{i}^{(s)}|s)$ by evaluating the likelihood of each $a_{i}^{(s)}$ as the completion to a string of the form \"STATE: $\\mathbb{\\left<\\mathbf{s}\\mathbf{\\scriptscriptstyle{\\ge}}\\right.}$ ; POLICY:\". We read out the value of a state in a similar way \u2014 by considering the likelihood of 1 or 0 as being the next token following \"STATE: $\\leqslant\\mathbf{s}\\gg$ ; VALUE:\". In both cases, the probability of the next token is normalized over only the choices that can lead to a valid completion. ", "page_idx": 5}, {"type": "text", "text": "States and actions in Peano are similar to several other interactive theorem provers, such as Lean and Coq. The state consists of a set of typed objects, along with a set of open proof goals (which are types). Objects in the state whose type is a proposition type are interpreted as evidence for that proposition (either a proof or an assumption). Actions might add new objects to the state (e.g., take the successor of one existing natural number, or use an equality to rewrite an existing proposition into a new one), change the goal (by backward chaining), or both (e.g., if the goal is to prove $A\\rightarrow B$ , which is used to represent both logical implication and universal quantification, the intro action will add object of type $A$ to the state and change the goal to $B$ ). ", "page_idx": 5}, {"type": "text", "text": "When proof search succeeds, we can extract examples to train both the policy and the value function. For the policy, we simply extract the actions that lead to the proof as language modeling examples, using the same format we use to query $\\pi_{\\theta}$ . For the value function, we take the states in the subtree that complete the proof as examples with value 1, and a random set of other states as examples with value 0 for training. When proof search fails, however, this na\u00efve procedure does not extract any training examples. This happens often at the beginning, since our model initially generates a large number of conjectures it cannot prove (either because they are false, or because na\u00efve proof search times out). But forward actions in the proof search tree often construct proofs for other propositions, even if they are irrelevant for proving the original goal. We levarage this fact for generating training data by hindsight relabeling, as we describe next. ", "page_idx": 5}, {"type": "text", "text": "3.3 Hindsight Relabeling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Even a conjecture that fails to be proven can be highly informative about the domain. During proof search, forward actions that apply functions whose result type is a proposition type (e.g., concluding A from (and A B)) produce proofs, even when those proofs might not be useful for proving the original conjecture. In Reinforcement Learning, the well-known method of Hindsight Experience Replay [1] extracts training data for the policy from such failed trajectories by relabeling the trajectories with goals that were in fact achieved, as opposed to the original goal. For those alternative goals, the trajectory then represents a successful sequence of actions. We apply this idea to extract training examples for both the policy and value functions from proof search trees, by picking nodes after forward actions that produced a proof, and walking upwards in the tree until we find a backward action (since those change the goal). That path then becomes a successful trajectory after we relabel the goal. Two important steps to improve data quality are (1) we clean up the solutions by eliminating steps irrelevant for the proof of the new goal, and (2) we only add proofs of goals never seen before, to avoid oversampling trivial facts that are rediscovered extremely often (such as $0=0$ ). ", "page_idx": 5}, {"type": "text", "text": "We go one step further and observe that hindsight relabeling can also be useful for training the conjecturer. Concretely, the procedure we described above produces a set of proofs $p_{i}$ for relabeled statements $g_{i}$ . All of these statements are therefore true in the mathematical domain, and we use them as examples of true conjectures. As a concrete example, in arithmetic, the agent will often conjecture simple expressions such as $2+1=0$ . Most equalities generated at random will be false. However, by applying the Peano Axioms and facts about equality in succession, the agent eventually finds a proof of $2+1=3$ . Evaluating the likelihood of the proof under $\\pi_{\\theta}$ gives a measure of the difficulty of this alternative statement for the current policy. This insight allows our agent to learn about hundreds of new unique, true statements in each proof search. As our experiments show, we find hindsight relabeling to be crucial for enabling the agent to steadily conjecture harder statements that it is able to prove. ", "page_idx": 5}, {"type": "image", "img_path": "uNKlTQ8mBD/tmp/cbb11c4e97ab8c805e39fe618b5f74685411e8ad8f1a10e5fc10baab5d53bfbb.jpg", "img_caption": ["Figure 2: Difficulty of proved conjectures found in each iteration of MINIMO, evaluate as the logprobability of the proof under the policy checkpoints after each iteration of the training loop (with standard error bands for runs with 3 random seeds). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.4 Training loop ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we tie all components together by alternating between (a) generating a batch of conjectures, (b) running proof search on them, (c) extracting training examples from the search trees, and finally (d) training the underlying LM using the standard cross-entropy objective. When a proof is found, either directly or by hindsight relabeling, we first compute a continuous difficulty metric of the statement by taking the log-likelihood of the proof under the current policy. To discretize difficulties, we then consider difficulty percentiles of the last batch of conjectures: we take the $10\\%$ least likely proofs to be associated with \u201chard\u201d conjectures, the $50\\%$ most likely to be considered \u201ctrivial\u201d, and the remaining are taken as \u201ceasy\u201d. When put together, these components form a self-improving loop that starts only from the axioms of the given mathematical domain, as our experiments show. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now evaluate MINIMO2 on three mathematical domains. We experiment with axiomatic systems for (a) propositional logic, (b) arithmetic, and (c) abstract groups. All the axioms are given in the Appendix. We then train agents over 5 iterations of conjecturing and theorem proving, generating 200 conjectures in each batch, running MCTS for proof search with 1000 expansions, and evaluate our agents guided by the following research questions: ", "page_idx": 6}, {"type": "text", "text": "RQ1: Can our conjecturing method generate increasingly harder conjectures as training progresses? RQ2: Do agents improve at theorem proving as they train on their own generated problems? RQ3: Is hindsight relabeling effective at improving conjecturing and theorem proving? RQ4: Do our agents improve at proving an externally given set of human-selected theorems, even if these are not given during training? ", "page_idx": 6}, {"type": "text", "text": "The first three questions require intrinsic evaluations \u2014 they ask whether the learning dynamics of agents trained with MINIMO matches the desiderata of self-improving at both conjecturing and theorem proving while given only the axioms. The last question is an extrinsic assessment of what our agents learn \u2014 we evaluate whether the learning progresses in a way that aligns with an external criteria \u2014 namely, the proficiency of the agent at proving theorems from human sources (a textbook and a Lean game). ", "page_idx": 6}, {"type": "text", "text": "4.1 Learning dynamics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first investigate RQ1 and RQ2. Figure 2 shows the progression of difficulty across 5 iterations of the MINIMO learning loop (first conjecturing, then running proof search, and training on collected examples). Here, we evaluate the average log-likelihood (y-axis) of conjectures proven at each conjecturing iteration $\\mathbf{\\dot{X}}$ -axis) under the policy after each iteration (line color). Policy 0 is the initial (randomly initialized) policy, while subsequent policies were trained on the examples obtained during proof search, with hindsight relabeling, up to the previous iteration. Lower log-likelihood generally means harder conjectures (i.e., they tend to take more search iterations, see Appendix). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Difficulty increases as training progresses (RQ1). Fixing an early policy, the log-likelihood of proofs under that policy steadily decreases across training iterations. This is reflected in the negative slope of difficulty across iterations when the policy is fixed. In particular, the policy at iteration 0 serves as a na\u00efve search baseline, since it is essentially uniform. We observe that, as training progresses, this policy struggles increasingly more with each new batch of conjectures. The same pattern repeats for subsequent policies when we consider conjectures generated in future iterations, giving a generally downward trend in log-likelihood of the solutions for any given policy, showing that conjectures get increasingly more challenging. This provides positive evidence for answering RQ1: in all 3 domains, the conjecturer is able to increasingly propose harder provable conjectures as training progresses. ", "page_idx": 7}, {"type": "text", "text": "Proof policy improves as training progresses (RQ2). At each iteration, we sample a new set of unseen conjectures for training. From Figure 2, we see that later policies generally perform better than earlier ones, at a fixed conjecture iteration. This reflects the fact that each new policy assigns higher likelihood to the proofs, even for unseen conjectures at each iteration. For example, after training on the data generated from the first iteration, the policy on iteration 1 has higher log-likelihood for the proofs to the new conjectures found at iteration 1, when compared to the initial policy from iteration 0. This pattern repeats at each iteration, showing that the prover is also progressing and generalizing to unseen problems, though with diminishing gains in the final iterations. This suggests a positive answer to our second research question: our agents effectively self-improve in their ability to prove new statements. ", "page_idx": 7}, {"type": "text", "text": "Hindsight relabeling is necessary for joint self-improvement (RQ3). The results so far all used hindsight relabeling on all proof searches\u2014successful or not\u2014to extract training data for the policy and value function, as well as conjecturing. To evaluate whether our agents would still show the same continuous self-improvement regardless of the data efficiency gains from hindsight relabeling, Figure 3 compares agents trained with and without hindsight relabeling across 5 iterations over the same 3 axiomatic domains. Here, we look at the ability of the agent to achieve the goal of constantly proposing provable but challenging conjectures for itself. Ideally, the difficulty of conjectures should not fluctuate significantly during the course of training: we would like the agent to always find new challenging conjectures that it nevertheless still proves. We find that, in all 3 domains, the agent fails to achieve this goal when not trained with hindsight relabeling. Instead, as it trains on its own proofs, the agent\u2019s conjectures fail to remain challenging\u2014all provable conjectures end up with high log-likelihood as training progresses, and the conjecturer is unable to leave that regime. We attribute this to the volume of signal that the conjecturer receives: at each initial batch, only around $10\u201320\\%$ of the conjectures are proven. When not using hindsight relabeling, the only feedback that the conjecturer recevies is that proof search timed out on these statements. On the other hand, with hindsight relabeling, even these failures lead the conjecturer to observe hundreds of actual true statements in each domain (along with their proofs), leading to better learning dynamics. This provides positive evidence for RQ3: hindsight relabeling significantly helps the agent to jointly improve in conjecturing and theorem proving\u2014without it, training tends to collapse to by proposing only easy conjectures. ", "page_idx": 7}, {"type": "text", "text": "4.2 Proving human-written theorems (RQ4) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finally, we evaluate whether our agent, trained only on problems that it proposes to itself, also improves in solving problems that are of interest to humans. Since our agent does not grow its library of theorems over time, starting every new proof from the axioms, a meaningful evaluation requires theorems that can be reasonably proven straight from the axioms, without lemmas. We thus use two human-written sources of theorems meeting this criterion, for the domains of propositional logic and arithmetic. For logic, we take the set of 35 theorems of propositional logic from Stephen Kleene\u2019s textbook \u201cIntroduction to Metamathematics\u201d [20]. Precisely, in Theorem 41, Kleene states (and proves a subset of) 35 useful statements of Propositional Logic (such as contraposition rules, commutativity and transitivity laws of logical connectives, and properties of double negation). For arithmetic, we use the Natural Number Game [6], a popular game used to introduce formal mathematics in the Lean theorem prover. We take levels of the game that are (a) theorems about natural numbers, and (b) do not refer to previous lemmas, only the axioms; this results in 10 levels spanning the Tutorial, Addition, and Multiplication worlds. We translate the statements into Peano, and evaluate our agents on their success rate on those problems within 2000 MCTS expansions. ", "page_idx": 7}, {"type": "image", "img_path": "uNKlTQ8mBD/tmp/3f95f1f62a750458f2e8326da4b2dfbd4e150d5e4c7a102ce40cca1e44dbbb2e.jpg", "img_caption": ["Figure 3: Difficulty of proved conjectures proposed in each iteration under the current policy at that same iteration, comparing when using and not using hindsight relabeling to generate new proofs and conjectures, with standard error bands for runs with 3 random seeds. Ideal behavior would be a flat line, representing constant relative difficulty. Hindsight significantly helps the agent conjecture propose harder problems. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 4 shows the results. We find that, as our agents train on their self-generated problems, they steadily become more successful at proving theorems from both Kleene\u2019s book and the Natural Number Game. This happens even though these theorems are not targeted during training, since our agent only uses its own conjectures. Four theorems in Propositional Logic are only proved after the last iteration of training: commutativity and transitivity of the \u201cif and only if\u201d logical connector, a law connecting double negation and implication $\\operatorname{\\neg}\\neg(A\\,\\Rightarrow\\,B)$ , $\\neg\\neg A\\;\\vdash\\;\\neg\\neg B)$ , and the \u201ccurrying law\u201d of the conjunction \u2014 $(A\\land B)\\Rightarrow{\\dot{C}}\\vdash{\\dot{A}}\\Rightarrow(B\\Rightarrow C)$ . In the Natural Number game, only the final agent proves $\\forall n,\\forall m,s u c c(n)+m=s u c c(n+m)$ , a theorem requiring induction on the correct variable ${(\\mathrm{m})}$ and a non-trivial sequence of rewrites in the inductive case (we include the full proof in the Appendix). While admitedly small scale, these results suggest a positive answer to our last research question: ", "page_idx": 8}, {"type": "image", "img_path": "uNKlTQ8mBD/tmp/fb32fd1145cb8019e79ca646e25e16e22b1528b95d9a8eff46e2ca4cf66c3d11.jpg", "img_caption": ["", "Figure 4: Success rate of our agents at proving theorems from the \u201cIntroduction to Metamathematics\u201d textbook and the Natural Number Game. As agents train on their own conjectures, they also improve at solving problems from these two human-written sources. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "agents trained on their own conjectures can also improve at solving human-written problems, which are not given during training. ", "page_idx": 8}, {"type": "text", "text": "5 Limitations and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present MINIMO: an approach to training agents for formal mathematical reasoning starting from only the axioms of a given domain. The agent jointly learns to propose challenging conjectures and to prove them. Our experiments show evidence of MINIMO improving its performance across training iterations. In the Groups domain, the average proof length it finds on generated conjectures (i.e., not found by hindsight) increased from 2.67 steps in the first iteration, when the model is randomly initialized, to 5 steps by iteration 4, with proofs for \u2018hard\u2019 conjectures growing from 3.67 to 6.10 steps. The longest proofs found grew from 4 steps to 9 steps from the first to last iteration. Similar trends appear in Propositional Logic (average length from 2.75 to 4.21 steps, longest proofs from 5 to 11 steps) and Arithmetic (average from 2.36 to 3.35 steps, longest from 4 to 7 steps). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "However, MINIMO currently has two crucial limitations that prevent it from (a) discovering deep mathematical theories, and (b) scaling up to large theories. First, even if the agent\u2019s policy improves, its library remains fixed to the definitions and axioms that it starts with. Proofs that do not use lemmas (cut-free proofs in logic) can be exponentially longer than equivalent ones that do, and thus quickly grow beyond the reach of search. With this constraint, our agent most often finds harder conjectures by making the statements longer and more complicated. For example, in Groups, early conjectures include trivial statements like $e=e$ ; by the last iteration, the conjecture requiring the longest proof reads as $\\forall g\\in G$ , if $e=(g^{-1})^{2}$ then $e^{\\bar{2}}=e(e(e((g^{-1})^{2})))$ (proved in 9 steps). In contrast, human mathematicians develop deep theories by accumulating results and definitions along the way, in such a way that even very high-level results can be described succinctly at the right level of abstraction. Understanding how to bootstrap such cumulative learning in an agent (e.g., exploring notions of usefulness or interestingness, several of which have been posited [4, 11]) will be a key direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Another bottleneck in our current setup is that a large library can cause the current action enumeration algorithm in Peano to become prohibitively slow (a finite action space can still become intractable). A method that scales unboundedly should incorporate some form of premise selection [17]. In past work, premise selection has either been based on symbolic heuristics or in learning useful premises from human-written proofs. We believe that developing a premise selection method that bootstraps together with the other learned components will be as important as understanding how to grow the agent\u2019s library. ", "page_idx": 9}, {"type": "text", "text": "Together, lifting these limitations from our method might lead to a completely compute-bound, self-improving agent for formal mathematics capable of discovering deep mathematical theories starting only from basic axioms \u2014 the rules of the game of mathematics. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Daniel Selsam and Laetitia Teodorescu for insightful discussions on preliminary ideas related to this work, as well as the anonymous reviewers for their helpful feedback. This work was supported by the NSF Expeditions Grant with Award Number (FAIN) 1918771, and by NSF grant #2302701. GP was also supported by the Stanford Interdisciplinary Graduate Fellowship (SIGF). This work was also partially supported by the Wallenberg Al, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.   \n[2] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.   \n[3] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.   \n[4] Yoshua Bengio and Nikolay Malkin. Machine learning and information theory concepts towards an ai mathematician. arXiv preprint arXiv:2403.04571, 2024.   \n[5] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.   \n[6] Kevin Buzzard. Natural number game. https://www.ma.imperial.ac.uk/\\~buzzard/ xena/natural_number_game/, 2019. [7] Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.   \n[8] Nuttapong Chentanez, Andrew Barto, and Satinder Singh. Intrinsically motivated reinforcement learning. Advances in neural information processing systems, 17, 2004. [9] Joshua Clune, Yicheng Qian, Alexander Bentkamp, and Jeremy Avigad. Duper: A proofproducing superposition theorem prover for dependent type theory. In International Conference on Interactive Theorem Proving, 2024.   \n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[11] Simon Colton, Alan Bundy, and Toby Walsh. On the notion of interestingness in automated mathematical discovery. International Journal of Human-Computer Studies, 53(3):351\u2013375, 2000.   \n[12] Thierry Coquand and G\u00e9rard Huet. The calculus of constructions. PhD thesis, INRIA, 1986.   \n[13] Nick Haber, Damian Mrowca, Stephanie Wang, Li F Fei-Fei, and Daniel L Yamins. Learning to play with intrinsically-motivated, self-aware agents. Advances in neural information processing systems, 31, 2018.   \n[14] Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations, 2023.   \n[15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[16] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1051\u20131068, Singapore, December 2023. Association for Computational Linguistics.   \n[17] Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas E\u00e9n, Fran\u00e7ois Chollet, and Josef Urban. Deepmath-deep sequence models for premise selection. Advances in neural information processing systems, 29, 2016.   \n[18] Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timoth\u00e9e Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.   \n[19] Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and Miroslav Ol\u0161\u00e1k. Reinforcement learning of theorem proving. Advances in Neural Information Processing Systems, 31, 2018.   \n[20] Stephen Cole Kleene. Introduction to metamathematics. Van Nostrand, 1952.   \n[21] Levente Kocsis and Csaba Szepesv\u00e1ri. Bandit based monte-carlo planning. In European conference on machine learning, pages 282\u2013293. Springer, 2006.   \n[22] Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. Advances in neural information processing systems, 35:26337\u201326349, 2022.   \n[23] Douglas Bruce Lenat. AM: an artificial intelligence approach to discovery in mathematics as heuristic search. Stanford University, 1976.   \n[24] David McAllester. Mathzero, the classification problem, and set-theoretic type theory. arXiv preprint arXiv:2005.05512, 2020.   \n[25] Allen Newell and Herbert Simon. The logic theory machine\u2013a complex information processing system. IRE Transactions on information theory, 2(3):61\u201379, 1956.   \n[26] Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265\u2013 286, 2007.   \n[27] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u2013 2787. PMLR, 2017.   \n[28] Gabriel Poesia and Noah D Goodman. Peano: learning formal mathematical reasoning. Philosophical Transactions of the Royal Society A, 381(2251):20220044, 2023.   \n[29] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations, 2021.   \n[30] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344, 2022.   \n[31] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.   \n[32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821\u20138831. Pmlr, 2021.   \n[33] J\u00fcrgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE transactions on autonomous mental development, 2(3):230\u2013247, 2010.   \n[34] J\u00fcrgen Schmidhuber. Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Frontiers in psychology, 4:46492, 2013.   \n[35] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \u017d\u00eddek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction using potentials from deep learning. Nature, 577(7792):706\u2013710, 2020.   \n[36] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.   \n[37] Laetitia Teodorescu, C\u00e9dric Colas, Matthew Bowers, Thomas Carta, and Pierre-Yves Oudeyer. Codeplay: Autotelic learning through collaborative self-play in programming environments. In IMOL 2023-Intrinsically Motivated Open-ended Learning workshop at NeurIPS 2023, 2023.   \n[38] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476\u2013482, 2024.   \n[39] Josef Urban and Jan Jakubu\u02d8v. First neural conjecturing datasets and experiments. In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 26\u201331, 2020, Proceedings 13, pages 315\u2013323. Springer, 2020.   \n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[41] Mingzhe Wang and Jia Deng. Learning to prove theorems by learning to generate theorems. Advances in Neural Information Processing Systems, 33:18146\u201318157, 2020.   \n[42] Daniel Whalen. Holophrasm: a neural automated theorem prover for higher-order logic. arXiv preprint arXiv:1608.02644, 2016.   \n[43] Minchao Wu, Michael Norrish, Christian Walder, and Amir Dezfouli. Tacticzero: Learning to prove theorems from scratch with deep reinforcement learning. Advances in Neural Information Processing Systems, 34:9330\u20139342, 2021.   \n[44] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36, 2023.   \n[45] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.   \n[46] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Training details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We represent our agents with GPT-2-style character-level Transformer models totalling approximately 8.45M parameters each (8 layers, 8 attention heads, hidden size 512, feed-forward size 2048, vocabulary size 128, absolute positional embeddings, with maximum context size of 1024). After each training iteration, we train the LM for a fixed number of 2000 steps of the AdamW optimizer (learning rate of $1e-4)$ with a dynamic batch size of most 10000 characters (random examples are added until their padded sequence lengths add up to more than 10000). We found these parameters to generally lead to stable training across all runs, without divergence, and 2000 steps was enough to bring each iteration to convergence in training loss. ", "page_idx": 13}, {"type": "text", "text": "Our training runs (5 iterations of generating and proving 200 conjectures in each) were done on 2 machines with 5 NVIDIA A40 40GB GPUs each. Each run took from 8 to 16h on a single GPU, totalling 288 GPU hours for the runs underlying our main results. ", "page_idx": 13}, {"type": "text", "text": "B Proof log-likelihood and MCTS Expansions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Throughout the paper, we used the likelihood of the proof under the policy as a measure of difficulty. Figure 5 shows that this quantity is linked to the number of iterations that MCTS takes to find the proof. This allows us to estimate difficulty easily without running search for theorems that we have proofs for (such as those we find by hindsight relabeling). ", "page_idx": 13}, {"type": "image", "img_path": "uNKlTQ8mBD/tmp/f6682776c5c0274bc78a68c142023e949fc569612ce18528e98de50f30fc795c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 5: Relation between MCTS iterations until the proof is found, vs log-likelihood of the proof that is found. The higher the likelihood, the faster MCTS is in finding the proof. ", "page_idx": 13}, {"type": "text", "text": "C Fraction of provable conjectures across iterations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our conjecturing procedure steers to LM to generate \u201chard\u201d, where that means they can still be proven by the current agent. Figure 6 shows how the ratio of proven conjectures evolves across training. In Groups and Propositional Logic, this ratio steadily increases both when and when not using hindsight. In Arithmetic, we observe that the ratio remains consistent up to iteration 3, but then decreases, whereas it increases sharply without hindsight. We note that this sharp increase is due to the conjecturer generating mostly trivial conjectures, by adding new combinations of assumptions that are not necessary for the proof (e.g., the most common conclusion for the conjecturer to generate without hindsight is often $0=0$ ). Even though the hard conjectures with hindisight often fail to be proven, they still seem to generate useful training data for the prover, as we see in our extrinsic evaluation with problems from the Natural Number Game. Thus, this analysis points out that ideal behavior for the conjecturer is unclear \u2014 it is still perhaps positive to generate false conjectures, as long as the process of proving them leads to learning progress (as seems to be the case in Arithmetic). ", "page_idx": 13}, {"type": "image", "img_path": "uNKlTQ8mBD/tmp/13d2fbe22084b73c470b92cf4af3a389be5f286b16a6515aaa07eb54de103a22.jpg", "img_caption": ["Figure 6: Ratio of proven conjectures in each batch of 200, at each iteration of training. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Axioms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We here provide the full Peano axiomatization of each domain. ", "page_idx": 14}, {"type": "text", "text": "D.1 Arithmetic ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our axiomatization of arithmetic includes the classical unary definition of the natural numbers (zero and successor function), axioms about addition, multiplication, and the principle of induction. ", "page_idx": 14}, {"type": "text", "text": "$=\\;:\\;\\;[(\\,\"t\\;\\;:\\;\\{\\mathrm{ype}\\}\\;\\;\\rightarrow\\;\\;\"t\\;\\;->\\;\\;^{\\prime}t\\;\\;->\\;\\;\\mathrm{prop}]\\;.$   \nnat : type.   \nz : nat.   \ns : [nat -> nat].   \n+ : [nat $->$ nat $->$ nat].   \n\\* : [nat -> nat $->$ nat].   \n$\\begin{array}{r l}&{+_{-}\\textbf{z}:\\,\\,[(\\mathfrak{s}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,\\mathfrak{s}\\,\\,(=\\,(+\\,\\mathfrak{n}_{\\mathfrak{n}}\\,\\mathfrak{z})\\,\\,\\mathfrak{n})]\\,.}\\\\ &{+_{-}\\textbf{s}:\\,\\,[(\\mathfrak{s}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,\\mathfrak{s}\\,\\,(\\,\\mathfrak{s}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,(+\\,\\mathfrak{n}_{\\mathfrak{n}}\\,\\mathfrak{(s}\\,\\mathfrak{n}))\\,\\,(\\textbf{s}_{\\mathfrak{n}}(+\\,\\mathfrak{n}\\,\\mathfrak{n})))]\\,.}\\\\ &{*_{-}\\textbf{z}:\\,\\,[(\\mathfrak{s}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,\\mathfrak{s}\\,\\,(*_{\\mathfrak{n}}\\,\\mathfrak{s}_{\\mathfrak{n}})\\,\\,\\mathfrak{z})]\\,.}\\\\ &{*_{-}\\textbf{s}:\\,\\,[(\\mathfrak{s}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,\\mathfrak{s}\\,\\,(\\,\\mathfrak{s}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,(*_{\\mathfrak{n}}\\,\\mathfrak{n}_{\\mathfrak{n}}(\\textbf{s}^{\\,\\prime}\\mathfrak{n}))\\,\\,(+\\,\\mathfrak{n}_{\\mathfrak{n}}\\,\\,(*_{\\mathfrak{n}}\\,\\mathfrak{n}))]\\,.}\\\\ &{\\mathrm{nat\\\\_ind}\\,\\,:\\,[(\\mathfrak{s}_{\\mathfrak{p}}:\\,[\\mathfrak{n}\\mathrm{at}\\,\\rightarrow\\,\\mathfrak{p}_{\\mathrm{rop}}])\\,\\,\\rightarrow\\,(\\,\\mathfrak{p}_{\\mathfrak{p}}\\,\\,\\mathfrak{z})\\,\\,-\\,[(\\,\\mathfrak{n}\\,\\mathfrak{}{\\lambda}_{\\mathfrak{n}}:\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,}\\\\ &{\\,\\,\\,\\,(\\,\\mathfrak{s}_{\\mathfrak{p}}\\,\\,\\mathfrak{n})\\,\\,-\\,(\\,\\mathfrak{p}_{\\mathfrak{p}}\\,\\,(\\textbf{s}^{\\,\\prime}\\mathfrak{n}))]\\,\\,-\\,\\,[(\\,\\mathfrak{n}\\,\\mathrm{~:~}\\,\\mathfrak{n}\\mathrm{at})\\,\\,-\\,(\\,\\mathfrak{p}\\,\\,\\mathfrak{n})]\\,.}\\end{array}$ #backward nat_ind.   \n#forward $+\\_z\\texttt{((+)n z)}:\\mathtt{n a t})$ .   \n#forward +_s ((+ \u2019n (s \u2019m)) : nat).   \n#forward \\*_z ((\\* \u2019n z) : nat).   \n#forward \\*_s ((\\* \u2019n (s \u2019m)) : nat). ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D.2 Propositional Logic ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "prop : type.   \nfalse : prop.   \n$/*$ Connectives \\*/   \nnot : [prop $->$ prop]. and : [prop $->$ prop $->$ prop].   \nor : [prop $->$ prop -> prop].   \niff : [prop $->$ prop $->$ prop]. $/*$ Introduction rule for conjunction \\*/   \n#backward and_i.   \nand_i : [(\u2019P : prop) -> (\u2019Q : prop) -> \u2019P -> \u2019Q -> (and \u2019P \u2019Q)]. $/*$ Elimination rules for conjunction \\*/   \n#forward and_el (\u2019_ : (and \u2019P \u2019Q)).   \nand_el : [(\u2019P : prop) -> (\u2019Q : prop) -> (and \u2019P \u2019Q) -> \u2019P]. #forward and_er (\u2019_ : (and \u2019P \u2019Q)).   \nand_er : [(\u2019P : prop) -> (\u2019Q : prop) -> (and \u2019P \u2019Q) -> \u2019Q]. $/*$ Introduction rules for disjunction \\*/   \n#backward or_il.   \nor_il : [(\u2019P : prop) -> (\u2019Q : prop) -> \u2019P -> (or \u2019P \u2019Q)]. #backward or_ir.   \nor_ir : [(\u2019P : prop) -> (\u2019Q : prop) -> \u2019Q -> (or \u2019P \u2019Q)]. $/*$ Elimination rule for disjunction \\*/   \n#backward or_e infer infer infer infer subgoal subgoal. or_e : [(\u2019P : prop) -> (\u2019Q : prop) -> (\u2019R : prop) -> (or \u2019P \u2019Q) -> [\u2019P -> \u2019R] -> [\u2019Q -> \u2019R] -> \u2019R]. $/*$ Introduction rule for negation \\*/   \n#backward not_i.   \n$\\mathrm{not_{-^{i}}\\;:\\;\\;[(\\,^{\\circ}P\\;:\\;p r o p)\\;\\;->\\;\\;[\\,^{\\circ}P\\;\\;->\\;\\;f a l s e]\\;\\;->\\;\\;(n o t\\;\\;^{\\circ}P)]}\\;.}$ $/*$ Elimination rule for negation \\*/   \nnot_e : [(\u2019P : prop) -> (not \u2019P) -> \u2019P -> false]. #backward exfalso.   \nexfalso : [false -> (\u2019P : prop) -> \u2019P]. /\\* Introduction rules for equivalence \\*/   \n#backward iff_i.   \niff_i : [(\u2019P : prop) -> (\u2019Q : prop) -> [\u2019P -> \u2019Q] -> [\u2019Q -> \u2019P] -> (iff \u2019P \u2019Q)]. $/*$ Elimination rules for equivalence \\*/   \n#forward iff_el (\u2019_ : (iff \u2019P \u2019Q)).   \niff_el $\\colon\\left[\\,\\left(\\,^{\\,\\circ}\\,\\mathrm{P}\\ :\\ \\mathrm{prop}\\right)\\ -\\ \\left(\\,^{\\,\\circ}\\,\\mathbb{Q}\\ :\\ \\mathrm{prop}\\right)\\ -\\ >\\ \\left(\\,\\mathrm{iff}\\ \\ ^{\\prime}\\ \\mathbb{P}\\ ^{\\ \\ \\circ}\\mathbb{Q}\\right)\\ ->\\ \\left[\\,^{\\,\\circ}\\,\\mathbb{P}\\ ->\\ ^{\\,\\circ}\\mathbb{Q}\\right]\\,\\right]\\,.$   \n#forward iff_er (\u2019_ : (iff \u2019P \u2019Q)).   \niff_er : [(\u2019P : prop) -> (\u2019Q : prop) -> (iff \u2019P \u2019Q) -> [\u2019Q -> \u2019P]]. $/*$ Excluded middle \\*/   \n#forward em.   \nem : [(\u2019P : prop) -> (or \u2019P (not \u2019P))]. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D.3 Groups ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n=\\;:\\;\\;[(\\,\"t\\;\\;:\\;\\{\\mathrm{ype}\\}\\;\\;\\rightarrow\\;\\;\"t\\;\\;->\\;\\;^{\\prime}t\\;\\;->\\;\\;\\mathrm{prop}]\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "G : type. ", "page_idx": 15}, {"type": "text", "text": "op : [G -> G -> G].   \nid : G. /\\* Associativity \\*/   \n#forward op_assoc ((op (op \u2019a \u2019b) \u2019c) : G).   \nop_assoc : [(\u2019a : G) -> (\u2019b : G) -> (\u2019c : G) -> (= (op (op \u2019a \u2019b) \u2019c) (op \u2019a (op \u2019b \u2019c)))]. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "$/*$ Commutativity \\*/ #forward op_comm ((op \u2019a \u2019b) : G). ", "page_idx": 15}, {"type": "text", "text": "/\\* Identity \\*/   \n#forward id_l.   \n$\\mathrm{id\\_~\\gimel~}:\\ \\ \\left[(\\mathbf{\\Lambda}^{\\prime}\\mathsf{a}\\ :\\ \\mathsf{G})\\ \\mathbf{\\Lambda}\\to\\ \\mathsf{\\Lambda}(=\\ (\\mathsf{o p\\ \\ i d\\ \\Lambda}^{,}\\mathsf{a})\\ \\ ^{,}\\mathsf{a})\\ \\right]\\,.$ /\\* Inverse \\*/   \ninv : $[{\\mathfrak{G}}~\\;->\\;{\\mathfrak{G}}]$ .   \n#forward inv_l.   \n$\\mathrm{{\\sf~inv}_{-}1~:~\\Delta[(\\,'a~:~\\Delta G)~\\bar{~}{}\\neg~>~(=~(o p~\\Delta(i n v~~^{\\prime}a)~~^{\\prime}a)~~i d)]}~.$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "E Extrinsic evaluation problems ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we list all problems used in the extrinsic evaluation in Arithmetic and Propositional Logic. ", "page_idx": 16}, {"type": "text", "text": "E.1 Arithmetic ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following are the 10 problems we extracted from from the Natural Number Game that (a) don\u2019t use previous lemmas, being reasonably provable straight from the axioms, (b) are about natural numbers (so, for example, this excludes the Proposition World, which is essentially about propositional logic). The prefix in these problems tell which \u201cworld\u201d of the game it came from: t stands for the Tutorial World, a is for the Addition World, and finally m is for the Multiplication World. ", "page_idx": 16}, {"type": "text", "text": "t_exampl $\\begin{array}{r l}&{\\mathfrak{z}1.\\quad[(\\mathfrak{x}_{{\\textbf{x}}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{p}_{{\\textbf{y}}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{p}_{{\\textbf{z}}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{c}_{{\\textbf{x}}}(+\\mathrm{~(**~x~y)~z)~}\\ (+\\ (*~*\\textbf{y})\\ ~z))]}\\\\ &{\\mathfrak{z}2.\\ \\ [(\\mathfrak{x}_{{\\textbf{}}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{p}_{{\\textbf{y}}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{p}_{{\\textbf{y}}}(+\\ \\mathfrak{p}_{{\\textbf{x}}}~\\mathrm{n7}))\\ \\to\\ (\\textbf{(*}(\\mathfrak{x}_{{\\textbf{2}}}~\\mathrm{>})\\ {\\textbf{y}})\\ (*~\\mathrm{n2}~(+\\ ~\\mathfrak{p}_{\\textbf{x}}~\\mathrm{n7})))]}\\\\ &{\\mathfrak{z}3.\\ \\ [(\\mathfrak{p}_{\\textbf{a}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{p}_{{\\textbf{b}}}:\\mathrm{~nat})\\ \\to(\\textbf{(=~(s~**~\\mathfrak{n})~}^{\\ *}~\\mathfrak{b}))\\ \\to\\ (\\textbf{(*}~(\\mathfrak{s}_{\\textbf{3}}~\\mathrm{)})\\ (\\mathrm{~*~\\mathfrak{b}}))]}\\\\ &{\\mathfrak{z}_{\\mathbf{c}_{\\textbf{-}}\\mathbf{zero}_{{\\textbf{0}}},\\ \\ [(\\mathfrak{r}_{{\\textbf{a}}}:\\mathrm{~nat})\\ \\to\\ (\\textbf{0}_{{\\textbf{c}}}(+\\ \\mathfrak{p}_{\\textbf{a}}(\\textbf{s}_{\\textbf{2}}))\\ (\\textbf{s}_{\\textbf{0}}^{\\ *}))]}\\\\ &{\\mathrm{id}.\\ \\ [(\\mathfrak{p}_{\\textbf{n}}:\\mathrm{~nat})\\ \\to\\ (\\textbf{(*}~\\mathrm{z}_{\\textbf{n}})~\\mathfrak{p}_{\\textbf{n}})]}\\\\ &{\\mathrm{soc.}\\ \\ [(\\mathfrak{r}_{\\textbf{a}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{p}_{{\\textbf{b}}}:\\mathrm{~nat})\\ \\to\\ (\\mathfrak{s}_{\\textbf{c}}:\\mathrm{~nat})\\ \\to\\ (\\textbf{(+}~(+\\ \\ '_{\\textbf{a}}~\\mathrm{\\gg})$ )]   \nt_exampl n7)))]   \nt_exampl   \nt_add_su   \na_zero_a   \na_add_as \u2019b \u2019c)))]   \na_succ_a   \na_succ_e   \nm_zero_m   \nm_mul_on ", "page_idx": 16}, {"type": "text", "text": "Notably, we give below the full proof our best agent for Arithmetic finds for a_succ_add: ", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "theorem a_succ_add $:\\mathrm{~\\small~\\left[\\{~\\mathfrak~{\\mu}^{\\prime}\\mathfrak{a}0~:~\\pi a t\\}~-~\\mathfrak{\\mu}^{\\prime}\\mathfrak{a}1~:~\\pi a t\\right)~\\mathfrak{\\mu}^{\\gamma}~}\\mathfrak{a}~\\mathrm{~\\small~\\left(\\!~\\mathfrak~{\\mu}^{\\prime}~\\mathfrak{a}~\\mathrm{~\\mu~(~\\mathfrak~{\\mu}~\"~\\mathfrak~{~a}0)~}~^{\\gamma}\\mathfrak{a}1\\right)~}\\mathrm{~\\small~\\left(\\!~\\mathfrak{\\mu}(\\mathfrak~{\\mu}^{\\prime}~\\mathfrak{a}0~:~\\pi a)~\\right)~}\\mathrm{~\\small~\\left.~\\left(\\!~\\mathfrak~{\\mu}^{\\prime}~\\mathfrak{a}~\\right)~\\right.~}\\,.$ intro x : nat. apply nat_ind. goal (= (+ (s x) z) (s (+ x z))) { show (= (+ (s x) z) (s x)) by +_z. show $\\left(=\\begin{array}{l}{(+\\texttt{x z})\\ \\texttt{x})}\\end{array}\\right$ by $+_{-2}$ . show $(=\\textbf{x}\\ (+\\textbf{x}\\,z)\\,)$ by eq_symm. show $\\tiny\\begin{array}{r l}{(=}&{{}(+\\tiny\\begin{array}{c c}{(\\textbf{s}\\textbf{x})}\\end{array}\\textbf{z})}\\end{array}$ (s (+ x z))) by rewrite. } goal $\\begin{array}{r l}&{\\mathrm{~[\\tiny~(\\textnormal{\\textsf{s}}~:~\\pi a t)~\\pi\\to~\\widehat{\\Sigma}~(=~\\pi(\\bullet~\\times~)~\\pi_n)~\\pi(s~\\pi(+~x~\\pi'n))~]~}\\,->}\\\\ &{\\mathrm{~[\\tiny~(\\textnormal{\\textsf{s}}~(s~\\mathbf{x})~\\pi(s~\\pi'n))~\\pi(s~\\pi(+~x~\\pi(s~\\pi~'n))))]~\\xi~\\!~\\!~\\!~\\!~\\!~\\!~}}\\end{array}$ intro x0 : nat. intro $\\bf{x1}\\,\\,\\,:\\,\\,\\,(=\\,\\,(+\\,\\,\\,(\\bf{s}\\,\\,\\,\\bf{x})\\,\\,\\,\\,\\bf{x}0)\\,\\,\\,\\,(\\bf{s}\\,\\,\\,\\,(+\\,\\,\\bf{x}\\,\\,\\,\\bf{x}0)\\,)\\,)\\,\\,\\,)$ . show $\\left(=\\begin{array}{c c c}{\\left(\\mathbf{s}\\ \\ \\left(+\\ \\mathbf{x}\\ \\ \\mathbf{x}0\\right)\\right)}&{\\left(+\\ \\ \\left(\\mathbf{s}\\ \\ \\mathbf{x}\\right)\\ \\ \\mathbf{x}0\\right)}\\end{array}\\right)$ by eq_symm. show $\\mathrm{~\\boldmath~\\xi~}(=\\mathrm{~\\boldmath~\\left(~\\ne~\\left(~{\\bf~x~}~\\right)~}\\,\\mathrm{\\boldmath~\\x0~})\\,\\mathrm{\\boldmath~\\xi~}(+\\mathrm{~\\boldmath~\\left(~{\\bf~s~}~\\right)~}\\,\\mathrm{\\boldmath~\\x0~})\\,\\mathrm{\\boldmath~\\xi~}$ by rewrite. show $\\left(=\\begin{array}{c c}{\\left(\\mathbf{s}\\end{array}\\right.\\left(+\\begin{array}{c c}{\\mathbf{x}\\ }\\end{array}\\right.\\mathbf{x}0\\right)}\\end{array}\\right)\\ \\ \\left(\\mathbf{s}\\ \\ \\left(+\\begin{array}{c c}{\\mathbf{x}\\ \\ \\mathbf{x}0}\\end{array}\\right)\\right)\\ \\mathbf{\\sigma}$ ) by rewrite. show $\\left(=\\begin{array}{c c}{\\left(+\\texttt{x}\\left(\\mathbf{s}\\texttt{x}0\\right)\\right)}\\end{array}\\right(\\mathbf{s}\\texttt{}\\left(+\\texttt{x x}0\\right)\\right)\\,\\right)$ ) by $+_{-8}$ . show $\\left(=\\begin{array}{c c c}{\\left(+\\texttt{x}\\left(\\mathbf{s}\\texttt{x}0\\right)\\right)}&{\\left(+\\texttt{}\\left(\\mathbf{s}\\texttt{x}\\right)\\textbf{x}0\\right)}\\end{array}\\right)$ by rewrite. show $\\left(=\\begin{array}{c c}{\\left(\\mathbf{s}\\end{array}\\right.\\left(+\\begin{array}{c c}{\\mathbf{x}\\ \\ \\mathbf{x}0}\\end{array}\\right)\\right)\\ \\ \\left(+\\begin{array}{c c}{\\mathbf{x}\\ \\ \\left(\\mathbf{s}\\ \\ \\mathbf{x}0\\right)}\\end{array}\\right)}\\end{array}$ by eq_symm. show $\\left(=\\begin{array}{l}{\\left(+\\begin{array}{l l l}{\\left(\\mathbf{s}}&{\\mathbf{x}}\\end{array}\\right)}&{\\mathbf{x}0}\\end{array}\\right)}\\end{array}$ (+ x (s x0))) by rewrite. show $C=(+\\texttt{x}\\left(\\mathbf{s}\\textbf{}\\mathbf{x}0\\right))$ $(+{\\textbf{x}}\\left(\\mathbf{s}~\\ \\mathbf{x}0\\right))$ ) by rewrite. show $\\bf\\Psi(=\\tau(+\\tau(s\\,\\ x)\\tau(s\\,\\ x0)\\tau(s\\,\\ x0)}$ ) $(\\textbf{s}\\,\\,(+\\,\\,(\\textbf{s}\\,\\textbf{x})\\,\\,\\textbf{x}0)\\,)$ ) by $+_{-8}$ . ", "page_idx": 16}, {"type": "text", "text": "show (= (+ (s x) (s x0)) (s (+ x (s x0)))) by rewrite. } } ", "page_idx": 17}, {"type": "text", "text": "This proof is not minimal \u2013 our human-written Peano proof for this level of the game has 7 steps in the inductive case, compared to 12 in the proof the agent finds. Nevertheless, this is a non-trivial theorem to prove by hand in this low-level axiomatic system, and our agent was not explicitly trained to target this theorem. Thus, this shows that it does learn general patterns that are useful in proof search. ", "page_idx": 17}, {"type": "text", "text": "E.2 Propositional Logic ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We here list all the statements in Theorem 41 of Kleene\u2019s book [20], \u201cIntroduction to Metamathematics\u201d, represented in Peano: ", "page_idx": 17}, {"type": "text", "text": "1. [(\u2019A : prop) -> [\u2019A -> \u2019A]]   \n2. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [\u2019B -> \u2019C] -> [\u2019A -> \u2019C]]   \n3. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> [\u2019B -> \u2019C]] -> [\u2019B -> [\u2019A -> \u2019C]]]   \n4. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> [\u2019B -> \u2019C]] -> [(and \u2019A \u2019B) -> \u2019C]]   \n5. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [(and \u2019A \u2019B) -> \u2019C] -> [\u2019A -> [\u2019B -> \u2019C]]]   \n6. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [[\u2019B -> \u2019C] -> [\u2019A -> \u2019C]]]   \n7. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [[\u2019C -> \u2019A] -> [\u2019C -> \u2019B]]]   \n8a. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [(and \u2019A \u2019C) -> (and \u2019B \u2019C)]]   \n8b. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [(and \u2019C \u2019A) -> (and \u2019C \u2019B)]]   \n9a. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [(or \u2019A \u2019C) -> (or \u2019B \u2019C)]]   \n9b. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> \u2019B] -> [(or \u2019A \u2019C) -> (or \u2019B \u2019C)]]   \n10a. [(\u2019A : prop) -> (\u2019B : prop) -> [(not \u2019A) -> [\u2019A -> \u2019B]]]   \n10b. [(\u2019A : prop) -> (\u2019B : prop) -> [\u2019A -> [(not \u2019A) -> \u2019B]]]   \n11. [(\u2019A : prop) -> (\u2019B : prop) -> [\u2019B -> [\u2019A -> \u2019B]]]   \n12. [(\u2019A : prop) -> (\u2019B : prop) -> [\u2019A -> \u2019B] -> [(not \u2019B) -> (not \u2019A)]]   \n13. [(\u2019A : prop) -> (\u2019B : prop) -> [\u2019A -> (not \u2019B)] -> [\u2019B -> (not \u2019A)]]   \n14. [(\u2019A : prop) -> (\u2019B : prop) -> [(not \u2019A) -> \u2019B] -> [(not \u2019B) -> \u2019A]]   \n15. [(\u2019A : prop) -> (\u2019B : prop) -> [(not \u2019A) -> (not \u2019B)] -> [\u2019B -> \u2019A]]   \n16. [(\u2019A : prop) -> (\u2019B : prop) -> [\u2019A -> \u2019B] -> [\u2019B -> \u2019A] -> (iff \u2019A \u2019B)]   \n17a. [(\u2019A : prop) -> (\u2019B : prop) -> (iff \u2019A \u2019B) -> [\u2019A -> \u2019B]]   \n17b. [(\u2019A : prop) -> (\u2019B : prop) -> (iff \u2019A \u2019B) -> [\u2019B -> \u2019A]]   \n18a. [(\u2019A : prop) -> (\u2019B : prop) -> (iff \u2019A \u2019B) -> \u2019A -> \u2019B]   \n18b. [(\u2019A : prop) -> (\u2019B : prop) -> (iff \u2019A \u2019B) -> \u2019B -> \u2019A]   \n19. [(\u2019A : prop) -> (iff \u2019A \u2019A)]   \n20. [(\u2019A : prop) -> (\u2019B : prop) -> (iff \u2019A \u2019B) -> (iff \u2019B \u2019A)]   \n21. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> (iff \u2019A \u2019B) -> (iff \u2019B \u2019C) -> (iff \u2019A \u2019C)]   \n22. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [\u2019A -> [\u2019B -> \u2019C]] ->   \n[(not (not \u2019A)) -> [(not (not \u2019B)) -> (not (not \u2019C))]]]   \n23. [(\u2019A : prop) -> (\u2019B : prop) -> [(not (not [\u2019A -> \u2019B]))] ->   \n[(not (not \u2019A)) -> (not (not \u2019B))]]   \n24. [(\u2019A : prop) -> (\u2019B : prop) -> (\u2019C : prop) -> [(not (not [\u2019A -> \u2019B]))] ->   \n[(not (not [\u2019B -> \u2019C]))] -> [(not (not [\u2019A -> \u2019C]))]]   \n25. [(\u2019A : prop) -> (\u2019B : prop) ->   \n(iff (not (not (and \u2019A \u2019B))) (and (not (not \u2019A)) (not (not \u2019B))))] ", "page_idx": 17}, {"type": "text", "text": "We use the numbering from the book here \u2014 there are 30 statements, even if the last one is labeled 25. ", "page_idx": 17}, {"type": "text", "text": "F Discussion: extension to other proof assistants ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "While our approach is implemented in Peano, it can in principle be extended to mainstream theorem provers like Lean and Coq. Peano\u2019s typing rules (a version of the Calculus of Constructions, without native inductive types) are essentially a subset of these systems\u2019 more expressive calculi, and its minimalism makes it particularly suitable for automated proof search. The most practical path to integration would be similar to recent tools like Duper [9]: embedding problems from the target system (e.g., Lean) into a Peano representation, running the prover, and then reconstructing the discovered proofs in the original system. While developing a Peano-to-Lean proof object translator would be relatively straightforward given the simplicity of Peano\u2019s type system, translating arbitrary Lean problems to Peano would require significant more engineering effort (e.g., we would need to generate explicit axioms for inductive types and quotients, which are not part of Peano). However, we believe a simplified environment is valuable for investigating how self-improving agents can build towards deeper theorems, eventually being able to invent new definitions and accumulate a growing library (which MINIMO does not, yet). Once these capabilities are demonstrated convincingly, the engineering investment in proof translation will become more compelling. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We claim an approach to training a self-improving mathematical reasoning agent starting just from axioms. This is demonstrated in 3 axiomatic domains in Section 4. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our key limitations are explicitly discussed in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: No theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We list all of the axioms in Peano we use in the Appendix. For conjecturing, we use a public implementation of Synchromesh, together with the completion engine algorithm we describe in Section 3.1. The MCTS proof search procedure we use is standard, and present in previous papers with open-source implementations [42]. Nevertheless, we acknowledge that the paper required significant implementation, and thus reproducibility will be much easier with our code release, which we will do after the submission. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide a zip flie containing all of our implementation in our supplementary material, and the configuration flies and commands used to run the main experiments. This was also released on Github (https://github.com/gpoesia/minimo). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: These are provided in the Appendix, and in the code release. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Figures 2 and 3 contain standard error bands, together with a description of those in the captions. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provided these details in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: our paper only uses synthetic data of formal mathematics, and thus not incurr in any of the issues listed in the Ethics guideline. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We don\u2019t foresee any social impact of our work in its current form, since we only train agents on synthetic data, collected from formal mathematical environments, and evaluate their behavior there. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our models have no such potential for misuse. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide a README in our code release, which we plan to gradually improve in our open-source repository. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: N/A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: N/A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]