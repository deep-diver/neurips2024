[{"type": "text", "text": "Statistical Efficiency of Distributional Temporal Difference Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang Peng\\* Liangyu Zhangt Zhihua Zhang+ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributional reinforcement learning (DRL) has achieved empirical success in various domains. One core task in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\\eta^{\\pi}$ for a given policy $\\pi$ . The distributional temporal difference learning has been accordingly proposed, which is an extension of the temporal difference learning (TD) in the classic RL area. In the tabular case, Rowland et al. [2018] and Rowland et al. [2024a] proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (QTD), respectively. In this paper, we go a step further and analyze the finitesample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric distributional temporal difference learning (NTD). For a $\\gamma$ -discounted infinite-horizon tabular Markov decision process, we show that for NTD we need O ( - $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)}\\end{array}$ iterations to achieve an $\\varepsilon$ -optimal estimator with high probability, when the estimation error is measured by the $p$ Wasserstein distance. This sample complexity bound is minimax optimal up to logarithmic factors in the case of the 1-Wasserstein distance. To achieve this, we establish a novel ", "page_idx": 0}, {"type": "text", "text": "Freedman's inequality in Hilbert spaces, which would be of independent interest. In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$ Wasserstein distance for $p\\geq1$ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In high-stake applications of reinforcement learning (RL), such as healthcare [Lavori and Dawson, 2004, Bock et al., 2022] and finance[Ghysels et al., 2005], only considering the mean of returns is insufficient. It is necessary to take risk and uncertainties into consideration. Distributional reinforcement learning (DRL) Morimura et al. [2010], Bellemare et al. [2017, 2023] addresses such issues by modeling the complete distribution of returns instead of their expectations. ", "page_idx": 0}, {"type": "text", "text": "In the field of DRL, one of the most fundamental tasks is to estimate the return distribution $\\eta^{\\pi}$ for agiven policy $\\pi$ , which is referred to as distributional policy evaluation. Distributional temporal difference learning (TD) is probably the most widely-used approach for solving the distributional policy evaluation problem. A key aspect of implementing a distributional TD algorithm is how to represent the return distribution, an infinite-dimensional object, via a computationally feasible finite-dimensional parametrization. This has led to the development of two special instances of distributional TD: categorical temporal difference learning (CTD) [Bellemare et al., 2017] and quantile temporal difference learning (QTD) [Dabney et al., 2018]. These algorithms provide computationally tractable parametrizations and updating schemes of the return distribution. ", "page_idx": 0}, {"type": "text", "text": "Previous theoretical works have primarily focused on the asymptotic behaviors of distributional TD. In particular, Rowland et al. [2018] and Rowland et al. [2024a] showed the asymptotic convergences of CTD and QTD in the tabular case, respectively. A natural question arises: can we depict the statistical effciency of distributional TD by non-asymptotic results similar to the classic TD algorithm [Liet al.,2024j? ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we manage to answer the above question affirmatively in the synchronous setting [Kakade, 2003, Kearns et al., 2002]. Firstly, we introduce non-parametric distributional temporal difference learning (NTD) in Section 3, which is not practical but aids theoretical understanding. Weshowthat $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)}\\end{array}$ 4 iterations are suffcient to yield an estimator $\\hat{\\eta}^{\\pi}$ , such that the $p$ Wassersteinmetricbetween $\\hat{\\eta}^{\\pi}$ and $\\eta^{\\pi}$ is less than $\\varepsilon$ with high probability (Theorem 4.1). This bound is minimax optimal (Theorem B.1) in the 1-Wasserstein metric case, if we neglect all logarithmic terms. Next, we revisit the more practical CTD, and show that, in terms of the $p_{\\|}$ Wasserstein metric, CTD and NTD have the same non-asymptotic convergence bounds (Theorem 4.2). It is worth pointing out that to attain such tight bounds in Theorem 4.1, we establish a Freedman's inequality in Hilbert spaces (Theorem A.2). We would believe it is of independent interest beyond the current work. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Non-asymptotic results of DRL. Recently, there has been an emergence of work focusing on finite-sample/iteration results of the distributional policy evaluations. ", "page_idx": 1}, {"type": "text", "text": "Wu et al. [2023] studied the offline distributional policy evaluation problem. They solved the problem via fitted likelihood estimation (FLE) inspired by the classic offine policy evaluation algorithm fitted Q evaluation (FQE), and provided a generalization bound in the $p$ -Wasserstein metric case. Zhang et al. [2023] proposed to solve distributional policy evaluation by the model-based approach and derivd coresponding sample complexity bounds, namely $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+2}}\\right)}\\end{array}$ in the $p$ Wasserstein metric case, and $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{4}}\\right)}\\end{array}$ in both the Kolmogorov-Smirnov metric and total variation metric under different conditions. Rowland et al. [2024b] proposed direct categorical fixedpoint computation (DCFP), a model-based version of CTD, in which they constructed the estimator by solving a linear system directly instead of performing an iterative algorithm. They showed that the sample complexity of DCFP is $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{3}}\\right)}\\end{array}$ in the 1-Wasserstein metric case by introducing the novel stochastic categorical CDF Bellman operator and equation. Their result matches the minimax lower bound (up to logarithmic factors) ((1-)a ) proposed in [Zhang et al., 2023], which implies that learning the full return distribution can be as sample-efficient as learning just its expectation. It's worth noting that the algorithms analyzed in both [Zhang et al., 2023] and [Rowland et al., 2024b] are model-based, hence they are less similar to practical algorithms. While distributional TD analyzed in this paper, as a model-free method, is more practical, and also involves a more complicated theoretical analysis. ", "page_idx": 1}, {"type": "text", "text": "Bock and Heitzinger [2022] also considered model-free method. They proposed speedy categorical policy evaluation (SCPE), which can be regarded as CTD with an additional acceleration term. They showed that the sample complexity of SCPE is O ( - $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{4}}\\right)}\\end{array}$ in the 1-Wasserstein metric case. Compared to [Bock and Heitzinger, 2022], our work shows that even if we do not introduce any acceleration techniques to the original CTD algorithm, it is still possible to attain the near-minimax optimal sample complexity bounds. Thus, we give sharper bounds based on a simpler algorithm. ", "page_idx": 1}, {"type": "text", "text": "Table 1 gives more detailed comparisons of sample complexity with the previous work in the 1- Wasserstein metric. Note that solving distributional policy evaluation can also address the traditional policy evaluation task by taking expectation of the return distribution estimator. And the supreme 1-Wasserstein metric error of the return distribution estimator is not smaller than the $\\ell_{\\infty}$ errorof the induced value function estimator (see the proof of Theorem B.1 in Appendix B), we have also listed the sample complexity of the policy evaluation task in Table 1 for comparison. ", "page_idx": 1}, {"type": "table", "img_path": "eWUM5hRYgH/tmp/e8d33305409243b9d5b1b6e87b7279fb40e2cc1cb43fc1ae6e7f504f29ad7d6e.jpg", "table_caption": ["Table 1. Sample complexity of algorithms for solving policy evaluation (PE) in the $\\ell_{\\infty}$ norm, and distributional policy evaluation (DPE) in the supreme 1-Wasserstein metric. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Freedman's inequality. Freedman's inequality was originally proposed in [Freedman, 1975]. It can be viewed as a Bernstein's inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Tropp [2011] generalized Freedman's inequality to matrix martingales. And Talebi et al. [2022] established Freedman inequalities for martingales in the setting of noncommutative probability spaces. To the best of our knowledge, we are the first to present a concrete version of Freedman's inequality in Hilbert spaces. ", "page_idx": 2}, {"type": "text", "text": "The remainder of this paper is organized as follows. In Section 2, we introduce some background of DRL and state Freedman's inequality in Hilbert spaces. In Section 3, we revisit distributional TD and propose NTD for further theoretical analysis. In Section 4, we analyze the non-asymptotic convergence bounds of NTD and CTD. Section 5 presents proof outlines of our theoretical results, and Section 6 concludes our work. We put the detailed results with Freedman's inequality in Hilbert spaces in Appendix A, and the minimax lower bound of the distributional policy evaluation task in AppendixB. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An infinite-horizon tabular Markov decision process (MDP) is defined by a 5-tuple $M\\;=\\;$ $\\langle S,A,\\mathcal{P}_{R},P,\\gamma\\rangle$ , where $\\boldsymbol{S}$ represents a finite state space, $\\boldsymbol{\\mathcal{A}}$ a finite action space, $\\mathcal{P}_{R}$ the distribution of rewards, $P$ the transition dynamics, i.e., $\\mathcal{P}_{R}(\\cdot|s,a)\\in\\Delta\\,([0,1]),P(\\cdot|s,a)\\in\\Delta\\,(S)$ for any state action pair $(s,a)\\in S\\times A$ , and $\\gamma\\,\\in\\,(0,1)$ a discount factor. Here we use $\\Delta(\\cdot)$ to represent the set of all probability distributions over some set. Given a policy $\\pi\\colon S\\to\\Delta\\left(A\\right)$ and an initial state $s_{0}=s\\in\\mathcal{S}$ a random trajectory $\\{(s_{t},a_{t},t_{t})_{t=0}^{\\infty}\\}$ can be sampled from $M$ at $\\left\\vert s_{t}\\sim\\pi(\\cdot\\mid s_{t})\\right\\vert$ $r_{t}\\mid(s_{t},a_{t})\\sim\\mathcal{P}_{R}(\\cdot\\mid s_{t},a_{t}),\\,s_{t+1}\\mid(s_{t},a_{t})\\sim P(\\cdot\\mid s_{t},a_{t})$ for any $t\\in\\mathbb N$ . Given a trajectory, we define the return by $\\begin{array}{r}{G^{\\pi}(s):=\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\,\\in\\,\\left[0,\\frac{1}{1-\\gamma}\\right]}\\end{array}$ . We denote return distribution $\\eta^{\\pi}(s)$ as the probability distribution of $G^{\\pi}(s)$ , and $\\eta^{\\pi}:=\\bar{(\\eta^{\\pi}(s))_{s\\in S}}^{\\bullet}$ The expected return $V^{\\pi}(s)=\\mathbb{E}G^{\\pi}(s)$ is the value function in the traditional RL setting. ", "page_idx": 2}, {"type": "text", "text": "2.1  Distributional Bellman Equation and Operator ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recall that the classic policy evaluation aims at computing the value functions $V^{\\pi}$ . It is known that $V^{\\pi}=(V^{\\pi}(s))_{s\\in S}$ satisfy the Bellman equation. That is, for any $s\\in S$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nV^{\\pi}(s)=\\left[T^{\\pi}(V^{\\pi})\\right](s)=\\mathbb{E}_{a\\sim\\pi(\\cdot\\vert s),r\\sim\\mathcal{P}_{R}(\\cdot\\vert s,a),s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[r+\\gamma V^{\\pi}(s^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The operator $T^{\\pi}\\colon\\mathbb{R}^{S}\\rightarrow\\mathbb{R}^{S}$ is called the Bellman operator, and $V^{\\pi}$ is a fixed point of $T^{\\pi}$ ", "page_idx": 2}, {"type": "text", "text": "The task of distribution policy evaluation is finding $\\eta^{\\pi}$ given some fixed policy $\\pi.\\ \\eta^{\\pi}$ satisfies a distributional version of the Bellman equation (1). That is, for any $s\\in S$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta^{\\pi}(s)=(T^{\\pi}\\eta^{\\pi})\\left(s\\right)=\\mathbb{E}_{a\\sim\\pi(\\cdot|s),r\\sim\\mathcal{P}_{R}(\\cdot|s,a),s^{\\prime}\\sim P\\left(\\cdot|s,a\\right)}\\left[(b_{r,\\gamma})_{\\#}\\,\\eta^{\\pi}(s^{\\prime})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $b_{r,\\gamma}\\colon\\mathbb{R}\\rightarrow\\mathbb{R}$ is an affine function defined by $b_{r,\\gamma}(x)=r+\\gamma x$ .And $f_{\\#}\\mu$ is the push forward measure of $\\mu$ through any function $f\\colon\\ensuremath{\\mathbb{R}}\\to\\ensuremath{\\mathbb{R}}$ , so that $f_{\\#}\\mu(A)=\\mu(f^{-1}(A))$ for any Borel set $A$ where $f^{-1}(A):=\\{x\\colon f(x)\\in A\\}$ The operator $\\begin{array}{r}{\\mathcal{T}^{\\pi}\\colon\\Delta\\left(\\left[0,\\frac{1}{1-\\gamma}\\right]\\right)^{S}\\rightarrow\\Delta\\left(\\left[0,\\frac{1}{1-\\gamma}\\right]\\right)^{S}}\\end{array}$ is known as the distributional Bellman operator, and $\\eta^{\\pi}$ is a fixed point of $\\dot{\\mathcal{T}}^{\\pi}$ . For notational simplicity, we denote $\\textstyle\\Delta\\left(\\left[0,{\\frac{1}{1-\\gamma}}\\right]\\right)$ $\\mathcal{P}$ from now on. ", "page_idx": 3}, {"type": "text", "text": "2.2 $\\mathcal{T}^{\\pi}$ as Contraction in $\\mathcal{P}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A key property of the Bellman operator $T^{\\pi}$ is that it is a $\\gamma$ -contraction w.r.t. the supreme norm (i.e. $\\ell_{\\infty}$ norm). However, before we can properly discuss the contraction properties of $\\mathcal{T}^{\\pi}$ , we need to specify a metric $d$ on $\\mathcal{P}$ . And for any metric $d$ on $\\mathcal{P}$ , we denote $\\bar{d}$ as the corresponding supreme metric on ${\\mathcal{P}}^{S}$ , i.e., $\\bar{d}\\,(\\eta,\\eta^{\\prime}):=\\operatorname*{max}_{s\\in\\mathcal{S}}\\,d\\,\\big(\\eta(s),\\eta^{\\prime}(s)\\big)$ for any $\\eta,\\eta^{\\prime}\\in\\mathcal{P}^{s}$ ", "page_idx": 3}, {"type": "text", "text": "Suppose $\\mu$ and $\\nu$ are_two probability distributions on $\\mathbb{R}$ with_ finite $p$ moments  for $\\begin{array}{r l r}{p}&{{}\\in}&{[1,\\infty]}\\end{array}$ \uff0eThe $p$ Wasserstein  metric  between $\\mu$ and $\\nu$ is defined as $\\begin{array}{r l}{W_{p}(\\mu,\\nu)}&{{}:=}\\end{array}$ $\\begin{array}{r}{\\left(\\operatorname*{inf}_{\\kappa\\in\\Gamma(\\mu,\\nu)}\\int_{\\mathbb{R}^{2}}\\vert x-y\\vert^{p}\\,\\kappa(d x,d y)\\right)^{1/p}}\\end{array}$ . Each element $\\kappa\\,\\in\\,\\Gamma(\\mu,\\nu)$ is a coupling of $\\mu$ and $\\nu$ ie, a joint distribution on $\\mathbb{R}^{2}$ with prescribed marginals $\\mu$ and $\\nu$ on each \u201caxis. When $p=1$ we have $\\begin{array}{r}{\\dot{W_{1}}(\\mu,\\nu)\\,=\\,\\int_{\\mathbb{R}}|F_{\\mu}(x)-F_{\\nu}(\\bar{x)}|d x.}\\end{array}$ where $F_{\\mu}$ and $F_{\\nu}$ are the cumulative distribution function of $\\mu$ and $\\nu$ respectively. It can be shown that $\\mathcal{T}^{\\pi}$ is a $\\gamma$ -contraction w.r.t. the supreme $p$ Wasserstein metric $\\bar{W}_{p}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1. [Bellemare et al., 2023, Propositions 4.15] The distributional Bellman operator is a $\\gamma$ contraction on $\\mathcal{P}^{S}~_{W.r.t.}$ the supreme $p$ Wassersteinmetricfor $p\\in[1,\\infty]$ That is, for any $\\eta,\\eta^{\\prime}\\in\\mathcal{P}^{s}$ , we have $\\bar{W}_{p}\\,({\\mathcal T}^{\\pi}\\eta,{\\mathcal T}^{\\pi}\\eta^{\\prime})\\overset{\\cdot}{\\leq}\\gamma\\bar{W}_{p}(\\eta,\\eta^{\\prime})$ ", "page_idx": 3}, {"type": "text", "text": "The $\\ell_{p}$ metric between $\\mu$ and $\\nu$ is defined as $\\begin{array}{r}{\\ell_{p}(\\mu,\\nu)=\\left(\\int_{\\mathbb{R}}|F_{\\mu}(x)-F_{\\nu}(x)|^{p}\\,d x\\right)^{\\frac{1}{p}}}\\end{array}$ for $p\\in[1,\\infty)$ \uff0c and $\\mathcal{T}^{\\pi}$ .s $\\gamma^{\\frac{1}{p}}$ -contraction w.t. the supreme $\\ell_{p}$ metric $\\bar{\\ell}_{p}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.2. [Bellemare et al., 2023, Propositions 4.20] The distributional Bellman operator is $a\\,\\gamma^{\\frac{1}{p}}$ -contraction on $\\mathcal{P}^{S}\\nsim_{W.r.t.}$ the supreme $\\ell_{p}$ metric for $p\\in[1,\\infty)$ . That is, for any $\\eta,\\eta^{\\prime}\\in\\mathcal{P}^{s}$ \uff0c we have $\\bar{\\ell}_{p}\\,(\\mathcal{T}^{\\pi}\\eta,\\mathcal{T}^{\\pi}\\eta^{\\prime})\\le\\gamma^{\\frac{1}{p}}\\bar{\\ell}_{p}(\\eta,\\eta^{\\prime})$ ", "page_idx": 3}, {"type": "text", "text": "Notethat the $\\ell_{1}$ metric coincides with the 1-Wasserstein metric. And the $\\ell_{2}$ metric is also called the Cram\u00e9r metric, which plays an important role in subsequent analysis because the zero-mass signed measure space equipped with this metric $\\left(\\mathcal{M},\\Vert\\cdot\\Vert_{\\ell_{2}}\\right)$ (defined in Section 5.1) is a Hilbert space5. Thereby, we can apply Freedman's inequality in Hilbert spaces. ", "page_idx": 3}, {"type": "text", "text": "2.3  Freedman's Inequality in Hilbert Spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Just as Freedman's inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]), a Hilbert space version of Freedman's inequality is indispensable for deriving the minimax nonasymptotic convergence bound for distributional TD. At the moment, we state a Hilbert space version of the original Freedman's inequality (Theorem 1.6 in [Freedman, 1975]), and more detailed results can be found in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{X}$ be a Hilbert space, $\\{X_{i}\\}_{i=1}^{n}$ be an $\\mathcal{X}$ -valued martingale difference sequence adapted to the filtration $\\{\\mathcal{F}_{i}\\}_{i=1}^{n}$ $\\begin{array}{r}{Y_{i}:=\\sum_{j=1}^{i}X_{j}}\\end{array}$ be the corresponding martingale, and $\\begin{array}{r}{W_{i}:=\\sum_{j=1}^{i}\\sigma_{j}^{2}}\\end{array}$ be the corresponding quadratic variation process. Here $\\sigma_{j}^{2}:=\\mathbb{E}_{j-1}\\left\\|X_{j}\\right\\|^{2}$ , and $\\mathbb{E}_{i}\\left[\\cdot\\right]:=\\mathbb{E}\\left[\\cdot|\\mathcal{F}_{i}\\right]$ denotes the conditional expectation. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1 (Freedman's inequality in Hilbert spaces). Suppose $\\operatorname*{max}_{i\\in[n]}\\left\\|X_{i}\\right\\|\\leq\\,b$ forsome constant $b>0$ .Then, for any $\\varepsilon$ and $\\sigma>0$ thefollowing inequality holds ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists k\\in[n],s.t.\\ \\|Y_{k}\\|\\ge\\varepsilon\\,a n d\\,W_{k}\\le\\sigma^{2}\\right)\\le2\\exp\\left\\{-\\frac{\\varepsilon^{2}/2}{\\sigma^{2}+b\\varepsilon/3}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3   Distributional Temporal Difference Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "If the MDP $M=\\langle S,A,\\mathcal{P}_{R},P,\\gamma\\rangle$ is known, and because $V^{\\pi}$ is the fixed point of the contraction $T^{\\pi}$ \uff0c $V^{\\pi}$ can be evaluated via the famous dynamic programming (DP) algorithm. To be concrete, for any initialization $V^{(0)}\\,\\in\\,\\mathbb{R}^{S}$ , if we define the iteration sequence $V^{(k+1)}\\,=\\,T^{\\pi}(V^{(k)})$ for $k\\,\\in\\,\\mathbb{N}$ $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\bigl\\|V^{(k)}-V^{\\pi}\\bigr\\|_{\\infty}\\,=\\,0}\\end{array}$ by te coractomapingthorem Propston 4. in ", "page_idx": 4}, {"type": "text", "text": "Similarly, the distributional dynamic programming algorithm defines the iteration sequence as $\\eta^{(k+1)}\\overset{\\cdot}{=}\\mathcal{T}^{\\pi}\\eta^{(k)}$ for any initialization $\\eta^{(0)}$ In the same way, we have $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\bar{W}_{p}(\\eta^{(k)},\\eta^{\\pi})=0}\\end{array}$ for $p\\in[1,\\infty]$ and $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\bar{\\ell}_{p}(\\eta^{(k)},\\eta^{\\pi})=0}\\end{array}$ for $p\\in[1,\\infty)$ ", "page_idx": 4}, {"type": "text", "text": "In most application scenarios, the transition dynamic $P$ and reward distribution $\\mathcal{P}_{R}$ are unknown, and instead we can only get samples of $P$ and $\\mathcal{P}_{R}$ in a streaming manner. In this paper, we assume a generative model [Kakade, 2003, Kearns et al., 2002] is accessible, which generates independent samples for all states in each iteration, i.e., in the $t$ -th iteration, we collect sample $a_{t}{\\mathit{\\hat{(s)}}}\\ \\sim\\ \\pi(\\cdot|s{\\mathit{\\hat{)}}},s_{t}(s)\\ \\sim\\ P(\\cdot|s,a_{t}(s)),r_{t}(s)\\ \\sim\\ \\mathcal{P}_{R}(\\cdot|s,a_{t}(s))$ for each $s~\\in~s$ .Similar to TD [Sutton, 1988] in classic RL, distributional TD also employs the stochastic approximation (SA) [Robbins and Monro, 1951] technique to address the aforementioned problem and can be viewed as an approximate version of distributional DP. ", "page_idx": 4}, {"type": "text", "text": "Non-parametric Distributional TD We first introduce non-parametric distributional temporal difference learning (NTD), which is helpful in the theoretical understanding of distributional TD. In the setting of NTD, we assume the return distributions can be precisely updated without any parametrization. For any initialization $\\eta_{0}^{\\pi}\\in\\mathcal{P}^{s}$ , the updating scheme is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\pi}=(1-\\alpha_{t})\\eta_{t-1}^{\\pi}+\\alpha_{t}T_{t}^{\\pi}\\eta_{t-1}^{\\pi}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for any $t\\geq1$ .Here $\\alpha_{t}$ is the step size. The empirical Bellman operator at the $t$ -th iteration $\\textstyle\\mathcal{T}_{t}^{\\pi}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\mathcal{T}_{t}^{\\pi}\\eta\\right)(s)=\\big(b_{r_{t}(s),\\gamma}\\big)_{\\#}\\big(\\eta\\big(s_{t+1}\\big)\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is an unbiased estimator of $(\\tau^{\\pi}\\eta)\\left(s\\right)$ . It is evident that NTD is a SA modification of distributional DP. Consequently, we can analyze NTD using the techniques from the SA area. ", "page_idx": 4}, {"type": "text", "text": "Categorical Distributional TD Now, we revisit the more practical CTD. In this case, the updates in CTD is computationally tractable, due to the following categorical parametrization of probability distributions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{P}_{K}:=\\left\\{\\sum_{k=0}^{K}p_{k}\\delta_{x_{k}}:p_{0},\\dotsc,p_{K}\\geq0\\,,\\sum_{k=0}^{K}p_{k}=1\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $K\\,\\in\\,\\mathbb{N}$ and $\\begin{array}{r}{0\\,\\le\\,x_{0}\\,<\\,\\cdot\\,\\cdot\\,<\\,x_{K}\\,\\le\\,\\frac{1}{1-\\gamma}}\\end{array}$ are fixed points of the support. For simplity,. $\\left\\{x_{k}\\right\\}_{k=0}^{K}$ $\\begin{array}{r}{x_{k}=\\frac{k}{K(1-\\gamma)}}\\end{array}$ . We denote the gap between two points by lK = K(i-) \u00b7 When updating the return distributions, we need to evaluate the $\\ell_{2}$ -projection of $\\mathcal{P}_{K}$ \uff0c\uff1a $\\Pi_{K}\\colon{\\mathcal{P}}\\rightarrow{\\mathcal{P}}_{K}$ \uff0c $\\Pi_{K}\\mu:=\\operatorname*{argmin}_{\\hat{\\mu}\\in\\mathcal{P}_{K}}\\ell_{2}(\\mu,\\hat{\\mu})$ :It can be shown (Proposition 5.14 in [Bellemare et al., 2023]) that the projection is uniquely given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Pi_{K}\\mu=\\sum_{k=0}^{K}p_{k}(\\mu)\\delta_{x_{k}},\\mathrm{~where~}\\;\\;p_{k}(\\mu)=\\mathbb{E}_{X\\sim\\mu}\\left[\\left(1-\\left|\\frac{X-x_{k}}{\\iota_{K}}\\right|\\right)_{+}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$(x)_{+}:=\\operatorname*{max}\\left\\{x,0\\right\\}$ for any $x\\in\\mathbb R$ . It is known that $\\Pi_{K}$ is non-expansive w.r.t. the Cram\u00e9r metric (Lemma 5.23 in [Bellemare et al., 2023]), i.e., $\\ell_{2}(\\Pi_{K}\\mu,\\Pi_{K}\\nu)\\,\\le\\,\\ell_{2}(\\mu,\\nu)$ for any $\\mu,\\nu\\in\\mathcal{P}$ . For any $\\eta\\in\\mathcal{P}^{S}$ $s\\in S$ , we slightly abuse the notation and define $\\left(\\Pi_{K}\\eta\\right)(s):=\\Pi_{K}\\eta(s)$ $\\Pi_{K}$ is still ", "page_idx": 4}, {"type": "text", "text": "non-expansive w.r.t. $\\bar{\\ell}_{2}$ Hence $\\begin{array}{r}{\\mathcal{T}^{\\pi,K}:=\\Pi_{K}\\mathcal{T}^{\\pi}}\\end{array}$ is a $\\sqrt{\\gamma}$ -contraction w.r.t. $\\bar{\\ell}_{2}$ , we denote its unique fixed point as $\\eta^{\\pi,K}\\in\\mathcal{P}_{K}^{S}$ . The approximation error induced by categorical parametrization is given by (Proposition 3 in Rowland et al. [2018]) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{2}(\\eta^{\\pi},\\eta^{\\pi,K})\\le\\frac{1}{\\sqrt{K}(1-\\gamma)},\\quad\\bar{W}_{1}(\\eta^{\\pi},\\eta^{\\pi,K})\\le\\frac{1}{\\sqrt{1-\\gamma}}\\bar{\\ell}_{2}(\\eta^{\\pi},\\eta^{\\pi,K})\\le\\frac{1}{\\sqrt{K}(1-\\gamma)^{3/2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now, we are ready to give the updating scheme of CTD, given any initialization $\\eta_{0}^{\\pi}\\in\\mathcal{P}_{K}^{S}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\pi}=(1-\\alpha_{t})\\eta_{t-1}^{\\pi}+\\alpha_{t}\\Pi_{K}\\mathcal{T}_{t}^{\\pi}\\eta_{t-1}^{\\pi}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "forany $t\\geq1$ . We can find that the only difference between CTD and NTD lies in the additional application of the projection operator $\\Pi_{K}$ at each iteration in CTD. ", "page_idx": 5}, {"type": "text", "text": "4 Statistical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we state our main results. For both NTD and CTD, we give the non-asymptotic convergencerates of $\\bar{W}_{p}(\\eta_{T}^{\\pi},\\eta^{\\pi})$ and $\\bar{\\ell}_{2}(\\eta_{T}^{\\pi},\\eta^{\\pi})$ ,respectively. ", "page_idx": 5}, {"type": "text", "text": "4.1  Non-asymptotic Analysis of NTD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first provide a non-asymptotic convergence rate of $\\bar{W}_{1}(\\eta_{T}^{\\pi},\\eta^{\\pi})$ for NTD, which is minimax optimal (Theorem B.1) up to logarithmic factors. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Sample complexity of NTD in the 1-Wasserstein metric). Given any $\\delta\\in(0,1)$ and $\\varepsilon\\in(0,1)$ let theinitializationbe $\\eta_{0}^{\\pi}\\in\\mathcal{P}^{s}$ ,thetotal updatenumber $T$ satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\nT\\geq\\frac{C_{1}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{3}}\\log\\frac{|S|\\,T}{\\delta}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "forsomelargeuniveralnstant $C_{1}>1$ i.e, $\\begin{array}{r}{T=\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{3}}\\right)}\\end{array}$ and the step ize $\\alpha_{t}$ satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{2}(1-\\sqrt{\\gamma})t}{\\log t}}\\le\\alpha_{t}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})t}{\\log t}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some small universal constants $c_{2}>c_{3}>0$ Then, with probability at least $1-\\delta$ the last iterate estimator satisfies $\\bar{W}_{1}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)\\leq\\varepsilon$ ", "page_idx": 5}, {"type": "text", "text": "Because $\\begin{array}{r}{\\bar{W}_{1}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)\\leq\\frac{1}{1-\\gamma}}\\end{array}$ always holds, we can translate the high probability bound to a mean error bound, that is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\bar{W}_{1}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)\\right]\\le\\varepsilon(1-\\delta)+\\frac{\\delta}{1-\\gamma}\\le2\\varepsilon\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "ifwetake $\\delta\\,\\leq\\,\\varepsilon(1\\,-\\,\\gamma)$ . In the subsequent discussion, we will not state the mean error bound conclusions for the sake of brevity. ", "page_idx": 5}, {"type": "text", "text": "The key idea of our proof is to first expand the error term $\\bar{W}_{1}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)$ over the time steps. Then it can be decomposed into an initial error term and a martingale term. The initial error term becomes smaller as the iteration goes due to the contraction properties of $\\mathcal{T}^{\\pi}$ . To control the martingale term, we first use the basic inequality Lemma E.1) $\\begin{array}{r}{\\dot{W_{1}}\\left(\\dot{\\mu},\\nu\\right)\\leq\\frac{1}{\\sqrt{1-\\gamma}}\\ell_{2}\\left(\\mu,\\nu\\right)}\\end{array}$ V--|2 (\u03bc, ), whichallows us to analyze this error term in the Hilbert space $(\\mathcal{M},\\|\\cdot\\|_{\\ell_{2}})$ defined in Section 5.1. Consequently, we can bound it using Freedman's inequality in the Hilbert space (Theorem A.2). A more detailed outline of proof can be found in Section 5.2. ", "page_idx": 5}, {"type": "text", "text": "Combining Theorem 4.1 with the basic inequality $\\begin{array}{r}{\\bar{W}_{p}(\\eta,\\eta^{\\prime})\\le\\frac{1}{\\left(1-\\gamma\\right)^{1-\\frac{1}{p}}}\\bar{W}_{1}^{\\frac{1}{p}}(\\eta,\\eta^{\\prime})}\\end{array}$ for any $\\eta,\\eta^{\\prime}\\in$ ${\\mathcal{P}}^{S}$ (Lemma E.1), we can derive that $\\begin{array}{r}{T\\,=\\,\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}\\left(1-\\gamma\\right)^{2p+1}}\\right)}\\end{array}$ iterations are sufficient to ensure $\\bar{W}_{p}(\\eta_{T}^{\\pi},\\eta^{\\pi})\\leq\\varepsilon$ . As pointed out in the example after Corollary 3.1 in [Zhang et al., 2023], when $p>1$ , the slow rate in terms of $\\varepsilon$ is inevitable without additional regularity conditions. ", "page_idx": 5}, {"type": "text", "text": "Although the 1-Wasserstein metric cannot bound the Cramer metric properly, by making slight modifications to the proof we have the following non-asymptotic convergence rate of $\\bar{\\ell}_{2}(\\bar{\\eta}_{T}^{\\pi},\\bar{\\eta}^{\\pi})$ .See Appendix C.5 for our proof. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.1 (Sample complexity of NTD in the Cram\u00e9r metric). Given any $\\delta\\,\\in\\,(0,1)$ and $\\varepsilon\\in$ $(0,1)$ let theinitial value $\\bar{\\eta_{0}^{\\pi}}\\in\\bar{\\mathcal{P}^{s}}$ ,thetotal updatenumber $T$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\geq\\frac{C_{1}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{5/2}}\\log\\frac{|S|\\,T}{\\delta}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for somelarge miveral onstan $C_{1}>1$ $\\begin{array}{r}{T=\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{5/2}}\\right)}\\end{array}$ and the se ie $\\alpha_{t}$ satisy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{2}(1-\\sqrt{\\gamma})t}{\\log t}}\\le\\alpha_{t}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})t}{\\log t}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some small universalconstants $c_{2}>c_{3}>0$ Then, with probability at least $1-\\delta$ the last iterate estimator satisfies $\\bar{\\ell}_{2}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)\\leq\\varepsilon$ ", "page_idx": 6}, {"type": "text", "text": "4.2  Non-asymptotic Analysis of CTD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first state a parallel result to Theorem 4.1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Sample complexity of CTD in the 1-Wasserstein metric). Given any $\\delta\\in(0,1)$ and $\\varepsilon\\in(0,1)$ suppose $\\begin{array}{r}{\\dot{K}>\\frac{\\dot{4}}{1-\\gamma}}\\end{array}$ the initial value $\\eta_{0}^{\\pi}\\in\\mathcal{P}_{K}^{S}$ , the total update number $T$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\geq\\frac{C_{1}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{3}}\\log\\frac{|S|\\,T}{\\delta}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some large univeral constant $C_{1}>1$ i.e, $\\begin{array}{r}{T=\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{3}}\\right)}\\end{array}$ and the sep size $\\alpha_{t}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{2}(1-\\sqrt{\\gamma})t}{\\log t}}\\le\\alpha_{t}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})t}{\\log t}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some small universal constants $c_{2}\\;>\\;c_{3}\\;>\\;0$ Then, with probability at least $1-\\delta$ the last iterate estimator satisfies $\\bar{W}_{1}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi,K}\\right)\\leq\\frac{\\varepsilon}{2}$ Frtheorordintupodf the appoxination eror $\\bar{W}_{1}\\left(\\eta^{\\pi,K},\\eta^{\\pi}\\right)$ ifwe take $\\begin{array}{r}{K>\\frac{4}{\\varepsilon^{2}(1-\\gamma)^{3}}}\\end{array}$ we have $\\bar{W}_{1}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)\\leq\\varepsilon$ ", "page_idx": 6}, {"type": "text", "text": "Note that the order (modulo logarithmic factors) of sample complexity of CTD is better than the previous results of SCPE [Bock and Heitzinger, 2022], and we do not need the additional term introduced in the updating scheme of SCPE. ", "page_idx": 6}, {"type": "text", "text": "The proof of this theorem is almost the same as that of Theorem 4.1, we outline the proof in Section 5.2. The $\\Bar{W}_{1}$ metric result can be translated into sample complexity bound $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)}\\end{array}$ the $\\bar{W}_{p}$ metric. We comment that this theoretical result matches the sample complexity bound in the model-based setting [Rowland et al., 2024b]. ", "page_idx": 6}, {"type": "text", "text": "As in the NTD setting, we have the following non-asymptotic convergence rate of $\\bar{\\ell}_{2}(\\eta_{T}^{\\pi},\\eta^{\\pi})$ as a corollary of Theorem 4.2. See Appendix C.5 for the proof. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2 (Sample complexity of CTD in the Cram\u00e9r metric). For any given $\\delta\\,\\in\\,(0,1)$ and $\\varepsilon\\in(0,1)$ suppose $\\begin{array}{r}{\\dot{K}>\\frac{4}{1-\\gamma}}\\end{array}$ the initialization is $\\eta_{0}^{\\pi}\\in\\mathcal{P}_{K}^{S}$ the total update number $T$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\geq\\frac{C_{1}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{5/2}}\\log\\frac{|S|\\,T}{\\delta}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for somelrge wniversal constant $C_{1}>1$ i.e, $\\begin{array}{r}{T=\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1-\\gamma)^{5/2}}\\right)}\\end{array}$ and the sep size $\\alpha_{t}$ satisies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{2}(1-\\sqrt{\\gamma})t}{\\log t}}\\le\\alpha_{t}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})t}{\\log t}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some small universal constants $c_{2}\\;>\\;c_{3}\\;>\\;0$ Then, with probability at least $1-\\delta$ the last iterate estimator satisfies $\\bar{\\ell}_{2}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi,K}\\right)\\le\\frac{\\varepsilon}{2}$ Furthereaccordingtotheupeboundf th aproxiation eror $\\bar{\\ell}_{2}\\left(\\eta^{\\pi,K},\\eta^{\\pi}\\right)$ \uff0c $i f$ wetake $\\begin{array}{r}{K>\\frac{4}{\\varepsilon^{2}(1-\\gamma)^{2}}}\\end{array}$ wehave $\\bar{\\ell}_{2}\\left(\\eta_{T}^{\\pi},\\eta^{\\pi}\\right)\\leq\\varepsilon$ ", "page_idx": 6}, {"type": "text", "text": "5  Proof Outlines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we will outline the proofs of our main theoretical results (Theorem 4.1, Corollary 4.1, Theorem 4.2, and Corollary 4.2). Before diving into the details of the proofs, we first define some notation. ", "page_idx": 7}, {"type": "text", "text": "5.1  Zero-mass Signed Measure Space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To analyze the distance between the estimator and the ground-truth $\\eta^{\\pi}$ , we will work with the zeromass signed measure space $\\mathcal{M}$ defined as follows ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathcal{M}}:=\\left\\{\\mu:\\mu{\\mathrm{~is~a~signed~measure~with~}}|\\mu|\\left(\\mathbb{R}\\right)<\\infty,\\mu(\\mathbb{R})=0,\\operatorname{supp}(\\mu)\\subseteq[0,{\\frac{1}{1-\\gamma}}]\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $|\\mu|$ is the total variation measure of $\\mu$ , and $\\operatorname{supp}(\\mu)$ is the support of $\\mu$ . See [Bogachev, 2007] for more details about signed measures. ", "page_idx": 7}, {"type": "text", "text": "For any $\\mu\\in\\mathcal{M}$ , we define its cumulative function as $F_{\\mu}(x):=\\mu[0,x)$ . We can check that $F_{\\mu}$ is linear w.r.t. $\\mu$ , that is, $F_{\\alpha\\mu+\\beta\\nu}=\\alpha F_{\\mu}+\\beta F_{\\nu}$ for any $\\alpha,\\beta\\in\\mathbb{R},\\mu,\\nu\\in\\mathcal{M}$ ", "page_idx": 7}, {"type": "text", "text": "To analyze the Cram\u00e9r metric case, we define the following Cram\u00e9r inner product on $\\mathcal{M}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\langle\\mu,\\nu\\right\\rangle_{\\ell_{2}}:=\\int_{0}^{\\frac{1}{1-\\gamma}}F_{\\mu}(x)F_{\\nu}(x)d x.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It is easy to verify that $\\langle\\cdot,\\cdot\\rangle_{\\ell_{2}}$ is indeed an inner product on $\\mathcal{M}$ The corresponding norm, called the Cram\u00e9r norm, is given by $\\begin{array}{r}{\\|\\mu\\|_{\\ell_{2}}=\\sqrt{\\langle\\mu,\\mu\\rangle_{\\ell_{2}}}=\\sqrt{\\int_{0}^{\\frac{1}{1-\\gamma}}\\left(F_{\\mu}(x)\\right)^{2}d x}}\\end{array}$ We have $\\nu_{1}-\\nu_{2}\\in\\mathcal{M}$ and $\\|\\nu_{1}-\\nu_{2}\\|_{\\ell_{2}}=\\ell_{2}\\left(\\nu_{1},\\nu_{2}\\right)$ for any $\\nu_{1},\\nu_{2}\\in\\mathcal{P}$ ", "page_idx": 7}, {"type": "text", "text": "The $W_{1}$ norm on $\\mathcal{M}$ is defined as $\\begin{array}{r}{\\|\\mu\\|_{W_{1}}:=\\int_{0}^{\\frac{1}{1-\\gamma}}|F_{\\mu}(x)|\\,d x}\\end{array}$ We have $\\|\\nu_{1}-\\nu_{2}\\|_{W_{1}}=W_{1}\\left(\\nu_{1},\\nu_{2}\\right)$ for any $\\nu_{1},\\nu_{2}\\in\\mathcal{P}$ ", "page_idx": 7}, {"type": "text", "text": "We can extend the distributional Bellman operator $\\mathcal{T}^{\\pi}$ and the Cram\u00e9r projection operator $\\Pi_{K}$ naturally to $\\mathcal{M}^{\\mathcal{S}}$ .Here,theproduct space $\\mathcal{M}^{\\bar{s}}$ is also a Banach space, and we use the supreme norm: $\\|\\eta\\|_{\\bar{\\ell}_{2}}:=\\operatorname*{max}_{s\\in\\mathcal{S}}\\|\\eta(s)\\|_{\\ell_{2}}$ , and $\\Vert\\eta\\Vert_{\\bar{W}_{1}}:=\\operatorname*{max}_{s\\in\\cal{S}}\\Vert\\eta(s)\\Vert_{W_{1}}$ for any $\\eta\\in\\mathcal{M}^{s}$ . We denote by $\\mathcal{T}$ the identity operator in $\\mathcal{M}^{\\bar{\\mathcal{S}}}$ ", "page_idx": 7}, {"type": "text", "text": "When the norm $\\left\\Vert\\cdot\\right\\Vert$ is applied to $\\mathit{\\textbf{A}}\\in\\mathit{\\textbf{C}}(\\mathcal{X})$ , where $\\mathcal{X}$ is any Banach space, and $\\mathcal{L}(\\mathcal{X})$ is  the  space  of  all  bounded  linear  operators  in $\\mathcal{X}$ , we  refer $\\|A\\|$ to the operator norm of $A$ . which is defined as $\\begin{array}{r l r}{\\|A\\|}&{:=}&{\\operatorname*{sup}_{\\eta\\in\\mathcal{X},\\|\\eta\\|=1}\\|A\\eta\\|}\\end{array}$ .\u3001 With this notation, ${\\cal{L}}({\\mathcal{X}})~~=$ $\\{A\\colon A$ is a linear operator mapping from $\\mathcal{X}$ to $\\ X,a n d\\left\\|A\\right\\|<\\infty\\}$ ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.1. $\\mathcal{T}^{\\pi}$ and $\\Pi_{K}$ are linear operators in $\\mathcal{M}^{\\mathcal{S}}$ .Furthermore, $\\|\\mathcal{T}^{\\pi}\\|_{\\bar{\\ell}_{2}}~\\le~\\sqrt{\\gamma},$ $\\|T^{\\pi}\\|_{\\bar{W}_{1}}\\leq\\gamma,\\|\\Pi_{K}\\|_{\\bar{\\ell}_{2}}=1,$ and $\\|\\Pi_{K}\\|_{\\bar{W}_{1}}\\leq1$ ", "page_idx": 7}, {"type": "text", "text": "The proof of the last inequality can be found in the proof of Lemma C.4, while the remaining results are trivial. We omit the proofs for brevity. ", "page_idx": 7}, {"type": "text", "text": "Moreover, we have the following matrix (of operators) representations of $\\mathcal{T}^{\\pi}$ and $\\Pi_{K}$ $\\mathcal{T}^{\\pi}~\\in$ $\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\mathcal{M}})^{s\\times\\bar{s}}$ forany $\\eta\\in\\mathcal{M}^{s}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\mathcal{T}^{\\pi}\\eta\\right)(s)=\\sum_{a\\in A,s^{\\prime}\\in\\mathcal{S}}\\pi(a\\mid s)P(s^{\\prime}\\mid s,a)\\int_{0}^{1}\\left(b_{r,\\gamma}\\right)_{\\#}\\eta(s^{\\prime})\\mathcal{P}_{R}(d r\\mid s,a)=\\sum_{s^{\\prime}\\in\\mathcal{S}}\\mathcal{T}^{\\pi}(s,s^{\\prime})\\eta(s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathscr{T}^{\\pi}(s,s^{\\prime})\\in\\mathcal{L}(\\mathcal{M})$ for any $\\nu\\in\\mathcal{M}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{T}^{\\pi}(s,s^{\\prime})\\nu=\\sum_{a\\in\\mathcal{A}}\\pi(a\\mid s)P(s^{\\prime}\\mid s,a)\\int_{0}^{1}\\big(b_{r,\\gamma}\\big)_{\\#}\\,\\nu\\mathcal{P}_{R}(d r\\mid s,a).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It can be verified that $\\begin{array}{r}{\\|\\mathcal{T}(s,s^{\\prime})\\|_{\\ell_{2}}\\le\\sqrt{\\gamma}\\sum_{a\\in\\mathcal{A}}\\pi(a\\mid s)P(s^{\\prime}\\mid s,a)=:\\sqrt{\\gamma}P^{\\pi}(s^{\\prime}|s).}\\end{array}$ Similarly, $\\|{\\mathcal{T}}(s,s^{\\prime})\\|_{W_{1}}\\,\\leq\\,\\gamma P^{\\pi}(s^{\\prime}|s)$ and $\\Pi_{K}\\;=\\;\\mathrm{diag}\\big(\\Pi_{K}\\big|_{\\mathcal{M}}\\big)_{s\\in\\mathcal{S}}\\;\\in\\;\\mathcal{L}(\\mathcal{M})^{s\\times{\\cal S}}$ . With these representations, $\\Pi_{K}{\\mathcal T}^{\\pi}\\;\\in\\;{\\mathcal L}({\\mathcal M})^{{\\mathcal S}\\times{\\mathcal S}}$ can be interpreted as matrix multiplication, where the scalar multiplication is replaced by the composition of operators. It can be verified that $\\left(\\Pi_{K}{\\mathcal T}^{\\pi}\\right)(s,s^{\\prime})\\,=$ $\\bar{\\Pi}_{K}\\mathcal{T}^{\\pi}(s,s^{\\prime})$ , and $\\|(\\Pi_{K}{\\bar{T}}^{\\pi})\\left(s,s^{\\bar{\\prime}}\\right)\\|_{\\ell_{2}}\\leq\\sqrt{\\gamma}\\bar{P^{\\pi}}(s^{\\prime}|s)$ ", "page_idx": 7}, {"type": "text", "text": "Remark 1: Although the spaces $\\left(\\mathcal{M},\\|\\cdot\\|_{\\ell_{2}}\\right)_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ and $\\left(\\mathcal{M},\\Vert\\cdot\\Vert_{W_{1}}\\right)$ are not complete, we will use their completions to replace them without loss of generality, because the completeness property does not affect the non-asymptotic analysis. For simplicity, we still use $\\mathcal{M}$ to denote the completion space. And according to the BLT theorem (Theorem 5.19 in [Hunter and Nachtergaele, 2001]), any bounded linear operator can be extended to the completion space, and still preserves its operator norm. ", "page_idx": 8}, {"type": "text", "text": "5.2 Analysis of Theorems 4.1 and 4.2 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For simplicity, we abbreviate both $\\lVert\\cdot\\rVert_{\\bar{\\ell}_{2}}$ and $\\lVert\\cdot\\rVert_{\\ell_{2}}$ as $\\lVert\\cdot\\rVert$ in this part. For all $t~\\in~[T]~:=$ $\\{1,2,\\cdots,T\\}$ , we denote $\\mathcal T_{t}:=\\mathcal T_{t}^{\\pi}$ \uff0c $\\tau:=\\tau^{\\pi}$ \uff0c $\\eta:=\\eta^{\\pi}$ for NTD; $\\mathcal{T}_{t}:=\\Pi_{K}\\mathcal{T}_{t}^{\\pi}$ \uff0c $\\mathcal{T}:=\\Pi_{K}\\mathcal{T}^{\\pi}$ $\\stackrel{\\cdot}{\\eta}:=\\eta^{\\pi,K}$ for CTD; and $\\iota\\,:=\\eta_{t}^{\\pi},\\,\\Delta_{t}\\,:=\\eta_{\\underline{{\\ell}}}-\\eta\\,\\in\\,\\mathcal{M}^{s}$ for both NTD and CTD. According to Lemma E.2, $\\eta_{t}\\in\\mathcal{P}^{S}$ for NTD and $\\eta_{t}\\in\\mathcal{P}_{K}^{S}$ for CTD. Our goal is to bound the $\\Bar{W}_{1}$ norm of the error term $\\|\\Delta_{T}\\|_{\\bar{W}_{1}}$ . This can be achieved by bounding $\\Vert\\Delta_{T}\\Vert$ ,as $\\begin{array}{r}{\\|\\Delta_{T}\\|_{\\bar{W}_{1}}\\leq\\frac{1}{\\sqrt{1-\\gamma}}\\,\\|\\Delta_{T}\\|}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "According to the updating rule, we have the error decomposition ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{t}=\\eta_{t}-\\eta}\\\\ &{\\quad=(1-\\alpha_{t})\\eta_{t-1}+\\alpha_{t}\\mathcal{T}_{t}\\eta_{t-1}-\\eta}\\\\ &{\\quad=(1-\\alpha_{t})\\Delta_{t-1}+\\alpha_{t}\\left(\\mathcal{T}_{t}\\eta_{t-1}-\\mathcal{T}\\eta\\right)}\\\\ &{\\quad=(1-\\alpha_{t})\\Delta_{t-1}+\\alpha_{t}\\left(\\mathcal{T}_{t}-\\mathcal{T}\\right)\\eta_{t-1}+\\alpha_{t}\\mathcal{T}\\left(\\eta_{t-1}-\\eta\\right)}\\\\ &{\\quad=\\left[(1-\\alpha_{t})\\mathcal{T}+\\alpha_{t}\\mathcal{T}\\right]\\Delta_{t-1}+\\alpha_{t}\\left(\\mathcal{T}_{t}-\\mathcal{T}\\right)\\eta_{t-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Applying it recursively, we can further decompose the error into two terms ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Delta_{T}=\\underbrace{\\prod_{t=1}^{T}\\left[(1-\\alpha_{t})\\mathcal{Z}+\\alpha_{t}\\mathcal{T}\\right]\\Delta_{0}}_{\\Omega}+\\underbrace{\\sum_{t=1}^{T}\\alpha_{t}}_{i=t+1}\\prod_{i=t+1}^{T}\\,\\left[(1-\\alpha_{i})\\mathcal{Z}+\\alpha_{i}\\mathcal{T}\\right]\\left(\\mathcal{T}_{t}-\\mathcal{T}\\right)\\eta_{t-1}}_{\\Omega\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Term (I) is an initial error term that becomes negligible when $T$ islargebecause $\\tau$ is a contraction. Term (Il) can be bounded via Freedman's inequality in the Hilbert space (Theorem A.2). Combining the two upper bound, we can establish a recurrence relation. Solving this relation will lead to the conclusion. ", "page_idx": 8}, {"type": "text", "text": "We first establish the conclusion for step sizes that depend on $T$ . Specifically, we consider ", "page_idx": 8}, {"type": "equation", "text": "$$\nT\\geq\\frac{C_{4}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{3}}\\log\\frac{|S|\\,T}{\\delta},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{5}(1-\\sqrt{\\gamma})T}{\\log^{2}T}}\\leq\\alpha_{t}\\leq\\frac{1}{1+\\frac{c_{6}(1-\\sqrt{\\gamma})t}{\\log^{2}T}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $c_{5}~>~c_{6}~>~0$ are small constants satisfying $c_{5}c_{6}\\ \\leq\\ \\frac{1}{8}$ , and $C_{4}~>~1$ is a large constant depending only on $c_{5}$ and $c_{6}$ . As shown in Appendix C.1, once we have established the conclusion in this setting, we can recover the original conclusion stated in the theorem. ", "page_idx": 8}, {"type": "text", "text": "Now, we introduce the following useful quantities involving step sizes and $\\gamma$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta_{k}^{(t)}:=\\left\\{\\begin{array}{l l}{\\prod_{i=1}^{t}\\left(1-\\alpha_{i}(1-\\sqrt{\\gamma})\\right),}&{\\mathrm{if~}k=0,}\\\\ {\\alpha_{k}\\prod_{i=k+1}^{t}\\left(1-\\alpha_{i}(1-\\sqrt{\\gamma})\\right),}&{\\mathrm{if~}0<k<t,}\\\\ {\\alpha_{T},}&{\\mathrm{if~}k=t.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The folowing lmma provides useful bounds for $\\beta_{k}^{(t)}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.1. Supose $c_{5}c_{6}\\leq\\frac{1}{8}$ Then, for al $\\begin{array}{r}{t\\geq\\frac{T}{c_{6}\\log T}}\\end{array}$ we havethat ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta_{k}^{(t)}\\leq\\frac{1}{T^{2}},\\,f o r\\;\\;0\\leq k\\leq\\frac{t}{2};\\qquad\\beta_{k}^{(t)}\\leq\\frac{2\\log^{3}T}{(1-\\sqrt{\\gamma})T},\\,f o r\\;\\;\\frac{t}{2}<k\\leq t.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof can be found in Appendix C.2. From now on, we only consider $\\begin{array}{r}{t\\geq\\frac{T}{c_{6}\\log T}}\\end{array}$ The upper bound of term (I) is given by ", "page_idx": 9}, {"type": "text", "text": "$(\\mathrm{I})\\leq\\prod_{k=1}^{t}\\|(1-\\alpha_{k})\\mathcal{Z}+\\alpha_{k}\\mathcal{T}\\|\\,\\|\\Delta_{0}\\|\\leq\\prod_{k=1}^{t}\\left((1-\\alpha_{k})+\\alpha_{k}\\sqrt{\\gamma}\\right)\\frac{1}{\\sqrt{1-\\gamma}}=\\frac{\\beta_{0}^{(t)}}{\\sqrt{1-\\gamma}}\\leq\\frac{1}{\\sqrt{1-\\gamma}T^{2}},$ where I\u25b3oll\u2264\u221a S d = ", "page_idx": 9}, {"type": "text", "text": "As for term (Il), we have the following upper bound with high probability by utilizing Freedman's inequality (Theorem A.2). ", "page_idx": 9}, {"type": "text", "text": "Lemma 5.2. For any $\\delta\\in(0,1)$ wih probhabiliry a least $1-\\delta$ we have for al $\\begin{array}{r}{t\\geq\\frac{T}{c_{6}\\log T}}\\end{array}$ in the NTD case, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{k=1}^{t}\\alpha_{k}\\,\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})\\bar{Z}+\\alpha_{i}\\bar{T}\\right](\\mathcal{T}_{k}-\\mathcal{T})\\,\\eta_{k-1}\\right\\|}\\\\ &{\\leq34\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\,\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{2}T}\\left(1+\\operatorname*{max}_{k:\\,t/2<k\\leq t}\\left\\|\\Delta_{k-1}\\right\\|_{\\bar{W}_{1}}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Theconclusonstillhldsfor thcase fwe t $\\begin{array}{r}{K\\ge\\frac{4}{\\varepsilon^{2}(1-\\gamma)^{2}}+1.}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "The proof can be found in Appendix C.3. Combining the two results, we find the following recurrence relation in terms of the $\\Bar{W}_{1}$ norm holds given the choice of $T$ , with probability at least $1-\\delta$ for all t \u2265 cg logT ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\|\\Delta_{t}\\|_{\\bar{W}_{1}}\\leq\\frac{1}{\\sqrt{1-\\gamma}}\\left\\|\\Delta_{t}\\right\\|\\leq35\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}\\left(1+\\operatorname*{max}_{k:\\,t/2<k\\leq t}\\left\\|\\Delta_{k-1}\\right\\|_{\\bar{W}_{1}}\\right)}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In Theorem C.1, we solve the relation and obtain the error bound of the last iterate estimator: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\left\\|\\Delta_{T}\\right\\|_{\\bar{W}_{1}}\\leq C_{7}\\left(\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}}+\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}\\right),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "$C_{7}>1$ $c_{6}$ $C_{4}\\geq2C_{7}^{2}$ $\\begin{array}{r}{T\\ge\\frac{C_{4}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{3}}\\log\\frac{|S|T}{\\delta}}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we have studied the statistical performance of the distributional temporal difference learning (TD) from a non-asymptotic perspective. Specifically, we have considered two instances of distributional TD, namely, the non-parametric distributional TD (NTD) and the categorical distributional TD (CTD). For both NTD and CTD, we have shown that $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)}\\end{array}$ iterations are sufficient to achieve a $p$ Wasserstein $\\varepsilon$ -optimal estimator, which is minimax optimal (up to logarithmic factors). We have established a novel Freedman's inequality in Hilbert spaces to prove these theoretical results, which has independent theoretical value beyond the current work. We leave the details to Appendix A. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work has been supported by the National Key Research and Development Project of China (No. 2022YFA1004002), the National Natural Science Foundation of China (No. 12271011 and No. 12350001), and the MOE Project of Key Research Institute of Humanities and Social Sciences (No.22JJD110001). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In International conference on machine learning, pages 449-458. PMLR, 2017.   \nM. G. Bellemare, W. Dabney, and M. Rowland. Distributional Reinforcement Learning. MIT Press, 2023. http://www.distributional-rl.org.   \nM. Bock and C. Heitzinger. Speedy categorical distributional reinforcement learning and complexity analysis. SIAM Journal on Mathematics of Data Science, 4(2):675-693, 2022. doi: 10.1137/ 20M1364436. URL https ://doi . org/10 .1137/20M1364436.   \nM. Bock, J. Mall, D. Pasterk, H. Kukina, R. Hasani, and C. Heitzinger. Superhuman performance on sepsis mimic-ii data by distributional reinforcement learning. PLoS One, 17(11):e0275358, 2022.   \nV. 1. Bogachev. Measure theory, volume 1. Springer, 2007.   \nW. Dabney, M. Rowland, M. Bellemare, and R. Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAl Conference on Artificial Intelligence, 2018.   \nV. H. de la Pena. A general class of exponential inequalities for martingales and ratios. The Annals of Probability, 27(1):537-564, 1999.   \nR. Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019.   \nD. A. Freedman. On tail probabilities for martingales. The Annals of Probability, pages 100-118, 1975.   \nM. Gheshlaghi Azar, R.Munos, and H. J. Kappen. Minimax pac bounds on the samle complexity of reinforcement learning with a generative model. Machine learning, 91:325-349, 2013.   \nE. Ghysels, P. Santa-Clara, and R. Valkanov. There is a risk-return trade-off after all. Journal of financial economics, 76(3):509-548, 2005.   \nJ. K. Hunter and B. Nachtergaele. Applied analysis. World Scientific Publishing Company, 2001.   \nS. M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.   \nM. Kearns, Y. Mansour, and A. Y. Ng. A sparse sampling algorithm for near-optimal planning in large markov decision processes. Machine learning, 49:193-208, 2002.   \nP. W. Lavori and R. Dawson. Dynamic treatment regimes: practical design considerations. Clinical trials, 1(1):9-20, 2004.   \nG. Li, C. Cai, Y. Chen, Y. Wei, and Y. Chi. Is q-learning minimax optimal? a tight sample complexity analysis. Operations Research, 72(1):222-236, 2024.   \nT. Morimura, M. Sugiyama, H. Kashima, H. Hachiya, and T. Tanaka. Nonparametric return distribution approximation for reinforcement learning. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 799-806, 2010.   \nA. Pananjady and M. J. Wainwright. Instance-dependent $\\ell_{\\infty}$ -bounds for policy evaluation in tabular reinforcement learning. IEEE Transactions on Information Theory, 67(1):566-585, 2020.   \nI. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, pages 1679-1706, 1994.   \nG. Pisier. Martingales in Banach Spaces. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2016. doi: 10.1017/CB09781316480588.   \nH. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400-407, 1951.   \nM. Rowland, M. Bellemare, W. Dabney, R. Munos, and Y. W. Teh. An analysis of categorical distributional reinforcement learning. In International Conference on Artijficial Intelligence and Statistics, pages 29-37. PMLR, 2018.   \nM. Rowland, R. Munos, M. G. Azar, Y. Tang, G. Ostrovski, A. Harutyunyan, K. Tuyls, M. G. Bellemare, and W. Dabney. An analysis of quantile _ temporal-difference learning.  Journal of Machine Learning Research, 25(163):1-47, 2024a.  URL http://jmlr.org/papers/v25/23-0154.html.   \nM. Rowland, L. K. Wenliang, R. Munos, C. Lyle, Y. Tang, and W. Dabney. Near-minimax-optimal distributional reinforcement learning with a generative model. arXiv preprint arXiv:2402.07598, 2024b.   \nR. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:9-44, 1988.   \nA. Talebi, G. Sadeghi, and M. Moslehian. Freedman inequality in noncommutative probability spaces. Complex Analysis and Operator Theory, 16(2):22, 2022.   \nJ. Tropp._ , Freedman's inequality for matrix martingales.a 1 Electronic Communications in Probability, 16(none):262 - 270, 2011. doi:10.1214/ECP.v16-1624. URL https://doi.0rg/10.1214/ECP.v16-1624.   \nC. Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \nR. Wu, M. Uehara, and W. Sun. Distributional offine policy evaluation with predictive error guarantees. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 4Oth International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 37685-37712. PMLR, 23-29 Jul 2023. URL https: //proceedings.mlr.press/v202/wu23s.html.   \nL. Zhang, Y. Peng, J. Liang, W. Yang, and Z. Zhang._Estimation and inference in distributional reinforcement learning. arXiv preprint arXiv:2309.17262, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A The Key Lemma: Freedman's Inequality in Hilbert Spaces ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Freedman's inequality, proposed in [Freedman, 1975], can be viewed as a Bernstein's inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Compared to the Azuma-Hoeffding inequality which only utilizes the boundedness of martingale difference sequences, Freedman's inequality incorporates second-order information, namely the quadratic variation (cumulative conditional variance) of martingales. This may leads to a sharper concentration result. It has various generalizations, such as matrix Freedman's inequality [Tropp, 2011]. However, to the best of our knowledge, a Freedman's inequality in Hilbert spaces has not been established yet. Just as Freedman's inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]), it is indispensable for deriving the minimax non-asymptotic convergence bound for distributional TD. ", "page_idx": 12}, {"type": "text", "text": "In this section, we will present a Freedman's inequalities in Hilbert spaces. Firstly, we will state a Hilbert space version of the original Freedman's inequality (Theorem 1.6 in [Freedman, 1975]). After that, we state a generalization of a more powerful version (Theorem 6 in [Li et al., 2024]) to Hilbert spaces. We will provide self-contained proofs in Appendix A.1, primarily inspired by Theorem 3.2 in [Pinelis, 1994]. The necessary knowledge of martingale theory for the proofs can be found in any standard textbook, such as [Durrett, 2019]. ", "page_idx": 12}, {"type": "text", "text": "Let $\\mathcal{X}$ be a Hilbert space, $\\{X_{i}\\}_{i=1}^{n}$ be an $\\mathcal{X}$ -valued martingale difference sequence adapted to the filtration $\\begin{array}{r}{\\{\\mathcal{F}_{i}\\}_{i=1}^{n},Y_{i}:=\\sum_{j=1}^{i}X_{j}}\\end{array}$ be the corresponding martingale, $\\begin{array}{r}{W_{i}:=\\sum_{j=1}^{i}\\sigma_{j}^{2}}\\end{array}$ be the corresponding quadratic variation process. Here $\\sigma_{j}^{2}\\::=\\:\\mathbb{E}_{j-1}\\left\\|X_{j}\\right\\|^{2}$ , and $\\mathbb{E}_{i}\\left[\\cdot\\right]\\;:=\\;\\mathbb{E}\\left[\\cdot\\left|\\mathcal{F}_{i}\\right]$ is the conditional expectation. ", "page_idx": 12}, {"type": "text", "text": "Theorem A.1 (Freedman's inequality in Hilbert spaces). Suppose $\\mathrm{max}_{i\\in[n]}\\left\\|X_{i}\\right\\|\\leq\\,b$ for some constant $b>0$ Then, for any $\\varepsilon,\\sigma>0$ the following inequality holds ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists k\\in[n],s.t.\\ \\|Y_{k}\\|\\ge\\varepsilon\\,a n d\\,W_{k}\\le\\sigma^{2}\\right)\\le2\\exp\\left\\{-\\frac{\\varepsilon^{2}/2}{\\sigma^{2}+b\\varepsilon/3}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now, we are ready to state the generalization of Theorem 6 in [Li et al., 2024] to Hilbert spaces, which is used in our non-asymptotic analysis. ", "page_idx": 12}, {"type": "text", "text": "Theorem A.2 (Freedman's inequality in Hilbert spaces with bounded quadratic variation). Suppose $\\mathrm{max}_{i\\in[n]}\\left\\|X_{i}\\right\\|\\leq b$ and $W_{n}\\leq\\sigma^{2}$ for some constant $b,\\sigma>0$ .Then, for any $\\delta\\,\\in\\,(0,1)$ , and any positive integer $H\\ge1$ ,thefollowinginequalityholdswithprobabilityatleast $1-\\delta$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\lVert Y_{n}\\rVert\\leq\\sqrt{8\\operatorname*{max}\\left\\{W_{n},\\frac{\\sigma^{2}}{2^{H}}\\right\\}\\log\\frac{2H}{\\delta}}+\\frac{4}{3}b\\log\\frac{2H}{\\delta}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The proof can be found in Appendix A.2 ", "page_idx": 12}, {"type": "text", "text": "Remark 2: Theorem 2.1 can be straightforwardly extended to the case where $\\left(\\Vert X_{i}\\Vert\\right)_{i=1}^{n}$ satisfies the Bernstein condition (Theorem 1.2A in [de la Pena, 1999]), thereby relaxing the boundedness assumption on $\\|X_{i}\\|$ . Namely, $\\begin{array}{r}{\\mathbb{E}_{i-1}\\left\\|X_{i}\\right\\|^{k}\\,\\leq\\,\\frac{1}{2}k!\\sigma_{i}^{2}b^{k-2}}\\end{array}$ for some $b~>~0$ , and for all $i\\;\\in\\;[n]$ $k\\in\\{2,3,\\cdots\\}$ . In this case, Freedman's inequality stili holds, albeit with a worse constant. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists k\\in[n],\\mathrm{s.t.}\\ \\Vert Y_{k}\\Vert\\ge\\varepsilon\\mathrm{~and~}W_{k}\\le\\sigma^{2}\\right)\\le2\\exp\\left\\{-\\frac{\\varepsilon^{2}/2}{\\sigma^{2}+b\\varepsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The proof only requires making appropriate modifications after the fth line of Equation (12). Note that Bernstein condition holds if $\\mathrm{max}_{i\\in[n]}\\left\\|X_{i}\\right\\|\\leq b$ ", "page_idx": 12}, {"type": "text", "text": "A.1Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. For any $\\lambda>0$ $t\\in[0,1]$ and $j\\in[n]$ , let $\\phi(t)=\\phi_{j,\\lambda}(t):=\\mathbb{E}_{j-1}\\cosh\\left(\\lambda\\left\\|Y_{j-1}+t X_{j}\\right\\|\\right)=$ $\\mathbb{E}_{j-1}\\cosh\\left(\\lambda u(t)\\right)$ , where $u(t):=\\|Y_{j-1}+t X_{j}\\|$ . We aim to use the Newton-Leibniz formula to establish the relationship between $\\phi(1)=\\mathbb{E}_{j-1}\\cosh{(\\lambda\\,\\|Y_{j}\\|)}$ and $\\phi(0)=\\cosh{(\\lambda\\,\\|Y_{j-1}\\|)}$ . This will allow usto construct a positive supermartingale $(B_{i})_{i=0}^{n}$ By utizing the positive supermartingale and optional stopping theorem, we can derive the desired concentration inequality. ", "page_idx": 12}, {"type": "text", "text": "Firstly, we calculate the derivative of $\\phi$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nu^{\\prime}(t)=\\frac{\\langle Y_{j-1}+t X_{j},X_{j}\\rangle}{u(t)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{\\prime}(t)=\\lambda\\mathbb{E}_{j-1}\\left[\\sinh\\left(\\lambda u(t)\\right)u^{\\prime}(t)\\right]}\\\\ &{\\qquad=\\lambda\\mathbb{E}_{j-1}\\left[\\sinh\\left(\\lambda u(t)\\right)\\frac{\\left\\langle Y_{j-1}+t X_{j},X_{j}\\right\\rangle}{u(t)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{\\prime}(0)=\\lambda\\mathbb{E}_{j-1}\\left[\\sinh\\left(\\lambda u(0)\\right)\\frac{\\langle Y_{j-1},X_{j}\\rangle}{u(0)}\\right]}\\\\ &{\\qquad=\\lambda\\sinh\\left(\\lambda\\left\\|Y_{j-1}\\right\\|\\right)\\frac{\\langle Y_{j-1},\\mathbb{E}_{j-1}\\left[X_{j}\\right]\\rangle}{\\left\\|Y_{j-1}\\right\\|}}\\\\ &{\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By utilizing Newton-Leibniz formula, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\phi(1)=\\phi(0)+\\int_{0}^{1}\\phi^{\\prime}(s)d s}\\\\ {\\displaystyle\\qquad=\\phi(0)+\\int_{0}^{1}\\int_{0}^{s}\\phi^{\\prime\\prime}(t)d t d s}\\\\ {\\displaystyle\\qquad=\\phi(0)+\\int_{0}^{1}(1-t)\\phi^{\\prime\\prime}(t)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, we calculate the second order derivate of $\\phi$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{\\prime\\prime}(t)=\\lambda\\mathbb{E}_{j-1}\\left\\{\\frac{d}{d t}\\left[\\sinh\\left(\\lambda u(t)\\right)u^{\\prime}(t)\\right]\\right\\}}\\\\ &{\\qquad=\\lambda\\mathbb{E}_{j-1}\\left[\\lambda\\left(u^{\\prime}(t)\\right)^{2}\\cosh\\left(\\lambda u(t)\\right)+u^{\\prime\\prime}(t)\\sinh\\left(\\lambda u(t)\\right)\\right]}\\\\ &{\\qquad\\le\\lambda^{2}\\mathbb{E}_{j-1}\\left[\\left(\\left(u^{\\prime}(t)\\right)^{2}+u^{\\prime\\prime}(t)u(t)\\right)\\cosh\\left(\\lambda u(t)\\right)\\right]}\\\\ &{\\qquad=\\frac{\\lambda^{2}}{2}\\mathbb{E}_{j-1}\\left[\\left(u^{2}\\right)^{\\prime\\prime}(t)\\cosh\\left(\\lambda u(t)\\right)\\right]}\\\\ &{\\qquad=\\lambda^{2}\\mathbb{E}_{j-1}\\left[\\|X_{j}\\|^{2}\\cosh\\left(\\lambda\\left\\|Y_{j-1}+t X_{j}\\right\\|\\right)\\right]}\\\\ &{\\qquad\\le\\lambda^{2}\\cosh\\left(\\lambda\\left\\|Y_{j-1}\\right\\|\\right)\\mathbb{E}_{j-1}\\left[\\left\\|X_{j}\\right\\|^{2}\\exp\\left(\\lambda t\\left\\|X_{j}\\right\\|\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in the third line, we used $\\begin{array}{r}{u^{\\prime\\prime}(t)\\ =\\ \\frac{\\|X_{j}\\|^{2}u(t)-\\frac{\\left\\langle Y_{j-1}+t X_{j},X_{j}\\right\\rangle^{2}}{u(t)}}{u^{2}(t)}\\ \\geq\\ 0}\\end{array}$ by Cauchy-Schwarz inequality, and $h(x)\\ =\\ x\\cosh(x)\\ -\\sinh(x)\\ \\geq\\ 0$ for any $x~\\ge~0$ , the inequality holds because $h(0)~=~0$ and $h^{\\prime}(x)~=~x\\sinh(x)~\\geq~0$ for any $x~\\ge~0$ . In the fourth line, we used $\\left(u^{2}\\right)^{\\prime\\prime}(t)=2\\left(\\left(u^{\\prime}(t)\\right)^{2}+u^{\\prime\\prime}(t)u(t)\\right)$ . In the fifth line, we used ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(u^{2}\\right)^{\\prime\\prime}(t)={\\frac{d^{2}}{d t^{2}}}\\left\\|Y_{j-1}+t X_{j}\\right\\|^{2}={\\frac{d}{d t}}\\left(2\\left\\langle Y_{j-1}+t X_{j},X_{j}\\right\\rangle\\right)=2\\left\\|X_{j}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "And in the last line, we used ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\cosh\\left(\\lambda\\left\\|Y_{j-1}+t X_{j}\\right\\|\\right)\\leq\\cosh\\left(\\lambda\\left\\|Y_{j-1}\\right\\|\\right)\\exp\\left(\\lambda t\\left\\|X_{j}\\right\\|\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "this holds since ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(\\lambda\\left\\|Y_{j-1}+t X_{j}\\right\\|\\right)\\leq\\exp\\left\\{\\lambda\\left(\\left\\|Y_{j-1}\\right\\|+t\\left\\|X_{j}\\right\\|\\right)\\right\\}=\\exp\\left(\\lambda\\left\\|Y_{j-1}\\right\\|\\right)\\exp\\left(\\lambda t\\left\\|X_{j}\\right\\|\\right),}\\\\ &{\\exp\\left(-\\lambda\\left\\|Y_{j-1}+t X_{j}\\right\\|\\right)\\leq\\exp\\left\\{-\\lambda\\left(\\left\\|Y_{j-1}\\right\\|-t\\left\\|-X_{j}\\right\\|\\right)\\right\\}=\\exp\\left(-\\lambda\\left\\|Y_{j-1}\\right\\|\\right)\\exp\\left(\\lambda t\\left\\|X_{j}\\right\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{j-1}\\left|\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j}\\right|\\right)=\\delta(1)-\\delta(0)+\\int_{0}^{1}(1-t)\\delta^{\\prime\\prime}(\\delta)d t}\\\\ &{\\le\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}\\eta+{\\lambda}^{2}\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}-\\left[{\\lambda}\\right]\\right)^{2}\\int_{0}^{1}(1-t)\\exp\\left({\\lambda}\\pi\\left|{\\mathbb X}_{j}\\right|\\right)d t\\right]}\\\\ &{=\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}\\eta+{\\lambda}^{2}\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}\\eta\\right)\\mathbb{E}_{j-1}\\left[{\\lambda}\\right]{\\mathbb Y}_{j}^{2}\\exp\\left({\\lambda}\\left|{\\mathbb X}_{j}\\right|\\right)-\\lambda\\mathbb{E}_{j}{\\mathbb Y}_{j}^{1}\\left[-\\left|{\\lambda}\\right|{\\mathbb X}_{j}\\right|\\right]}\\\\ &{=\\mathbb{E}_{j-1}\\left[\\exp\\left({\\lambda}\\right|{\\mathbb X}_{j}\\right|\\right)-\\lambda\\left|{\\mathbb X}_{j}\\right|\\right]\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}\\eta\\right)}\\\\ &{=\\mathbb{E}_{j-1}\\left[1+\\displaystyle\\sum_{s=0}^{1}\\left(k-{\\frac{\\lambda}{2}}\\right)(\\lambda\\left|{\\mathbb X}_{j}\\right|)^{1+2}\\left[\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}\\right])\\right.}\\\\ &{\\left.\\le\\mathbb{E}_{j-1}\\left[1+\\displaystyle\\sum_{s=2}^{1}\\left({\\frac{\\lambda}{2}}\\right)^{2}\\displaystyle\\sum_{k=0}^{2}\\left({\\frac{\\lambda}{3}}\\right)^{k}\\right]\\cosh\\left({\\lambda}\\right|{\\mathbb Y}_{j-1}^{1}\\eta\\right)}\\\\ &{=\\left(1+\\displaystyle\\sum_{s=0}^{\\lambda/2}\\lambda^{2}\\right)\\cosh\\left(\\lambda\\left|{\\mathbb Y}_{j-1}^{1}\\right|\\right)}\\\\ &{\\le\\exp\\left\\{\\frac{\\lambda^{2}\\sigma^{2}}{(1-\\lambda\\rho/3)}}\\right)\\cosh\\left(\\lambda\\left|{\\mathbb Y}_{j-1}^{1}\\right|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "whih hlds for any $\\textstyle\\lambda\\in(0,{\\frac{3}{b}})$ $\\begin{array}{r}{e^{x}=\\sum_{k=0}^{\\infty}\\frac{x^{k}}{k!}}\\end{array}$ Inthe sixth line, we used $(k+2)!\\geq2(3^{k})$ and $\\|X_{j}\\|\\leq b$ In the seventh line, we used Taylor expansion $\\textstyle{\\frac{1}{1-x}}=\\sum_{k=0}^{\\infty}x^{k}$ for $x\\in(-1,1)$ ", "page_idx": 14}, {"type": "equation", "text": "$\\begin{array}{r}{B_{0}:=1,\\,B_{i}:=\\exp\\left\\{-\\frac{\\lambda^{2}W_{i}}{2\\left(1-\\lambda b/3\\right)}\\right\\}\\cosh{\\left(\\lambda\\left\\Vert Y_{i}\\right\\Vert\\right)},\\,\\mathrm{then}}\\end{array}$ ", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{i-1}\\left[B_{i}\\right]=\\exp\\left\\{-\\frac{\\lambda^{2}W_{i-1}}{2(1-\\lambda b/3)}\\right\\}\\exp\\left\\{-\\frac{\\lambda^{2}\\sigma_{i}^{2}}{2(1-\\lambda b/3)}\\right\\}\\mathbb{E}_{i-1}\\left[\\cosh\\left(\\lambda\\left\\Vert Y_{i}\\right\\Vert\\right)\\right]}\\\\ {\\displaystyle\\leq\\exp\\left\\{-\\frac{\\lambda^{2}W_{i-1}}{2(1-\\lambda b/3)}\\right\\}\\cosh\\left(\\lambda\\left\\Vert Y_{i-1}\\right\\Vert\\right)}\\\\ {\\displaystyle=B_{i-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "i.e., $(B_{i})_{i=0}^{n}$ is positive supermartingale. By optional stopping theorem (Theorem 4.8.4 in [Durrett, 2019]), for any stopping time $\\tau$ ,wehave $\\mathbb{E}\\left[B_{\\tau}\\right]\\leq\\mathbb{E}\\left[B_{0}\\right]=\\bar{1}$ ", "page_idx": 14}, {"type": "text", "text": "Let $\\tau:=\\operatorname*{inf}\\left\\{k\\in[n]:\\|Y_{k}\\|\\geq\\varepsilon\\right\\}$ be a stopping time, and inf $\\varnothing:=\\infty$ . Define an event ", "page_idx": 14}, {"type": "equation", "text": "$$\nA:=\\left\\{\\exists k\\in[n],\\mathbf{s.t.}\\ \\lVert Y_{k}\\rVert\\geq\\varepsilon\\ \\mathrm{and}\\ W_{k}\\leq\\sigma^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then on $A$ , we have $\\tau<\\infty$ \uff0c\uff0c $\\|Y_{\\tau}\\|\\geq\\varepsilon$ and $W_{\\tau}\\leq\\sigma^{2}$ , noting that $W_{k}$ is non-decreasing with $k$ . Our goal is to provide an upper bound for $\\mathbb{P}(A)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(A)=\\mathbb{E}\\left[\\sqrt{B_{T}}\\frac{1}{\\sqrt{B_{T}}}\\mathbf{I}(A)\\right]}\\\\ &{\\quad\\quad\\leq\\sqrt{\\mathbb{E}\\left[B_{T}\\right]\\mathbb{E}\\left[\\frac{1}{B_{T}}\\mathbf{I}(A)\\right]}}\\\\ &{\\quad\\quad\\leq\\sqrt{\\mathbb{E}\\left[\\frac{\\exp\\Big\\{\\frac{3\\pi W_{\\tau}}{2(1-\\lambda\\kappa)/3}\\Big\\}_{\\mathbf{I}(A)}\\right]}{\\cosh\\left(\\lambda\\right)\\left[Y\\right]}}\\mathbf{I}(A)\\Bigg]}\\\\ &{\\quad\\quad\\leq\\sqrt{\\mathbb{E}\\left[\\frac{\\exp\\Big\\{\\frac{3\\pi\\sigma^{2}}{2(1-\\lambda\\kappa)/3}\\Big\\}_{\\mathbf{I}(A)}\\right]}{\\cosh\\left(\\lambda\\right)}}}\\\\ &{\\quad\\quad\\leq\\sqrt{\\mathbb{E}\\left[\\frac{\\exp\\Big\\{-\\lambda\\kappa/3\\}}{\\cosh\\left(\\lambda\\right)}\\mathbf{I}(A)\\right]}}\\\\ &{\\quad\\quad\\leq\\sqrt{2\\exp\\Big\\{-\\lambda\\varepsilon+\\frac{\\lambda^{2}\\sigma^{2}}{2(1-\\lambda\\kappa/3)}\\Big\\}}\\mathbb{P}(A),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the second line, we used Cauchy-Schwarz inequality. In the third line, we used $\\mathbb{E}\\left[B_{\\tau}\\right]\\leq1$ In the fourth line, we used $\\|Y_{\\tau}\\|\\geq\\varepsilon$ and $W_{\\tau}\\leq\\sigma^{2}$ on $A$ , and $\\cosh(x)$ is increasing when $x\\geq0$ . In the last line, we used $\\cosh(\\chi)\\geq{\\frac{1}{2}}e^{x}$ ", "page_idx": 15}, {"type": "text", "text": "Hence for any $\\textstyle\\lambda\\in(0,{\\frac{3}{b}})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(A)\\leq2\\exp\\left\\{-\\lambda\\varepsilon+\\frac{\\lambda^{2}\\sigma^{2}}{2\\left(1-\\lambda b/3\\right)}\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we an coose $\\begin{array}{r}{\\lambda^{\\star}=\\frac{\\varepsilon}{\\sigma^{2}+\\varepsilon b/3}\\in(0,\\frac{3}{b})}\\end{array}$ then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\operatorname{\\mathbb{P}}(A)\\leq2\\exp\\left\\{-\\lambda^{\\star}\\varepsilon+\\frac{\\left(\\lambda^{\\star}\\right)^{2}\\sigma^{2}}{2\\left(1-\\lambda^{\\star}b/3\\right)}\\right\\}}\\\\ {\\,}&{\\,=2\\exp\\left\\{-\\frac{\\varepsilon^{2}}{\\sigma^{2}+\\varepsilon b/3}+\\frac{\\sigma^{2}}{2\\left(1-\\frac{\\varepsilon b/3}{\\sigma^{2}+\\varepsilon b/3}\\right)}\\frac{\\varepsilon^{2}}{\\left(\\sigma^{2}+\\varepsilon b/3\\right)^{2}}\\right\\}}\\\\ {\\,}&{\\,=2\\exp\\left\\{-\\frac{\\varepsilon^{2}/2}{\\sigma^{2}+\\varepsilon b/3}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is the desired conclusion. ", "page_idx": 15}, {"type": "text", "text": "A.2Proof of Theorem A.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. According to Theorem 2.1, for any $\\varepsilon,\\tilde{\\sigma}>0$ ,wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|Y_{n}\\|\\ge\\varepsilon\\mathrm{~and~}W_{n}\\le\\tilde{\\sigma}^{2}\\right)\\le2\\exp\\left\\{-\\frac{\\varepsilon^{2}/2}{\\tilde{\\sigma}^{2}+b\\varepsilon/3}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can check that, when $\\begin{array}{r}{\\varepsilon=\\sqrt{4\\tilde{\\sigma}^{2}\\log\\frac{2}{\\delta}}+\\frac{4}{3}b\\log\\frac{2}{\\delta}}\\end{array}$ we have the upper bound on RHS is less than $\\delta$ ,hence ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|Y_{n}\\|\\ge\\sqrt{4\\tilde{\\sigma}^{2}\\log\\frac{2}{\\delta}}+\\frac{4}{3}b\\log\\frac{2}{\\delta}\\mathrm{~and~}W_{n}\\le\\tilde{\\sigma}^{2}\\right)\\le\\delta.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define the events ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}_{H}:=\\left\\{\\|Y_{n}\\|\\ge\\sqrt{8\\operatorname*{max}\\left\\{W_{n},\\frac{\\sigma^{2}}{2^{H}}\\right\\}\\log\\frac{2H}{\\delta}}+\\frac{4}{3}b\\log\\frac{2H}{\\delta}\\right\\},}\\\\ &{\\mathfrak{H}_{0,H}:=\\left\\{\\|Y_{n}\\|\\ge\\sqrt{8\\frac{\\sigma^{2}}{2^{H}}\\log\\frac{2H}{\\delta}}+\\frac{4}{3}b\\log\\frac{2H}{\\delta}\\mathrm{~and~}W_{n}\\le\\frac{\\sigma^{2}}{2^{H-1}}\\right\\}}\\\\ &{\\mathfrak{H}_{k,H}:=\\left\\{\\|Y_{n}\\|\\ge\\sqrt{8\\frac{\\sigma^{2}}{2^{k}}\\log\\frac{2H}{\\delta}}+\\frac{4}{3}b\\log\\frac{2H}{\\delta}\\mathrm{~and~}\\frac{\\sigma^{2}}{2^{k}}\\le W_{n}\\le\\frac{\\sigma^{2}}{2^{k-1}}\\right\\},\\qquad1\\le k\\le H-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $W_{n}\\leq\\sigma^{2}$ , we can verify that $\\mathcal{H}_{H}\\subseteq\\bigcup_{0\\leq k<H}\\mathcal{B}_{k,H}$ By th inequality (19) with $\\begin{array}{r}{\\tilde{\\sigma}^{2}=\\frac{\\sigma^{2}}{2^{k-1}}}\\end{array}$ and $\\delta$ set to be $\\frac{\\delta}{H}$ we have $\\begin{array}{r}{\\mathbb{P}\\left(\\mathcal{B}_{k,H}\\right)\\,\\leq\\,\\frac{\\delta}{H}}\\end{array}$ for all $k=0,1,\\cdots\\,,H-1$ .By the union bound, we arrive at the conclusion ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{H}_{H}\\right)\\leq\\sum_{k=0}^{H-1}\\mathbb{P}\\left(\\mathcal{B}_{k,H}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B  Minimax Lower Bound of Distributional Policy Evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we still consider infinite-horizon tabular MDP defined in Section 2, and assume a generative model is accessible. For any positive integer $D$ , we define ${\\mathfrak{M}}(D)$ as the set of all MDPs with state space size $|S|=D$ . For any MDP $M$ and policy $\\pi$ , we denote $V_{M}^{\\pi}$ as the corresponding value function, and $\\eta_{M}^{\\pi}$ as the corresponding return distribution. ", "page_idx": 16}, {"type": "text", "text": "Now, we can state the minimax lower bound of the distributional policy evaluation task in the 1- Wassersteinmetric. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.1 (Minimax lower bound of distributional policy evaluation in the 1-Wasserstein metric). For anypositive integer $D\\geq3$ and samplesize $\\begin{array}{r}{T\\ge\\frac{`C}{1-\\gamma}\\log\\frac{D}{2}}\\end{array}$ the followingresult holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\eta}}\\operatorname*{sup}_{M\\in\\mathfrak{M}(D)}\\operatorname*{sup}_{\\pi}\\mathbb{E}\\left[\\bar{W}_{1}\\left(\\hat{\\eta},\\eta_{M}^{\\pi}\\right)\\right]\\geq\\frac{c}{(1-\\gamma)^{3/2}}\\sqrt{\\frac{\\log\\frac{D}{2}}{T}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $c,C\\;>\\;0$ are universal constants, and the infimum $\\hat{\\eta}\\,\\in\\,\\mathcal{P}^{D}$ ranges over all measurable functionsof $T$ samples from the generative model. ", "page_idx": 16}, {"type": "text", "text": "The theorem states that for any algorithm, there exist corresponding MDP $M$ and policy $\\pi$ , such that to ensure $\\mathbb{E}\\left[\\bar{W}_{1}\\left(\\hat{\\eta},\\eta_{M}^{\\pi}\\right)\\right]\\le\\varepsilon$ for some $\\varepsilon>0$ , at least $\\tilde{\\Omega}\\left(\\frac{1}{\\varepsilon^{2}(1\\!-\\!\\gamma)^{3}}\\right)$ samples are required. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem B.1. For any $\\eta\\in\\mathcal P^{D}$ ,we define $\\mathcal{V}(\\eta)\\in\\mathbb{R}^{D}$ as the entry-wise expectation of $\\eta$ It is easy to check that $\\gamma_{\\left(\\eta_{M}^{\\pi}\\right)\\l}=V_{M}^{\\pi}$ . And recall the dual representation of 1-Wasserstein metric (Corollary 5.16 in [Villani et al., 2009]) ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{1}(\\mu,\\nu)=\\operatorname*{sup}_{f:\\ f{\\mathrm{is~}}1\\cdot\\mathrm{Lipschitz}}\\left|\\mathbb{E}_{X\\sim\\mu}\\left[f(X)\\right]-\\mathbb{E}_{Y\\sim\\nu}\\left[f(Y)\\right]\\right|,\\quad\\forall\\mu,\\nu\\in\\mathcal{P},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have $\\bar{W}_{1}\\left(\\hat{\\eta},\\eta_{M}^{\\pi}\\right)\\geq\\|\\mathcal{V}(\\hat{\\eta})-V_{M}^{\\pi}\\|_{\\infty}.$ Hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\hat{\\eta}}{\\operatorname*{inf}}\\ \\underset{M\\in\\mathfrak{M}(D)}{\\operatorname*{sup}}\\ \\underset{\\pi}{\\operatorname*{sup}}\\mathbb{E}\\left[\\bar{W}_{1}\\left(\\hat{\\eta},\\eta_{M}^{\\pi}\\right)\\right]\\geq\\underset{\\hat{\\eta}}{\\operatorname*{inf}}\\ \\underset{M\\in\\mathfrak{M}(D)}{\\operatorname*{sup}}\\ \\underset{\\pi}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\lVert\\mathcal{V}(\\hat{\\eta})-V_{M}^{\\pi}\\right\\rVert_{\\infty}\\right]}\\\\ &{\\geq\\underset{\\hat{V}}{\\operatorname*{inf}}\\ \\underset{M\\in\\mathfrak{M}(D)}{\\operatorname*{sup}}\\ \\underset{\\pi}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left\\lVert\\hat{V}-V_{M}^{\\pi}\\right\\rVert_{\\infty}\\right]}\\\\ &{\\geq\\frac{c}{(1-\\gamma)^{3/2}}\\sqrt{\\frac{\\log\\frac{D}{2}}{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality holds because $\\mathcal{V}\\left(\\hat{\\eta}\\right)\\,\\in\\,\\mathbb{R}^{D}$ is also a measurable function of $T$ samples from the generative model, and the infimum $\\hat{V}\\,\\in\\,\\mathbb{R}^{D}$ ranges over all measurable functions of $T$ samples from the generative model. And the last inequality is due to Theorem 2(b) in [Pananjady and Wainwright, 2020]. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C  Omitted Proofs in Section 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Remove the Dependence on T for Step Sizes ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We have shown that the conclusion holds for ", "page_idx": 17}, {"type": "equation", "text": "$$\nT\\geq\\frac{C_{4}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{3}}\\log\\frac{|S|\\,T}{\\delta},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{5}(1-\\sqrt{\\gamma})T}{\\log^{2}T}}\\leq\\alpha_{t}\\leq\\frac{1}{1+\\frac{c_{6}(1-\\sqrt{\\gamma})t}{\\log^{2}T}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c_{5}c_{6}\\leq\\frac{1}{8}$ \uff0c $c_{5}>c_{6}>0$ and $C_{4}>0$ ", "page_idx": 17}, {"type": "text", "text": "Then for some $c_{2}>c_{3}>0$ to be determined, now we assume ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{c_{2}(1-\\sqrt{\\gamma})t}{\\log^{2}t}}\\le\\alpha_{t}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})t}{\\log^{2}t}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we will show that if we consider the result of the $\\textstyle{\\frac{T}{2}}$ -th iteration with this step size scheme as the initialization of a new iteration process, then the step sizes in the subsequent $\\textstyle{\\frac{T}{2}}$ iterations lie in the previously established range. If this is done, the conclusion still holds if we choose $T\\ \\geq$ log , since the initialization m/2 E  (or inthe case of CTD) is independent of the samples obtained for $\\begin{array}{r}{\\frac{T}{2}<t\\leq T}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "For any $\\begin{array}{r}{\\frac{T}{2}<t\\leq T}\\end{array}$ wedenote $\\begin{array}{r}{\\tau:=t-\\frac{T}{2}}\\end{array}$ we can see that there exist $c_{2}>c_{3}>0$ such that the last inequality in both of the following lines hold simultaneously, which is desired. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}_{\\tau}:=\\alpha_{t}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})(\\tau+T/2)}{\\log^{2}(\\tau+T/2)}}\\le\\frac{1}{1+\\frac{c_{3}(1-\\sqrt{\\gamma})\\tau}{\\log^{2}T}}\\le\\frac{1}{1+\\frac{c_{6}(1-\\sqrt{\\gamma})\\tau}{\\log^{2}(T/2)}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}_{\\tau}=\\alpha_{t}\\geq\\frac{1}{1+\\frac{c_{2}(1-\\sqrt{\\gamma})(\\tau+T/2)}{\\log^{2}(\\tau+T/2)}}\\geq\\frac{1}{1+\\frac{2c_{2}(1-\\sqrt{\\gamma})T/2}{\\log^{2}(T/2)}}\\geq\\frac{1}{1+\\frac{c_{5}(1-\\sqrt{\\gamma})T/2}{\\log^{2}(T/2)}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2  Range of Step Size ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 5.1. ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1-\\sqrt{\\gamma})\\alpha_{t}\\geq\\frac{1-\\sqrt{\\gamma}}{1+\\frac{c_{5}(1-\\sqrt{\\gamma})T}{\\log^{2}T}}\\geq\\frac{1-\\sqrt{\\gamma}}{\\frac{2c_{5}(1-\\sqrt{\\gamma})T}{\\log^{2}T}}=\\frac{\\log^{2}T}{2c_{5}T}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For any $0\\leq k\\leq\\frac{t}{2}$ \uff0c", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{k}^{(t)}\\leq\\left[1-\\alpha_{t/2}(1-\\sqrt{\\gamma})\\right]^{t/2}}\\\\ &{\\leq\\left(1-\\frac{\\log^{2}T}{2c_{5}T}\\right)^{t/2}}\\\\ &{\\leq\\left(1-\\frac{\\log^{2}T}{2c_{5}T}\\right)^{\\frac{T}{2c_{6}\\log T}}}\\\\ &{=\\left\\{\\left(1-\\frac{\\log^{2}T}{2c_{5}T}\\right)^{\\frac{2c_{5}T}{\\log^{2}T}}\\right\\}^{\\frac{\\log T}{4c_{5}c_{6}}}}\\\\ &{\\leq\\frac{1}{T^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the last inequality, we used $c_{5}c_{6}\\leq\\frac{1}{8}$ ", "page_idx": 17}, {"type": "text", "text": "And for any $\\begin{array}{r}{\\frac{t}{2}<k\\leq t}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta_{k}^{(t)}\\leq\\alpha_{k}\\leq\\frac{1}{\\frac{c_{6}(1-\\sqrt{\\gamma})k}{\\log^{2}T}}\\leq\\frac{2\\log^{3}T}{(1-\\sqrt{\\gamma})T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.3  Concentration of the Martingale Term ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 5.2.We wil show that the inequality holds for each $\\begin{array}{r}{t\\geq\\frac{T}{c_{6}\\log T}}\\end{array}$ and then apply the union bound. For any $s\\in S$ ,wedenote ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\zeta_{k}(s):=\\zeta_{k}^{(t)}(s)=\\alpha_{k}\\left\\{\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})\\mathbb{Z}+\\alpha_{i}\\mathcal{T}\\right](\\mathcal{T}_{k}-\\mathcal{T})\\,\\eta_{k-1}\\right\\}(s),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we omit the superscript $(t)$ for brevity,then LHS in the lemma equals $\\left\\|\\sum_{k=1}^{t}\\zeta_{k}\\right\\|$ for each $t$ . Let $\\mathcal{F}_{k}$ denote the $\\sigma$ -field that contains all information up to time step $k$ , then $\\left\\{\\zeta_{k}(s)\\right\\}_{k=1}^{t}$ is a $\\left\\{\\mathcal{F}_{k}\\right\\}_{k=1}^{t}$ -martingale difference sequence: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{k-1}\\left[\\zeta_{k}(s)\\right]=\\alpha_{k}\\left\\{\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})\\mathcal{T}+\\alpha_{i}\\mathcal{T}\\right]\\mathbb{E}_{k-1}\\left[\\left(\\mathcal{T}_{k}-\\mathcal{T}\\right)\\eta_{k-1}\\right]\\right\\}(s)=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the first equality holds because a Bochner integral can be exchanged with a bounded linear operator (see Pisier [2016] for more details about Bochner integral), and the second equality holds due to the definition of the empirical distributional Bellman operator. ", "page_idx": 18}, {"type": "text", "text": "We hope to use Freedman's inequality (Theorem A.2) to bound this martingale. To this end, we need to give a deterministic upper bound of the martingale difference sequence, and an upper bound of its quadratic variation. ", "page_idx": 18}, {"type": "text", "text": "Deterministic upper bound of $\\mathrm{max}_{k\\in[t]}\\,\\|\\zeta_{k}(s)\\|$ .The norm of the martingale difference $\\|\\zeta_{k}(s)\\|$ can be bounded as follow ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\zeta_{k}(s)\\right\\|\\leq\\left\\|\\zeta_{k}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\alpha_{k}\\left\\|\\displaystyle\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})Z+\\alpha_{i}T\\right]\\right\\|\\left\\|({\\mathcal{T}}_{k}-{\\mathcal{T}})\\,\\eta_{k-1}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\alpha_{k}\\displaystyle\\prod_{i=k+1}^{t}\\left((1-\\alpha_{i})+\\alpha_{i}\\sqrt{\\gamma}\\right)\\displaystyle\\frac{1}{\\sqrt{1-\\gamma}}}\\\\ &{\\qquad=\\displaystyle\\frac{{\\beta}_{k}^{(t)}}{\\sqrt{1-\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{k\\in[t]}\\|\\zeta_{k}(s)\\|\\leq\\frac{\\operatorname*{max}_{k\\in[t]}\\beta_{k}^{(t)}}{\\sqrt{1-\\gamma}}\\leq\\frac{1}{\\sqrt{1-\\gamma}}\\operatorname*{max}\\left\\{\\frac{1}{T^{2}},\\frac{2\\log^{3}T}{(1-\\sqrt{\\gamma})T}\\right\\}\\leq\\frac{4\\log^{3}T}{(1-\\gamma)^{3/2}T}=:b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Upper bound of quadratic variation. Now, let's calculate the quadratic variation. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first introduce some notations. For any $k\\in\\mathbb N$ we denote $\\mathsf{V a r}(\\pmb{\\xi}):=\\left(\\mathbb{E}\\left[\\left\\|\\pmb{\\xi}(s)\\right\\|^{2}\\right]\\right)_{s\\in\\mathcal{S}}\\in\\mathbb{R}^{S},$ $\\mathsf{V a r}_{k}(\\pmb{\\xi}):=\\left(\\mathbb{E}_{k}\\left[\\left\\|\\pmb{\\xi}(s)\\right\\|^{2}\\right]\\right)_{s\\in\\mathcal{S}}\\in\\mathbb{R}^{S}$ for any random element $\\xi$ .n $\\mathcal{M}^{\\mathcal{S}}$   \nFor any $\\xi\\in\\mathcal{M}^{s}$ , we define its one-step update Cram\u00e9r variation as $\\pmb{\\sigma}(\\xi):=\\mathsf{V a r}\\left((\\widehat{T}-T)\\xi\\right)\\in$ $\\mathbb{R}^{S}$ , where $\\widehat{\\tau}$ is a random operator and has the same distribution as $\\tau_{1}$   \nFor any $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}\\ \\in\\ \\mathbb{R}^{S}$ , we say $\\textbf{\\textit{x}}\\leq\\textbf{\\textit{y}}$ if $\\pmb{x}(s)~\\leq~\\pmb{y}(s)$ for all $s\\ \\in\\ S$ . In this part, $\\left\\|x\\right\\|\\;:=$ $\\begin{array}{r}{\\|\\pmb{x}\\|_{\\infty}\\,=\\,\\operatorname*{max}_{s\\in\\mathcal{S}}|\\pmb{x}(s)|,\\,\\sqrt{\\pmb{x}}\\,:=\\,\\left(\\sqrt{\\pmb{x}(s)}\\right)_{\\textnormal{\\tiny{s}c}}.}\\end{array}$ And for any $\\boldsymbol{U}\\,\\in\\,\\mathbb{R}^{S\\times S}$ \uff0c $\\|\\pmb{U}\\|\\,:=\\,\\|\\pmb{U}\\|_{\\infty}\\,=$ $\\begin{array}{r}{\\operatorname*{sup}_{\\pmb{x}\\in\\mathbb{R}^{S},\\|\\pmb{x}\\|=1}\\|\\pmb{U}\\pmb{x}\\|=\\operatorname*{max}_{\\pmb{s}\\in S}\\sum_{\\substack{s^{\\prime}\\in S}}|\\pmb{U}(\\mathscr{s},\\mathscr{s}^{\\prime})|}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "We denote $\\pmb{I}\\in\\mathbb{R}^{S\\times S}$ as the identity matrix, $\\mathbf{1}\\in\\mathbb{R}^{S}$ as the all-ones vector, and $P:=P^{\\pi}\\in\\mathbb{R}^{S\\times S}$ \uff0c i.e., $\\begin{array}{r}{\\pmb{P}(s,s^{\\prime}):=\\pmb{P}^{\\pi}(s^{\\prime}|s)=\\sum_{a\\in\\mathcal{A}}\\dot{\\pi}(a|s)\\b{P}(s^{\\prime}|s,a)}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "With these notations, the quadratic variation is $\\begin{array}{r}{W_{t}:=\\sum_{k=1}^{t}\\mathsf{V a r}_{k-1}\\left(\\zeta_{k}\\right)}\\end{array}$ . To bound the quadratic variation $W_{t}$ , we need to bound $\\mathsf{V a r}_{k-1}\\left(\\zeta_{k}\\right)$ ", "page_idx": 19}, {"type": "text", "text": "Lemma C.1. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{V a r}_{k-1}\\left(\\zeta_{k}\\right)\\leq\\alpha_{k}\\beta_{k}^{(t)}\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]\\pmb{\\sigma}(\\eta_{k-1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, the quadratic variation $W_{t}$ can be bounded as follow ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{t}=\\displaystyle\\sum_{k=1}^{t}\\mathrm{Su}_{\\ell-1}\\left(\\zeta_{k}\\right)}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{t}\\alpha_{k}\\beta_{k}^{(t)}\\prod_{i=1}^{t}\\left\\{(1-\\alpha_{i})F^{*}P_{i}\\right\\}\\sigma(\\eta_{k-1})}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{t/2}\\alpha_{k}\\beta_{k}^{(t)}\\left\\|\\prod_{i=1}^{t}\\left\\{(1-\\alpha_{i})F^{*}P_{i}\\right\\}\\left\\|\\sigma(\\eta_{k-1})\\right\\|+\\displaystyle\\sum_{k=t/2+1}^{t}\\alpha_{k}\\beta_{k}^{(t)}\\prod_{i=t+1}^{t}\\left\\{(1-\\alpha_{i})\\right\\}}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{t/2}\\beta_{k}^{(t)}\\left\\|\\operatorname*{lim}_{1\\to\\gamma}^{(t)}\\right\\|+\\Big(\\operatorname*{max}_{i\\in\\mathbb{R}^{d}}\\beta_{k}^{(t)}\\Big)\\displaystyle\\sum_{k=t/2+1}^{t}\\alpha_{k}\\prod_{i=t+1}^{t}\\left\\{(1-\\alpha_{i})F^{*}P_{i}\\right\\}\\sigma(\\eta_{k-1})}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{t}\\displaystyle\\sum_{1\\gamma=1}^{1}+\\frac{2\\log^{3}T}{\\left(1-\\gamma\\right)T}\\left\\{\\sum_{k=t/2+1}^{t}\\alpha_{k}\\prod_{i=t+1}^{t}\\left\\{(1-\\alpha_{i})F^{*}P_{i}\\right\\}\\right\\}\\operatorname*{max}_{i\\in\\mathbb{R}^{d}}\\gamma(\\eta_{k-1})}\\\\ &{\\le\\displaystyle\\sum_{j=1-\\gamma}^{1}\\sum_{k=1}^{16}\\displaystyle+\\frac{4\\log^{3}T}{\\left(1-\\gamma\\right)T}\\left\\{(t-\\sqrt{P})^{D}\\log\\frac{t}{\\log(t-1)}\\right\\}\\operatorname*{max}_{i\\in\\mathbb{R}^{d}}\\alpha(\\eta_{k-1}),}\\\\ &{\\le2(1-\\gamma)T^{3}+\\displaystyle\\frac{4\\log^{3}T}{\\left(1-\\gamma\\right)T}(t-\\sqrt{P})^{D}\\log\\frac{t}{\\log(t-1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in the fourth line, we used ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{k}\\left\\|\\prod_{i=k+1}^{t}[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P]\\right\\|\\leq\\alpha_{k}\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})+\\alpha_{i}\\sqrt{\\gamma}\\right]=\\beta_{k}^{(t)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pmb{\\sigma}(\\eta_{k-1})\\|\\leq\\int_{0}^{\\frac{1}{1-\\gamma}}d x=\\frac{1}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the last line, we used the fact that $\\operatorname*{max}_{k:t/2\\leq k<t}\\pmb{\\sigma}\\big(\\eta_{k-1}\\big)\\geq\\mathbf{0}$ and the following lemma: ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2. For any $t\\in\\mathbb{N},\\,(\\alpha_{i})_{i\\in[t]}\\in[0,1]^{t},$ thefollowing nequality lds entri: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=t/2+1}^{t}\\alpha_{k}\\prod_{i=k+1}^{t}\\left[I-\\alpha_{i}\\left(I-\\sqrt{\\gamma}P\\right)\\right]\\le(I-\\sqrt{\\gamma}P)^{-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to (35), we have the following deterministic upper bound for $\\|W_{t}\\|=\\operatorname*{max}_{s\\in S}W_{t}(s)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{t}\\|\\leq\\displaystyle\\frac{1}{2(1-\\gamma)T^{3}}+\\frac{4\\log^{3}T}{(1-\\gamma)T}\\left\\|(I-\\sqrt{\\gamma}P)^{-1}\\right\\|\\operatorname*{max}_{k:\\,t/2<k<\\underline{{\\epsilon}}t}\\|\\sigma(\\eta_{k-1})\\|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{2(1-\\gamma)T^{3}}+\\frac{8\\log^{3}T}{(1-\\gamma)^{3}T}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{9\\log^{3}T}{(1-\\gamma)^{3}T}}\\\\ &{\\qquad=:\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\begin{array}{r}{H=\\left\\lceil2\\log_{2}\\frac{1}{1-\\gamma}\\right\\rceil}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\sigma^{2}}{2^{H}}\\leq\\frac{9\\log^{3}T}{(1-\\gamma)T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By applying Freedman's inequality (Theorem A.2) and utilizing the union bound over $s\\,\\in\\,S$ ,we obtain with probability at least $1-\\delta$ ,for all $t\\in[T]$ and $s\\in S$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left\\|\\displaystyle\\sum_{k=1}^{t}\\zeta_{k}(s)\\right\\|\\right)_{s\\in\\mathcal{S}}}\\\\ &{\\leq\\sqrt{8\\left(W_{t}+\\frac{\\sigma^{2}}{2^{H}}\\right)\\log\\frac{8|\\mathcal{S}|T\\log\\frac{1}{1-\\gamma}}{\\delta}}+\\frac{4}{3}b\\log\\frac{8|\\mathcal{S}|T\\log\\frac{1}{1-\\gamma}}{\\delta}}\\\\ &{\\leq\\sqrt{16\\left(W_{t}+\\frac{9\\log^{3}T}{(1-\\gamma)T}\\right)\\log\\frac{|\\mathcal{S}|T}{\\delta}}+3b\\log\\frac{|\\mathcal{S}|T}{\\delta}}\\\\ &{\\leq8\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)T}}\\left[\\left(I-\\sqrt{\\gamma}P\\right)^{-1}\\operatorname*{max}_{k:t/2\\<k\\leq t}\\sigma(\\eta_{k-1})+3\\cdot1\\right]+\\frac{12\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{\\left(1-\\gamma\\right)^{3/2}T}\\left(1-\\gamma\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "h and CTD. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.3. For any $t\\in[T]$ ", "page_idx": 20}, {"type": "text", "text": "Lemma C.4. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\pmb\\sigma(\\eta_{t})-\\pmb\\sigma(\\eta)\\leq4\\,\\|\\Delta_{t}\\|_{\\bar{W}_{1}}\\,{\\bf1}.}}\\\\ {{\\qquad}}\\\\ {{({\\pmb I}-\\sqrt{\\gamma}{\\pmb P})^{-1}\\pmb\\sigma(\\eta)\\leq\\frac{4}{1-\\gamma}{\\bf1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining the upper bound with the two lemmas, we get the desired conclusion ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left\\lVert\\displaystyle\\sum_{k=1}^{t}\\zeta_{k}(s)\\right\\rVert\\right)_{s\\in\\mathcal{S}}}\\\\ &{\\le\\!\\sqrt{\\frac{(\\log^{3}T)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)T}}\\left[4\\!\\!\\!\\begin{array}{c}{\\!\\!\\!\\operatorname*{max}_{k:t/2<k\\le t}\\!\\!\\left\\lVert\\Delta_{k-1}\\right\\rVert_{\\bar{W}_{1}}(I-\\sqrt{\\gamma}P)^{-1}\\mathbf{1}\\!+\\!\\frac{8}{1-\\gamma}\\right\\rVert+\\!\\frac{12\\left(\\log^{3}T\\right)\\left(1-\\gamma\\right)}{(1-\\gamma)^{3}}\\right]}\\\\ {\\le\\!\\!22\\sqrt{\\frac{(\\log^{3}T)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{2}T}}\\left(1+\\!\\!\\!\\operatorname*{max}_{k:t/2<k\\le t}\\!\\!\\left\\lVert\\Delta_{k-1}\\right\\rVert_{\\bar{W}_{1}}\\right)\\mathbf{1}\\!+\\!\\frac{12\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{3/2}T}\\mathbf{1}}\\\\ {\\le\\!\\!34\\sqrt{\\frac{(\\log^{3}T)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{2}T}}\\left(1+\\!\\!\\operatorname*{max}_{k:t/2<k\\le t}\\!\\!\\left\\lVert\\Delta_{k-1}\\right\\rVert_{\\bar{W}_{1}}\\right)\\mathbf{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{T\\ge\\frac{C_{4}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{3}}\\log\\frac{|S|T}{\\delta}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "C.4 Solve the Recurrence Relation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem C.1. Suppose for all $\\begin{array}{r}{t\\geq\\frac{T}{c_{6}\\log T}}\\end{array}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Vert\\Delta_{t}\\Vert_{\\bar{W}_{1}}\\leq35\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}\\left(1+\\operatorname*{max}_{k:\\,t/2<k\\leq t}\\Vert\\Delta_{k-1}\\Vert_{\\bar{W}_{1}}\\right)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then there exists some large universal constant ${\\cal C}_{7}>0$ suchthat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\Delta_{T}\\right\\|_{\\bar{W}_{1}}\\leq C_{7}\\left(\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}}+\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For any $k\\geq0$ , we denote ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k}:=\\operatorname*{max}\\left\\{\\|\\Delta_{t}\\|_{\\bar{W}_{1}}~\\Big|~2^{k}\\frac{T}{c_{6}\\log T}\\leq t\\leq T\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $0\\le k\\le\\log_{2}\\left(c_{6}\\log T\\right)$ . We can see that $\\|\\Delta_{T}\\|_{\\bar{W}_{1}}\\leq u_{k}$ for any valid $k$ . Hence, it suffices to show the upper bound holds for $u_{k}$ for any valid $k$ . It can be verified that $\\begin{array}{r}{u_{0}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ , and for $k\\geq0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k+1}\\leq35\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}\\left(1+u_{k}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first show that once $u_{k}\\leq1$ , the subsequent values of $u_{k+l}$ will also remain upper bounded by $1$ Namely, if $u_{k}\\leq1$ for some $k\\geq1$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k+1}\\leq35\\sqrt{\\frac{2\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}}\\leq1,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "if $\\begin{array}{r}{T\\geq\\frac{2450\\log^{3}T\\log\\frac{|S|T}{\\delta}}{(1-\\gamma)^{3}}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Let $\\tau:=\\operatorname*{inf}\\left\\{k:u_{k}\\leq1\\right\\}$ , then for any $k>\\tau$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k}\\leq35\\sqrt{\\frac{2\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}}=:a.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $k\\leq\\tau$ , we have $u_{k}\\geq1$ and thereby ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k+1}\\leq35\\sqrt{\\frac{2\\left(\\log^{3}T\\right)\\left(\\log\\frac{|S|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}}u_{k}=a\\sqrt{u_{k}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log u_{k+1}-2\\log a\\leq\\frac12\\left(\\log u_{k}-2\\log a\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Apply it recursively, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log u_{k+1}\\leq2\\log a+\\left(\\frac12\\right)^{k+1}\\left(\\log u_{0}-2\\log a\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k+1}\\leq a^{2}\\left(\\frac{u_{0}}{a^{2}}\\right)^{1/2^{k}}=a^{2\\left(1-1/2^{k}\\right)}u_{0}^{1/2^{k}}\\leq a^{2\\left(1-1/2^{k}\\right)}\\frac{1}{(1-\\gamma)^{1/2^{k}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To sum up, for any $k\\geq0,\\,u_{k+1}$ is always less than the sum of the upper bounds in cases of $k>\\tau$ and $k\\leq\\tau$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nu_{k+1}\\leq a+a^{2\\left(1-1/2^{k}\\right)}\\frac{1}{(1-\\gamma)^{1/2^{k}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that, $a^{2\\left(1-1/2^{k}\\right)}\\leq\\operatorname*{max}\\left\\{a,\\sqrt{a}\\right\\}$ an if wetake $\\begin{array}{r}{k\\,\\geq\\,c_{8}\\log\\log\\frac{1}{1-\\gamma}}\\end{array}$ for any constant $c_{8}$ we have $\\begin{array}{r}{\\frac{1}{(1-\\gamma)^{1/2^{k}}}\\,=\\,O(1)}\\end{array}$ We can take the constant $c_{8}$ small enough such that $\\begin{array}{r}{c_{8}\\log\\log\\frac{1}{1-\\gamma}\\ <}\\end{array}$ $\\log_{2}\\left(c_{6}\\log T\\right)$ (this can be done and $c_{8}$ is universal since $\\begin{array}{r}{\\frac{1}{1-\\gamma}=o(T))}\\end{array}$ , and thereby we can find a valid $\\begin{array}{r}{k^{\\star}\\geq c_{8}\\log\\log\\frac{1}{1-\\gamma}+1}\\end{array}$ Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\Delta_{T}\\|_{\\bar{W}_{1}}\\leq u_{k^{\\star}}\\leq C_{7}\\left(\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T\\right)}{\\delta}}{(1-\\gamma)^{3}T}}+\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{3}T}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is the desired conclusion, and $C_{7}$ is some large universal constant related to $c_{8}$ ", "page_idx": 21}, {"type": "text", "text": "C.5  Analysis of Corollaries 4.1 and 4.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The difference in the proof compared to Section 5.2 arises in Lemma 5.2 when we control term (II). Now we further bound the result in Lemma C.3 by the Cram\u00e9r norm of the error term, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma(\\eta_{t})-\\sigma(\\eta)\\leq4\\left\\|\\Delta_{t}\\right\\|_{\\bar{W}_{1}}\\mathbf{1}\\leq\\frac{1}{\\sqrt{1-\\gamma}}\\left\\|\\Delta_{t}\\right\\|\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the same way, we can derive the following recurrence relation: with probability at least $1-\\delta$ for allt \u2265 cg logT ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\Delta_{t}\\right\\|\\leq35\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{5/2}T}\\left(1+\\operatorname*{max}_{k:\\,t/2<k\\leq t}\\left\\|\\Delta_{k-1}\\right\\|\\right)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By repeating the reasoning of Theorem C.1, we can obtain the desired conclusion, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\Delta_{T}\\|\\leq C_{7}\\left(\\sqrt{\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{5/2}T}}+\\frac{\\left(\\log^{3}T\\right)\\left(\\log\\frac{|\\mathcal{S}|T}{\\delta}\\right)}{(1-\\gamma)^{5/2}T}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is less than $\\varepsilon$ if we take $C_{4}\\,\\geq\\,2C_{7}^{2}$ and $\\begin{array}{r}{T\\,\\geq\\,\\frac{C_{4}\\log^{3}T}{\\varepsilon^{2}(1-\\gamma)^{5/2}}\\log\\frac{|S|T}{\\delta}}\\end{array}$ . Here, $C_{7}>1$ is a large universal constant depending on $c_{6}$ ", "page_idx": 22}, {"type": "text", "text": "C.6 Proof of Lemma C.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We frst inroduce some ntations. For any matrix of operators $\\mathcal{U}\\in\\mathcal{L}\\left(\\mathcal{M}\\right)^{S\\times S}$ , we denote $\\mathcal{U}(s)=(\\mathcal{U}(s,s^{\\prime}))_{s^{\\prime}\\in\\mathcal{S}}\\in\\mathcal{L}\\left(\\mathcal{M}\\right)^{s}$ as the $s$ -row of $\\boldsymbol{\\mathcal{U}}$ . And for any $\\xi\\in\\mathcal{M}^{s}$ , we define the vector inner product operation $\\begin{array}{r}{\\mathcal{U}(s)\\xi:=\\sum_{s^{\\prime}\\in\\mathcal{S}}\\mathcal{U}(s,s^{\\prime})\\xi(s^{\\prime})\\in\\mathcal{M}.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "We need the following lemma, which holds for both cases of NTD and CTD ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. For any $\\nu\\,\\in\\,{\\mathcal{M}}$ $n\\,\\in\\,\\mathbb{N},$ $(\\alpha_{i})_{i\\in[n]}\\;\\in\\;[0,1]^{n}$ ,let $\\begin{array}{r}{\\mathcal{U}_{n}\\,=\\,\\prod_{i=1}^{n}\\left[(1-\\alpha_{i})\\mathcal{Z}+\\alpha_{i}\\mathcal{T}\\right]\\!,}\\end{array}$ $\\begin{array}{r}{U_{n}\\,=\\,\\prod_{i=1}^{n}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]}\\end{array}$ \uff0c $\\begin{array}{r}{u_{n}\\,=\\,\\prod_{i=1}^{n}\\left[\\left(1-\\alpha_{i}\\right)+\\alpha_{i}\\sqrt{\\gamma}\\right]}\\end{array}$ then for any $s,s^{\\prime}\\,\\in\\,{\\mathcal{S}}_{}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{U}_{n}(s,s^{\\prime})\\nu\\right\\|^{2}\\leq u_{n}U_{n}(s,s^{\\prime})\\left\\|\\nu\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Utilizing this lemma, we get the following result. Recall that $\\widehat{\\tau}$ is a random operator and has the same distribution as $\\mathcal{T}_{1}$ . Then, for any non-random $\\xi\\in\\mathcal{M}^{s}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|U_{\\alpha}(s)(\\hat{T}-T)\\hat{\\mathcal{X}}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\sum_{\\ell\\in\\mathcal{S}}U_{\\alpha}(s,s^{\\prime})\\left[\\hat{T}(\\hat{T}-T)\\xi\\right](s^{\\prime})\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\sum_{\\ell\\in\\mathcal{S}}U_{\\alpha}(s,s^{\\prime})\\left[\\hat{T}(s^{\\prime})\\xi-T(s^{\\prime})\\xi\\right]\\right\\|^{2}\\right]}\\\\ &{=\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}\\mathbb{E}\\left[\\left\\|U_{\\alpha}(s,s^{\\prime})\\left[\\hat{T}(s^{\\prime})\\xi-T(s^{\\prime})\\xi\\right]\\right\\|^{2}\\right]}\\\\ &{=\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}\\mathbb{E}\\left[\\left\\|U_{\\alpha}(s,s^{\\prime})\\left[\\hat{T}(s^{\\prime})\\xi-T(s^{\\prime})\\xi\\right]\\right\\|^{2}\\right]}\\\\ &{\\overset{\\leq}c u_{s}\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}U_{\\alpha}(s,s^{\\prime})\\mathbb{E}\\left[\\left\\|\\hat{T}(s^{\\prime})\\xi-T(s^{\\prime})\\xi\\right\\|^{2}\\right]}\\\\ &{=\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}U_{\\alpha}(s,s^{\\prime})\\sigma(\\xi)(s^{\\prime})}\\\\ &{=\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}u_{s}(s)\\sigma(\\xi)(s^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used different rows of $\\widehat{\\tau}$ are independent, and $\\widehat{\\mathcal{T}}(s^{\\prime})\\xi$ is an unbiased estimator of $\\mathcal{T}(s^{\\prime})\\xi\\in$ $\\mathcal{M}$ Hence, Var $\\mathbf{\\nabla}\\cdot\\left(\\mathcal{U}_{n}(\\widehat{\\mathcal{T}}-\\mathcal{T})\\xi\\right)\\leq u_{n}U_{n}\\overset{\\cdot}{\\sigma}(\\xi)$ ", "page_idx": 23}, {"type": "text", "text": "Now, we are ready to bound $\\mathsf{V a r}_{k-1}\\left(\\zeta_{k}\\right)$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathsf{V a r}_{k-1}\\left(\\zeta_{k}\\right)=\\alpha_{k}^{2}\\mathsf{V a r}_{k-1}\\left(\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})\\bar{Z}+\\alpha_{i}\\bar{T}\\right](\\mathcal{T}_{k}-\\mathcal{T})\\,\\eta_{k-1}\\right)}\\\\ {\\displaystyle\\le\\alpha_{k}^{2}\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})+\\alpha_{i}\\sqrt{\\gamma}\\right]\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})\\bar{I}+\\alpha_{i}\\sqrt{\\gamma}\\,\\bar{P}\\right]\\sigma\\big(\\eta_{k-1}\\big)}\\\\ {\\displaystyle=\\alpha_{k}\\beta_{k}^{(t)}\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})\\bar{I}+\\alpha_{i}\\sqrt{\\gamma}\\,\\bar{P}\\right]\\sigma\\big(\\eta_{k-1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.7  Proof of Lemma C.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{k=t/2+1}^{t}\\alpha_{k}\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]}}\\\\ {{\\displaystyle=(I-\\sqrt{\\gamma}P)^{-1}\\sum_{k=t/2+1}^{t}\\alpha_{k}(I-\\sqrt{\\gamma}P)\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]}}\\\\ {{\\displaystyle=(I-\\sqrt{\\gamma}P)^{-1}\\sum_{k=t/2+1}^{t}\\left\\{\\prod_{i=k+1}^{t}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]-\\prod_{i=k}^{t}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]\\right\\}}}\\\\ {{\\displaystyle=(I-\\sqrt{\\gamma}P)^{-1}-(I-\\sqrt{\\gamma}P)^{-1}\\prod_{i=t/2+1}^{t}\\left[(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P\\right]}}\\\\ {{\\displaystyle\\leq(I-\\sqrt{\\gamma}P)^{-1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the inequality holds entry-wise since we can verify that all entries of $({\\cal I}\\mathrm{~-~}\\sqrt{\\gamma}{\\cal P})^{-1}\\;=\\;$ $\\scriptstyle\\sum_{k=0}^{\\infty}\\left({\\sqrt{\\gamma}}P\\right)^{k}$ and $(1-\\alpha_{i})I+\\alpha_{i}\\sqrt{\\gamma}P$ are non-negative. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.8  Proof of Lemma C.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. For any $s\\in S$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\sigma(\\eta_{t})(s)-\\sigma(\\eta)(s)}\\\\ &{=\\int_{0}^{1+\\frac{\\gamma}{2}}\\left\\{\\mathbb{E}\\left[F_{(\\bar{T}_{\\eta})(s)}^{2}(x)\\right]-F_{(T_{\\eta})(s)}^{2}(x)-\\mathbb{E}\\left[F_{(\\bar{T}_{\\eta})(s)}^{2}(x)\\right]+F_{(T_{\\eta})(s)}^{2}(x)\\right\\}d x}\\\\ &{=\\int_{0}^{1+\\frac{\\gamma}{2}}\\left\\{\\mathbb{E}\\left[F_{(\\bar{T}_{\\eta})(s)}^{2}(x)-F_{(\\bar{T}_{\\eta})(s)}^{2}(x)\\right]+F_{(T_{\\eta})(s)}^{2}(x)-F_{(T_{\\eta})(s)}^{2}(x)\\right\\}d x}\\\\ &{=\\int_{0}^{1+\\frac{\\gamma}{2}}\\left\\{\\mathbb{E}\\left[\\left(F_{(\\bar{T}_{\\eta})(s)}(x)-F_{(\\bar{T}_{\\eta})(s)}(x)\\right)\\left(F_{(\\bar{T}_{\\eta})(s)}(x)+F_{(\\bar{T}_{\\eta})(s)}(x)\\right)\\right]\\right.}\\\\ &{~~~~+\\left.\\left(F_{(T_{\\eta})(s)}(x)-F_{(T_{\\eta})(s)}(x)\\right)\\left(F_{(T_{\\eta})(s)}(x)+F_{(T_{\\eta})(s)}(x)\\right)\\right\\}d x}\\\\ &{\\leq2\\int_{0}^{1+\\frac{\\gamma}{2}}\\left\\{\\mathbb{E}\\left[\\left|F_{(\\bar{T}_{\\eta})(s)}(x)-F_{(\\bar{T}_{\\eta})(s)}(x)\\right|\\right]+\\left|F_{(T_{\\eta})(s)}(x)-F_{(T_{\\eta})(s)}(x)\\right|\\right\\}d x}\\\\ &{=2\\left(\\mathbb{E}\\left[\\left\\|\\hat{T}(\\eta_{n}-\\eta)(s)\\right\\|_{W_{s}}\\right]+\\left\\|T(\\eta_{n}-\\eta)(s)\\right\\|_{W_{s}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the case of NTD, $\\tau$ and $\\widehat{\\tau}$ are $\\gamma$ -contraction w.r.t. the supreme 1-Wasserstein metric, hence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\sigma}(\\eta_{t})(s)-\\pmb{\\sigma}(\\eta)(s)\\leq2\\left(\\mathbb{E}\\left[\\left\\Vert\\widehat{\\mathcal{T}}\\left(\\eta_{t}-\\eta\\right)(s)\\right\\Vert_{W_{1}}\\right]+\\left\\Vert\\mathcal{T}\\left(\\eta_{t}-\\eta\\right)(s)\\right\\Vert_{W_{1}}\\right)}\\\\ &{\\leq4\\gamma\\left\\Vert\\eta_{t}-\\eta\\right\\Vert_{\\bar{W}_{1}}}\\\\ &{\\leq4\\left\\Vert\\Delta_{t}\\right\\Vert_{\\bar{W}_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the case of CTD, if we can show $\\Pi_{K}$ is non-expansive w.r.t. 1-Wasserstein metric, the conclusion stll holds. For any $x,y\\in\\left[0,{\\frac{1}{1-\\gamma}}\\right]$ such that $x<y$ , we denote $x\\in[x_{k},x_{k+1})$ and $y\\in[x_{l},x_{l+1})$ \uff0c then $k\\leq l$ , by the definition of $\\Pi_{K}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi_{K}(\\delta_{x})=\\frac{x_{k+1}-y}{\\iota_{K}}\\delta_{x_{k}}+\\frac{y-x_{k}}{\\iota_{K}}\\delta_{x_{k+1}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi_{K}(\\delta_{y})=\\frac{x_{l+1}-y}{\\iota_{K}}\\delta_{x_{l}}+\\frac{y-x_{l}}{\\iota_{K}}\\delta_{x_{l+1}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$k\\ =\\ l$ $\\begin{array}{r l r}{W_{1}\\left(\\Pi_{K}\\delta_{x},\\Pi_{K}\\delta_{y}\\right)}&{{}\\!\\!=}&{\\!\\!\\iota_{K}\\frac{y-x}{\\iota_{K}}\\;=\\;y\\,-\\,x}\\end{array}$ $k<\\ l$ we have \u0418 $V_{1}\\left(\\Pi_{K}\\delta_{x},\\Pi_{K}\\delta_{y}\\right)\\leq W_{1}\\left(\\Pi_{K}\\delta_{x},x_{k+1}\\right)+W_{1}\\left(x_{k+1},x_{l}\\right)+W_{1}\\left(x_{l},\\Pi_{K}\\delta_{y}\\right)=\\left(x_{k+1}-x\\right)+\\left(x_{l}-x_{l}\\right)\\delta_{x}$ $x_{k+1})+(y-x_{x_{l}})=y-x$ . Hence, for any $\\nu_{1},\\nu_{2}\\in\\mathcal{P}$ and for anytransport plan $\\kappa\\in\\Gamma(\\nu_{1},\\nu_{2})$ \uff0c the previous results tell us the cost of the transport plan $\\Pi_{K^{K}}\\in\\Gamma\\left(\\Pi_{K}\\nu_{1},\\Pi_{K^{\\nu_{2}}}\\right)$ induced by $\\Pi_{K}$ is no greater than the cost of $\\kappa$ . Consequently, $W_{1}\\left(\\Pi_{K}\\nu_{1},\\Pi_{K}\\nu_{2}\\right)\\leq W_{1}(\\nu_{1},\\nu_{2})$ ,i.e., $\\Pi_{K}$ is nonexpansive w.r.t. 1-Wasserstein metric, which is desired. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.9  Proof of Lemma C.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Firstly, we show that for any $\\pmb{v}\\geq\\mathbf{0}$ ,we have $\\left\\|(\\pmb{I}-\\sqrt{\\gamma}\\pmb{P})^{-1}\\pmb{v}\\right\\|\\leq2\\left\\|(\\pmb{I}-\\gamma\\pmb{P})^{-1}\\pmb{v}\\right\\|$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|(I-\\sqrt{\\gamma}P)^{-1}v\\|=\\left\\|(I-\\sqrt{\\gamma}P)^{-1}(I-\\gamma P)(I-\\gamma P)^{-1}v\\right\\|}&{}\\\\ {=\\left\\|(I-\\sqrt{\\gamma}P)^{-1}\\left[(1-\\sqrt{\\gamma})I+\\sqrt{\\gamma}(I-\\sqrt{\\gamma}P)\\right](I-\\gamma P)^{-1}v\\right\\|}\\\\ {=\\left\\|\\left[(1-\\sqrt{\\gamma})(I-\\sqrt{\\gamma}P)^{-1}+\\sqrt{\\gamma}I\\right](I-\\gamma P)^{-1}v\\right\\|}\\\\ {\\leq(1-\\sqrt{\\gamma})\\left\\|(I-\\sqrt{\\gamma}P)^{-1}(I-\\gamma P)^{-1}v\\right\\|+\\sqrt{\\gamma}\\left\\|(I-\\gamma P)^{-1}v\\right\\|}\\\\ {\\leq\\left(\\frac{1-\\sqrt{\\gamma}}{1-\\sqrt{\\gamma}}+\\sqrt{\\gamma}\\right)\\left\\|(I-\\gamma P)^{-1}v\\right\\|}\\\\ {\\leq2\\left\\|(I-\\gamma P)^{-1}v\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the case of NTD, by Corollary D.1, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|(I-\\gamma P)^{-1}\\pmb{\\sigma}\\left(\\eta\\right)\\right\\|\\leq\\frac{1}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the case of CTD, by Corollary 5.12 in [Rowland et al., 2024b], we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|(\\boldsymbol{I}-\\gamma\\boldsymbol{P})^{-1}\\sigma\\left(\\eta\\right)\\right\\|\\leq\\frac{2}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "given $K>{\\frac{4}{1-\\gamma}}$ ", "page_idx": 24}, {"type": "text", "text": "C.10 Proof of Lemma C.5 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. We proof this result by induction. For $n=0$ , we have $\\mathcal{U}_{0}=\\mathcal{T}$ $U_{0}=I$ $u_{0}=1$ , thereby the inequality holds trivially. Suppose the inequality holds true for $n-1$ . To prove that the inequality holds for $n$ , it is sufficient to show that, for any $\\mu\\in\\mathcal{M}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left[(1-\\alpha_{n})\\delta_{s,s^{\\prime}}+\\alpha_{n}\\mathcal{T}(s,s^{\\prime})\\right]\\mu\\right\\|^{2}\\leq\\left[(1-\\alpha_{n})+\\alpha_{n}\\sqrt{\\gamma}\\right]\\left[(1-\\alpha_{n})\\delta_{s,s^{\\prime}}+\\alpha_{n}\\sqrt{\\gamma}P(s,s^{\\prime})\\right]\\left\\|\\mu\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\delta_{s,s^{\\prime}}=1$ if $s=s^{\\prime}$ , and O otherwise. ", "page_idx": 24}, {"type": "text", "text": "LHS can be bounded as follow ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left[(1-\\alpha_{n})\\delta_{s,s^{\\prime}}+\\alpha_{n}\\mathcal{T}(s,s^{\\prime})\\right]\\mu\\right\\|^{2}}\\\\ &{=\\!(1-\\alpha_{n})^{2}\\delta_{s,s^{\\prime}}\\left\\|\\mu\\right\\|^{2}+2(1-\\alpha_{n})\\alpha_{n}\\delta_{s,s^{\\prime}}\\left\\langle\\mu,\\mathcal{T}(s,s^{\\prime})\\mu\\right\\rangle+\\alpha_{n}^{2}\\left\\|\\mathcal{T}(s,s^{\\prime})\\mu\\right\\|^{2}}\\\\ &{\\le\\!(1-\\alpha_{n})^{2}\\delta_{s,s^{\\prime}}\\left\\|\\mu\\right\\|^{2}+2(1-\\alpha_{n})\\alpha_{n}\\delta_{s,s^{\\prime}}\\left\\|\\mu\\right\\|\\left\\|\\mathcal{T}(s,s^{\\prime})\\mu\\right\\|+\\alpha_{n}^{2}\\left\\|\\mathcal{T}(s,s^{\\prime})\\mu\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used Cauchy-Schwarz inequality. We need to give an upper bound for $\\|\\boldsymbol{\\mathcal{T}}(s,s^{\\prime})\\boldsymbol{\\mu}\\|^{2}$ ", "page_idx": 25}, {"type": "text", "text": "Note that $\\left(\\Pi_{K}{\\mathcal{T}}^{\\pi}\\right)\\left(s,s^{\\prime}\\right)=\\Pi_{K}\\left({\\mathcal{T}}^{\\pi}(s,s^{\\prime})\\right)$ and $\\|\\boldsymbol{\\Pi}_{K}\\|=1$ , we only need to consider the case of NTD, by the definition of $\\mathcal{T}(s,s^{\\prime})$ ,we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T(s,s^{\\prime})\\mu\\|^{2}=\\int_{0}^{1\\frac{1}{\\gamma-\\gamma}}\\left[\\displaystyle\\sum_{\\alpha\\in A}\\pi(a|s)P(s^{\\prime}|s,a)\\int_{0}^{1}F_{\\mu}\\left(\\frac{x-r}{\\gamma}\\right)\\mathcal{P}_{R}(d r|s,a)\\right]^{2}d x}\\\\ &{\\qquad=P(s,s^{\\prime})^{2}\\int_{0}^{1\\frac{1}{\\gamma-\\gamma}}\\left[\\displaystyle\\sum_{\\alpha\\in A}\\frac{\\pi\\left(a|s\\right)P(s^{\\prime}|s,a)}{P(s,s^{\\prime})}\\int_{0}^{1}F_{\\mu}\\left(\\frac{x-r}{\\gamma}\\right)\\mathcal{P}_{R}(d r|s,a)\\right]^{2}d x}\\\\ &{\\qquad=P(s,s^{\\prime})^{2}\\int_{0}^{1\\frac{1}{\\gamma-\\gamma}}\\left\\{\\mathbb{E}_{a\\sim\\pi(\\cdot|s),r\\sim\\mathcal{P}_{R}(\\cdot|s,a)}\\left[F_{\\mu}\\left(\\frac{x-r}{\\gamma}\\right)|s^{\\prime}\\right]\\right\\}^{2}d x}\\\\ &{\\qquad\\leq P(s,s^{\\prime})^{2}\\mathbb{E}_{a\\sim\\pi(\\cdot|s),r\\sim\\mathcal{P}_{R}(\\cdot|s,a)}\\left\\{\\int_{0}^{1\\frac{1}{\\gamma-\\gamma}}\\left[F_{\\mu}\\left(\\frac{x-r}{\\gamma}\\right)\\right]^{2}d x\\right|s^{\\prime}\\right\\}}\\\\ &{\\qquad=\\gamma P(s,s^{\\prime})^{2}\\|\\mu\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used Jensen\u2019s inequality and Fubini's theorem. Substitute it back to the upper bound, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left[(1-\\alpha_{n})\\delta_{s,s^{\\prime}}+\\alpha_{n}\\mathcal{T}(s,s^{\\prime})\\right]\\mu\\right\\|^{2}}\\\\ &{\\leq\\!(1-\\alpha_{n})^{2}\\delta_{s,s^{\\prime}}\\left\\|\\mu\\right\\|^{2}+2(1-\\alpha_{n})\\alpha_{n}\\delta_{s,s^{\\prime}}\\left\\|\\mu\\right\\|\\left\\|\\mathcal{T}(s,s^{\\prime})\\mu\\right\\|+\\alpha_{n}^{2}\\left\\|\\mathcal{T}(s,s^{\\prime})\\mu\\right\\|^{2}}\\\\ &{\\leq\\left[(1-\\alpha_{n})^{2}\\delta_{s,s^{\\prime}}+2(1-\\alpha_{n})\\alpha_{n}\\delta_{s,s^{\\prime}}\\sqrt{\\gamma}P(s,s^{\\prime})+\\alpha_{n}^{2}\\gamma P(s,s^{\\prime})^{2}\\right]\\left\\|\\mu\\right\\|^{2}}\\\\ &{=\\left[(1-\\alpha_{n})^{2}\\delta_{s,s^{\\prime}}+\\alpha_{n}\\sqrt{\\gamma}P(s,s^{\\prime})\\right]^{2}\\left\\|\\mu\\right\\|^{2}}\\\\ &{\\leq\\left[(1-\\alpha_{n})+\\alpha_{n}\\sqrt{\\gamma}\\right]\\left[(1-\\alpha_{n})\\delta_{s,s^{\\prime}}+\\alpha_{n}\\sqrt{\\gamma}P(s,s^{\\prime})\\right]\\left\\|\\mu\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is desired. ", "page_idx": 25}, {"type": "text", "text": "D   Stochastic Distributional Bellman Equation and Operator ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we use the same notations as in Appendix $\\mathbf{C}$ and only consider the NTD setting. Inspired by stochastic categorical CDF Bellman operator introduced in [Rowland et al., 2024b], we introduce stochastic distributional Bellman operator $\\mathcal{T}\\colon\\Delta\\left(\\mathcal{P}^{S}\\right)\\rightarrow\\bar{\\Delta}\\left(\\mathcal{P}^{S}\\right)$ to derive an upper bound for $\\left\\|(\\boldsymbol{I}-\\gamma\\boldsymbol{P})^{-1}\\sigma(\\eta)\\right\\|$ in the case of NTD. For any $\\phi\\in\\Delta\\left({\\mathcal{P}}^{S}\\right)$ , we denote $\\eta_{\\phi}$ be the random element in ${\\mathcal{P}}^{S}$ with law $\\phi$ \uff1a ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{T}\\phi:=\\mathrm{Law}\\left(\\widehat{T}\\eta_{\\phi}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(\\widehat{\\mathcal{T}}\\eta_{\\phi})(\\omega)\\,:=\\,(\\widehat{\\mathcal{T}})(\\omega)(\\eta_{\\phi})(\\omega)\\,\\in\\,\\mathcal{P}^{S}$ for any $\\omega\\,\\in\\,\\Omega$ \uff0c $\\Omega$ is the corresponding probability space, and $\\widehat{\\tau}$ is independent of $\\eta_{\\phi}$ . In this part, $\\widehat{\\tau}$ does not consist of $\\Pi_{K}$ since we only consider the NTD setting. ", "page_idx": 25}, {"type": "text", "text": "We consider the 1-Wasserstein metric $W_{1}$ on $\\Delta\\left(\\mathcal{P}^{S}\\right)$ , the space of all probability measures on the space $\\left(\\mathcal{P}^{S},\\bar{\\ell}_{2}\\right)$ .Since $\\left(\\mathcal{P}^{S},\\bar{\\ell}_{2}\\right)$ is Polish (complete and separable), the space $\\left(\\Delta\\left(\\mathcal{P}^{S}\\right),W_{1}\\right)$ is also Polish (Theorem 6.18 in [Villani et al., 2009]). ", "page_idx": 25}, {"type": "text", "text": "Proposition D.1. The stochastic distributional Bellman operator $\\mathcal{T}$ is $a\\ \\sqrt{\\gamma}$ contractionon $\\Delta\\left(\\mathcal{P}^{S}\\right)$ , i.e., for any $\\phi,\\phi^{\\prime}\\in\\Delta\\left(\\mathcal{P}^{S}\\right)$ wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\nW_{1}\\left(\\mathcal{T}\\phi,\\mathcal{T}\\phi^{\\prime}\\right)\\leq\\sqrt{\\gamma}W_{1}\\left(\\phi,\\phi^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let $\\kappa^{\\star}\\,\\in\\,\\Gamma\\left(\\phi,\\phi^{\\prime}\\right)$ be the optimal coupling between $\\phi$ and $\\phi^{\\prime}$ .The existence of $\\kappa^{\\star}$ is guaranteed by Theorem 4.1 in [Villani et al., 2009]. And let the random element $\\pmb{\\xi}\\,=\\,(\\pmb{\\xi}_{1},\\pmb{\\xi}_{2})$ in $\\left(\\mathcal{P}^{S}\\right)^{2}$ has the law $\\kappa^{\\star}$ , where $\\pmb{\\xi}_{1}$ and $\\pmb{\\xi}_{2}$ are both random elements in ${\\mathcal{P}}^{S}$ . We denote $\\mathcal{T}\\kappa^{\\star}:=$ Law $\\left[\\left(\\widehat{\\mathcal{T}}\\xi_{1},\\widehat{\\mathcal{T}}\\xi_{2}\\right)\\right]\\in\\Gamma\\left(\\mathcal{T}\\phi,\\mathcal{T}\\phi^{\\prime}\\right).$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}\\left(\\mathcal{F}\\phi,\\mathcal{F}\\phi^{\\prime}\\right)=\\underset{\\kappa\\in\\Gamma}{\\mathrm{,inf}}\\frac{\\mathrm{inf}}{\\mathcal{F}\\phi},\\mathcal{F}\\phi^{\\prime}\\right)\\int_{(\\mathcal{F}^{\\delta})^{\\delta}}\\overline{{\\ell}}_{2}\\left(\\xi,\\xi^{\\prime}\\right)\\kappa\\left(d\\xi,d\\xi^{\\prime}\\right)}\\\\ &{\\leq\\int_{\\left(\\mathcal{F}^{\\delta}\\right)^{2}}\\overline{{\\ell}}_{2}\\left(\\xi,\\xi^{\\prime}\\right)\\mathcal{F}\\kappa^{*}\\left(d\\xi,d\\xi^{\\prime}\\right)}\\\\ &{=\\mathbb{E}\\left[\\overline{{\\ell}}_{2}\\left(\\widehat{T}\\xi_{1},\\widehat{T}\\xi_{2}\\right)\\right]}\\\\ &{\\leq\\sqrt{\\gamma}\\mathbb{E}\\left[\\overline{{\\ell}}_{2}\\left(\\xi_{1},\\xi_{2}\\right)\\right]}\\\\ &{=\\sqrt{\\gamma}\\int_{\\left(\\mathcal{F}^{\\delta}\\right)^{2}}\\overline{{\\ell}}_{2}\\left(\\xi,\\xi^{\\prime}\\right)\\kappa^{*}\\left(d\\xi,d\\xi^{\\prime}\\right)}\\\\ &{=\\sqrt{\\gamma}\\ \\underset{\\kappa\\in\\Gamma}{\\mathrm{,inf}}\\ \\int_{\\left(\\mathcal{F}^{\\delta}\\right)^{\\delta}}\\Big|_{\\left(\\xi,\\xi^{\\prime}\\right)^{\\delta}}\\kappa\\left(d\\xi,d\\xi^{\\prime}\\right)}\\\\ &{=\\sqrt{\\gamma}W_{1}\\left(\\phi,\\phi^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the proposition and contraction mapping theorem, there exists a unique fixed point of $\\mathcal{T}$ ,we denote $\\pmb{\\dot{\\psi}}\\in\\Delta\\left(\\mathcal{P}^{S}\\right)$ as the fixed point. Hence, the stochastic distributional Bellman equation reads ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\psi={\\mathcal{T}}\\psi.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We denote $\\eta_{\\psi}$ as the random element in $\\mathcal{P}$ with law $\\psi$ , then $\\hat{\\tau}\\eta_{\\psi}$ and $\\eta_{\\psi}$ have the same law. As shown in the following proposition, $\\eta_{\\psi}$ can be regarded as a noisy version of $\\eta$ ", "page_idx": 26}, {"type": "text", "text": "Proposition D.2. ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\eta_{\\psi}\\right]=\\eta,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the expectation is regarded as the Bochner integral in the space of all finite measures on ${\\mathcal{P}}^{S}$ which is a normed linear space equipped with Cramer metric as its norm. ", "page_idx": 26}, {"type": "text", "text": "Proof. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\eta_{\\psi}\\right]=\\mathbb{E}\\left[\\left.\\widehat{T}\\eta_{\\psi}\\right]}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left\\{\\mathbb{E}\\left[\\left.\\widehat{T}\\eta_{\\psi}\\middle|\\eta_{\\psi}\\right]\\right\\}}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left[T\\eta_{\\psi}\\right]}\\\\ &{\\qquad\\quad=\\mathcal{T}\\mathbb{E}\\left[\\eta_{\\psi}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used $\\widehat{\\tau}$ is independent of $\\eta_{\\psi}$ . Since $\\mathbb{E}\\left[\\eta_{\\psi}\\right]$ is the fxed point of $\\tau$ ,we have $\\mathbb{E}\\left[\\eta_{\\psi}\\right]\\;=$ $\\eta$ ", "page_idx": 26}, {"type": "text", "text": "Based on the concepts of $\\mathcal{T}$ and $\\psi$ , we can obtain the following second order distributional Bellman equation, which is similar to the classic second-order Bellman equation (Lemma 7 in [Gheshlaghi Azar et al., 2013]). ", "page_idx": 26}, {"type": "text", "text": "Recal the one-step rame variation $\\pmb{\\sigma}(\\eta)=\\left(\\mathbb{E}\\left[\\left\\|\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s)\\right\\|^{2}\\right]\\right)_{s\\in\\mathcal{S}}\\in\\mathbb{R}^{S}$ used in the NTD seting. We denote $\\pmb{\\sigma}:=\\pmb{\\sigma}(\\eta)$ for simplity, and $\\pmb{\\Sigma}:=\\left(\\mathbb{E}\\left[\\left\\|\\pmb{\\eta}_{\\psi}(s)-\\eta(s)\\right\\|^{2}\\right]\\right)_{s\\in\\mathcal{S}}\\in\\mathbb{R}^{S},$ ", "page_idx": 26}, {"type": "text", "text": "Proposition D.3 (Second order distributional Bellman equation). ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Sigma={\\pmb\\sigma}+\\gamma P\\Sigma.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. For any $s\\in S$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(s)=\\mathbb{E}\\left[\\left\\Vert\\eta_{\\psi}(s)-\\eta(s)\\right\\Vert^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\left\\Vert\\left(\\widehat{\\mathcal{T}}\\eta_{\\psi}\\right)(s)-\\eta(s)\\right\\Vert^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\left\\Vert\\left(\\widehat{\\mathcal{T}}\\eta_{\\psi}\\right)(s)-\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)+\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s)\\right\\Vert^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\left\\Vert\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s)\\right\\Vert^{2}\\right]+\\mathbb{E}\\left[\\left\\Vert\\left(\\widehat{\\mathcal{T}}\\eta_{\\psi}\\right)(s)-\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last equality holds since the cross term is zero as below ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s),\\left(\\widehat{\\mathcal{T}}\\eta_{\\psi}\\right)(s)-\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)\\right\\rangle\\right]}\\\\ &{=\\!\\mathbb{E}\\left\\{\\mathbb{E}\\left[\\left\\langle\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s),\\left(\\widehat{\\mathcal{T}}\\eta_{\\psi}\\right)(s)-\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)\\right\\rangle\\left|\\widehat{\\mathcal{T}}\\right]\\right\\}}\\\\ &{=\\!\\mathbb{E}\\left\\{\\left\\langle\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s),\\mathbb{E}\\left[\\left(\\widehat{\\mathcal{T}}\\eta_{\\psi}\\right)(s)\\middle|\\widehat{\\mathcal{T}}\\right]-\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)\\right\\rangle\\right\\}}\\\\ &{=\\!\\mathbb{E}\\left[\\left\\langle\\left(\\widehat{\\mathcal{T}}\\eta\\right)(s)-\\eta(s),\\mathbf{0}\\right\\rangle\\right]}\\\\ &{=\\!0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first term in (71) is ${\\pmb\\sigma}(s)$ , we need to deal with the second term. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\Vert\\left(\\hat{\\mathcal{T}}\\eta_{\\psi}\\right)\\left(s\\right)-\\left(\\hat{\\mathcal{T}}\\eta\\right)\\left(s\\right)\\right\\Vert^{2}\\right]}\\\\ &{=\\mathbb{E}\\left\\{\\mathbb{E}\\left[\\left\\Vert\\left(\\hat{\\mathcal{T}}\\eta_{\\psi}\\right)\\left(s\\right)-\\left(\\hat{\\mathcal{T}}\\eta\\right)\\left(s\\right)\\right\\Vert^{2}\\left\\Vert\\eta_{\\psi}\\right]\\right\\}}\\\\ &{=\\mathbb{E}\\left\\{\\mathbb{E}_{a\\left(s\\right)\\sim\\pi\\left(\\cdot\\left\\vert s\\right\\vert,s^{\\prime}\\left(s\\right)\\sim P\\left(\\cdot\\left\\vert s,a\\right(s)\\right),r\\left(s\\right)\\sim\\mathcal{P}_{R}\\left\\langle\\cdot\\left\\vert s,a(s)\\right\\rangle\\right)}\\left[\\int_{0}^{\\frac{1}{1-\\gamma}}\\left(F_{\\left(\\eta_{\\psi}\\right)\\left(s^{\\prime}\\left(s\\right)\\right)}\\left(\\frac{x-r}{\\gamma}\\right)-F_{\\eta\\left(s^{\\prime}\\left(s\\right)\\right)}\\left(\\frac{x}{\\gamma}\\right)\\right)\\right]\\right\\}}\\\\ &{=\\gamma\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}\\mathbb{E}\\left[\\left\\Vert\\eta_{\\psi}(s^{\\prime})-\\eta(s^{\\prime})\\right\\Vert^{2}\\right]\\sum_{a\\in A}\\pi(a|s)P(s^{\\prime}|s,a)}\\\\ &{\\quad-\\gamma\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s,s^{\\prime})\\Sigma(s^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Put these together, and we can arrive at the conclusion. ", "page_idx": 27}, {"type": "text", "text": "Now, we can derive a tighter upper bound for $\\left\\|(\\boldsymbol{I}-\\gamma\\boldsymbol{P})^{-1}\\sigma(\\eta)\\right\\|$ ", "page_idx": 27}, {"type": "text", "text": "Corollary D.1. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|(I-\\gamma P)^{-1}\\pmb{\\sigma}(\\eta)\\right\\|\\leq\\left\\|(I-\\gamma P)^{-1}\\pmb{\\sigma}\\right\\|=\\|\\pmb{\\Sigma}\\|\\leq\\frac{1}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Note that all entries of $\\begin{array}{r}{(I{-}\\gamma P)^{-1}=\\sum_{k=0}^{\\infty}\\left(\\gamma P\\right)^{k}}\\end{array}$ are positive, thereby $(I\\!-\\!\\gamma P)^{-1}\\sigma(\\eta)\\leq$ $(\\boldsymbol{I}-\\gamma\\boldsymbol{P})^{-1}\\boldsymbol{\\sigma}=\\boldsymbol{\\Sigma}$ and $\\begin{array}{r}{\\Sigma(s)=\\mathbb{E}\\left[\\left\\Vert\\eta_{\\psi}(s)-\\eta(s)\\right\\Vert^{2}\\right]\\leq\\int_{0}^{\\frac{1}{1-\\gamma}}d x=\\frac{1}{1-\\gamma}}\\end{array}$ for any $s\\in S$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "E Other Technical Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma E.1 (Basic inequalities for metrics on the space of probability measures). For any $\\mu,\\nu\\in\\mathcal{P}$ wehave $\\begin{array}{r}{W_{1}(\\mu,\\nu)\\leq\\frac{1}{\\sqrt{1-\\gamma}}\\ell_{2}(\\mu,\\nu)}\\end{array}$ and $\\begin{array}{r}{W_{p}(\\mu,\\nu)\\overset{\\cdot}{\\leq}\\frac{\\overset{\\cdot}{1}}{(1-\\gamma)^{1-\\frac{1}{p}}}W_{1}^{\\frac{1}{p}}(\\mu,\\nu).}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Proof. By Cauchy-Schwarz inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{W_{1}(\\mu,\\nu)=\\displaystyle\\int_{0}^{\\frac{1}{1-\\gamma}}|F_{\\mu}(x)-F_{\\nu}(x)|d x}}\\\\ {{\\displaystyle\\leq\\sqrt{\\displaystyle\\int_{0}^{\\frac{1}{1-\\gamma}}1^{2}d x}\\sqrt{\\displaystyle\\int_{0}^{\\frac{1}{1-\\gamma}}|F_{\\mu}(x)-F_{\\nu}(x)|^{2}d x}}}\\\\ {{\\displaystyle=\\frac{1}{\\sqrt{1-\\gamma}}\\ell_{2}(\\mu,\\nu).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "And ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{p}(\\mu,\\nu)=\\left(\\underset{\\kappa\\in\\Gamma(\\mu,\\nu)}{\\operatorname*{inf}}\\int_{\\left[0,\\frac{1}{1-\\gamma}\\right]^{2}}\\vert x-y\\vert^{p}\\,\\kappa(d x,d y)\\right)^{1/p}}\\\\ &{\\hphantom{\\sum_{\\kappa\\in\\Gamma(\\mu,\\nu)}^{(1)}\\,\\int_{\\left[0,\\frac{1}{1-\\gamma}\\right]^{2}}\\,\\vert x-y\\vert^{2}}\\leq\\frac{1}{(1-\\gamma)^{1-\\frac{1}{p}}}\\left(\\underset{\\kappa\\in\\Gamma(\\mu,\\nu)}{\\operatorname*{inf}}\\int_{\\left[0,\\frac{1}{1-\\gamma}\\right]^{2}}\\vert x-y\\vert\\,\\kappa(d x,d y)\\right)^{1/p}}\\\\ &{\\hphantom{\\sum_{\\kappa\\in\\Gamma(\\mu,\\nu)}^{(1)}\\,\\int_{\\left[0,\\frac{1}{1-\\gamma}\\right]^{2}}\\vert y\\vert^{\\frac{1}{p}}}=\\frac{1}{(1-\\gamma)^{1-\\frac{1}{p}}}W_{1}^{\\frac{1}{p}}(\\mu,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma E.2 (Range of $\\eta_{t}^{\\pi}$ ).Suppose that $\\alpha_{t}\\in[0,1]$ for all $t\\geq0$ Assume that $\\eta_{0}^{\\pi}\\in\\mathcal{P}^{s}$ , then we have, for all $t\\geq0$ $\\eta_{t}^{\\pi}\\in\\mathcal{P}^{s}$ in the case of NTD. Similarly, assume that $\\eta_{0}^{\\pi}\\in\\mathcal{P}_{K}^{S}$ , then we have, for all $t\\geq0$ $\\eta_{t}^{\\pi}\\in\\mathcal{P}_{K}^{S}$ in the case of CTD. ", "page_idx": 28}, {"type": "text", "text": "Proof. We will only prove the case of NTD, and the proof for CTD is similar by utilizing the property of the projectionoperator $\\Pi_{K}\\colon{\\mathcal{P}}^{S}\\ \\to\\ {\\mathcal{P}}_{K}^{S}$ .We prove the result by induction. It is trivial that $\\eta_{t}^{\\pi}\\in\\mathcal{P}^{S}$ for $t=0$ .Supposethat $\\eta_{t-1}^{\\pi}\\in\\mathcal{P}^{s}$ , recall the updating scheme of NTD ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\pi}=(1-\\alpha_{t})\\eta_{t-1}^{\\pi}+\\alpha_{t}T_{t}^{\\pi}\\eta_{t-1}^{\\pi}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is evident that ${\\mathcal{P}}^{S}$ is a convex set, considering that ${\\mathcal{P}}^{S}$ is a subset of the product signed measure space, which is a linear space. Therefore, we only need to show that $T_{t}^{\\pi}\\eta_{t-1}^{\\pi}\\in\\mathcal{P}^{s}$ , which trivially holds since $\\mathcal{T}_{t}^{\\pi}$ is a random operator mapping from ${\\mathcal{P}}^{S}$ to ${\\mathcal{P}}^{S}$ , and $\\eta_{t-1}^{\\pi}\\in\\mathcal{P}^{s}$ . By applying the induction argument, we can arrive at the conclusion. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The main claims reflect the paper's contributions and scope. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The theoretical assumptions used in the paper are widely accepted and supported by a rich body of references in the literature, and the references are cited in our paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper provide the full set of assumptions and a self-contained proof. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please  see  the  NeurIPs  code and data  submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropri ate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The research conform with NeurIPs Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper only focuses on theory, hence there is no negative societal impact. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper only focuses on theory. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetis used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the datacollector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}]