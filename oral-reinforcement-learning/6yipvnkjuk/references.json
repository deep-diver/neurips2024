{"references": [{"fullname_first_author": "M. G. Azar", "paper_title": "Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model", "publication_date": "2013-01-01", "reason": "This paper establishes fundamental sample complexity lower bounds for reinforcement learning, providing a benchmark against which the proposed Federated Q-learning algorithm is evaluated."}, {"fullname_first_author": "M. Wainwright", "paper_title": "Variance-reduced Q-learning is minimax optimal", "publication_date": "2019-01-01", "reason": "This paper introduces variance-reduced Q-learning, which serves as a key algorithmic component in Fed-DVR-Q, improving its sample efficiency."}, {"fullname_first_author": "G. Li", "paper_title": "Sample complexity of asynchronous Q-learning: Sharper analysis and variance reduction", "publication_date": "2021-01-01", "reason": "This paper provides a sharper analysis of asynchronous Q-learning, which is highly relevant to the federated setting, and introduces variance reduction techniques that are adapted in the Fed-DVR-Q algorithm."}, {"fullname_first_author": "J. Woo", "paper_title": "Federated offline reinforcement learning: Collaborative single-policy coverage suffices", "publication_date": "2024-01-01", "reason": "This paper offers a state-of-the-art algorithm for Federated Q-learning with intermittent communication, against which Fed-DVR-Q's performance is compared."}, {"fullname_first_author": "J. Woo", "paper_title": "Sample complexity of asynchronous Q-learning: Sharper analysis and variance reduction", "publication_date": "2023-01-01", "reason": "This paper proposes new algorithms for Federated Q-learning under both synchronous and asynchronous settings, offering improved sample and communication complexities that help contextualize the contributions of the current work."}]}