[{"type": "text", "text": "The Sample-Communication Complexity Trade-off in Federated Q-Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sudeep Salgia Carnegie Mellon University ssalgia@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Yuejie Chi Carnegie Mellon University yuejiechi@cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of Federated Q-learning, where $M$ agents aim to collaboratively learn the optimal Q-function of an unknown infinite horizon Markov Decision Process with finite state and action spaces. We investigate the trade-off between sample and communication complexity for the widely used class of intermittent communication algorithms. We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least $\\Omega\\big(\\frac{\\tilde{\\Gamma}}{1\\!-\\!\\gamma}\\big)$ where $\\gamma$ is the discount factor. We also propose a new Federated Q-learning algorithm, called Fed-DVR-Q, which is the first Federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in Federated Q-learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) [Sutton and Barton, 2018] refers to an online sequential decision making paradigm where the learning agent aims to learn an optimal policy, i.e., a policy that maximizes the long-term reward, through repeated interactions with an unknown environment. RL finds applications across a diverse array of fields including, but not limited to, autonomous driving, games, recommendation systems, robotics and Internet of Things (IoT) [Kober et al., 2013, Yurtsever et al., 2020, Silver et al., 2016, Lim et al., 2020]. ", "page_idx": 0}, {"type": "text", "text": "The primary hurdle in RL applications is often the high-dimensional nature of the decision space that necessitates the learning agent to have to access to an enormous amount of data in order to have any hope of learning the optimal policy. Moreover, the sequential collection of such an enormous amount of data through a single agent is extremely time-consuming and often infeasible in practice. Consequently, practical implementations of RL involve deploying multiple agents to collect data in parallel. This decentralized approach to data collection has fueled the design and development of distributed or federated RL algorithms that can collaboratively learn the optimal policy without actually transferring the collected data to a centralized server. Such a federated approach to RL, which does not require the transfer of local data, is gaining interest due to lower bandwidth requirements and lower security and privacy risks. In this work, we focus on federated variants of Q-learning algorithms where the agents collaborate to directly learn the optimal Q-function without forming an estimate of the underlying unknown environment. ", "page_idx": 0}, {"type": "text", "text": "A particularly important aspect of designing Federated RL algorithms, including Federated Q-learning algorithms, is to address the natural tension between sample and communication complexity. At one end of the spectrum lies the naive approach of running a centralized algorithm with optimal sample complexity after transferring and combining all the collected data at a central facility/server. Such an approach trivially achieves the optimal sample complexity while suffering from a very high and infeasible communication complexity. On the other hand, several recently proposed algorithms [Khodadadian et al., 2022, Woo et al., 2023] operate in more practical regimes, offering significantly lower communication complexities as compared to the naive approach at the cost of sub-optimal sample complexities. These results suggest the existence of underlying trade-off between sample and communication complexities of Federated RL algorithms. The primary goal of this work is to better understand this trade-off in context of Federated Q-learning by investigating these following fundamental questions: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u00b7Fundamental limit of communication:What is the minimum amount of communication required by a federated Q-learning algorithm to achieve any statistical benefit of collaboration? \u00b7Optimal algorithm design:How does one design a federated $Q$ -Learningalgorithmthatsimultaneously offers optimal order sample and communication complexity guarantees i.e., operates on the optimalfrontierofsample-communicationcomplexitytrade-off? ", "page_idx": 1}, {"type": "text", "text": "1.1 Main Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a setup where $M$ distributed agents collaborate to learn the optimal Q-function of an infinite horizon Markov Decision Process which is defined over a finite state space $\\boldsymbol{S}$ and a finite actionset $\\boldsymbol{\\mathcal{A}}$ , and has a discount factor of $\\gamma\\in(0,1)$ . We consider a commonly considered setup in federated learning called the intermittent communication setting, where the clients intermittently share information among themselves with the help of a central server. In this work, we provide a complete characterization of the trade-off between sample and communication complexity under the aforementioned setting by providing answers to both the questions. The main result of this work is twofold and is summarized below. ", "page_idx": 1}, {"type": "text", "text": "\u00b7Fundamental bounds on communication complexity of Federated $Q$ learning:We establishlower bounds on the communication complexity of Federated Q-learning, both in terms of number of communication rounds and the overall number of bits that need to be transmitted in order to achieve any speed up in convergence with respect to the number of agents. Specifically, we show that in order for an intermittent communication algorithm to obtain any benefit of collaboration, i.e., any order of speed up w.r.t. the number of agents, the number of communication rounds   \nmust beleast $\\Omega(\\frac{\\mathbf{1}}{(1\\!-\\!\\gamma)\\log^{2}N})$ and the numbero bis sent by each agent to the servr must e $\\Omega\\big(\\frac{|S||\\mathcal{A}|}{(1-\\gamma)\\log^{2}N}\\big)$ $N$ state-actionpair. ", "page_idx": 1}, {"type": "text", "text": "\u00b7Achieving the optimal sample-communication complexity trade-off: We propose a new Federated Q-Learning algorithm called Federated Doubly Variance Reduced Q Learning, Fed-DVR-Q for short, that simultaneously achieves optimal order of sample complexity and the minimal order of communication as dictated by the lower bound. We show that Fed-DVR-Q learns an $\\varepsilon,$ optimal Q-function in the lo sense with O ( $\\tilde{\\mathcal{O}}\\left(\\frac{|S||A|}{M\\varepsilon^{2}(1\\!-\\!\\gamma)^{3}}\\right)$ i.i.d. samples from the generative model at each agent while incurring a total communication cost of O (S\uff09 bits per agent across $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(\\frac{1}{(1-\\gamma)}\\right)}\\end{array}$ rounds of communication. Thus, Fed-DVR-Q not only improves upon both the sample and communication complexities of existing algorithms, but also is the first algorithm to achieve both order-optimal sample and communication complexities (See Table 1 for a comparison). ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Single agent Q-Learning. Q-Learning has been extensively studied in the single-agent setting in terms of both its asymptotic convergence [Jaakkola et al., 1993, Tsitsiklis, 1994, Szepesvari, 1997, Borkar and Meyn, 2000] and its finite-time sample complexity in both synchronous [Even-Dar and Mansour, 2004, Beck and Srikant, 2012, Wainwright, 2019a, Chen et al., 2020, Li et al., 2023] and asynchronous settings [Chen et al., 2021b, Li et al., 2023, 2021, Qu and Wierman, 2020]. ", "page_idx": 1}, {"type": "text", "text": "Distributed RL. There has also been a considerable effort towards developing distributed and federated RL algorithms. The distributed variants of the classical TD learning algorithm have been investigated in a series of studies [Chen et al., 2021c, Doan et al., 2019, 2021, Sun et al., 2020, Wai, 2020, Wang et al., 2020, Zeng et al., 2021b]. The impact of environmental heterogeneity in federated TD learning was studied in Wang et al. [2023]. A distributed version of actor-critic algorithms was studied by Shen et al. [2023] where the authors established convergence of their algorithm and demonstrated a linear speed up in the number of agents in their sample complexity bound. Chen et al. [2022] proposed a new distributed actor-critic algorithm which improved the dependence of sample complexity on the error $\\varepsilon$ and incurs a communication cost of $\\bar{\\mathcal O}(\\bar{\\varepsilon}^{-1})$ .Chen et al. [2021a] have proposed a communication efficient distributed policy gradient algorithm and have analyzed its convergence and established a communication complexity of ${\\mathcal{O}}(1/(M\\varepsilon))$ . Xie and Song [2023] adopts a distributed policy optimization perspective, which is different from the Q-learning paradigm considered in this work. Moreover, the algorithm in Xie and Song [2023] obtains a linear communication cost, which is worse than that obtained in our work. Similarly, Zhang et al. [2024] focuses on on-policy learning and incurs a communication cost that depends polynomially on the required error $\\varepsilon$ . Several other studies [Yang et al., 2023, Zeng et al., 2021a, Lan et al., 2024] have also developed and analyzed other distributed/federated variants of the classical natural policy gradient method [Kakade, 2001]. Assran et al. [2019], Espeholt et al. [2018], Mnih et al. [2016] have developed distributed algorithms to train deep RL networks more efficiently. ", "page_idx": 1}, {"type": "table", "img_path": "6YIpvnkjUK/tmp/d9369d57df4aa9fbf1341559002336d6210598330d50016c2364a81fa0201d98.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an $\\varepsilon$ -optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ represent state and action spaces respectively and $\\gamma$ denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Distributed Q-learning. Federated Q-learning has been explored relatively recently. Khodadadian   \net al. [2022] proposed and analyzed a federated Q-learning algorithm in the asynchronous setting with a sample complexity of (M() ), where \u03bcmn i te minimum etry of the stationary state-action occupancy distribution of the sample trajectories over all agents. Jin et al. [2022]   \nstudy the impact of environmental heterogeneity across clients in Federated Q-learning. They   \npropose an algorithm where the local environments are different at each client but each client $\\mathcal{O}(\\frac{1}{(1\\!-\\!\\gamma)^{3}\\varepsilon})$ $\\mathcal{O}\\big(\\frac{1}{(1-\\gamma)^{3}\\varepsilon}\\big)$ et al. [2023] proposed new algorithms with improved analysis for Federated Q-learning under both   \nsynchronous and asynchronous settings. Their proposed algorithm achieves a sample complexity and   \ncommunicaton complextyof $\\tilde{\\mathcal{O}}\\big(\\frac{\\bar{|}S||A|}{M(1-\\gamma)^{5}\\varepsilon^{2}}\\big)$ and $\\tilde{\\mathcal{O}}\\big(\\frac{M|\\bar{S(|\\boldsymbol{A}|}}{1-\\gamma}\\big)$ real numbersrespetivly underthe   \nsychronous setin and that of $\\tilde{\\mathcal{O}}\\big(\\frac{1}{M\\mu_{\\mathrm{avg}}(1\\!-\\!\\gamma)^{5}\\varepsilon^{2}}\\big)$ and $\\tilde{\\mathcal{O}}\\left(\\frac{M|S||A|}{1-\\gamma}\\right)$ real numbers respectively under   \nthe asynchronous setting. Here, $\\mu_{\\mathrm{avg}}$ denotes the minimum entry of the average stationary state-action   \noccupancy distribution of all agents. In a follow up work, Woo et al. [2024] propose a Federated Qlearning_for offline RL in finite horizon setting and establish a sample and communication complexity $\\tilde{\\mathcal{O}}\\big(\\frac{H^{7}|S|C_{\\mathrm{avg}}}{M\\varepsilon^{2}}\\big)$ and $\\tilde{\\cal O}(H)$ where $H$ denotesthenthftrid $C_{\\mathrm{avg}}$ denotes th average   \nsingle-policy concentrability coefficient of all agents. ", "page_idx": 2}, {"type": "text", "text": "Accuracy-Communication Trade-off in Federated Learning.The trade-off between communication complexity and accuracy (equivalently, sample complexity) has been studied in various federated and distributed learning problems, including stochastic approximation algorithms for convex optimization. Duchi et al. [2014], Braverman et al. [2016] establish the celebrated inverse linear relationship between the error and the communication cost the problem of distributed mean estimation. Similar trade-off for distributed stochastic optimization, multi-armed bandits and linear bandits has been studied and established across numerous studies [Woodworth et al., 2018, 2021, Tsitsiklis and Luo, 1987, Shi and Shen, 2021, Salgia and Zhao, 2023]. ", "page_idx": 3}, {"type": "text", "text": "2  Problem Formulation and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide a brief background of Markov Decision Processes, outline the performance measures for Federated Q-learning algorithms and describe the class of intermittent communication algorithms considered in this work. ", "page_idx": 3}, {"type": "text", "text": "2.1 Markov Decision Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we focus on an infinite-horizon Markov Decision Process (MDP), denoted by $\\mathcal{M}$ ,over a state space $\\boldsymbol{S}$ and an action space $\\boldsymbol{\\mathcal{A}}$ and with a discount factor $\\gamma\\in(0,1)$ . Both the state and action spaces are assumed to be finite sets. In an MDP, the state $s$ evolves dynamically under the influence of actions based on a probability transition kernel, $P:(S\\times A)\\times S\\rightarrow[0,1]$ . The entry $P(s^{\\prime}|s,a)$ denotes the probability of moving to state $s^{\\prime}$ when an action $a$ is taken in the state $s$ . An MDP is also associated with a deterministic reward function $r:S\\times A\\to[0,1]$ ,where $r(s,a)$ denotes the immediate reward obtained for taking the action $a$ in the state $s$ . Thus, the transition kernel $P$ along with the reward function $r$ completely characterize an MDP. In this work, we consider the synchronous setting, where each agent has access to an independent generative model or simulator from which they can draw independent samples from the unknown underlying distribution $P(\\cdot|s,a)$ for each state-action pair $(s,a)$ [Kearns and Singh, 1998]. ", "page_idx": 3}, {"type": "text", "text": "A policy $\\pi:S\\to\\Delta(A)$ is a rule for selecting actions across different states, where $\\Delta(A)$ denotes the simplex over $\\boldsymbol{\\mathcal{A}}$ and $\\pi(a|s)$ denotes the probability of choosing action $a$ in a state $s$ . Each policy $\\pi$ is associated with a state value function and a state-action value function, or the Q-function, denoted by $V^{\\pi}$ and $Q^{\\pi}$ respectively. $V^{\\pi}$ and $Q^{\\pi}$ measure the expected discounted cumulative reward attained by $\\pi$ starting from a particular state $s$ and state-action pair $(s,a)$ respectively. Mathematically, $V^{\\pi}$ and $Q^{\\pi}$ are given as ", "page_idx": 3}, {"type": "equation", "text": "$$\nV^{\\pi}(s):=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})\\bigg|\\;s_{0}=s\\right];\\quad Q^{\\pi}(s,a):=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t},a_{t})\\bigg|\\;s_{0}=s,a_{0}=a\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $a_{t}\\,\\sim\\,\\pi(\\cdot|s_{t})$ and $s_{t+1}\\,\\sim\\,P(\\,\\cdot\\,\\,|s_{t},a_{t})$ for all $t\\,\\geq\\,0$ .The expectation is taken w.r.t. the randomness in the trajectory $\\{s_{t},a_{t}\\}_{t=1}^{\\infty}$ . Since the rewards lie in $[0,1]$ , it follows immediately that both the value function and Q-function lie in the range $[0,\\frac{1}{1-\\gamma}]$ ", "page_idx": 3}, {"type": "text", "text": "An optimal policy $\\pi^{\\star}$ is a policy that maximizes the value function uniformly over all the states and it has been shown that such an optimal policy $\\pi^{\\star}$ always exists [Puterman, 2014]. The optimal value and Q-functions are those corresponding to that of an optimal policy $\\pi^{\\star}$ are denoted as $V^{\\star}:=V^{\\pi^{\\star}}$ and $Q^{\\star}:=Q^{\\pi^{\\star}}$ respectively. The optimal Q-function, $Q^{\\star}$ , is also the unique fixed point of the Bellman operator $\\mathcal{T}:S\\times\\mathcal{A}\\rightarrow S\\times\\mathcal{A}$ , given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n(T Q)(s,a)=r(s,a)+\\gamma\\cdot\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}Q(s^{\\prime},a^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Q-learning [Watkins and Dayan, 1992] aims to learn the optimal policy by first learning. $Q^{\\star}$ asthe solution to the fixed point equation $\\tau Q=Q$ and then obtain a deterministic optimal policy via the maximization $\\pi^{\\star}(s)=\\arg\\operatorname*{max}_{a}Q^{\\star}(s,a)$ ", "page_idx": 3}, {"type": "text", "text": "Let $Z\\in S^{|S||A|}$ be a random vector whose $(s,a)^{\\mathrm{th}}$ coordinate is drawn from the distribution $P(\\cdot|s,a)$ independently of all other coordinates. We define the random operator $T_{Z}:(S\\times A)\\to(S\\times A)$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n(T_{Z}Q)(s,a)=r(s,a)+\\gamma V(Z(s,a)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $V(s^{\\prime})\\,=\\,\\operatorname*{max}_{a^{\\prime}\\in A}Q(s^{\\prime},a^{\\prime})$ .The operator $\\tau_{z}$ can be interpreted as the sample Bellman Operator, where we have the relation $\\mathcal{T}Q=\\bar{\\mathbb{E}}_{Z}[\\mathcal{T}_{Z}Q]$ for all Q-functions. ", "page_idx": 4}, {"type": "text", "text": "Lastly, the federated learning setup considered in this work consists of $M$ agents, where all the agents face a common, unknown MDP, i.e., the transition kernel and the reward functions are the same across agents, which is popularly known as the homogeneous etting.For a given value of $\\begin{array}{r}{\\varepsilon\\in(0,\\frac{1}{1-\\gamma})}\\end{array}$ the objective of agents is to collaboratively learn an $\\varepsilon$ -optimal estimate (in the $\\ell_{\\infty}$ sense) of the optimal Q-function of the unknown MDP. ", "page_idx": 4}, {"type": "text", "text": "2.2 Performance Measures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We measure the performance of a Federated Q-learning algorithm $\\mathcal{A}$ using two metrics \u2014- sample complexity and communication complexity. For a given MDP $\\mathcal{M}$ , let $\\widehat{Q}_{{\\mathcal{M}}}({\\mathcal{A}},N,M)$ denote the estimate of $Q_{\\mathcal{M}}^{\\star}$ , the optimal Q-function of the MDP $\\mathcal{M}$ , returned by an algorithm $\\mathcal{A}$ , when given access to $N$ i.i.d. samples from the generative model for each $(s,a)$ pair at all the $M$ agents. The minimax error rate of the algorithm $\\mathcal{A}$ , denoted by $\\mathsf{E R}(\\mathcal{A};N,M)$ , is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{E R}(\\mathcal{A};N,M):=\\operatorname*{sup}_{\\mathcal{M}=(P,r)}\\mathbb{E}\\left[\\|\\widehat{Q}_{\\mathcal{M}}(\\mathcal{A},N,M)-Q_{\\mathcal{M}}^{\\star}\\|_{\\infty}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation is taken over the samples and any randomness in the algorithm. Given a value Oof $\\varepsilon>0$ ,the sample complexity of $\\mathcal{A}$ ,denotedby $\\mathsf{S C}(\\mathcal{A};\\varepsilon,M)$ isgiven as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{S C}(\\mathcal{A};\\varepsilon,M):=|S||\\mathcal{A}|\\cdot\\operatorname*{min}\\{N\\in\\mathbb{N}:\\mathsf{E R}(\\mathcal{A};N,M)\\leq\\varepsilon\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly, we can also define a high-probability version for any $\\delta\\in(0,1)$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathtt{S C}(\\mathcal{A};\\varepsilon,M,\\delta):=|\\mathcal{S}||A|\\cdot\\operatorname*{min}\\{N\\in\\mathbb{N}:\\operatorname*{Pr}_{\\mathtt{M}}(\\operatorname*{sup}_{M}\\|\\widehat{Q}_{M}(\\mathcal{A},N,M)-Q_{M}^{\\star}\\|_{\\infty}\\le\\varepsilon)\\ge1-\\delta\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We measure the communication complexity of any federated learning algorithm both in terms of   \nfrequency of information exchange and total number of bits uploaded by the agents. For each agent $m$ $C_{\\mathsf{r o u n d}}^{m}(\\mathcal{A};N)$ $C_{\\mathsf{b i t}}^{m}(\\mathcal{A};\\bar{N})$ $m$ $m$   \n$\\mathcal{A}$ is run with $N$ i.i.d. samples from the generative model for each $(s,a)$ pair at each agent. The   \ncommunication complexity of $\\mathcal{A}$ , when measured in terms of frequency of communication and total   \nnumber of bits exchanged, is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{C G}_{\\sf r o u n d}({\\mathcal A};N):=\\frac{1}{M}\\sum_{m=1}^{M}C_{\\sf r o u n d}^{m}({\\mathcal A};N);\\quad\\mathsf{C G}_{\\sf b i t}({\\mathcal A};N):=\\frac{1}{M}\\sum_{m=1}^{M}C_{\\sf b i t}^{m}({\\mathcal A};N),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "respectively. Similarly, for a given value of $\\begin{array}{r}{\\varepsilon\\in(0,\\frac{1}{1-\\gamma})}\\end{array}$ , we can also define $\\mathsf{C C}_{\\mathsf{r o u n d}}(\\mathcal{A};\\varepsilon)$ and $\\mathsf{C C}_{\\mathsf{b i t}}(\\mathcal{A};\\varepsilon)$ based on when $\\mathcal{A}$ is run to guarantee a minimax error of at most $\\varepsilon$ ", "page_idx": 4}, {"type": "text", "text": "2.3  Intermittent Communication Algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this work, we consider a popular class of federated learning algorithms referred to as algorithms with intermittent communication. The intermittent communication setting provides a natural framework to extend single agent Qlearning algorithms to the distributed setting. As the name suggests, under this setting, the agents intermittently communicate with each other, sharing their updated beliefs about $Q^{\\star}$ Between two communication rounds, each agent updates their belief about $Q^{\\star}$ using stochastic fixed point iteration based on the locally available data, similar to a single agent setup. Such intermittent communication algorithms ", "page_idx": 4}, {"type": "table", "img_path": "6YIpvnkjUK/tmp/5ca422f485bae4881ddc0179c399c2eb8fc07e97e8289e4308d611a576ef66c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "have been extensively studied and used to establish lower bounds on communication complexity of distributed stochastic convex optimization [Woodworth et al., 2018, 2021]. ", "page_idx": 4}, {"type": "text", "text": "A generic Federated Q-learning algorithm with intermittent communication is outlined in Algorithm 1. It is characterized by the following five parameters: (i) total number of updates $T$ ; (i) the number of communication rounds $R$ :(iia sep size schedule $\\{\\eta_{t}\\}_{t=1}^{T}$ (iv acommunicatin schedule $\\{t_{r}\\}_{r=1}^{R}$ (v) batch size $B$ During the $t^{\\mathrm{th}}$ iteration,each aget $m$ computes $\\{\\widehat{\\mathcal{T}}_{Z_{b}}(Q_{t-1}^{m})\\}_{b=1}^{B}$ aminibatch of sample Bellman operators at the current estimate $Q_{t-1}^{m}$ ,using samples from the generativemodel for each $(s,a)$ pair, and obtains an intermediate local estimate using the Q-learning update as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{t-\\frac{1}{2}}^{m}=(1-\\eta_{t})Q_{t-1}^{m}+\\frac{\\eta_{t}}{B}\\sum_{b=1}^{B}\\mathcal{T}_{Z_{b}}(Q_{t-1}^{m}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\eta_{t}\\in(0,1]$ is the step-size chosen corresponding to the $t^{\\mathrm{th}}$ time step. The intermediate estimates are averaged based on a communication schedule $\\bar{C}=\\{t_{r}\\}_{r=1}^{R}$ consisting of $R$ rounds, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{t}^{m}=\\left\\{\\frac{1}{M}\\sum_{j=1}^{M}Q_{t-\\frac{1}{2}}^{j}\\right.\\quad\\mathrm{if~}t\\in\\mathcal{C},}\\\\ {Q_{t-\\frac{1}{2}}^{m}}&{\\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the above equation, the averaging step can also be replaced with any distributed mean estimation routine that includes compression to control the bit level costs. Without loss of generality, we assume that $Q_{0}^{m}=0$ for all agents $m$ and $t_{R}=T$ , i.e.,the last iterates are always averaged. It is straightforward to note that the number of samples taken by an intermittent communication algorithm .s $B T$ ,i.e, $N=B T$ andthecommunicationcomplexity $\\mathsf{C C}_{\\mathsf{r o u n d}}=R$ ", "page_idx": 5}, {"type": "text", "text": "3Lower Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we investigate the first of the two questions regarding the lower bound on communication complexity. The following theorem establishes a lower bound on the communication complexity of a Federated Q-learning algorithm with intermittent communication. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Assume that $\\gamma\\in[5/6,1)$ and the state and action spaces satisfy $\\vert{\\cal S}\\vert\\ge4$ and $|{\\mathcal{A}}|\\ge$ 2. Let $\\mathcal{A}$ be a Federated $Q$ -learning algorithm with intermittent communication that is run for $T\\geq\\operatorname*{max}\\{16,\\frac{1}{1-\\gamma}\\}$ $\\begin{array}{r}{\\eta_{t}:=\\frac{1}{1+c_{\\eta}(1-\\gamma)t}}\\end{array}$ $\\eta_{t}:=\\eta$ Jor al $1\\leq t\\leq T$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nR=C C_{r o u n d}(\\mathcal{A};N)\\leq\\frac{c_{0}}{(1-\\gamma)\\log^{2}N};\\;o r\\;C C_{b i t}(\\mathcal{A};N)\\leq\\frac{c_{1}|S||A|}{(1-\\gamma)\\log^{2}N}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some universal constants $c_{0},c_{1}>0$ then, for all choices of communication schedule, batch size $B$ $c_{\\eta}>0$ and $\\eta\\in(0,1)$ ,theminimax errorof $\\mathcal{A}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nE R(\\mathcal{A};N,M)\\ge\\frac{C_{\\gamma}}{\\log^{3}N\\sqrt{N}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $M\\geq2$ and $N=B T$ Here $C_{\\gamma}>0$ is a constant that depends only on $\\gamma$ ", "page_idx": 5}, {"type": "text", "text": "The above theorem states that in order for an intermittent communication algorithm to obtain any benefit of collaboration, i.e., for the error rate $\\mathsf{E R}(\\mathcal{A};N,M)$ to decrease w.r.t. number of agents, the number ofcommunication rounds must be least $\\Omega(\\frac{1}{(1\\!-\\!\\gamma)\\log^{2}N})$ This implie ha any Federated Q-learning algorithm that offers order optimal sample complexity, and thereby also a linear speed up with respect to the number of agents, must have at east $\\dot{\\Omega}(\\frac{1}{(1\\!-\\!\\gamma)\\log^{2}N})$ rounds of communication and transmit S( (\u00b1S)lAg2 N ) (3)Iog-) bis o information per agent This characterizes the converse rlation for the sample-communication tradeoff in Federated Q-learning. We would like to point out that our lower bound extends to the asynchronous setting as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in asynchronous setting. ", "page_idx": 5}, {"type": "text", "text": "The lower bound on the communication complexity of Federated Q-learning is a consequence of the bias-variance trade-off that governs the convergence of the algorithm. While a careful choice of step-sizes alone is sufficient to balance this trade-off in the centralized setting, the choice of communication schedule also plays an important role in balancing this trade-off in the federated setting. The local steps between two communication rounds induce a positive estimation bias that depends on the standard deviation of the iterates and is a well-documented issue of \u201cover-estimation' in Q-learning [Hasselt, 2010]. Since such a bias is driven by local updates, it does not reflect any benefit of collaboration. During a communication round, the averaging of iterates across agents allows the algorithm an opportunity to counter this bias by reducing the effective variance of the updates through averaging. In our analysis, we show that if the communication is infrequent, the local bias becomes the dominant term and averaging of iterates is insuffcient to counter the impact of the positive bias induced by the local steps. As a result, we do not observe any statistical gains when the communication is infrequent. The analysis is inspired the analysis of Q-learning by Li et al. [2023] and is based on analyzing the convergence of an intermittent communication algorithm on a specifically chosen \u201chard\" instance of MDP. Please refer to Appendix B for a detailed proof. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Remark 1 (Communication complexity of policy evaluation). Several recent studies [Liu and Olshevsky, 2023, Tian et al., 2024] established that a single round of communication is sufficient to achieve linear speedup of TD learning for policy evaluation, which do not contradict with our results focusing on Q-learning for policy learning. The latter is more involved due to the nonlinearity of the Bellman optimality operator. Specifically, if the operator whose fixed point is to be found is linear in the decision variable (e.g., the value function in TD learning) then the fixed point update only induces a variance term corresponding to the noise. However, if the operator is non-linear, then in addition to the variance term, we also obtain a bias term in the fixed point update. While the variance term can be controlled with one-shot averaging, more frequent communication is necessary to ensure that the bias term is small enough. ", "page_idx": 6}, {"type": "text", "text": "Remark 2 (Extension to asynchronous Q-learning). We would like to point out that our lower bound extends to the asynchronous setting [Li et al., 2023] as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in the asynchronous setting. ", "page_idx": 6}, {"type": "text", "text": "4  The Fed-DVR-Q algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Having characterized the lower bound on the communication complexity of Federated Q-learning, we explore our second question of interest \u2014- designing a federated Q-learning algorithm that achieves this lower bound while simultaneously offering an optimal order of sample complexity. ", "page_idx": 6}, {"type": "text", "text": "We propose a new Federated Q-learning algorithm, Fed-DVR-Q, that achieves not only a communicationcomplexityof $\\begin{array}{r}{\\mathsf{C C}_{\\mathsf{r o u n d}}=\\tilde{\\mathcal{O}}(\\frac{1}{1-\\gamma})}\\end{array}$ $\\begin{array}{r}{\\mathsf{C C}_{\\mathrm{bit}}=\\tilde{\\mathcal{O}}(\\frac{|S||A|}{1-\\gamma})}\\end{array}$ but alsotheoptimalorderof sarmple complexity (upto logarithmic factors), thereby providing a tight characterization of the achievability frontier that matches with the converse result derived in the previous section. ", "page_idx": 6}, {"type": "text", "text": "4.1  Algorithm Description ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fed-DVR-Q proceeds in epochs. During an epoch $k\\ \\geq\\ 1$ , the agents collaboratively update ${\\cal Q}^{(k-1)}$ , the estimate of $Q^{\\star}$ obtained at the end of previous epoch, to a new estimate $Q^{(k)}$ , with the aid of thesub-routinecalledREFINEEsTIMATE. Thesub-routineREFINEEsTIMATEis designed to ensure that the suboptimality gap, $\\|\\bar{Q}^{(k)}-Q^{\\star}\\|_{\\infty}$ , reduces by a factor of 2 at the end of every epoch. Thus, at the end of $K={\\mathcal{O}}(\\log(1/\\varepsilon))$ epochs, Fed-DVR-Q obtains a $\\varepsilon$ -optimal estimate of $Q^{\\star}$ , which is then set to be the output of the algorithm. Please refer to Alg. 2 for a pseudocode. ", "page_idx": 6}, {"type": "table", "img_path": "6YIpvnkjUK/tmp/461720f13e99bc71e9804c2fcac26a1e3561f70d3b30db88ec16721749530d41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1.1 The REFINEEsTIMATE sub-routine ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "REFINEESTIMATE, starting from $\\overline{{Q}}$ , an initial estimate of $Q^{\\star}$ , uses variance reduced Q-learning updates to obtain an improved estimate of $Q^{\\star}$ . It is characterized by four parameters \u2014- the initial estimate $\\overline{{Q}}$ , the number of local iterations $I$ , the recentering sample size $L$ and the batch size $B$ ", "page_idx": 6}, {"type": "text", "text": "which can be appropriately tuned to control the quality of the returned estimate. Additionally, it also takes input two parameters $D$ and $J$ required by the compressor. ", "page_idx": 7}, {"type": "text", "text": "The first step in REFINEEsTIMATE is to collaboratively approximate $\\tau\\overline{{Q}}$ for the variance reduced updates. To this effect, each agent $m$ builds an approximation of $\\tau\\overline{{Q}}$ asfollows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{\\cal T}_{L}^{(m)}(\\overline{{{Q}}}):=\\frac{1}{\\lceil L/M\\rceil}\\sum_{l=1}^{\\lceil L/M\\rceil}\\mathcal{T}_{Z_{l}^{(m)}}(\\overline{{{Q}}}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Where(Z(rm),Z(m). $\\{Z_{1}^{(m)},Z_{2}^{(m)},\\ldots,Z_{\\lceil L/M\\rceil}^{(m)}\\}$ \\* , ZL/)} ae L/MI id. samples with Z(m) \\~ Z. Each agent sends $\\mathcal{C}\\left(\\widetilde{T}_{L}^{(m)}(\\overline{{Q}})-\\overline{{Q}};D,J\\right)$ , a compressed version of the difference $\\widetilde{T}_{L}^{(m)}(\\overline{{Q}})-\\overline{{Q}}$ , to the server, which collects all the estimates from the agents and constructs the estimate ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{T}_{L}(\\overline{{Q}})=\\overline{{Q}}+\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{C}\\left(\\widetilde{T}_{L}^{(m)}(\\overline{{Q}})-\\overline{{Q}};D,J\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and sends it back to the agents for the variance reduced updates. We defer the description of the compression routine to the end of this section. Equipped with the estimate $\\widetilde{\\mathcal{T}}_{L}(\\overline{{Q}})$ , REFINEESTIMATE constructs a sequence $\\{Q_{i}\\}_{i=1}^{I}$ using the following iterative update scheme initialized with $Q_{0}=\\overline{{Q}}$ During the $i^{\\mathrm{th}}$ iteration, each agent $m$ carries out the following update: ", "page_idx": 7}, {"type": "equation", "text": "$$\nQ_{i-\\frac{1}{2}}^{m}=(1-\\eta)Q_{i-1}+\\eta\\left[\\widehat{T}_{i}^{(m)}Q_{i-1}-\\widehat{T}_{i}^{(m)}\\overline{{{Q}}}+\\widetilde{T}_{L}(\\overline{{{Q}}})\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the aboveguation, $\\eta\\in(0,1)$ $\\begin{array}{r}{\\widehat{\\mathcal{T}}_{i}^{(m)}Q:=\\frac{1}{B}\\sum_{z\\in\\mathcal{Z}_{i}^{(m)}}\\mathcal{T}_{z}Q_{\\cdot}}\\end{array}$ where z(m) is the minibatch of $B$ i.i.d. random variables drawn according to $Z$ , independently at each agent $m$ for all iterations $i$ . Each agent then sends a compressed version of the update,i.. $\\mathcal{C}\\left(Q_{i-\\frac{1}{2}}^{m}-Q_{i-1};D,J\\right)$ to the server, which uses them to compute the next iterate ", "page_idx": 7}, {"type": "equation", "text": "$$\nQ_{i}=Q_{i-1}+\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{C}\\left(Q_{i-\\frac{1}{2}}^{m}-Q_{i-1};D,J\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and broadcast it to the clients. After $I$ such updates, the obtained iterate $Q_{I}$ is returned by the routine.   \nA pseudocode of the REFINEEsTIMATE routine is given in Algorithm 3 in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "4.1.2 The Compression Operator ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The compressor, $\\mathcal{C}(\\cdot;D,J)$ , used in the proposed algorithm Fed-DVR-Q is based on the popular stochastic quantization scheme. In addition to the input vector $Q$ to be quantized, the quantizer $\\mathcal{C}$ takes two input parameters $D$ and $J,\\;D$ corresponds to an upper bound on $\\ell_{\\infty}$ norm of $Q$ i.e., $\\|Q\\|_{\\infty}\\leq D$ $J$ corresponds to the resolution of the compressor, i.e., number of bits used by the compressor to represent each coordinate of the output vector. ", "page_idx": 7}, {"type": "text", "text": "The compressor first splits the interval $[0,D]$ into $2^{J}-1$ intervals of equal length where $0=d_{1}<$ $d_{2},\\cdot\\cdot\\cdot<d_{2^{J}}=D$ correspond to end points of the intervals. Each coordinate of $Q$ is then separately quantized as follows. The value of the $n^{\\mathrm{th}}$ coordinate, $\\mathcal{C}(Q)[n]$ , is set to be $d_{j_{n}-1}$ with probability $\\frac{d_{j_{n}}-Q[n]}{d_{j_{n}}-d_{j_{n}-1}}$ and to $d_{j_{n}}$ with the remaining probity, where $j_{n}:=\\operatorname*{min}\\{j:d_{j}<Q[i]\\leq d_{j+1}\\}$ .Itis straightforward to note that each coordinate of $\\mathcal{C}(Q)$ can be represented using $J$ bits. ", "page_idx": 7}, {"type": "text", "text": "4.1.3 Setting the parameters ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The desired convergence of the iterates $\\{Q^{(k)}\\}$ is obtained by carefully choosing the parameters of   \nthe sub-routine REFINEEsTIMATE and the compression operator $\\mathcal{C}$ . For all epochs $k\\geq1$ ,we set the $I$ $B$ $J$ $\\mathcal{C}$ $\\textstyle\\lceil{\\frac{2}{\\eta(1-\\gamma)}}\\rceil$ $\\begin{array}{r}{\\frac{2}{-\\gamma)}\\rceil,\\lceil\\frac{2}{M}\\big(\\frac{12\\gamma}{(1-\\gamma)}\\big)^{2}\\log\\bigl(\\frac{8K I|S||\\mathcal{A}|}{\\delta}\\bigr)\\rceil}\\end{array}$ $\\begin{array}{r}{\\lceil\\log_{2}\\bigl(\\frac{70}{\\eta(1-\\gamma)}\\sqrt{\\frac{2}{M}\\log(\\frac{8K I|S||\\mathcal{A}|}{\\delta})}\\bigr)\\rceil}\\end{array}$   \nspectively. The total number of epochs is set to $\\begin{array}{r}{K\\,=\\,\\lceil\\frac{1}{2}\\log_{2}(\\frac{1}{1-\\gamma})\\rceil+\\lceil\\frac{1}{2}\\log_{2}(\\frac{1}{(1-\\gamma)\\varepsilon^{2}})\\rceil}\\end{array}$ The   \nrecentering sample sizes $L_{k}$ and bounds $D_{k}$ are set to be the following functions of epoch index $k$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nL_{k}:=\\frac{19600}{(1-\\gamma)^{2}}\\log\\left(\\frac{8K I|S||A|}{\\delta}\\right)\\cdot\\left\\{\\!\\!\\begin{array}{l l}{4^{k}}&{\\mathrm{~if~}k\\le K_{0}}\\\\ {4^{k-K_{0}}}&{\\mathrm{~if~}k>K_{0}}\\end{array}\\!;\\quad D_{k}:=16\\cdot\\frac{2^{-k}}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{K_{0}=\\lceil\\frac{1}{2}\\log_{2}(\\frac{1}{1-\\gamma})\\rceil}\\end{array}$ . The piecewise definition of $L_{k}$ is crucial to obtain the optimal dependence with respect to $\\textstyle{\\frac{1}{1-\\gamma}}$ similar to the tw-step proedure outlined in Wanwright [2019b]. ", "page_idx": 8}, {"type": "text", "text": "4.2 Performance Guarantees ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The following theorem characterizes the sample and communication complexity of Fed-DVR-Q. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2. Consider any $\\delta\\in(0,1)$ and $\\varepsilon\\in(0,1]$ Under the federated learning setup described in Section 2.1, the sample and communication complexities of the Fed-DVR-Q algorithm, when run with the choice of parameters described in Sec. 4.1.3 and a learning rate $\\eta\\in(0,1)$ ,satisfy the following relations for some universal constant $C_{1}>0$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{S C}(\\mathsf{F e d}\\!-\\!\\mathsf{D V}\\mathsf{R}\\!-\\!\\mathsf{Q};\\varepsilon,M,\\delta)\\le\\displaystyle\\frac{C_{1}}{\\eta M(1-\\gamma)^{3}\\varepsilon^{2}}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right)\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right),}\\\\ &{\\mathsf{S C}_{\\mathrm{round}}(\\mathsf{F e d}\\!-\\!\\mathsf{D V}\\mathsf{R}\\!-\\!\\boldsymbol{\\Omega};\\varepsilon,\\delta)\\le\\displaystyle\\frac{16}{\\eta(1-\\gamma)}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right),}\\\\ &{\\mathsf{C C}_{\\mathrm{bit}}(\\mathsf{F e d}\\!-\\!\\mathsf{D V}\\mathsf{R}\\!-\\!\\boldsymbol{\\Omega};\\varepsilon,\\delta)\\le\\displaystyle\\frac{32|\\mathcal{S}|\\mathcal{A}|}{\\eta(1-\\gamma)}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right)\\log_{2}\\left(\\frac{70}{\\eta(1-\\gamma)}\\sqrt{\\frac{2}{M}\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "A proof of Theorem 2 can be found in Appendix C. A few implications of the theorem are in order. ", "page_idx": 8}, {"type": "text", "text": "Optimal Sample-Communication complexity trade-off. As shown by the above theorem, FedDVR-Q offers a linear speed up in the sample complexity with respect to the number of agents while simultaneously achieving the same order of communication complexity as dictated by the lower bound derived in Theorem 1, both in terms of frequency and bit level complexity. Moreover, Fed-DVR-Q is the first Federated Q-Learning algorithm that achieves a sample complexity with optimal dependence on allthe salient parameter, i.e., $|{\\cal S}|,|{\\cal A}|$ and $\\textstyle{\\frac{1}{1-\\gamma}}$ , in addition to linear speedup w.t. to number of agents and thereby bridges the existing gap between upper and lower bounds on sample complexity for Federated Q-learning. Thus, Theorem 1 and 2 together provide a characterization of optimal operating point of the sample-communication complexity trade-off in Federated Q-learning. ", "page_idx": 8}, {"type": "text", "text": "Role of Minibatching.  The commonly adopted approach in intermittent communication algorithm is to use a local update scheme that takes multiple small (i.e., $B=\\mathcal{O}(1))$ ,noisyupdatesbetween communication rounds, as evident from the algorithm design in Khodadadian et al. [2022], Woo et al. [2023] and even numerous FL algorithms for stochastic optimization McMahan et al. [2017], Haddadpour et al. [2019], Khaled et al. [2020]. In Fed-DVR-Q, we replace the local update scheme of taking multiple small, noisy updates by a single, large update with smaller variance, obtained by averaging the noisy updates over a minibatch of samples. The use of updates with smaller variance in variance reduced Q-learning yields the algorithm its name. While both the approaches result in similar sample complexity guarantees, the local update scheme requires more frequent averaging across clients to ensure that the bias of the estimate, also commonly referred to as \u201cclient drift\", is not too large. On the other hand, the minibatching approach does not encounter the problem of bias accumulation from local updates and hence can afford more infrequent averaging allowing Fed-DVR-Q to achieve optimal order of communication complexity. ", "page_idx": 8}, {"type": "text", "text": "Compression. Fed-DVR-Q is the first algorithm in Federated Q-Learning to analyze and establish communication complexity at the bit level. All existing studies on Federated RL focus only on the frequency of communication and assume transmission of real numbers with infinite bit precision. On the other hand, the our analysis provides a more holistic view point of communication complexity and provides bounds at the bit level, which is of great practical significance. While some recent other studies [Wang et al., 2023] also consider quantization in Federated RL, their objective is to understand the impact of message size on convergence with no constraint on the frequency of communication, unlike the holistic viewpoint adopted in this work. ", "page_idx": 8}, {"type": "text", "text": "5  Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented a complete characterization of the sample-communication trade-off for Federated Q-learning algorithms with intermittent communication. We showed that no Federated Q-learning algorithm with intermittent communication can achieve a linear speedup with respect to the number of agentsif its number of communicationroundsae sublinarin $\\frac{1}{1-\\gamma}$ Wealso proposedaewFeeratd Q-learning algorithm called Fed-DVR-Q that uses variance reduction along with minibatching to achieve optimal-order sample and communication complexities. In particular, we showed that FedDVR-Q has a sample complexiyof $\\tilde{\\mathcal{O}}\\big(\\frac{|S||A|}{M(1\\!-\\!\\gamma)^{3}\\varepsilon^{2}}\\big)$ whihis oreoaial alet rb parames, ad acicaton plxiyf $\\begin{array}{r}{\\tilde{\\mathcal O}(\\frac{1}{1-\\gamma})}\\end{array}$ rounds and $\\tilde{\\mathcal{O}}\\big(\\frac{|S||A|}{1\\!-\\!\\gamma}\\big)$ bits. ", "page_idx": 9}, {"type": "text", "text": "The results in this work raise several interesting questions that are worth exploring. While we focus on the tabular setting in this work, it is of great interest to investigate to the trade-off in other settings where we use function approximation to model the $Q^{\\star}$ and $V^{\\star}$ functions. Moreover, it is interesting to explore the trade-off in the finite horizon setting, where there is no discount factor. Furthermore, it is also worthwhile to explore if the communication complexity can be further reduced by going beyond the class of intermittent communication algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their constructive feedback. This work is supported in part by the grants NSF CCF-2007911, CCF-2106778, CNS-2148212, ECCS-2318441, ONR N00014-19-1-2404 and AFRL FA8750-20-2-0504, and in part by funds from federal agency and industry partners as specified in the Resilient & Intelligent NextG Systems (RINGS) program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. Assran, J. Romoff, N. Ballas, J. Pineau, and M. Rabbat. Gossip-based actor-learner architectures for deep reinforcement learning. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems, volume 32, 2019. ", "page_idx": 9}, {"type": "text", "text": "M. G. Azar, R. Munos, and H. J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3):325-349, 2013.   \nC. Beck and R. Srikant. Error bounds for constant step-size q-learning. Systems & Control Letters, 61(12):1203-1208, 2012. ISSN 0167-6911.   \nV. S. Borkar and S. P. Meyn. The o.d.e. method for convergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000. doi: 10.1137/S0363012997331639.   \nM. Braverman, A. Garg, T. Ma, H. L. Nguyen, and D. P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing, pages 1011-1020, 2016.   \nT. Chen, K. Zhang, G. B. Giannakis, and T. Basar. Communication-efficient policy gradient methods for distributed reinforcement learning. IEEE Transactions on Control of Network Systems, 9(2): 917-929, 2021a.   \nZ. Chen, S. T. Maguluri, S. Shakkottai, and K. Shanmugam. Finite-sample analysis of contractive stochastic approximation using smooth convex envelopes. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems, volume 33, pages 8223-8234, 2020.   \nZ. Chen, S. T. Maguluri, S. Shakkottai, and K. Shanmugam. A lyapunov theory for finite-sample guarantees of asynchronous q-learning and td-learning variants, 2021b.   \nZ. Chen, Y. Zhou, and R. Chen. Multi-agent off-policy tdc with near-optimal sample and communication complexity. In Proceedings of the 55th Asilomar Conference on Signals, Systems, and Computers, pages 504-508, 2021c.   \nZ. Chen, Y. Zhou, R.-R. Chen, and S. Zou. Sample and communication-efficient decentralized actorcritic algorithms with finite-time analysis. In Proceedings of the 39th International Conference on Machine Learning, pages 3794-3834. PMLR, 2022.   \nT. Doan, S. Maguluri, and J. Romberg. Finite-time analysis of distributed td (O) with linear function approximation on multi-agent reinforcement learning. In Proceedings of the 36th International Conference on Machine Learning, pages 1626-1635. PMLR, 2019.   \nT. T. Doan, S. T. Maguluri, and J. Romberg. Finite-time performance of distributed temporaldifference learning with linear function approximation. SIAM Journal on Mathematics of Data Science, 3(1):298-320, 2021.   \nJ. C. Duchi, M. I. Jordan, M. J. Wainwright, and Y. Zhang. Optimality guarantees for distributed statistical estimation, 2014. URL http: //arxiv. org/abs/1405.0782.   \nL. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, 1. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.InProceedings of the 35th International conference onmachinelearning, pages 1407-1416. PMLR, 2018.   \nE. Even-Dar and Y. Mansour. Learning rates for q-learning. Journal of Machine Learning Research, 5, 2004. ISSN 1532-4435.   \nD. A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100-118, 1975.   \nF. Haddadpour, M. M. Kamani, M. Mahdavi, and V. R. Cadambe. Local SGD with periodic averaging: Tighter analysis and adaptive synchronization. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems, volume 32, 2019.   \nH. v. Hasselt. Double q-learning. In Proceedings of the 23rd International Conference on Neural Information Processing Systems, page 2613-2621. Curran Associates Inc., 2010.   \nT. Jaakkola, M. Jordan, and S. Singh. Convergence of stochastic iterative dynamic programming algorithms. In Proceedings of the 7th Annual Conference on Neural Information Processing Systems, volume 6, 1993.   \nH. Jin, Y. Peng, W. Yang, S. Wang, and Z. Zhang. Federated reinforcement learning with environment heterogeneity. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, pages 18-37. PMLR, 2022.   \nS. M. Kakade. A natural policy gradient. Proceedings of the 15th Annual Conference on Neural Information Processing Systems, 14, 2001.   \nM. Kearns and S. Singh. Finite-sample convergence rates for q-learning and indirect algorithms. In Proceedings of the 12th Annual Conference on Neural Information Processing Systems, 1998.   \nA. Khaled, K. Mishchenko, and P Richtarik. Tighter Theory for Local SGD on Identical and Heterogeneous Data. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, AISTATS, pages 4519-4529. PMLR, 2020. URL http: //arxiv . org/abs/1909. 04746.   \nS. Khodadadian, P. Sharma, G. Joshi, and S. T. Maguluri. Federated reinforcement learning: Linear speedup under markovian sampling. In Proceedings of the 39th International Conference on Machine Learning, pages 10997-11057. PMLR, 2022.   \nJ. Kober, J. A.Bagnell, and J. Peters. Reinforcement larning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238-1274, 2013.   \nG. Lan, D.-J. Han, A. Hashemi, V. Aggarwal, and C. G. Brinton. Asynchronous federated reinforcement learning with policy gradient updates: Algorithm design and convergence analysis, 2024.   \nG. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen. Sample complexity of asynchronous q-learning: Sharper analysis and variance reduction. IEEE Transactions on Information Theory, 68(1):448-473, 2021.   \nG. Li, C. Cai, Y. Chen, Y. Wei, and Y Chi. Is q-learning minimax optimal? a tight sample complexity analysis. Operations Research, 2023.   \nH.-K. Lim, J.-B. Kim, J.-S. Heo, and Y.-H. Han. Federated reinforcement learning for training control policies on multiple iot devices. Sensors, 20(5), 2020. ISSN 1424-8220. doi: 10.3390/s20051359.   \nR. Liu and A. Olshevsky. Distributed TD(O) with almost no communication. IEEE Control Systems Letters, 7:2892-2897, 2023.   \nB. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS, pages 1273-1282. PMLR, 2017.   \nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, pages 1928-1937. PMLR, 2016.   \nM. Puterman. Markov decision processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.   \nG. Qu and . Wierman. Finite-time analysis of asynchronous stochastic approximation and q-learning. In Proceedings of the 33rd Conference on Learning Theory, pages 3185-3205. PMLR, 2020.   \nS. Salgia and Q. Zhao. Distributed linear bandits under communication constraints. In Proceedings of the 40th International Conference on Machine Learning, ICML, pages 29845-29875. PMLR, 2023.   \nH. Shen, K. Zhang, M. Hong, and T. Chen. Towards understanding asynchronous advantage actorcritic: Convergence and linear speedup. IEEE Transactions on Signal Processing, 2023.   \nC. Shi and C. Shen. Federated Multi-Armed Bandits. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, pages 9603-9611, 2021. URL http: //arxiv. org/abs/2101.12204.   \nD. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershalvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kaukuoglu, T. Graepel, and D. Hassabis. Mastering th game of go with deep neural networks and tree search. Nature, 529:484-489, 2016.   \nJ. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-time analysis of decentralized temporal-difference learning with linear function approximation. In Proceedings of the $23r d$ International Conference on Artificial Inteligence and Statistics, pages 4485-4495. PMLR, 2020.   \nR. Sutton and A. Barton. Reinforcement learning: An introduction. MIT Press, 2018.   \nC. Szepesvari. The asymptotic convergence-rate of q-learning. Proceedings of the 1lth Annual Conference on Neural Information Processing Systems, 10, 1997.   \nH. Tian,I C. Paschalidis, and A. Olshevsky. One-shot averaging for distributed TD (X) under Markov sampling. IEEE Control Systems Letters, 2024.   \nJ. N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine learning, 16: 185-202, 1994.   \nJ. N. Tsitsiklis and Z. Q. Luo. Communication complexity of convex optimization. Journal of Complexity, 3(3):231-243, 1987. ISSN 10902708. doi: 10.1016/0885-064X(87)90013-6.   \nR. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nH.-T. Wai. On the convergence of consensus algorithms with markovian noise and gradient bias. In Proceedings of 59th IEEE Conference on Decision and Control, pages 4897-4902. IEEE, 2020.   \nM. Wainwright. Stochastic approximation with cone-contractive operators: Sharp l-infinity-bounds for q -learning, 2019a. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "M. Wainwright. Variance-reduced q-learning is minimax optimal, 2019b. ", "page_idx": 12}, {"type": "text", "text": "G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized td tracking with linear function approximation and its finite-time analysis. Proceedings of the 34th Annual Conference on Neural Information Processing Systems, 33:13762-13772, 2020.   \nH. Wang, A. Mitra, H. Hassani, G. J. Pappas, and J. Anderson. Federated temporal difference learning with linear function approximation under environmental heterogeneity, 2023.   \nC. J. Watkins and P. Dayan. Q-learning. Machine learning, 8:279-292, 1992.   \nJ. Woo, G. Joshi, and Y. Chi. The blessing of heterogeneity in federated q-learning: Linear speedup and beyond. In Proceedings of the 4Oth International Conference on Machine Learning, page 37157-37216, 2023.   \nJ. Woo, L. Shi, G. Joshi, and Y. Chi. Federated offline reinforcement learning: Collaborative single-policy coverage suffices, 2024.   \nB. Woodworth, J. Wang, A. Smith, B. McMahan, and N. Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In Proceedings of the 32nd Annual Conference on Neural Information Processing Systems, volume 31, 2018.   \nB. Woodworth, B. Bullins, O. Shamir, and N. Srebro. The min-max complexity of distributed stochastic convex optimization with intermittent communication. In Proceedings of the 34th Conference on Learning Theory, COLT, pages 4386-4437. PMLR, 2021.   \nZ. Xie and S. Song. Fedkl: Tackling data heterogeneity in federated reinforcement learning by penalizing kl divergence. IEEE Journal on Selected Areas in Communications, 41(4):1227-1242, 2023.   \nT. Yang, S. Cen, Y. Wei, Y. Chen, and Y. Chi. Federated natural policy gradient methods for multi-task reinforcement learning, 2023.   \nE. Yurtsever, J. Lambert, A. Carballo, and K. Takeda. A survey of autonomous driving: Common practices and emerging technologies. IEEE access, 8:58443-58469, 2020.   \nS. Zeng, M. A. Anwar, T. T. Doan, A. Raychowdhury, and J. Romberg. A decentralized policy gradient approach to multi-task reinforcement learning. In Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence, UAl, pages 1002-1012. PMLR, 2021a.   \nS. Zeng, T. T. Doan, and J. Romberg. Finite-time analysis of decentralized stochastic approximation with applications in multi-agent and multi-task learning. In Proceedings of the 60th IEEE Conference on Decision and Control, pages 2641-2646. IEEE, 2021b.   \nC. Zhang, H. Wang, A. Mitra, and J. Anderson. Finite-time analysis of on-policy heterogeneous federated reinforcement learning, 2024. ", "page_idx": 12}, {"type": "text", "text": "We outline below the pseudocode of the REFINEEsTIMATE routine described in Sec. 4.1.1. ", "page_idx": 13}, {"type": "table", "img_path": "6YIpvnkjUK/tmp/0ed8b2532fdaf46162794b76886b8dae7f5ded5b22eeb7c41b885f59fc527a82.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we prove the main result of the paper, the lower bound on the communication complexity of federated Q-learning algorithms. At a high level, the proof consists of the following thre steps. ", "page_idx": 13}, {"type": "text", "text": "Introducing the \u201chard\" MDP instance. The proof builds upon analyzing the behavior of a generic algorithm $\\mathcal{A}$ outlined in Algorithm 1 over a particular instance of MDP. The particular choice of MDP is inspired by, and borrowed from, other lower bound proofs in the single-agent setting [Li et al., 2023] and helps highlight core issues that lie at the heart of the sample-communication complexity trade-off. Following Li et al. [2023], the construction is first over a small state-action space that allows us to focus on a simpler problem before generalizing it to larger state-action spaces. ", "page_idx": 13}, {"type": "text", "text": "Establishing the performance of intermittent communication algorithms. In the second step, we analyze the error of the iterates generated by an intermittent communication algorithm $\\mathcal{A}$ .The analysis is inspired by the single-agent analysis in Li et al. [2023], which highlights the underlying bias-variance trade-off. Through careful analysis of the algorithm dynamics in the federated setting, we uncover the impact of communication on the bias-variance trade-off and the resulting error of the iterates to obtain the lower bound on the communication complexity. ", "page_idx": 13}, {"type": "text", "text": "Generalization to larger MDPs. As the last step, we generalize our construction of the \u201chard\u2019\" instance to more general state-action space and extend our insights to obtain the statement of the theorem. ", "page_idx": 13}, {"type": "text", "text": "B.1 Introducing the \u201chard\u2019 instance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first introduce an MDP instance $\\mathcal{M}_{h}$ that we will use throughout the proof to establish the result. Note that this MDP is identical to the one considered in Li et al. [2023] to establish the lower bounds on the performance of single-agent Q-learning algorithm. It consists of four states $S=\\{0,1,2,3\\}$ Let $\\mathcal{A}_{s}$ denote the action set associated with the state $s$ . The probability transition kernel and the reward function of $\\mathcal{M}_{h}$ is given as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{0}=\\{1\\}\\qquad P(0|0,1)=1\\qquad\\qquad\\qquad\\qquad\\quad r(0,1)=0,}\\\\ {\\mathcal{A}_{1}=\\{1,2\\}\\qquad P(1|1,1)=p\\qquad\\qquad\\qquad\\quad P(0|1,1)=1-p\\quad r(1,1)=1,}\\\\ {\\quad\\;P(1|1,2)=p\\qquad\\qquad\\qquad\\quad P(0|1,2)=1-p\\quad r(1,2)=1,}\\\\ {\\mathcal{A}_{2}=\\{1\\}\\qquad P(2|2,1)=p\\qquad\\qquad\\qquad\\quad P(0|2,1)=1-p\\quad r(2,1)=1,}\\\\ {\\mathcal{A}_{3}=\\{1\\}\\qquad P(3|3,1)=1\\qquad\\qquad\\qquad\\quad r(3,1)=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the parameter $p=\\frac{4\\gamma-1}{3\\gamma}$ We have the folwing eults abut the om $Q$ and $V$ functions of this hard MDP instance. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 ([Li et al., 2023, Lemma 3]). Consider the MDP $\\mathcal{M}_{h}$ constructed in Eqn. (14). We have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V^{\\star}(0)=Q^{\\star}(0,1)=0}}\\\\ {{V^{\\star}(1)=Q^{\\star}(1,1)=Q^{\\star}(1,2)=V^{\\star}(2)=Q^{\\star}(2,1)=\\displaystyle\\frac{1}{1-\\gamma p}=\\displaystyle\\frac{3}{4(1-\\gamma)}}}\\\\ {{V^{\\star}(3)=Q^{\\star}(3,1)=\\displaystyle\\frac{1}{1-\\gamma}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Throughout the next section of the proof, we focus on this MDP with four states and two actions. In Appendix B.4, we generalize the proof to larger state-action spaces. ", "page_idx": 14}, {"type": "text", "text": "B.2   Notation and preliminary results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For convenience, we first define some notation that will be used throughout the proof. ", "page_idx": 14}, {"type": "text", "text": "Useful relations of the learning rates. We consider two kinds of step size sequences that are commonly used in Q-learning \u2014 the constant step size schedule, i.e., $\\eta_{t}=\\eta$ for all $\\bar{t^{\\dagger}}\\in\\{1,2,\\ldots,T\\}$ and therescale linear step sze schedule ie $\\begin{array}{r}{\\eta_{t}\\dot{=}\\;\\frac{1}{1+c_{\\eta}(1-\\gamma)t}}\\end{array}$ Where $c_{\\eta}>0$ is a univeralconstant that is independent of the problem parameters. ", "page_idx": 14}, {"type": "text", "text": "We define the following quantities: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\eta_{k}^{(t)}=\\eta_{k}\\prod_{i=k+1}^{t}(1-\\eta_{i}(1-\\gamma p))\\qquad\\mathrm{~for~all~}0\\leq k\\leq t,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "wherewetake $\\eta_{0}=1$ and use the convention throughout the proof that if a product operation does not have a valid index, we take the value of that product to be 1. For any integer $0\\leq\\tau<t$ wehave the following relation, which will be proved at the end of this subsection for completeness: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\prod_{k=\\tau+1}^{t}(1-\\eta_{k}(1-\\gamma p))+(1-\\gamma p)\\sum_{k=\\tau+1}^{t}\\eta_{k}^{(t)}=1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we also define, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{\\eta}_{k}^{(t)}=\\eta_{k}\\prod_{i=k+1}^{t}(1-\\eta_{i})\\qquad\\mathrm{~for~all~}0\\leq k\\leq t,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which satisfies the relation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\prod_{k=\\tau+1}^{t}(1-\\eta_{k})+\\sum_{k=\\tau+1}^{t}\\widetilde{\\eta}_{k}^{(t)}=1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any integer $0\\leq\\tau<t$ . The claim follows immediately by plugging $p=0$ in (16). Note that for constant step size, the sequence 7Kt) is clearly increasing. For the rescaled linear step size, we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\widetilde{\\eta}_{k-1}^{(t)}}{\\widetilde{\\eta}_{k}^{(t)}}=\\frac{\\eta_{k}}{\\eta_{k-1}(1-\\eta_{k})}=1-\\frac{(1-c_{\\eta}(1-\\gamma))\\eta_{k}}{1-c_{\\eta}(1-\\gamma)\\eta_{k}}\\le1\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whenever $\\begin{array}{r}{c_{\\eta}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ .Thus, $\\widetilde{\\eta}_{k}^{(t)}$ is anireain equas $\\begin{array}{r}{c_{\\eta}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ Similaly, $\\eta_{k}^{(t)}$ is also clearly increasing for the constant step size schedule. For the rescaled linear step size schedule, we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\eta_{k-1}^{(t)}}{\\eta_{k}^{(t)}}=\\frac{\\eta_{k}}{\\eta_{k-1}(1-\\eta_{k}(1-\\gamma p))}\\leq\\frac{\\eta_{k}}{\\eta_{k-1}(1-\\eta_{k})}\\leq1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whenever $\\begin{array}{r}{c_{\\eta}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ . The last bound follows from Eqn. (19) ", "page_idx": 15}, {"type": "text", "text": "Proof of (16). We can show the claim using backward induction. For the base case, note that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\gamma p)\\eta_{t}^{(t)}+(1-\\gamma p)\\eta_{t-1}^{(t)}=(1-\\gamma p)\\eta_{t}+(1-\\gamma p)\\eta_{t-1}(1-(1-\\gamma p)\\eta_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=1-(1-\\eta_{t}(1-\\gamma p))(1-\\eta_{t-1}(1-\\gamma p))=1-\\displaystyle\\prod_{k=t-1}^{t}\\left(1-\\eta_{k}(1-\\gamma p)\\eta_{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as required. Assume (16) is true for some $\\tau$ .Wehave, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\gamma p)\\displaystyle\\sum_{k=\\tau}^{t}\\eta_{k}^{(t)}=(1-\\gamma p)\\eta_{\\tau}^{t}+(1-\\gamma p)\\displaystyle\\sum_{k=\\tau+1}^{t}\\eta_{k}^{(t)}}\\\\ &{\\qquad\\qquad\\qquad=(1-\\gamma p)\\eta_{\\tau}\\displaystyle\\prod_{k=\\tau+1}^{t}\\left(1-\\eta_{k}(1-\\gamma p)\\right)+1-\\displaystyle\\prod_{k=\\tau+1}^{t}\\left(1-\\eta_{k}(1-\\gamma p)\\right)}\\\\ &{\\qquad\\qquad\\qquad=1-\\displaystyle\\prod_{k=\\tau}^{t}(1-\\eta_{k}(1-\\gamma p)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "thus completing the induction step. ", "page_idx": 15}, {"type": "text", "text": "Sample transition matrix. Recall $Z\\in S^{|S||A|}$ is a random vector whose $(s,a)$ -th coordinate is $P(\\cdot|s,a)$ $\\widehat{P}_{t}^{m}$ $t$ $m$ $B$ $\\{Z_{t,b}^{m}\\}_{b=1}^{B}$ denote a collection of $B$ i.i.d. copies of $Z$ collected at time $t$ at agent $m$ . Then, for all $s,a,s^{\\prime}$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{P}_{t}^{m}(s^{\\prime}|s,a)=\\frac{1}{B}\\sum_{b=1}^{B}P_{t,b}^{m}(s^{\\prime}|s,a),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $P_{t,b}^{m}(s^{\\prime}|s,a)=\\mathbb{1}\\{Z_{t,b}^{m}(s,a)=s^{\\prime}\\}$ for $s^{\\prime}\\in\\mathcal{S}$ ", "page_idx": 15}, {"type": "text", "text": "Preliminary relations of the iterates. We state some preliminary relations regarding the evolution ofthe $\\mathrm{^Q}$ -function and the value function across different agents that will be helpful for the analysis later. ", "page_idx": 15}, {"type": "text", "text": "We begin with the state O, where we have $Q_{t}^{m}(0,1)\\,=\\,V_{t}^{m}(0)\\,=\\,0$ for all agents $m\\in[M]$ and $t\\,\\in\\,[\\bar{T}]$ . This follows almost immediately from the fact that state O is an absorbing state with zero reward. Note that $Q_{0}^{m}(0,1)\\,=\\,V_{0}^{m}(0)\\,=\\,0$ holds for all clients $m\\,\\in\\,[M]$ .Assuming that $Q_{t-1}^{m}(0,1)=V_{t-1}^{m}(0)=0$ for all clients for some time instant $t-1$ , by induction, we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{t-1/2}^{m}(0,1)=(1-\\eta_{t})Q_{t-1}^{m}(0,1)+\\eta_{t}(\\gamma V_{t-1}^{m}(0))=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consequently, $Q_{t}^{m}(0,1)\\,=\\,0$ and $V_{t}^{m}(0)\\,=\\,0$ , for all agents $m$ , irrespective of whether there is averaging. ", "page_idx": 15}, {"type": "text", "text": "For state 3, the iterates satisfy the following relation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t-1/2}^{m}(3,1)=(1-\\eta_{t})Q_{t-1}^{m}(3,1)+\\eta_{t}(1+\\gamma V_{t-1}^{m}(3))}\\\\ &{\\qquad\\qquad\\qquad=(1-\\eta_{t})Q_{t-1}^{m}(3,1)+\\eta_{t}(1+\\gamma Q_{t-1}^{m}(3,1))}\\\\ &{\\qquad\\qquad\\quad=(1-\\eta_{t}(1-\\gamma))Q_{t-1}^{m}(3,1)+\\eta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second step follows by noting $V_{t}^{m}(3)=Q_{t}^{m}(3,1)$ Once again, one can note that averaging step does not affect the update rule implying that the following holds for all $m\\in[M]$ and $t\\in[T]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{t}^{m}(3)=Q_{t}^{m}(3,1)=\\sum_{k=1}^{t}\\eta_{k}\\left(\\prod_{i=k+1}^{t}(1-\\eta_{i}(1-\\gamma))\\right)=\\frac{1}{1-\\gamma}\\left[1-\\prod_{i=1}^{t}(1-\\eta_{i}(1-\\gamma))\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last step follows from Eqn. (16) with $p=1$ ", "page_idx": 16}, {"type": "text", "text": "Similarly, for state 1 and 2, we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{t-1/2}^{m}(1,1)=(1-\\eta_{t})Q_{t-1}^{m}(1,1)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,1)V_{t-1}^{m}(1)),}\\\\ {Q_{t-1/2}^{m}(1,2)=(1-\\eta_{t})Q_{t-1}^{m}(1,2)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,2)V_{t-1}^{m}(1)),}\\\\ {Q_{t-1/2}^{m}(2,1)=(1-\\eta_{t})Q_{t-1}^{m}(2,1)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(2|2,1)V_{t-1}^{m}(2)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the averaging makes a difference in the update rule, we further analyze the update as required in later proofs. ", "page_idx": 16}, {"type": "text", "text": "B.3 Main analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first focus on establishing a bound on the number of communication rounds, i.e., $\\mathsf{C C}_{\\mathsf{r o u n d}}(\\mathcal{A})$ (where we drop the dependency with other parameters for notational simplicity), and then use this lower bound to establish the bound on the bit level communication complexity $\\mathsf{C C}_{\\mathsf{b i t}}(\\mathcal{A})$ ", "page_idx": 16}, {"type": "text", "text": "To establish the lower bound on $\\mathsf{C C}_{\\mathsf{r o u n d}}(\\mathcal{A})$ for any intermittent communication algorithm $\\mathcal{A}$ we analyze the convergence behavior of $\\mathcal{A}$ on the MDP $\\mathcal{M}_{h}$ . We assume that the averaging step in line 6 of Algorithm 1 is carried out exactly. Since the use of compression only makes the problem harder, it is sufficient for us to consider the case where there is no loss of information in the averaging step for establishing a lower bound. Lastly, throughout the proof, without loss of generality we assume that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log N\\leq\\frac{1}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "otherwise, the lower bound in Theorem 1 reduces to the trivial lower bound. ", "page_idx": 16}, {"type": "text", "text": "We divide the proof into following three parts based on the choice of learning rates and batch sizes: ", "page_idx": 16}, {"type": "text", "text": "1. Small eaing rte Frostant leainrt. $\\begin{array}{r}{0\\leq\\eta<\\frac{1}{(1-\\gamma)T}}\\end{array}$ and for rescaled linear learning rates, the constant $c_{\\eta}$ satisfies $c_{\\eta}\\geq\\log T$   \n2. Large laning raes with smal $\\eta_{T}/(B M)$ Forconstat laming rates, $\\begin{array}{r}{\\eta\\geq\\frac{1}{(1-\\gamma)T}}\\end{array}$ and for rescaled linear learning rates, the constant $c_{\\eta}$ satisfies $\\begin{array}{r}{0\\le c_{\\eta}\\le\\log T\\le\\frac{1}{1-\\gamma}}\\end{array}$ (c.f. (25). Additionally, the ratio $\\frac{\\eta_{T}}{B M}$ satisfies $\\begin{array}{r}{\\frac{\\eta_{T}}{B M}\\leq\\frac{1-\\gamma}{100}}\\end{array}$   \n3. Large learning rates with large $\\eta_{T}/(B M)$ : We have the same condition on the learning rates as above. However, in this case the ratio $\\frac{\\eta_{T}}{B M}$ satisfies $\\begin{array}{r}{\\frac{\\eta_{T}}{B M}>\\frac{1-\\gamma}{100}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "We consider each of the cases separately in the following three subsections. ", "page_idx": 16}, {"type": "text", "text": "B.3.1 Small learning rates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection, we prove the lower bound for small learning rates, which follow from similar arguments in Li et al. [2023]. ", "page_idx": 16}, {"type": "text", "text": "For this case, we focus on the dynamics of state 2. We claim that the same relation established in Li et al. [2023] continues to hold, which will be established momentarily: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[V_{T}^{m}(2)]=\\left(\\frac{1}{M}\\sum_{j=1}^{M}\\mathbb{E}[V_{T}^{j}(2)]\\right)=\\sum_{k=1}^{T}\\eta_{k}^{(t)}=\\frac{1-\\eta_{0}^{(T)}}{1-\\gamma p}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, for all $m\\in[M]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nV^{\\star}(2)-\\mathbb{E}[V_{T}^{m}(2)]=\\frac{\\eta_{0}^{(T)}}{1-\\gamma p}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To obtain lower bound on $V^{\\star}(2)-\\mathbb{E}[V_{T}^{m}(2)]$ we need to obtain a lower bound on $\\eta_{0}^{(T)}$ , which from [Li et al., 2023, Eqn. (120)] we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log(\\eta_{0}^{(T)})\\geq-1.5\\sum_{t=1}^{T}\\eta(1-\\gamma p)\\geq-2\\sum_{t=1}^{T}\\frac{1}{t\\log T}\\geq-2\\qquad\\Longrightarrow\\qquad\\eta_{0}^{(T)}\\geq e^{-2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "when $T\\geq16$ for both choices of learning rates. On plugging this bound in (27), we obtain, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Vert Q_{T}^{m}-Q^{\\star}\\Vert_{\\infty}]\\ge\\mathbb{E}[\\vert Q^{\\star}(2)-Q_{T}^{m}(2)\\vert]\\ge V^{\\star}(2)-\\mathbb{E}[V_{T}^{m}(2)]\\ge\\frac{3}{4e^{2}(1-\\gamma)\\sqrt{N}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds for all $m\\in[M]$ $N\\geq1$ and $M\\geq2$ . Thus, it can be noted that the error rate $\\mathsf{E R}(\\mathcal{A};N,M)$ is bounded away from a constant value irrespective of the number of agents and the number of communication rounds. Thus, even with $\\mathsf{C C}_{\\mathsf{r o u n d}}^{\\mathsf{-}}=\\Omega(T)$ , we will not observe any collaborative gain if the step size is too small. ", "page_idx": 17}, {"type": "text", "text": "Proof of (26). Recall that from (24), we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ_{t-1/2}^{m}(2,1)=(1-\\eta_{t})V_{t-1}^{m}(2)+\\eta_{t}(1+\\gamma{\\widehat{P}}_{t}^{m}(2|2,1)V_{t-1}^{m}(2)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $Q_{t-1}^{m}(2,1)=V_{t-1}^{m}(2)$ as the second state has only a single action. ", "page_idx": 17}, {"type": "text", "text": "\u00b7 When $t$ is not an averaging instant, we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{t}^{m}(2)=Q_{t}^{m}(2,1)=(1-\\eta_{t})V_{t-1}^{m}(2)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(2|2,1)V_{t-1}^{m}(2)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On taking expectation on both sides of the equation, we obtain, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[V_{t}^{m}(2)]=(1-\\eta_{t})\\mathbb{E}[V_{t-1}^{m}(2)]+\\eta_{t}(1+\\gamma\\mathbb{E}[\\widehat{P}_{t}^{m}(2|2,1)V_{t-1}^{m}(2)])}\\\\ &{\\qquad\\qquad=(1-\\eta_{t})\\mathbb{E}[V_{t-1}^{m}(2)]+\\eta_{t}\\left(1+\\gamma\\mathbb{E}[\\widehat{P}_{t}^{m}(2|2,1)]\\mathbb{E}[V_{t-1}^{m}(2)]\\right)}\\\\ &{\\qquad\\qquad=(1-\\eta_{t})\\mathbb{E}[V_{t-1}^{m}(2)]+\\eta_{t}\\left(1+\\gamma p\\mathbb{E}[V_{t-1}^{m}(2)]\\right)}\\\\ &{\\qquad\\qquad=(1-\\eta_{t}(1-\\gamma p))\\mathbb{E}[V_{t-1}^{m}(2)]+\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the second step, we used the fact that $\\widehat{P}_{t}^{m}(2|2,1)$ is independent of $V_{t-1}^{m}(2)$ ", "page_idx": 17}, {"type": "text", "text": "\u00b7 Similarly, if $t$ is an averaging instant, we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{t}^{m}(2)=Q_{t}^{m}(2,1)=\\frac{1}{M}\\sum_{j=1}^{M}Q_{t-1/2}^{j}(2,1)}}\\\\ {{\\displaystyle\\qquad=(1-\\eta_{t})\\frac{1}{M}\\sum_{j=1}^{M}V_{t-1}^{j}(2)+\\frac{1}{M}\\sum_{j=1}^{M}\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{j}(2|2,1)V_{t-1}^{j}(2)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Once again, upon taking expectation we obtain, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[V_{t}^{m}(2)]=(1-\\eta_{t})\\frac{1}{M}\\sum_{j=1}^{M}\\mathbb{E}[V_{t-1}^{j}(2)]+\\frac{1}{M}\\sum_{j=1}^{M}\\eta_{t}(1+\\gamma\\mathbb{E}[\\widehat{P}_{t}^{j}(2|2,1)V_{t-1}^{j}(2)])\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=(1-\\eta_{t})\\frac{1}{M}\\sum_{j=1}^{M}\\mathbb{E}[V_{t-1}^{j}(2)]+\\frac{1}{M}\\sum_{j=1}^{M}\\eta_{t}(1+\\gamma p\\mathbb{E}[V_{t-1}^{j}(2)])}\\\\ &{\\displaystyle=(1-\\eta_{t}(1-\\gamma p))\\left(\\frac{1}{M}\\sum_{j=1}^{M}\\mathbb{E}[V_{t-1}^{j}(2)]\\right)+\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Eqns. (30) and (32) together imply that for all $t\\in[T]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}[V_{t}^{m}(2)]\\right)=\\left(1-\\eta_{t}(1-\\gamma p)\\right)\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}[V_{t-1}^{m}(2)]\\right)+\\eta_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On unrolling the above recursion with $V_{0}^{m}=0$ for all $m\\in[M]$ , we obtain the desired claim (26). ", "page_idx": 18}, {"type": "text", "text": "B.3.2 Large learning rates with small $\\frac{\\eta_{T}}{B M}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this subsection, we prove the lower bound for case of large learning rates when the ratio $\\frac{\\eta_{T}}{B M}$ small. For the analysis in this part, we focus on the dynamics of state 1. Unless otherwise specified, throughout the section we implicitly assume that the state is 1. ", "page_idx": 18}, {"type": "text", "text": "We further define a key parameter that will play a key role in the analysis: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau:=\\operatorname*{min}\\{k\\in\\mathbb{N}:\\forall\\,t\\geq k,\\eta_{t}\\leq\\eta_{k}\\leq3\\eta_{t}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It can be noted that for constant step size sequence $\\tau=1$ and for rescaled linear stepsize $\\tau=T/3$ ", "page_idx": 18}, {"type": "text", "text": "Step 1: introducing an auxiliary sequence.  We define an auxiliary sequence $\\widehat{Q}_{t}^{m}(a)$ for $a\\in$ $\\{1,{\\bar{2}}\\}$ and all $t=1,2,\\ldots,T$ to aid our analysis, where we drop the dependency with state $s=$ 1 for simplicity. The evolution of the sequence $\\widehat{Q}_{t}^{m}$ is defined in Algorithm 4, where $\\widehat V_{t}^{m}\\;=\\;$ $\\operatorname*{max}_{a\\in\\{1,2\\}}\\widehat{Q}_{t}^{m}(a)$ . In other words, the iterates $\\{\\widehat{Q}_{t}^{m}\\}$ evolve exactly as the iterates of Algorithm 1 except for the fact that sequence $\\{\\widehat{Q}_{t}^{m}\\}$ is initialized at the optimal $Q$ -function of the MDP. We would like to point out that we assume that the underlying stochasticity is also identical in the sense that the evolution of both $Q_{t}^{m}$ and $\\widehat{Q}_{t}^{m}$ is governed by the same $\\widehat{P}_{t}^{m}$ matrices. The following lemma controls the error between the iterates $Q_{t}^{m}$ and $\\widehat{Q}_{t}^{m}$ , allowing us to focus only on ${\\widehat{Q}}_{t}^{m}$ ", "page_idx": 18}, {"type": "table", "img_path": "6YIpvnkjUK/tmp/d231fef5060eb553f00adae1d6dfd9f8f77386cc77c147b0981b3e6b86eeeab8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Lemma 2. The following relation holds for all agents $m\\in[M]$ all $t\\in[T]$ and $a\\in\\{1,2\\}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ_{t}^{m}(1,a)-\\widehat{Q}_{t}^{m}(a)\\geq-\\frac{1}{1-\\gamma}\\prod_{i=1}^{t}(1-\\eta_{i}(1-\\gamma)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 2, bounding the error of the sequence $\\widehat{Q}_{t}^{m}$ allows us to obtain a bound on the error of $Q_{t}^{m}$ To that effect, we define the following terms for any $t\\leq T$ and all $m\\in[M]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta_{t}^{m}(a):=\\widehat{Q}_{t}^{m}(a)-Q^{\\star}(1,a);\\quad a=1,2;\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta_{t,\\mathrm{max}}^{m}=\\operatorname*{max}_{a\\in\\{1,2\\}}\\Delta_{t}^{m}(a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition, we use $\\begin{array}{r}{\\overline{{\\Delta}}_{t}=\\frac{1}{M}\\sum_{m=1}^{M}\\Delta_{t}^{m}}\\end{array}$ to denote the error of the averaged iterate', and similarly, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\overline{{\\Delta}}_{t,\\mathrm{max}}:=\\operatorname*{max}_{a\\in\\{1,2\\}}\\overline{{\\Delta}}_{t}(a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We first derive a basic recursion regarding $\\Delta_{t}^{m}(a)$ . From the iterative update rule in Algorithm 4, we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{t}^{m}(a)=(1-\\eta_{t})\\Delta_{t-1}^{m}(a)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,a)\\widehat{V}_{t-1}^{m}-Q^{\\star}(1,a))}\\\\ &{\\qquad\\quad=(1-\\eta_{t})\\Delta_{t-1}^{m}(a)+\\eta_{t}\\gamma(\\widehat{P}_{t}^{m}(1|1,a)\\widehat{V}_{t-1}^{m}-p V^{\\star}(1))}\\\\ &{\\qquad\\quad=(1-\\eta_{t})\\Delta_{t-1}^{m}(a)+\\eta_{t}\\gamma(p(\\widehat{V}_{t-1}^{m}-V^{\\star}(1))+(\\widehat{P}_{t}^{m}(1|1,a)-p)\\widehat{V}_{t-1})}\\\\ &{\\qquad\\quad=(1-\\eta_{t})\\Delta_{t-1}^{m}(a)+\\eta_{t}\\gamma(p\\Delta_{t-1,\\operatorname*{max}}^{m}+(\\widehat{P}_{t}^{m}(1|1,a)-p)\\widehat{V}_{t-1}^{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here in the last line, we used the following relation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{t,\\operatorname*{max}}^{m}=\\operatorname*{max}_{a\\in\\{1,2\\}}(\\widehat{Q}_{t}^{m}(a)-Q^{\\star}(1,a))=\\operatorname*{max}_{a\\in\\{1,2\\}}\\widehat{Q}_{t}^{m}(a)-V^{\\star}(1)=\\widehat{V}_{t-1}^{m}-V^{\\star}(1),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as $Q^{\\star}(1,1)=Q^{\\star}(1,2)=V^{\\star}(1)$ ", "page_idx": 19}, {"type": "text", "text": "Recursively unrolling the above expression, and using the expression (17), we obtain the following relation: for any $t^{\\prime}<t$ when there is no averaging during the interval $(t^{\\prime},t)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{t}^{m}(a)=\\left(\\prod_{k=t^{\\prime}+1}^{t}(1-\\eta_{k})\\right)\\Delta_{t^{\\prime}}^{m}(a)+\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma(p\\Delta_{k-1,\\operatorname*{max}}^{m}+(\\widehat{P}_{k}^{m}(1|1,a)-p)\\widehat{V}_{k-1}^{m}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For any $t^{\\prime},t$ with $t^{\\prime}<t$ , we define the notation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\varphi_{t^{\\prime},t}:=\\prod_{k=t^{\\prime}+1}^{t}(1-\\eta_{k}),}&{{}}\\\\ {\\displaystyle\\xi_{t^{\\prime},t}^{m}(a):=\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma(\\widehat{P}_{k}^{m}(1|1,a)-p)\\widehat{V}_{k-1}^{m},\\quad a=1,2;}\\\\ {\\displaystyle\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}:=\\operatorname*{max}_{a\\in\\{1,2\\}}\\xi_{t^{\\prime},t}^{m}(a).}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that by definition, $\\mathbb{E}[\\xi_{t^{\\prime},t}^{m}(a)]=0$ for $a\\in\\{1,2\\}$ and all $m,\\,t^{\\prime}$ and $t$ . Plugging them into the previous expression leads to the simplified expression ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{t}^{m}(a)=\\varphi_{t^{\\prime},t}\\Delta_{t^{\\prime}}^{m}(a)+\\left[\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\Delta_{k-1,\\mathrm{max}}^{m}\\right]+\\xi_{t^{\\prime},t}^{m}(a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We specifically choose $t^{\\prime}$ and $t$ to be the consecutive averaging instants to analyze the behaviour of $\\Delta_{t}^{m}$ across two averaging instants. Consequently, we can rewrite the above equation as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{t}^{m}(a)=\\varphi_{t^{\\prime},t}\\overline{{\\Delta}}_{t^{\\prime}}(a)+\\left[\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\Delta_{k-1,\\mathrm{max}}^{m}\\right]+\\xi_{t^{\\prime},t}^{m}(a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, after averaging, we obtain, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\overline{{\\Delta}}_{t}(a)=\\varphi_{t^{\\prime},t}\\overline{{\\Delta}}_{t^{\\prime}}(a)+\\frac{1}{M}\\sum_{m=1}^{M}\\left[\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\Delta_{k-1,\\mathrm{max}}^{m}\\right]+\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "IWe use this different notation in appendix as opposed to the half-time indices used in the main text to improve readability of the proof. ", "page_idx": 19}, {"type": "text", "text": "Step 2: deriving a recursive bound for $\\mathbb{E}[\\overline{{\\Delta}}_{t,\\operatorname*{max}}]$ .Bounding (40), we obtain, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{t,\\mathrm{max}}^{m}\\geq\\varphi_{t^{\\prime},t}\\overline{{\\Delta}}_{t^{\\prime},\\mathrm{max}}+\\left[\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\Delta_{k-1,\\mathrm{max}}^{m}\\right]+\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}-\\varphi_{t^{\\prime},t}|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|,}\\\\ &{\\Delta_{t,\\mathrm{max}}^{m}\\leq\\varphi_{t^{\\prime},t}\\overline{{\\Delta}}_{t^{\\prime},\\mathrm{max}}+\\left[\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\Delta_{k-1,\\mathrm{max}}^{m}\\right]+\\xi_{t^{\\prime},t,\\mathrm{max}}^{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the first step we used the fact that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{nax}\\{a_{1}+b_{1},a_{2}+b_{2}\\}\\geq\\operatorname*{min}\\{a_{1},a_{2}\\}+\\operatorname*{max}\\{b_{1},b_{2}\\}=\\operatorname*{max}\\{a_{1},a_{2}\\}+\\operatorname*{max}\\{b_{1},b_{2}\\}-|a_{1}-a_{2}|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On taking expectation, we obtain, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Im[\\Delta_{t,\\operatorname*{max}}^{m}]\\ge\\varphi_{t^{\\prime},t}\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]+\\left[\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\mathbb{E}[\\Delta_{k-1,\\operatorname*{max}}^{m}]\\right]+\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(\\Delta_{t^{\\prime},t^{\\prime}}^{m})|].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta_{t,\\operatorname*{max}}^{m}]\\leq\\varphi_{t^{\\prime},t}\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]+\\left[\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\mathbb{E}[\\Delta_{k-1,\\operatorname*{max}}^{m}]\\right]+\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, using (41) and (43) we can write, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\overline{{\\Delta}}_{t,\\operatorname*{max}}\\geq\\varphi_{t^{\\prime},t}\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}+\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left[\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\tilde{\\eta}_{k}^{(t)}\\gamma p\\Delta_{k-1,\\operatorname*{max}}^{m}\\right]-\\varphi_{t^{\\prime},t}|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|}\\\\ {\\displaystyle+\\operatorname*{max}\\left\\{\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(1),\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(2)\\right\\}}\\\\ {\\implies\\mathbb{E}[\\overline{{\\Delta}}_{t,\\operatorname*{max}}]\\geq\\varphi_{t^{\\prime},t}\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]+\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left[\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\tilde{\\eta}_{k}^{(t)}\\gamma p\\mathbb{E}[\\Delta_{k-1,\\operatorname*{max}}^{m}]\\right]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|]}\\\\ {\\displaystyle+\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(1),\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(2)\\right\\}\\right].\\qquad\\qquad(45\\mathrm{b})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On combining (44b) and (45b), we obtain, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\overline{{\\Delta}}_{t,\\mathrm{max}}]\\geq\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left[\\mathbb{E}[\\Delta_{t,\\mathrm{max}}^{m}]-\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]\\right]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|]\\,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(1),\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(2)\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In order to simplify (46), we make use of the following lemmas. ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. Let $t^{\\prime}<t$ be two consecutive averaging instants. Then for all $m\\in[M]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell[\\Delta_{t,\\mathrm{max}}^{m}]-\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]\\geq\\left(\\displaystyle\\prod_{k=t^{\\prime}+1}^{t}\\left(1-\\eta_{k}(1-\\gamma p)\\right)\\right)\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]+\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]\\left[\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}-1\\right]_{+}}\\\\ {-\\,\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|],\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $[x]_{+}=\\operatorname*{max}\\{x,0\\}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma 4.For all consecutive averaging instants $t^{\\prime},t$ satisfying $t-\\operatorname*{max}\\{t^{\\prime},\\tau\\}\\geq1/\\eta_{\\tau}$ and all $m\\in[M]$ we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]\\geq\\frac{1}{240\\log\\left(\\frac{180B}{\\eta_{T}(1-\\gamma)}\\right)}\\cdot\\frac{\\nu}{\\nu+1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(1),\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(2)\\right\\}\\right]\\geq\\frac{1}{240\\log\\left(\\frac{180B M}{\\eta_{T}(1-\\gamma)}\\right)}\\cdot\\frac{\\nu}{\\nu+\\sqrt{M}},$ where v := $\\nu:=\\sqrt{\\frac{20\\eta_{T}}{B(1-\\gamma)}}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 5. For all $t\\in\\{t_{r}\\}_{r=1}^{R}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[|\\overline{{\\Delta}}_{t}(1)-\\overline{{\\Delta}}_{t}(2)|]\\leq\\sqrt{\\frac{8\\eta_{T}}{3B M(1-\\gamma)}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, on combining the results from Lemmas 3, 4, and 5 and plugging them into (46), we obtain the following relation for $t,t^{\\prime}\\ge\\tau$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi|\\overline{{\\Delta}}_{t,\\operatorname*{max}}|\\geq\\left(\\displaystyle\\prod_{k=t^{\\prime}+1}^{t}\\left(1-\\eta_{k}(1-\\gamma p)\\right)\\right)\\mathbb{E}\\big[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}\\big]+\\mathbb{E}\\big[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}\\big]\\left[\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}-1\\right]_{+}}\\\\ &{\\qquad\\qquad-2\\varphi\\nu_{t^{\\prime},t}\\mathbb{E}\\big[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|\\big]+\\mathbb{E}\\left[\\operatorname*{max}\\left\\{\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(1),\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(2)\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\geq\\left(1-\\eta_{r}(1-\\gamma p)\\right)^{t-t^{\\prime}}\\mathbb{E}\\big[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}\\big]+\\left(\\displaystyle\\frac{1-\\left(1-\\eta_{r}\\left(1-\\gamma p\\right)\\right)^{t-t^{\\prime}}}{5760\\log\\left(\\frac{180B}{\\eta\\tau(1-\\gamma)}\\right)}\\right)\\cdot\\frac{\\nu}{\\nu+1}\\cdot1\\left\\{t-\\right.}}\\\\ &{\\qquad\\qquad-\\left.2(1-\\eta_{T})^{t-t^{\\prime}}\\sqrt{\\frac{8\\eta_{T}}{3B M(1-\\gamma)}}+\\frac{1}{2400\\log\\left(\\frac{180B}{\\eta\\tau(1-\\gamma)}\\right)}\\cdot\\frac{\\nu}{\\nu+\\sqrt{M}}\\cdot1\\left\\{t-t^{\\prime}\\geq\\frac{8}{\\eta_{T}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used the relation $\\varphi_{t^{\\prime},t}\\leq(1-\\eta_{T})^{t-t^{\\prime}}$ , as well as the value of $\\nu$ as defined in Lemma 4 along with the fact ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}-1\\ge\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{t-t^{\\prime}}}{24(1-\\gamma p)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $t,t^{\\prime}\\ge\\tau$ such that $t-t^{\\prime}\\geq8/\\eta_{\\tau}$ ", "page_idx": 21}, {"type": "text", "text": "Proof of (48). We have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}-1=\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\left(\\eta_{k}\\displaystyle\\prod_{i=k+1}^{t}\\left(1-\\eta_{i}(1-\\gamma)p\\right)\\right)-1}\\\\ {\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\left(\\eta_{i}\\displaystyle\\prod_{i=k+1}^{t}\\left(1-\\eta_{\\tau}(1-\\gamma p)\\right)\\right)-1}\\\\ {\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\left(1-\\eta_{\\tau}(1-\\gamma p)\\right)^{t-k}-1}\\\\ {\\displaystyle}&{\\geq\\eta_{i}\\cdot\\left(\\displaystyle\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{t-\\ell^{\\prime}}}{\\eta_{\\tau}(1-\\gamma p)}\\right)-1}\\\\ {\\displaystyle}&{\\geq\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{t-\\ell^{\\prime}}}{3(1-\\gamma p)}-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show (48), it is sufficient to show that $\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{t-t^{\\prime}}}{3(1-\\gamma p)}\\geq\\frac{8}{7}$ for $t-t^{\\prime}\\geq8/\\eta_{\\tau}$ .Thus, for $t-t^{\\prime}\\geq8/\\eta_{\\tau}$ we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{t-t^{\\prime}}}{3(1-\\gamma p)}\\geq\\frac{1-\\exp(-\\eta_{\\tau}(1-\\gamma p)\\cdot(t-t^{\\prime}))}{3(1-\\gamma p)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\geq{\\frac{1-\\exp(-8(1-\\gamma p))}{3(1-\\gamma p)}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\gamma\\geq5/6,1-\\gamma p\\leq2/9$ For $x\\leq2/9$ the function $\\begin{array}{r}{f(x)=\\frac{1-e^{-8x}}{3x}\\geq8/7}\\end{array}$ - \u2265 8/7, proving the claim. ", "page_idx": 22}, {"type": "text", "text": "Step3:lowerbounding $\\mathbb{E}[\\overline{{\\Delta}}_{T,\\mathrm{max}}]$ . We are now interested in evaluating $\\mathbb{E}[\\overline{{\\Delta}}_{T,\\operatorname*{max}}]$ based on the recursion (47). To this effect, we introduce some notation to simplify the presentation. Let ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{\\tau}:=\\operatorname*{min}\\{r:t_{r}\\geq\\tau\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For $r=R_{\\tau},\\ldots,R$ , we define the following terms: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{r}:=\\mathbb{E}[\\widetilde\\Delta_{t},\\operatorname*{max}],}\\\\ &{\\alpha_{r}:=(1-\\eta_{r}(1-\\gamma p))^{t_{r}-t_{r-1}},}\\\\ &{\\beta_{r}:=(1-\\eta_{r})^{t_{r}-t_{r-1}},}\\\\ &{T_{r}:=\\{r\\geq r^{\\prime}>R_{T}:t_{r^{\\prime}}-t_{r^{\\prime}-1}\\geq8/\\eta_{r}\\},}\\\\ &{C_{1}:=\\frac{1}{5760\\log\\left(\\frac{180R}{\\eta_{r}(1-\\gamma p)}\\right)(1-\\gamma p)}\\cdot\\frac{\\nu}{\\nu+1},}\\\\ &{C_{2}:=\\sqrt{\\frac{32\\eta_{r}}{3B M(1-\\gamma)}},}\\\\ &{C_{3}:=\\frac{1}{2401\\log\\left(\\frac{180R}{\\eta_{r}(1-\\gamma p)}\\right)}\\cdot\\frac{\\nu}{\\nu+\\sqrt{M}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With these notations in place, the recursion in (47) can be rewritten as ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{r}\\geq\\alpha_{r}x_{r-1}-\\beta_{r}C_{2}+C_{3}\\mathbb{1}\\{r\\in\\mathbb{Z}_{r}\\}+(1-\\alpha_{r})C_{1}\\mathbb{1}\\{r\\in\\mathbb{Z}_{r}\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $r\\geq R_{\\tau}$ . We claim that $x_{r}$ satisfies the following relation for all $r\\geq R_{\\tau}+1$ (whose proof is deferred to the end of this step): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{r}\\geq\\left(\\displaystyle\\prod_{i=R_{\\tau}+1}^{r}\\alpha_{i}\\right)x_{R_{\\tau}}-\\displaystyle\\sum_{k=R_{\\tau}+1}^{r}\\beta_{k}\\left(\\displaystyle\\prod_{i=k+1}^{r}\\alpha_{i}\\right)C_{2}+\\displaystyle\\sum_{k=R_{\\tau}+1}^{r}\\left(\\displaystyle\\prod_{i=k+1}^{r}\\alpha_{i}\\right)\\mathbb{1}\\left\\{k\\in\\mathbb{Z}_{k}\\right\\}C_{3}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle C_{1}\\left(\\displaystyle\\prod_{i\\notin\\mathbb{Z}_{r}}\\alpha_{i}\\right)\\left(1-\\displaystyle\\prod_{i\\in\\mathbb{Z}_{r}}\\alpha_{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we recall that if there is no valid index for a product, its value is taken to be 1. ", "page_idx": 22}, {"type": "text", "text": "Invoking (53) for $r=R$ and using the relation $x_{R_{\\tau}-1}\\geq0$ , we obtain, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\gamma_{R}\\geq-\\sum_{k=R_{\\tau}}^{R}\\beta_{k}\\left(\\prod_{i=k+1}^{R}\\alpha_{i}\\right)C_{2}+\\sum_{k=R_{\\tau}}^{R}\\left(\\prod_{i=k+1}^{R}\\alpha_{i}\\right)C_{3}\\mathbb{1}\\{k\\in\\mathbb{Z}_{k}\\}+C_{1}\\left(\\prod_{i\\not\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\left(1-\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)}}\\\\ {{\\displaystyle{\\geq-R C_{2}+C_{1}\\left(\\prod_{i\\not\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\left(1-\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)}}\\\\ {{\\displaystyle{\\geq-R\\cdot\\sqrt{\\frac{32\\eta_{T}}{3B M(1-\\gamma)}}+\\left(\\prod_{i\\not\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\left(1-\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\cdot\\frac{1}{5760\\log\\left(\\frac{180B}{\\eta_{T}(1-\\gamma)}\\right)(1-\\gamma p)}\\cdot\\frac{\\nu}{\\nu+1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the fact $\\begin{array}{r}{\\beta_{k}\\left(\\prod_{i=k+1}^{R}\\alpha_{i}\\right)\\leq1}\\end{array}$ and that $C_{3}\\geq0$ Consider the expression ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\prod_{i\\notin{\\cal Z}_{R}}\\alpha_{i}=\\prod_{i\\notin\\cal Z_{R}}\\left(1-\\eta_{\\tau}(1-\\gamma p)\\right)^{t_{i}-t_{i-1}}\\geq1-\\eta_{\\tau}(1-\\gamma p)\\cdot\\sum_{i\\notin{\\cal Z}_{R}}(t_{i}-t_{i-1})\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(1-\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)=1-(1-\\eta_{\\tau}(1-\\gamma p))^{T-\\tau-T_{1}}\\geq1-\\exp\\left(-\\eta_{\\tau}(1-\\gamma p)\\left(T-\\tau-T_{1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $T_{1}$ satisfies the following bound ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{1}:=\\sum_{i\\notin\\mathbb{Z}_{R}}(t_{i}-t_{i-1})\\leq(R-|\\mathbb{Z}_{R}|)\\cdot\\frac{8}{\\eta_{\\tau}}\\leq\\frac{8R}{\\eta_{\\tau}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We split the remainder of the analysis based on the step size schedule. ", "page_idx": 23}, {"type": "text", "text": "\u00b7For theconstantszeshede. $\\begin{array}{r}{\\eta_{t}=\\eta\\ge\\frac{1}{(1-\\gamma)T}}\\end{array}$ , we have, $R_{\\tau}=0$ with $\\tau=0$ and $t_{0}=0$ (as allagnsta at e $\\begin{array}{r}{R\\leq\\frac{1}{96000(1-\\gamma)\\log\\left(\\frac{180B}{\\eta(1-\\gamma)}\\right)}}\\end{array}$ then,(5),(56) and (57) yield the following relations: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{T_{1}\\leq\\frac{8R}{\\eta}\\leq\\frac{T}{12000\\log(180N)},}}\\\\ &{}&{\\displaystyle\\prod_{i\\notin\\mathbb{Z}_{R}}\\alpha_{i}\\geq1-\\eta(1-\\gamma p)\\cdot T_{1}\\geq1-\\frac{32R(1-\\gamma)}{3}\\geq1-\\frac{1}{9000\\log(180N)},}\\\\ &{}&{\\displaystyle\\left(1-\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\geq1-\\exp\\left(-\\eta(1-\\gamma p)\\left(T-T_{1}\\right)\\right)\\geq1-\\exp\\left(-\\frac{4}{3}\\left(1-\\frac{1}{9000\\log(180N)}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On plugging the above relations into (54), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\nx_{R}\\geq{\\frac{\\sqrt{40}}{96000\\log\\left({\\frac{180B}{\\eta(1-\\gamma)}}\\right)(1-\\gamma)}}\\cdot\\left({\\frac{\\nu}{\\nu+1}}-{\\frac{\\nu}{5{\\sqrt{M}}}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where recalltat v :=  3B(1 - ) Consider the function $\\textstyle f(x)\\,=\\,{\\frac{x}{x+1}}\\,-\\,{\\frac{x}{5{\\sqrt{M}}}}$ 5vM \u00b7 We claim that for $x\\in[0,\\sqrt{M}]$ and all $M\\geq2$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(x)\\geq{\\frac{7}{20}}\\operatorname*{min}\\{x,1\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The proof of the above claim is deferred to the end of the section. In light of the above claim, wehave, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{R}\\geq\\frac{\\sqrt{40}}{96000\\log\\left(\\frac{180B}{\\eta(1-\\gamma)}\\right)(1-\\gamma)}\\cdot\\frac{7}{20}\\cdot\\operatorname*{min}\\left\\{1,\\sqrt{\\frac{20\\eta}{3B(1-\\gamma)}}\\right\\}}}\\\\ {{\\geq\\frac{\\sqrt{40}}{96000\\log\\left(180N\\right)}\\cdot\\frac{7}{20}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{1-\\gamma},\\sqrt{\\frac{20}{3(1-\\gamma)^{4}N}}\\right\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used the fact that M \u2265 2, og(1/r) is an increasing function and the relation $\\frac{\\nu}{M}=\\frac{20\\eta}{3B M(1-\\gamma)}\\leq\\frac{1}{15}\\leq1.$ ", "page_idx": 23}, {"type": "text", "text": "\u00b7 Next, we consider the rescaled linear step size schedule, where $\\tau=T/3$ (cf. (34)). To begin, we assume $\\begin{array}{r}{t_{R_{\\tau}}\\leq\\operatorname*{max}\\lbrace\\frac{3T}{4},T-\\frac{1}{6\\eta_{\\tau}\\left(1-\\gamma p\\right)}\\rbrace}\\end{array}$ Itis straighforward to notethat ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\frac{3T}{4},T-\\frac{1}{6\\eta_{\\tau}(1-\\gamma p)}\\right\\}=\\left\\{\\frac{\\frac{3T}{4}}{T-\\frac{1}{6\\eta_{\\tau}(1-\\gamma p)}}\\right.\\quad\\mathrm{if~}c_{\\eta}\\geq3.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\begin{array}{r}{R\\,\\leq\\,\\frac{1}{384000(1-\\gamma)\\log\\Big(\\frac{180B}{\\eta_{T}(1-\\gamma)}\\Big)\\cdot(5+c_{\\eta})}}\\end{array}$ then, (55), (56) and (57) yield the following relations: ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{1}\\leq\\frac{8R}{\\eta_{\\tau}},\\qquad\\prod_{i\\notin\\cal Z_{R}}\\alpha_{i}\\geq1-\\eta_{\\tau}(1-\\gamma p)\\cdot T_{1}\\geq1-\\frac{32R(1-\\gamma)}{3}\\geq1-\\frac{1}{36000}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $c_{\\eta}\\geq3$ , we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left(1-\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\geq1-\\exp\\left(-\\eta_{\\tau}(1-\\gamma p)\\left(T-t_{R_{\\tau}}-T_{1}\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\geq1-\\exp\\left(-\\frac{(1-\\gamma)T}{(3+c_{\\eta}(1-\\gamma)T)}+\\frac{32R(1-\\gamma)}{3}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\frac{1}{2(3+c_{\\eta})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used $\\begin{array}{r}{T\\geq\\frac{1}{1-\\gamma}}\\end{array}$ in the second step. Similarly, for $c_{\\eta}<3$ , we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\displaystyle\\prod_{i\\in\\mathbb{Z}_{R}}\\alpha_{i}\\right)\\geq1-\\exp\\left(-\\eta_{\\tau}(1-\\gamma p)\\left(T-t_{R_{\\tau}}-T_{1}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq1-\\exp\\left(-\\displaystyle\\frac16+\\frac{32R(1-\\gamma)}3\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac1{10}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On plugging the above relations into (54), we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{R}\\geq\\frac{18\\sqrt{1.6}}{384000\\log\\left(\\frac{180B}{\\eta\\tau(1-\\gamma)}\\right)\\left(1-\\gamma\\right)\\left(5+c_{\\eta}\\right)}\\cdot\\left(\\frac{\\nu}{\\nu+1}-\\frac{\\nu}{18\\sqrt{M}}\\right)}\\\\ &{\\quad\\geq\\frac{18\\sqrt{1.6}}{384000\\log\\left(\\frac{180B}{\\eta\\tau(1-\\gamma)}\\right)\\left(1-\\gamma\\right)\\left(5+c_{\\eta}\\right)}\\cdot\\frac{7}{20}\\cdot\\operatorname*{min}\\left\\{1,\\sqrt{\\frac{20\\eta_{T}}{3B(1-\\gamma)}}\\right\\}}\\\\ &{\\quad\\geq\\frac{18\\sqrt{1.6}}{384000\\log\\left(\\frac{180B}{\\eta\\tau(1-\\gamma)}\\right)\\left(5+c_{\\eta}\\right)}\\cdot\\frac{7}{20}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{1-\\gamma},\\sqrt{\\frac{20\\eta_{T}}{3B(1-\\gamma)^{3}}}\\right\\}}\\\\ &{\\quad\\geq\\frac{18\\sqrt{1.6}}{384000\\log\\left(\\frac{180B}{\\eta\\tau(1-\\gamma)}\\right)\\left(5+\\log\\mathcal{N}\\right)}\\cdot\\frac{7}{20}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{1-\\gamma},\\sqrt{\\frac{20\\eta}{3B(1+\\log\\mathcal{N})(1-\\gamma)^{4}}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we again used the facts that $M\\geq2$ $c_{\\eta}\\leq\\log N$ \uff0c $\\frac{\\sqrt{x}}{\\log(1/x)}$ is an inereasing funtion and the relation M= 3BM(1-)\u22641. ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Last but not least, let us consider the rescaled linear step size schedule case when $t_{R_{\\tau}}>$ $\\begin{array}{r l}{\\operatorname*{max}\\lbrace\\frac{3T}{4},T-\\frac{1}{6\\eta_{\\tau}(1-\\gamma p)}\\rbrace}&{{}}\\end{array}$ 6n-(1-p) 1. The condition imples that the time between the communication rounds $R_{\\tau}-1$ and $R_{\\tau}$ is least $\\begin{array}{r}{T_{0}:=\\operatorname*{max}\\lbrace\\frac{5T}{12},\\frac{2T}{3}-\\frac{1}{6\\eta_{\\tau}\\left(1-\\gamma p\\right)}\\rbrace}\\end{array}$ Thus 47) ielstat $\\mathbb{E}[\\overline{{\\Delta}}_{t_{R_{\\tau}}}]\\geq\\left(\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{T_{0}}}{5760\\log\\left(\\frac{180}{B\\eta_{T}(1-\\gamma)}\\right)(1-\\gamma p)}\\right)\\cdot\\frac{\\nu}{\\nu+1}-2(1-\\eta_{T})^{T_{0}}\\sqrt{\\frac{8\\eta_{T}}{3B M(1-\\gamma)}}.$ ", "page_idx": 24}, {"type": "text", "text": "Using the above relation along with (53), we can conclude that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{x_{R}\\geq(1-\\eta_{\\tau}(1-\\gamma p))^{T-t_{R_{\\tau}}}\\left(\\frac{1-(1-\\eta_{\\tau}(1-\\gamma p))^{T_{0}}}{5760\\log\\left(\\frac{180}{B\\eta_{T}(1-\\gamma)}\\right)(1-\\gamma p)}\\right)\\cdot\\frac{\\nu}{\\nu+1}}}\\\\ &{}&{-\\,2(1-\\eta_{T})^{T_{0}}\\cdot(1-\\eta_{\\tau}(1-\\gamma p))^{T-t_{R_{\\tau}}}\\sqrt{\\frac{8\\eta_{T}}{3B M(1-\\gamma)}}-R C_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the above relation, we used the trivial bounds $C_{1},C_{3}\\geq0$ and a crude bound on the term corresponding to $C_{2}$ , similar to (54). Let us first consider the case of $c_{\\eta}\\geq3$ .We have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-\\left(1-\\eta_{\\tau}(1-\\gamma p)\\right)^{T_{0}}\\ge1-\\exp\\left(-\\eta_{\\tau}(1-\\gamma p)5T/12\\right)\\ge1-\\exp\\left(-\\frac{5(1-\\gamma)T}{3(3+c_{\\eta}(1-\\gamma)T)}\\right)\\ge\\frac{1}{3+b_{\\tau}}}}\\\\ {{\\displaystyle\\ -\\,\\eta_{\\tau}(1-\\gamma p))^{T-t_{R\\tau}}\\ge1-\\eta_{\\tau}(1-\\gamma p)\\frac{T}{4}\\ge1-\\frac{(1-\\gamma)T}{(3+c_{\\eta}(1-\\gamma)T)}\\ge1-\\frac{1}{c_{\\eta}}\\ge\\frac{2}{3}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, for $c_{\\eta}<3$ , we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-(1-\\eta_{\\tau}(1-\\gamma p))^{T_{0}}\\geq1-\\exp\\left(-\\eta_{\\tau}(1-\\gamma p)\\displaystyle\\frac{2T}{3}+\\displaystyle\\frac{1}{6}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq1-\\exp\\left(-\\displaystyle\\frac{8(1-\\gamma)T}{3(3+c_{\\eta}(1-\\gamma)T)}+\\displaystyle\\frac{1}{6}\\right)\\geq1-e^{-5/18},}\\\\ &{\\qquad\\qquad\\qquad(1-\\eta_{\\tau}(1-\\gamma p))^{T-t_{R_{\\tau}}}\\geq1-\\displaystyle\\frac{\\eta_{\\tau}(1-\\gamma p)}{6\\eta_{\\tau}(1-\\gamma p)}\\geq\\displaystyle\\frac{5}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above relations implies that $\\begin{array}{r}{(1-\\eta_{\\tau}(1-\\gamma p))^{T-t_{R_{\\tau}}}(1-(1-\\eta_{\\tau}(1-\\gamma p))^{T_{0}})\\geq c}\\end{array}$ for some constant $c$ which only depends on $c_{\\eta}$ . On plugging this into (63), we obtain a relation that is identical to that in (54) up to leading constants. Thus, by using a similar sequence of argument as used to obtain (61), we arrive at the same conclusion as for the case of $\\begin{array}{r}{t_{R_{\\tau}}\\leq\\operatorname*{max}\\lbrace\\frac{3T}{4},T-\\frac{1}{6\\eta_{\\tau}\\left(1-\\gamma p\\right)}\\rbrace}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Step 4: finishing up the proof. Thus, (60), (61) along with the above conclusion together imply that there exists a numerical constant $c_{0}>0$ suchthat ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[|\\widehat{V}_{T}^{m}(1)-V^{\\star}(1)|]\\ge\\mathbb{E}[\\overline{{\\Delta}}_{T,\\operatorname*{max}}]\\ge\\frac{c_{0}}{\\log^{3}N}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{1-\\gamma},\\sqrt{\\frac{1}{(1-\\gamma)^{4}N}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above equation along with Lemma 2 implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[|V_{T}^{m}-V^{\\star}(1)|]\\ge\\frac{c_{0}}{\\log^{3}N}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{1-\\gamma},\\sqrt{\\frac{1}{(1-\\gamma)^{4}N}}\\right\\}-\\frac{1}{1-\\gamma}\\prod_{i=1}^{T}(1-\\eta_{i}(1-\\gamma)).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, from (21) we know that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[|V_{T}^{m}(3)-V^{\\star}(3)|]\\ge\\frac{1}{1-\\gamma}\\prod_{i=1}^{T}(1-\\eta_{i}(1-\\gamma)).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\xi}[\\|Q_{T}^{m}-Q^{\\star}\\|_{\\infty}]\\geq\\mathbb{E}\\left[\\operatorname*{max}\\left\\{|V_{T}^{m}(3)-V^{\\star}(3)|,|V_{T}^{m}(1)-V^{\\star}(1)|\\}\\right]}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{max}\\left\\{\\mathbb{E}\\left[|V_{T}^{m}(3)-V^{\\star}(3)|\\right],\\mathbb{E}\\left[|V_{T}^{m}(1)-V^{\\star}(1)|\\right]\\right\\}}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{max}\\left\\{\\displaystyle\\frac{1}{1-\\gamma}\\prod_{i=1}^{T}(1-\\eta_{i}(1-\\gamma)),\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{1-\\gamma},\\sqrt{\\frac{1}{(1-\\gamma)^{4}N}}\\right\\}-\\frac{1}{1-\\gamma}\\prod_{i=1}^{T}(1-\\gamma)\\right\\}}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{2}\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{1-\\gamma},\\sqrt{\\frac{1}{(1-\\gamma)^{4}N}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the third step follows from (65) and (66) and the fourth step uses $\\operatorname*{max}\\{a,b\\}\\geq(a+b)/2$ ", "page_idx": 25}, {"type": "text", "text": "Thus, from (28) and (67) we can conclude that whenever $\\begin{array}{r c l}{{\\sf C C}_{\\mathrm{round}}}&{=}&{{\\mathcal O}\\left(\\frac{1}{\\left(1-\\gamma\\right)\\log^{2}N}\\right)}\\end{array}$ $\\begin{array}{r}{\\mathsf{E R}(\\mathcal{A};N,M)~=~\\Omega\\left(\\frac{1}{\\log^{3}N\\sqrt{N}}\\right)}\\end{array}$ for all values of $M~\\geq~2$ . In other words, for any algorithm to achieve any collaborative gain, its communication complexity should satisfy $\\mathsf{C C}_{\\mathsf{r o u n d}}=$ $\\begin{array}{r}{\\Omega\\left(\\frac{1}{\\left(1-\\gamma\\right)\\log^{2}N}\\right)}\\end{array}$ ,asrqguired. ", "page_idx": 25}, {"type": "text", "text": "Proof of (53). We now return to establish (53) using induction. For the base case, (52) yields $\\begin{array}{r}{^{:}R_{\\tau}+1\\geq\\alpha_{R_{\\tau}+1}x_{R_{\\tau}}-\\beta_{R_{\\tau}+1}C_{2}+C_{3}\\mathbb{1}\\{R_{\\tau}+1\\in{\\mathbb Z}_{R_{\\tau}+1}\\}+(1-\\alpha_{R_{\\tau}+1})C_{1}\\mathbb{1}\\{R_{\\tau}+1\\in{\\mathbb Z}_{R_{\\tau}+1}\\}.}\\end{array}$ (68) ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Note that this is identical to the expression in (53) for $r=R_{\\tau}+1$ as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(\\prod_{i\\notin{\\cal Z}_{R_{\\tau}+1}}\\alpha_{i}\\right)\\left(1-\\prod_{i\\in{\\cal Z}_{R_{\\tau}+1}}\\alpha_{i}\\right)=(1-\\alpha_{R_{\\tau}+1})\\mathbb{1}\\{R_{\\tau}+1\\in{\\cal Z}_{R_{\\tau}+1}\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "based on the adopted convention for products with no valid indices. For the induction step, assume (53) holds for some $r\\geq R_{\\tau}+1$ . On combining (52) and (53), we obtain, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\hat{r}}_{r+1}\\geq\\alpha_{r+1}x_{r}-\\beta_{r+1}C_{2}+C_{3}\\mathbb{1}\\{(r+1)\\in{\\cal Z}_{r+1}\\}+(1-\\alpha_{r+1})C_{1}\\mathbb{1}\\{r+1\\in{\\cal Z}_{r+1}\\}}\\\\ &{\\qquad\\geq\\alpha_{r+1}\\left(\\displaystyle\\prod_{i=R_{r}+1}^{r}\\alpha_{i}\\right)x_{R_{r}}-\\alpha_{r+1}\\displaystyle\\sum_{k=R_{r}+1}^{r}\\beta_{k}\\left(\\displaystyle\\prod_{i=k+1}^{r}\\alpha_{i}\\right)C_{2}+\\alpha_{r+1}\\displaystyle\\sum_{k=R_{r}+1}^{r}\\left(\\displaystyle\\prod_{i=k+1}^{r}\\alpha_{i}\\right)C_{3}}\\\\ &{\\qquad+\\alpha_{r+1}C_{1}\\left(\\displaystyle\\prod_{i\\notin{\\cal Z}_{r}}\\alpha_{i}\\right)\\left(1-\\displaystyle\\prod_{i\\in{\\cal Z}_{r}}\\alpha_{i}\\right)-\\beta_{r+1}C_{2}+C_{3}\\mathbb{1}\\{(r+1)\\in{\\cal Z}_{r+1}\\}+(1-\\alpha_{r+1})C_{1}\\mathbb{1}\\{(\\alpha_{r}+1)\\}}\\\\ &{\\qquad\\geq\\left(\\displaystyle\\prod_{i=R_{r+1}+1}^{r+1}\\alpha_{i}\\right)x_{R_{r}}-\\displaystyle\\sum_{k=R_{r+1}+1}^{r+1}\\beta_{k}\\left(\\displaystyle\\prod_{i=k+1}^{r+1}\\alpha_{i}\\right)C_{2}+\\displaystyle\\sum_{k=R_{r+1}+1}^{r+1}\\left(\\displaystyle\\prod_{i=k+1}^{r+1}\\alpha_{i}\\right)C_{3}\\mathbb{1}\\{k\\in{\\cal Z}_{k}\\}}\\\\ &{\\qquad+\\alpha_{r+1}C_{1}\\left(\\displaystyle\\prod_{i\\notin{\\cal Z}_{r}}\\alpha_{i}\\right)\\left(1-\\displaystyle\\prod_{i\\in{\\cal Z}_{r}}\\alpha_{i}\\right)+(1-\\alpha_{r+1})C_{1}\\mathbb{1}\\{(r+1)\\in{\\cal Z}_{r+1}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$(r\\;+\\;1)\\;\\;\\notin\\;\\;{\\cal Z}_{r+1}$ \uff0cthen $\\begin{array}{r c l}{\\left(1-\\prod_{i\\in{\\mathbb Z}_{r}}\\alpha_{i}\\right)}&{=}&{\\left(1-\\prod_{i\\in{\\mathbb Z}_{r+1}}\\alpha_{i}\\right)}\\end{array}$ and $\\begin{array}{r l}{\\alpha_{r+1}\\left(\\prod_{i\\notin\\mathbb{Z}_{r}}\\alpha_{i}\\right)}&{=}\\end{array}$", "page_idx": 26}, {"type": "text", "text": "$\\left(\\prod_{i\\notin\\mathbb{Z}_{r+1}}\\alpha_{i}\\right)$ . Consequently, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nu_{r+1}C_{1}\\left(\\prod_{i\\not=f_{r}}\\alpha_{i}\\right)\\left(1-\\prod_{i\\in{\\cal Z}_{r}}\\alpha_{i}\\right)+(1-\\alpha_{r+1})C_{1}\\mathbb{1}\\{(r+1)\\in{\\cal Z}_{r+1}\\}=C_{1}\\left(\\prod_{i\\not=f_{r+1}}\\alpha_{i}\\right)\\left(1-\\prod_{i\\in{\\cal Z}_{r}}\\alpha_{i}\\right)\\delta_{i}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand,if $(r+1)\\in\\mathcal{Z}_{r+1}$ , then $\\begin{array}{r}{\\left(\\prod_{i\\notin\\mathbb{Z}_{r}}\\alpha_{i}\\right)=\\left(\\prod_{i\\notin\\mathbb{Z}_{r+1}}\\alpha_{i}\\right)}\\end{array}$ . Consequently, we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{r+1}C_{1}\\displaystyle\\left(\\prod_{i\\notin\\mathbb{Z}_{r}}\\alpha_{i}\\right)\\left(1-\\displaystyle\\prod_{i\\in\\mathbb{Z}_{r}}\\alpha_{i}\\right)+(1-\\alpha_{r+1})C_{1}\\boldsymbol{1}\\{(r+1)\\in\\mathbb{Z}_{r+1}\\}}\\\\ &{=\\alpha_{r+1}C_{1}\\displaystyle\\left(\\prod_{i\\notin\\mathbb{Z}_{r+1}}\\alpha_{i}\\right)\\left(1-\\displaystyle\\prod_{i\\in\\mathbb{Z}_{r}}\\alpha_{i}\\right)+(1-\\alpha_{r+1})C_{1}}\\\\ &{\\geq C_{1}\\displaystyle\\left(\\prod_{i\\notin\\mathbb{Z}_{r+1}}\\alpha_{i}\\right)\\left[\\alpha_{r+1}\\left(1-\\displaystyle\\prod_{i\\in\\mathbb{Z}_{r}}\\alpha_{i}\\right)+(1-\\alpha_{r+1})\\right]}\\\\ &{\\geq C_{1}\\displaystyle\\left(\\prod_{i\\notin\\mathbb{Z}_{r+1}}\\alpha_{i}\\right)\\left(1-\\displaystyle\\prod_{i\\in\\mathbb{Z}_{r+1}}\\alpha_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining (69), (70) and (71) proves the claim. ", "page_idx": 26}, {"type": "text", "text": "Proof of (59). To establish this result, we separately consider the cases $x\\leq1$ and $x\\geq1$ ", "page_idx": 26}, {"type": "text", "text": "\u00b7 When $x\\leq1$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(x)={\\frac{x}{x+1}}-{\\frac{1}{5{\\sqrt{M}}}}\\geq x\\cdot\\left({\\frac{1}{2}}-{\\frac{x}{5{\\sqrt{M}}}}\\right)\\geq{\\frac{7x}{20}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last step, we used the relation $M\\geq2$ \u00b7 Let us now consider the case $x\\,\\geq\\,1$ .The second derivative of $f$ isgivenby $f^{\\prime\\prime}(x)\\;=$ 2(g+1)s. Clealy, for alx \u2265 1, f/ < 0 implying that f is a concave function.It is well-known that a continuous, bounded, concave function achieves its minimum values over a compact interval at the end points of the interval (Bauer's minimum principle). For all $M\\geq2$ ,wehave, ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\nf(1)=\\frac{1}{2}-\\frac{1}{5\\sqrt{M}}\\geq\\frac{7}{20};\\quad f(\\sqrt{M})=\\frac{\\sqrt{M}}{\\sqrt{M}+1}-\\frac{1}{5}\\geq\\frac{7}{20}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Consequently, we can conclude that for all $x\\in[1,\\sqrt{M}]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x)\\geq{\\frac{7}{20}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining (72) and (73) proves the claim. ", "page_idx": 27}, {"type": "text", "text": "B.3.3  Large learning rates with large $\\frac{\\eta_{T}}{B M}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In orderto bound therorin this sceario, notethat $\\frac{\\eta_{T}}{B M}$ controls the varianceof thestchastice updates in the fixed point iteration. Thus, when $\\frac{\\eta_{T}}{B M}$ is large, the variance of the iterates is large, resulting in a large error. To demonstrate this effect, we focus on the dynamics of state 2. This part of the proof is similar to the large learning rate case of Li et al. [2023]. For all $t\\in[T]$ ,define: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\overline{{V}}_{t}(2):=\\frac{1}{M}\\sum_{m=1}^{M}V_{t}^{m}(2).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, from (33), we know that $\\mathbb{E}[\\overline{{V}}_{t}(2)]$ obeys the following recursion: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\overline{{V}}_{t}(2)]=(1-\\eta_{t}(1-\\gamma p))\\mathbb{E}[\\overline{{V}}_{t-1}(2)]+\\eta_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Upon unrolling the recursion, we obtain, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\overline{{V}}_{T}(2)]=\\left(\\prod_{k=t+1}^{T}(1-\\eta_{k}(1-\\gamma p))\\right)\\mathbb{E}[\\overline{{V}}_{t}(2)]+\\sum_{k=t+1}^{T}\\eta_{k}^{(T)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, the above relation along with (16) and the value of $V^{\\star}(2)$ yields us, ", "page_idx": 27}, {"type": "equation", "text": "$$\nV^{\\star}(2)-\\mathbb{E}[\\overline{{V}}_{T}(2)]=\\prod_{k=t+1}^{T}(1-\\eta_{k}(1-\\gamma p))\\left(\\frac{1}{1-\\gamma p}-\\mathbb{E}[\\overline{{V}}_{t}(2)]\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similar to Li et al. [2023], we define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tau^{\\prime}:=\\operatorname*{min}\\left\\{0\\leq t^{\\prime}\\leq T-2\\,\\Big|\\,\\mathbb{E}[(\\overline{{V}}_{t})^{2}]\\geq\\frac{1}{4(1-\\gamma)^{2}}\\;\\mathrm{for\\,all}\\;t^{\\prime}+1\\leq t\\leq T\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "If sucha $\\tau^{\\prime}$ doesnot xs tmls tha ier $\\begin{array}{r}{\\mathbb{E}[(\\overline{{V}}_{T})^{2}]<\\frac{1}{4(1-\\gamma)^{2}}}\\end{array}$ 0 $\\begin{array}{r}{\\mathbb{E}[(\\overline{{V}}_{T-1})^{2}]<\\frac{1}{4(1-\\gamma)^{2}}}\\end{array}$ the former is true, then, ", "page_idx": 27}, {"type": "equation", "text": "$$\nV^{\\star}(2)-\\mathbb{E}[\\overline{{V}}_{T}(2)]=\\frac{3}{4(1-\\gamma)}-\\sqrt{\\mathbb{E}[(\\overline{{V}}_{T})^{2}]}>\\frac{1}{4(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly ift $\\begin{array}{r}{\\mathbb{E}[(\\overline{{V}}_{T-1})^{2}]<\\frac{1}{4(1-\\gamma)^{2}}}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}[\\overline{{V}}_{T-1}]<\\frac{1}{2(1-\\gamma)}}\\end{array}$ Uusing (3), we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathfrak{L}[\\overline{{V}}_{T}(2)]=(1-\\eta_{T}(1-\\gamma p))\\mathbb{E}[\\overline{{V}}_{T-1}(2)]+\\eta_{T}\\leq\\mathbb{E}[\\overline{{V}}_{T-1}(2)]+1<\\frac{1}{2(1-\\gamma)}+\\frac{1}{6(1-\\gamma)}=\\frac{2}{3(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Consequently, ", "page_idx": 27}, {"type": "equation", "text": "$$\nV^{\\star}(2)-\\mathbb{E}[\\overline{{V}}_{T}(2)]>\\frac{3}{4(1-\\gamma)}-\\frac{2}{3(1-\\gamma)}>\\frac{1}{12(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the case when $\\tau^{\\prime}$ exists, we divide the proof into two cases. ", "page_idx": 27}, {"type": "text", "text": "\u00b7 We first consider the case when the learning rates satisfy: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\prod_{k=\\tau^{\\prime}+1}^{T}(1-\\eta_{k}(1-\\gamma p))\\geq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The analysis for this case is identical to that considered in Li et al. [2023]. We explicitly write the steps for completeness. Specifically, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\star}(2)-\\mathbb{E}[\\overline{{V}}_{T}(2)]=\\left(\\displaystyle\\prod_{k=\\tau^{\\prime}+1}^{T}\\left(1-\\eta_{k}(1-\\gamma p)\\right)\\right)\\left(\\frac{1}{1-\\gamma p}-\\mathbb{E}[\\overline{{V}}_{\\tau^{\\prime}}(2)]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\cdot\\left(\\displaystyle\\frac{3}{4(1-\\gamma)}-\\sqrt{\\mathbb{E}[(\\overline{{V}}_{\\tau^{\\prime}}(2))^{2}]}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\cdot\\left(\\displaystyle\\frac{3}{4(1-\\gamma)}-\\frac{1}{2(1-\\gamma)}\\right)\\geq\\frac{1}{8(1-\\gamma)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first line follows from (75), the second line from the condition on step sizes and the third line from the definition of $\\tau^{\\prime}$ ", "page_idx": 28}, {"type": "text", "text": "\u00b7 We now consider the other case where, ", "page_idx": 28}, {"type": "equation", "text": "$$\n0\\leq\\prod_{k=\\tau^{\\prime}+1}^{T}(1-\\eta_{k}(1-\\gamma p))<\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using [Li et al., 2023, Eqn.(134)], for any $t^{\\prime}<t$ and all agents $m$ , we have the relation ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t}^{m}(2)=\\cfrac{1}{1-\\gamma p}-\\displaystyle\\prod_{k=t^{\\prime}+1}^{t}(1-\\eta_{k}(1-\\gamma p))\\left(\\cfrac{1}{1-\\gamma p}-V_{t^{\\prime}}^{m}(2)\\right)}\\\\ &{\\quad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}\\gamma(\\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The above equation is directly obtained by unrolling the recursion in (24) along with noting that $Q_{t}(2,1)\\bar{=}V_{t}(2)$ for all $t$ .Consequently, we have, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{V}}_{T}(2)=\\displaystyle\\frac{1}{1-\\gamma p}-\\prod_{k=t^{\\prime}+1}^{T}\\left(1-\\eta_{k}(1-\\gamma p)\\right)\\left(\\frac{1}{1-\\gamma p}-\\overline{{V}}_{t^{\\prime}}(2)\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\sum_{k=t^{\\prime}+1}^{T}\\eta_{k}^{(T)}\\gamma(\\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $\\{{\\mathcal{F}}_{t}\\}_{t=0}^{T}$ bea filtration such that $\\mathcal{F}_{t}$ isthe $\\sigma$ -algebracorresponding  to $\\{\\{\\hat{P}_{s}^{m}(2|2)\\}_{m=1}^{M}\\}_{s=1}^{t}$ It  is straightforward to note that $\\begin{array}{r}{\\left\\{\\frac{1}{M}\\sum_{m=1}^{M}\\eta_{k}^{(T)}\\gamma(\\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2)\\right\\}_{k}}\\end{array}$ is a maringle sqguenceadapted to the filtration $\\mathcal{F}_{k}$ . Thus, using the result from [Li et al., 2023, Eqn.(139)], we can conclude that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\overline{{V}}_{T}(2))\\geq\\mathbb{E}\\left[\\sum_{k=\\tau^{\\prime}+2}^{T}\\mathrm{Var}\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\eta_{k}^{(T)}\\gamma(\\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2)\\left|\\mathcal{F}_{k-1}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We have, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\left(\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\eta_{k}^{(T)}\\gamma(\\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2)\\left|\\mathcal{F}_{k-1}\\right)\\right.}\\\\ &{\\,\\left.=\\displaystyle\\frac{1}{M^{2}}\\sum_{m=1}^{M}\\mathrm{Var}\\left(\\eta_{k}^{(T)}\\gamma(\\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2)\\left|\\mathcal{F}_{k-1}\\right)\\right.}\\\\ &{\\,\\left.=\\displaystyle\\frac{(\\eta_{k}^{(T)})^{2}}{B M}\\gamma^{2}p(1-p)\\left(\\frac{1}{M}\\sum_{m=1}^{M}(V_{k-1}^{m}(2))^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\geq\\frac{(1-\\gamma)(4\\gamma-1)}{9B M}\\cdot(\\eta_{k}^{(T)})^{2}\\cdot(\\overline{{V}}_{k-1}(2))^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first line follows from that fact that variance of sum of i.i.d. random variables is the sum of their variances, the second line from variance of Binomial random variable and the third line from Jensen's inequality. Thus, (82) and (83) together yield, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Var}(\\overline{{V}}_{T}(2))\\geq\\frac{(1-\\gamma)(4\\gamma-1)}{9B M}\\cdot\\displaystyle\\sum_{k=\\tau^{\\prime}+2}^{T}(\\eta_{k}^{(T)})^{2}\\cdot\\mathbb{E}[(\\overline{{V}}_{k-1}(2))^{2}]}\\\\ {\\geq\\frac{(1-\\gamma)(4\\gamma-1)}{9B M}\\cdot\\frac{1}{4(1-\\gamma)^{2}}\\cdot\\displaystyle\\sum_{k=\\operatorname*{max}\\{\\tau,\\tau^{\\prime}\\}+2}^{T}(\\eta_{k}^{(T)})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second line follows from the definition of $\\tau^{\\prime}$ . We focus on bounding the third term in the above relation. We have, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=\\operatorname*{max}\\{r^{\\prime},r\\}+2}^{T}\\left(\\eta_{k}^{(r)}\\right)^{2}\\ge\\displaystyle\\sum_{k=\\operatorname*{max}\\{r^{\\prime},r\\}+2}^{T}\\left(\\eta_{k}\\prod_{i=k+1}^{T}\\left(1-\\eta_{i}(1-\\gamma p)\\right)^{2}\\right.}\\\\ &{\\ge\\displaystyle\\sum_{k=\\operatorname*{max}\\{r^{\\prime},r\\}+2}^{T}\\left(\\eta_{T}\\prod_{i=k+1}^{t}\\left(1-\\eta_{\\kappa}(1-\\gamma p)\\right)\\right)^{2}}\\\\ &{=\\eta_{T}^{2}\\displaystyle\\sum_{k=\\operatorname*{max}\\{r^{\\prime},r\\}+2}^{T}\\ (1-\\eta_{r}(1-\\gamma p))^{2(t-k)}}\\\\ &{\\ge\\eta_{T}^{2}\\cdot\\displaystyle\\frac{1-(1-\\eta_{r}(1-\\gamma p))^{2(T-\\operatorname*{max}\\{r^{\\prime},r\\}-1)}}{\\eta_{r}(1-\\gamma p)(2-\\eta_{r}(1-\\gamma p))}}\\\\ &{\\ge\\eta r\\cdot\\displaystyle\\frac{1}{4(1-\\gamma)}\\cdot c^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second line follows from monotonicity of $\\eta_{t}$ and the numerical constant $c^{\\prime}$ in the fifth step is given by the following claim whose proof is deferred to the end of the section: ", "page_idx": 29}, {"type": "text", "text": "for constant step sizes, ", "page_idx": 29}, {"type": "equation", "text": "$$\n1-(1-\\eta_{\\tau}(1-\\gamma p))^{2(T-\\operatorname*{max}\\{\\tau^{\\prime},\\tau\\}-1)}\\geq\\left\\{1-e^{-8/9}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, (84) and (85) together imply ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(\\overline{{V}}_{T}(2))\\geq\\frac{(4\\gamma-1)}{36B M(1-\\gamma)}\\cdot\\underset{k=\\tau^{\\prime}+2}{\\sum}(\\eta_{k}^{(T)})^{2}}\\\\ &{\\qquad\\qquad\\geq\\frac{c^{\\prime}(4\\gamma-1)}{144(1-\\gamma)}\\cdot\\frac{\\eta_{T}}{B M(1-\\gamma)}\\geq\\frac{c^{\\prime}(4\\gamma-1)}{144(1-\\gamma)}\\cdot\\frac{1}{100},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from the bound on $\\frac{\\eta_{T}}{B M}$ ", "page_idx": 29}, {"type": "text", "text": "Thus, for all $N\\geq1$ , we have, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[(V^{\\star}(2)-\\overline{{V}}_{T}(2))^{2}]=\\mathbb{E}[(V^{\\star}(2)-\\mathbb{E}[\\overline{{V}}_{T}(2)])^{2}]+\\mathrm{Var}(\\overline{{V}}_{T}(2))\\geq\\frac{c^{\\prime\\prime}}{(1-\\gamma)N},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for some numerical constant $c^{\\prime\\prime}$ . Similar to the small learning rate case, the error rate is bounded away from a constant value irrespective of the number of agents and the number of communication rounds. Thus, even with $\\mathsf{C C}_{\\mathsf{r o u n d}}=\\Omega(T)$ , we will not observe any collaborative gain in this scenario. ", "page_idx": 29}, {"type": "text", "text": "Proof of (86). To establish the claim, we consider two cases: ", "page_idx": 29}, {"type": "text", "text": "$\\tau^{\\prime}\\geq\\tau$ : Under this case, we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{(1-\\eta_{\\tau}(1-\\gamma p))^{2(T-\\operatorname*{max}\\{\\tau^{\\prime},\\tau\\}-1)}=(1-\\eta_{\\tau}(1-\\gamma p))^{2(T-\\tau^{\\prime}-1)}}}\\\\ &{}&{\\leq(1-\\eta_{\\tau}(1-\\gamma p))^{T-\\tau^{\\prime}}\\leq\\prod_{k=\\tau^{\\prime}+1}^{T}(1-\\eta_{k}(1-\\gamma p))\\leq\\frac{1}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality follows from (80) ", "page_idx": 30}, {"type": "text", "text": "$\\tau\\geq\\tau^{\\prime}$ : For this case, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-\\eta_{\\tau}(1-\\gamma p))^{2(T-\\operatorname*{max}\\{\\tau^{\\prime},\\tau\\}-1)}=(1-\\eta_{\\tau}(1-\\gamma p))^{2(T-\\tau-1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le(1-\\eta_{\\tau}(1-\\gamma p))^{T-\\tau}\\le\\exp\\left(-\\displaystyle\\frac{2T\\eta_{\\tau}(1-\\gamma p)}{3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the constant stepsize schedule, we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{2T\\eta_{\\tau}(1-\\gamma p)}{3}\\right)\\leq\\exp\\left(-\\frac{2T}{3}\\cdot\\frac{1}{(1-\\gamma)T}\\cdot\\frac{4(1-\\gamma)}{3}\\right)=\\exp\\left(-\\frac{8}{9}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For linearly rescaled stepsize schedule, we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{2T\\eta_{\\tau}(1-\\gamma p)}{3}\\right)\\leq\\exp\\left(-\\frac{2T}{3}\\cdot\\frac{1}{1+c_{\\eta}(1-\\gamma)T/3}\\cdot\\frac{4(1-\\gamma)}{3}\\right)=\\exp\\left(-\\frac{8}{3\\operatorname*{max}\\{1,c_{\\eta}\\}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "On combining (88), (89), (90) and (91), we arrive at the claim. ", "page_idx": 30}, {"type": "text", "text": "B.4  Generalizing to larger state action spaces ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We now elaborate on how we can extend the result to general state-action spaces along with the obtaining the lower bound on the bit level communication complexity. For the general case, we instead consider the following MDP. For the first four states $\\{0,1,2,3\\}$ , the probability transition kernel and reward function are given as follows. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{A}_{0}=\\{1\\}}&{\\quad P(0|0,1)=1}\\\\ {A_{1}=\\{1,2,\\ldots,|A|\\}}&{\\quad P(1|1,a)=p\\quad}&{P(0|1,a)=1-p\\quad r(1,a)=1,\\forall\\;a\\in A}\\\\ {\\mathcal{A}_{2}=\\{1\\}}&{\\quad P(2|2,1)=p\\quad}&{P(0|2,1)=1-p\\quad r(2,1)=1,}\\\\ {\\mathcal{A}_{3}=\\{1\\}}&{\\quad P(3|3,1)=1}&{r(3,1)=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the parameter 4 1 The overal DP s btained by eraring IS/ copie o the above MDP for all sets of the form $\\{4r,4r\\!+\\!1,4r\\!+\\!2,4r\\!+\\!3\\}$ for $r\\leq|S|/4\\!-\\!1$ . It is straightforward to note that the lower bound on the number of communication rounds immediately transfers to the general case as well. Moreover, note that the bound on $\\mathsf{C C}_{\\mathsf{r o u n d}}$ implies the bound $\\begin{array}{r}{\\dot{\\mathsf{C}}\\mathsf{C}_{\\mathsf{b i t}}=\\Omega\\left(\\frac{1}{(1-\\gamma)\\log^{2}N}\\right)}\\end{array}$ as every communication entails sending $\\Omega(1)$ bits. ", "page_idx": 30}, {"type": "text", "text": "To obtain the general lower bound on bit level communication complexity, note that we can carry out the analysis in the previous section for all $|{\\mathcal{A}}|/2$ pairs of actions in state 1 corresponding to the set of states $\\{0,1,2,3\\}$ . Moreover, the algorithm $\\mathcal{A}$ , needs to ensure that the error is low across all the $|{\\mathcal{A}}|/2$ pairs. Since the errors are independent across all these pairs, each of them require 2((1-2)log\u00b2 N) b bits of information to be transmitted during the learning horizon leading to a lower bound of $\\begin{array}{r}{\\Omega\\left(\\frac{\\left|A\\right|}{\\left(1-\\gamma\\right)\\log^{2}N}\\right)}\\end{array}$ Note that since we reguire low $\\ell_{\\infty}$ error, $\\mathcal{A}$ nedsto ensurethat the error is low across all the pairs, resulting in a communication cost linearly growing with $|{\\mathcal{A}}|$ Upon repeating the argument across all $|{\\cal S}|/4$ copies of the MDP, we arrive at the lower bound of $\\begin{array}{r}{\\mathsf{C C}_{\\mathsf{b i t}}=\\Omega\\left(\\frac{|S||A|}{(1-\\gamma)\\log^{2}N}\\right)}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "B.5  Proofs of auxiliary lemmas ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "B.5.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Note that a similar relationship is also derived in Li et al. [2023], but needing to take care of the averaging over multiple agents, we present the entire arguments for completeness. We prove the claim using an induction over $t$ . It is straightforward to note that the claim is true for $t=0$ and all agents $m\\in\\{1,2,\\dots,M\\}$ . For the inductive step, we assume that the claim holds for $t-1$ for all clients. Using the induction hypothesis, we have the following relation between $V_{t-1}^{m}(1)$ and $\\widehat V_{t-1}^{m}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathit{\\Sigma}_{t-1}^{m}(1)=\\operatorname*{max}_{a\\in\\{1,2\\}}Q_{t-1}^{m}(1,a)\\geq\\operatorname*{max}_{a\\in\\{1,2\\}}\\widehat{Q}_{t-1}^{m}(a)-\\frac{1}{1-\\gamma}\\prod_{i=1}^{t-1}(1-\\eta_{i}(1-\\gamma))=\\widehat{V}_{t-1}^{m}-\\frac{1}{1-\\gamma}\\prod_{i=1}^{t-1}(1-\\eta_{i}(1-\\gamma))\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $t\\notin\\{t_{r}\\}_{r=1}^{R}$ and $a\\in\\{1,2\\}$ , we have, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q_{t}^{m}(1,a)-\\widehat{Q}_{t}^{m}(a)=Q_{t-1/2}^{m}(1,a)-\\widehat{Q}_{t-1/2}^{m}(a)}\\\\ {=(1-\\eta_{t})Q_{t-1}^{m}(1,a)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,a)V_{t-1}^{m}(1))}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\left[(1-\\eta_{t})\\widehat{Q}_{t-1}^{m}(a)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,a)\\widehat{V}_{t-1}^{m})\\right]}\\\\ {=(1-\\eta_{t})(Q_{t-1}^{m}(1|1,a)-\\widehat{Q}_{t-1}^{m}(a))+\\eta_{t}\\gamma\\widehat{P}_{t}^{m}(1|1,a)(V_{t-1}^{m}(1)-\\widehat{V}_{t-1}^{m})}\\\\ {\\geq-\\frac{(1-\\eta_{t})^{t-1}}{1-\\gamma}\\displaystyle\\prod_{i=1}^{t-1}(1-\\eta_{t}(1-\\gamma))-\\widehat{P}_{t}^{m}(1|1,a)\\cdot\\displaystyle\\frac{\\eta_{t}\\gamma}{1-\\gamma}\\displaystyle\\prod_{i=1}^{t-1}(1-\\eta_{i}(1-\\gamma))}\\\\ {\\geq-\\displaystyle\\frac{(1-\\eta_{t})^{t-1}}{1-\\gamma}\\displaystyle\\prod_{i=1}^{t-1}(1-\\eta_{i}(1-\\gamma))-\\displaystyle\\frac{\\eta_{t}\\gamma}{1-\\gamma}\\displaystyle\\prod_{i=1}^{t-1}(1-\\eta_{i}(1-\\gamma))}\\\\ {\\geq-\\displaystyle\\frac{1}{1-\\gamma}\\displaystyle\\prod_{i=1}^{t}(1-\\eta_{i}(1-\\gamma)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $t\\in\\{t_{r}\\}_{r=1}^{R}$ and $a\\in\\{1,2\\}$ , we have, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t^{\\prime}}^{m}(1,a)-\\hat{Q}_{t}^{m}(a)=\\frac{1}{M}\\sum_{m=1}^{M}Q_{t-1/2}^{m}(1,a)-\\frac{1}{M}\\sum_{m=1}^{M}\\hat{Q}_{t-1/2}^{m}(a)}\\\\ {\\displaystyle=\\frac{1}{M}\\sum_{m=1}^{M}\\Big[(1-\\eta_{t})Q_{t-1}^{m}(1,a)+\\eta_{t}(1+\\gamma\\hat{P}_{t}^{m}(1|1,a)V_{t-1}^{m}(1))\\Big]}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -\\frac{1}{M}\\sum_{m=1}^{M}\\Big[(1-\\eta_{t})\\hat{Q}_{t-1}^{m}(a)+\\eta_{t}(1+\\gamma\\hat{P}_{t}^{m}(1|1,a)\\hat{V}_{t-1}^{m})\\Big]}\\\\ {\\displaystyle=\\frac{1}{M}\\sum_{m=1}^{M}\\Big[(1-\\eta_{t})(Q_{t-1}^{m}(1,a)-\\hat{Q}_{t-1}^{m}(a))+\\eta_{t}\\gamma\\hat{P}_{t}^{m}(1|1,a)(V_{t-1}^{m}(1)-\\hat{V}_{t-1}^{m})}\\\\ {\\displaystyle\\geq-\\frac{1}{1-\\gamma}\\prod_{m=1}^{M}(1-\\eta_{t}(1-\\gamma)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last step follows using the same set of arguments as used in (94). The inductive step follows from (94) and (95). ", "page_idx": 31}, {"type": "text", "text": "B.5.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In order to bound the term $\\mathbb{E}[\\Delta_{t,\\mathrm{max}}^{m}]-\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]$ we make use of the relation in (44a), whichwe recall ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Im[\\Delta_{t,\\operatorname*{max}}^{m}]\\ge\\varphi_{t^{\\prime},t}\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]+\\left[\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\gamma p\\mathbb{E}[\\Delta_{k-1,\\operatorname*{max}}^{m}]\\right]+\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(\\Delta_{t^{\\prime},\\operatorname*{max}}^{m})|].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "\u00b7 To aid the analysis, we consider the following recursive relation for any fixed agent $m$ ", "page_idx": 32}, {"type": "equation", "text": "$$\ny_{t}=(1-\\eta_{t})y_{t-1}+\\eta_{t}(\\gamma p y_{t-1}+\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Upon unrolling the recursion, we obtain, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t}=\\left(\\displaystyle\\prod_{k=t^{\\prime}+1}^{t}\\left(1-\\eta_{k}\\right)\\right)y_{t^{\\prime}}+\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\left(\\eta_{k}\\displaystyle\\prod_{i=k+1}^{t}\\left(1-\\eta_{i}\\right)\\right)\\gamma p y_{k-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\left(\\eta_{k}\\displaystyle\\prod_{i=k+1}^{t}\\left(1-\\eta_{i}\\right)\\right)\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]}\\\\ &{\\qquad=\\varphi_{t^{\\prime},t}y_{t^{\\prime}}+\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\tilde{\\eta}_{k}^{(t)}\\gamma p y_{k-1}+\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\tilde{\\eta}_{k}^{(t)}\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Initializing $y_{t^{\\prime}}=\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\mathrm{max}}]$ in (97) and plugging this into (44a), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\Delta_{t,\\mathrm{max}}^{m}]\\ge y_{t}-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Wwhere we used $\\begin{array}{r}{\\sum_{k=t^{\\prime}+1}^{t}\\widetilde{\\eta}_{k}^{(t)}\\le1}\\end{array}$ (f 8)flyxe $y_{t}$ By rewriting (96) as ", "page_idx": 32}, {"type": "equation", "text": "$$\ny_{t}=(1-\\eta_{t}(1-\\gamma p))y_{t-1}+\\eta_{t}\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "it is straight forward to note that $y_{t}$ is given as ", "page_idx": 32}, {"type": "equation", "text": "$$\ny_{t}=\\left(\\prod_{k=t^{\\prime}+1}^{t}(1-\\eta_{k}(1-\\gamma p))\\right)y_{t^{\\prime}}+\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]\\left[\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Consequently, we have, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\Delta_{t,\\operatorname*{max}}^{m}]-\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]\\geq\\left(\\displaystyle\\prod_{k=t^{\\prime}+1}^{t}(1-\\eta_{k}(1-\\gamma p))\\right)\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]\\left[\\displaystyle\\sum_{k=t^{\\prime}+1}^{t}\\eta_{k}^{(t)}-1\\right]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "\u00b7 We can consider a slightly different recursive sequence defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\nw_{t}=(1-\\eta_{t})w_{t-1}+\\eta_{t}(\\gamma p w_{t-1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using a similar sequence of arguments as outlined in (96)-(98), we can conclude that if $w_{t^{\\prime}}\\,=\\,\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\mathrm{max}}]$ ,then $\\mathbb{E}[\\Delta_{t,\\mathrm{max}}^{m}]\\,\\ge\\,w_{t}+\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|]$ and consequently, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta_{t,\\operatorname*{max}}^{m}]\\geq\\left(\\prod_{k=t^{\\prime}+1}^{t}(1-\\eta_{k}(1-\\gamma p))\\right)\\mathbb{E}[\\overline{{\\Delta}}_{t^{\\prime},\\operatorname*{max}}]+\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]-\\varphi_{t^{\\prime},t}\\mathbb{E}[|\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)|].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "On combining (99) and (101), we arrive at the claim. ", "page_idx": 32}, {"type": "text", "text": "B.5.3Proof of Lemma 4 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We begin with bounding the first term $\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]$ the second bound follows in an almost identical derivation. ", "page_idx": 32}, {"type": "text", "text": "Stepi $\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]$ FredUno $\\begin{array}{r}{\\operatorname*{max}\\{a,b\\}\\,=\\,\\frac{a+b+|a-b|}{2}}\\end{array}$ we can ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]=\\mathbb{E}\\left[\\frac{\\xi_{t^{\\prime},t}^{m}(1)+\\xi_{t^{\\prime},t}^{m}(2)}{2}+\\left|\\frac{\\xi_{t^{\\prime},t}^{m}(1)-\\xi_{t^{\\prime},t}^{m}(2)}{2}\\right|\\right]}\\\\ &{\\phantom{\\mathbb{E}\\left[\\frac{\\xi_{t^{\\prime},t}^{m}(1)+\\xi_{t^{\\prime},t}^{m}(2)}{2}+\\left|\\frac{\\xi_{t^{\\prime},t}^{m}(2)}{2}\\right|\\right]}=\\frac{1}{2}\\mathbb{E}\\left[\\bigg|\\frac{\\xi_{t^{\\prime},t}^{m}(1)-\\xi_{t^{\\prime},t}^{m}(2)}{2}\\right|\\right]}\\\\ &{\\phantom{\\mathbb{E}\\left[\\bigg|\\sum_{k=t^{\\prime}+1}^{t}\\tilde{\\eta}_{k}^{(t)}\\gamma(\\widehat{P}_{k}^{m}(1|1,1)-\\widehat{P}_{k}^{m}(1|1,2))\\widehat{V}_{k-1}^{m}\\bigg|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where weusd thedeniton 3andthefact h $\\mathbb{E}[\\xi_{t^{\\prime},t}^{m}(1)]=\\mathbb{E}[\\xi_{t^{\\prime},t}^{m}(2)]=0$ Decompose $\\zeta_{t^{\\prime},t}^{m}$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\zeta_{t^{\\prime},t}^{m}=\\sum_{k=t^{\\prime}+1}^{t}\\sum_{b=1}^{B}\\widetilde{\\eta}_{k}^{(t)}\\frac{\\gamma}{B}(P_{k,b}^{m}(1|1,1)-P_{k,b}^{m}(1|1,2))\\widehat{V}_{k-1}^{m}=:\\sum_{l=1}^{L}z_{l},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where for all $1\\leq l\\leq L$ ", "page_idx": 33}, {"type": "equation", "text": "$$\nz_{l}:=\\frac{\\gamma}{B}(P_{k(l),b(l)}^{m}(1|1,1)-P_{k(l),b(l)}^{m}(1|1,2))\\widehat{V}_{k(l)-1}^{m}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with ", "page_idx": 33}, {"type": "equation", "text": "$$\nk(l):=\\lfloor l/B\\rfloor+t^{\\prime}+1;\\quad b(l)=((l-1)\\mod B)+1;\\quad L=(t-t^{\\prime})B.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let $\\{\\mathcal{F}_{l}\\}_{l=1}^{L}$ be afltration such  that $\\mathcal{F}_{l}$ isthe $\\sigma$ -algebra   corresponding to $\\{P_{k(j),b(j)}^{m}(1|1,1),P_{k(j),b(j)}^{m}(1|1,2)\\}_{j=1}^{l}$ Iistraighforwardtnote that $\\{z_{l}\\}_{l=1}^{L}$ is a martingale sequence adapted to the filtration $\\{\\mathcal{F}\\}_{l=1}^{L}$ We will use the Freedman's inequality [Freedman, 1975,Lital, 2023]tobtan ahigh probabityoud n $|\\zeta_{t^{\\prime},t}^{m}|$ ", "page_idx": 33}, {"type": "text", "text": "\u00b7 To that effect, note that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{l}{\\operatorname*{sup}}\\,|z_{l}|\\leq\\underset{l}{\\operatorname*{sup}}\\,\\Big|\\widetilde{\\eta}_{k(l)}^{(t)}\\cdot\\frac{\\gamma}{B}\\cdot(P_{k(l),b(l)}^{m}(1|1,1)-P_{k(l),b(l)}^{m}(1|1,2))\\cdot\\widehat{V}_{k(l)-1}^{m}\\Big|}\\\\ &{\\qquad\\qquad\\leq\\widetilde{\\eta}_{k(l)}^{(t)}\\cdot\\frac{\\gamma}{B(1-\\gamma)}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\eta_{t}}{B(1-\\gamma)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where hsdspwsf $|(P_{k(l),b(l)}^{m}(1|1,1)-P_{k(l),b(l)}^{m}(1|1,2))|\\leq1$ d.n $\\begin{array}{r}{\\widehat{V}_{k(l)-1}^{m}\\le\\frac{1}{1-\\gamma}}\\end{array}$ $\\begin{array}{r}{c_{\\eta}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ $\\widetilde{\\eta}_{k}^{(T)}$ $k$ in this regime. (cf. (19)). ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Similarly, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{Var}(z_{l}|\\mathcal{F}_{l-1})\\le\\left(\\widetilde{\\eta}_{k(l)}^{(t)}\\right)^{2}\\frac{\\gamma^{2}}{B^{2}}\\cdot\\left(\\widehat{V}_{k(l)-1}^{m}\\right)^{2}\\cdot\\mathrm{Var}(P_{k(l),b(l)}^{m}(1|1,1)-P_{k(l),b(l)}^{m}(1|1,2))}\\\\ &{\\qquad\\qquad\\le\\left(\\widetilde{\\eta}_{k(l)}^{(t)}\\right)^{2}\\frac{\\gamma^{2}}{B^{2}(1-\\gamma)^{2}}\\cdot2p(1-p)}\\\\ &{\\qquad\\qquad\\le\\frac{2\\left(\\widetilde{\\eta}_{k(l)}^{(t)}\\right)^{2}}{3B^{2}(1-\\gamma)}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the above bounds (104) and (105) along with Freedman's inequality yield that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(|\\zeta_{t^{\\prime},t}^{m}|\\ge\\sqrt{\\frac{8\\log(2/\\delta)}{3B^{2}(1-\\gamma)}\\sum_{l=1}^{L}\\left(\\widetilde{\\eta}_{k(l)}^{(t)}\\right)^{2}}+\\frac{4\\eta_{t}\\log(2/\\delta)}{3B(1-\\gamma)}\\right)\\le\\delta.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Seting $\\begin{array}{r}{\\delta_{0}=\\frac{(1-\\gamma)^{2}}{2}\\cdot\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]}\\end{array}$ with probability at least $1-\\delta_{0}$ itholds ", "page_idx": 34}, {"type": "equation", "text": "$$\n|\\zeta_{t^{\\prime},t}^{m}|\\geq\\sqrt{\\frac{8\\log(2/\\delta_{0})}{3B(1-\\gamma)}\\sum_{k=t^{\\prime}+1}^{t}\\left(\\widetilde{\\eta}_{k}^{(t)}\\right)^{2}}+\\frac{4\\eta_{t}\\log(2/\\delta_{0})}{3B(1-\\gamma)}=:D.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Consequently, plugging this back to (102), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[\\xi_{t^{\\prime},t,\\operatorname*{max}}^{m}]=\\frac{1}{2}\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|]}}\\\\ &{\\geq\\frac{1}{2}\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|]\\left\\{|\\zeta_{t^{\\prime},t}^{m}|\\leq D\\right\\}]}\\\\ &{\\geq\\frac{1}{2D}\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}\\mathbb{I}\\left\\{|\\zeta_{t^{\\prime},t}^{m}|\\leq D\\right\\}]}\\\\ &{\\geq\\frac{1}{2D}\\left(\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]-\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}\\mathbb{I}\\left\\{|\\zeta_{t^{\\prime},t}^{m}|>D\\right\\}]\\right)}\\\\ &{\\geq\\frac{1}{2D}\\left(\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]-\\frac{\\mathrm{Pr}\\left(|\\zeta_{t^{\\prime},t}^{m}|>D\\right)}{\\left(1-\\gamma\\right)^{2}}\\right)\\geq\\frac{1}{4D}\\cdot\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, the penultimate step used the fact that $|\\zeta_{t^{\\prime},t}^{m}|\\leq\\sum_{k=t^{\\prime}+1}^{t}\\frac{\\widetilde{\\eta}_{k}^{(t)}}{(1-\\gamma)}\\leq\\frac{1}{(1-\\gamma)}$ and the last step used the definition of $\\delta_{0}$ . Thus, it is sufficient to obtain a lower bound on $\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]$ in order obtain a lower bound for E[,ax] ", "page_idx": 34}, {"type": "text", "text": "Step 2: lower bounding $\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]$ . To proceed, we introduce the following lemma pertaining to lower bounding $\\widehat{V}_{t}^{m}$ that will be useful later. ", "page_idx": 34}, {"type": "text", "text": "Lemma 6. For all time instants $t\\in[T]$ and all agent $m\\in[M]$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\widehat{V}_{t}^{m}\\right)^{2}\\right]\\geq\\frac{1}{2(1-\\gamma)^{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]=\\mathbb{E}\\left[\\displaystyle\\sum_{l=1}^{L}\\mathrm{Var}\\left(z_{l}|\\mathcal{F}_{l-1}\\right)\\right]=\\mathbb{E}\\left[\\displaystyle\\sum_{l=1}^{L}\\mathbb{E}\\left[z_{l}^{2}|\\mathcal{F}_{l-1}\\right]\\right]}\\\\ &{\\phantom{\\sum_{l=1}^{L}\\Big}\\ge\\displaystyle\\sum_{l=1}^{L}\\left(\\tilde{\\eta}_{k(l)}^{(t)}\\right)^{2}\\frac{\\gamma^{2}}{B^{2}}\\cdot2p(1-p)\\cdot\\mathbb{E}\\left[\\left(\\hat{V}_{k(l)-1}^{m}\\right)^{2}\\right]}\\\\ &{\\phantom{\\sum_{l=1}^{L}\\Big}\\ge\\displaystyle\\sum_{l=1}^{L}\\left(\\tilde{\\eta}_{k(l)}^{(t)}\\right)^{2}\\frac{\\gamma^{2}}{B^{2}}\\cdot2p(1-p)\\cdot\\displaystyle\\frac{1}{2(1-\\gamma)^{2}}}\\\\ &{\\phantom{\\sum_{l=1}^{L}\\Big}\\ge\\displaystyle\\frac{2}{9B(1-\\gamma)}\\cdot\\sum_{k=\\operatorname*{max}\\left\\{t^{\\prime},\\tau\\right\\}+1}^{t}\\left(\\tilde{\\eta}_{k}^{(t)}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the third line follows from Lemma 6 and the fourth line uses $\\gamma\\geq5/6$ ", "page_idx": 34}, {"type": "text", "text": "Step 3: finishing up. We fnish up the proof by bounding $\\begin{array}{r}{\\sum_{k=\\mathrm{max}\\left\\{t^{\\prime},\\tau\\right\\}+1}^{t}\\left(\\widetilde{\\eta}_{k}^{\\left(t\\right)}\\right)^{2}}\\end{array}$ for $t\\,-$ $\\operatorname*{max}\\{t^{\\prime},\\tau\\}\\geq1/\\eta_{\\tau}$ Wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{k=\\operatorname*{max}\\{t^{\\prime},\\tau\\}+1}^{t}\\left(\\widetilde{\\eta}_{k}^{(t)}\\right)^{2}\\geq\\sum_{k=\\operatorname*{max}\\{t^{\\prime},\\tau\\}+1}^{t}\\left(\\eta_{k}\\prod_{i=k+1}^{t}(1-\\eta_{i})\\right)^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\mathrm{(i)}}{\\geq}\\displaystyle\\sum_{k=\\operatorname*{max}\\{t^{\\prime},\\tau\\}+1}^{t}\\left(\\eta_{t}\\prod_{i=k+1}^{t}\\left(1-\\eta_{\\tau}\\right)\\right)^{2}}\\\\ &{=\\eta_{t}^{2}\\sum_{k=\\operatorname*{max}\\{t^{\\prime},\\tau\\}+1}^{t}\\left(1-\\eta_{\\tau}\\right)^{2(t-k)}}\\\\ &{\\geq\\eta_{t}^{2}\\cdot\\frac{1-\\left(1-\\eta_{\\tau}\\right)^{2(t-\\operatorname*{max}\\{t^{\\prime},\\tau\\})}}{\\eta_{\\tau}(2-\\eta_{\\tau})}}\\\\ &{\\geq\\eta_{t}\\cdot\\frac{1-\\exp(-2)}{6}\\geq\\frac{\\eta_{t}}{10}\\geq\\frac{\\eta_{T}}{10},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where (i) follows from the monotonicity of $\\eta_{k}$ . Plugging (110) into the expressions of $D$ (cf. (107)) wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D=\\sqrt{\\frac{8\\log(2/\\delta_{0})}{3B(1-\\gamma)}}\\,\\underset{k=t^{\\prime}+1}{\\overset{t}{\\sum}}\\,\\left(\\tilde{\\eta}_{k}^{(t)}\\right)^{2}+\\frac{4\\eta_{t}\\log(2/\\delta_{0})}{3B(1-\\gamma)}}\\\\ &{\\qquad\\le\\frac{9}{2}\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]\\cdot\\sqrt{\\frac{8\\log(2/\\delta_{0})}{3}}\\left(\\frac{1}{B(1-\\gamma)}\\,\\underset{k=t^{\\prime}+1}{\\overset{t}{\\sum}}\\,\\left(\\tilde{\\eta}_{k}^{(t)}\\right)^{2}\\right)^{-1/2}+60\\cdot\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]\\cdot\\log(2/\\delta_{0})}\\\\ &{\\qquad\\le3\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]\\cdot\\log(2/\\delta_{0})\\left[\\sqrt{\\frac{60B(1-\\gamma)}{\\eta_{t}}}+20\\right]}\\\\ &{\\qquad\\le60\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]\\cdot\\log(2/\\delta_{0})\\left[\\sqrt{\\frac{3B(1-\\gamma)}{20\\eta_{T}}}+1\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second line follows from (109) and (110), and the third line follows from (110). On combining the above bound with (108), we obtain, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\xi_{t^{\\prime},t,\\mathrm{max}}^{m}]\\geq\\frac{1}{240\\log(2/\\delta_{0})}\\cdot\\frac{\\nu}{\\nu+1},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "$\\nu:=\\sqrt{\\frac{20\\eta_{T}}{3B(1-\\gamma)}}$ .Note that we have, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\delta_{0}=\\frac{(1-\\gamma)^{2}}{2}\\cdot\\mathbb{E}[|\\zeta_{t^{\\prime},t}^{m}|^{2}]\\geq\\frac{(1-\\gamma)}{9B}\\cdot\\sum_{k=t^{\\prime}+1}^{t}\\left(\\widetilde{\\eta}_{k}^{(t)}\\right)^{2}\\geq\\frac{\\eta_{T}(1-\\gamma)}{90B}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining the above bound with (111) yields us the required bound. ", "page_idx": 35}, {"type": "text", "text": "Step 4: repeating the argument for the second claim. We note that second claim in the theorem, i.e., the lower bound on $\\begin{array}{r}{\\mathbb{\\bar{E}}\\left[\\operatorname*{max}\\left\\{\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(1),\\frac{1}{M}\\sum_{m=1}^{M}\\xi_{t^{\\prime},t}^{m}(2)\\right\\}\\right]}\\end{array}$ follows through an identical series of arguments where the bounds in Eqns. (104) and (105) contain an additional factor of $M$ in the denominator (effectively replacing $B$ with $B M$ ), which is carried through in all the following steps. ", "page_idx": 35}, {"type": "text", "text": "B.5.4 Proof of Lemma 5 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Using Eqns. (41) and (38), we can write ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\Delta}}_{t}(1)-\\overline{{\\Delta}}_{t}(2)=\\left(\\prod_{k=t^{\\prime}+1}^{t}\\left(1-\\eta_{k}\\right)\\right)\\left(\\overline{{\\Delta}}_{t^{\\prime}}(1)-\\overline{{\\Delta}}_{t^{\\prime}}(2)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\sum_{k=t^{\\prime}+1}^{t}\\left(\\eta_{k}\\prod_{i=k+1}^{t}\\left(1-\\eta_{i}\\right)\\right)\\gamma(\\widehat{P}_{k}^{m}(1|1,1)-\\widehat{P}_{k}^{m}(1|1,2))\\widehat{V}_{k-1}^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Upon unrolling the recursion, we obtain, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\overline{{\\Delta}}_{t}(1)-\\overline{{\\Delta}}_{t}(2)=\\sum_{k=1}^{t}\\sum_{m=1}^{M}\\left(\\eta_{k}\\prod_{i=k+1}^{t}(1-\\eta_{i})\\right)\\frac{\\gamma}{M}(\\widehat{P}_{k}^{m}(1|1,1)-\\widehat{P}_{k}^{m}(1|1,2))\\widehat{V}_{k-1}^{m}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If  we define afiltration $\\mathcal{F}_{k}$ as the $\\sigma$ -algebra    corresponding    to $\\{\\widehat{P}_{l}^{1}(1|1,1),\\widehat{P}_{l}^{1}(1|1,2),\\ldots,\\widehat{P}_{l}^{M}(1|1,1),\\widehat{P}_{l}^{M}(1|1,2)\\}_{l=1}^{k}$ , then it is straightforward to note that $\\{\\overline{{\\Delta}}{}_{t}(1)-\\overline{{\\Delta}}{}_{t}(2)\\}_{t}$ is a martingale sequence adapted to the filtration $\\{\\mathcal{F}_{t}\\}_{t}$ . Using Jensen's inequality, we know that if $\\{Z_{t}\\}_{t}$ is a martingale adapted to a filtration $\\{\\mathcal{G}_{t}\\}_{t}$ , then for a convex function $f$ such that $f(Z_{t})$ is integrable for all $t$ $\\{f(Z_{t})\\}_{t}$ is a sub-martingale adapted to $\\{\\mathcal{G}_{t}\\}_{t}$ Since $f(x)\\,=\\,\\left|x\\right|$ is a convex function, $\\{|\\overline{{\\Delta}}_{t}(1)\\rrangle-\\overline{{\\Delta}}_{t}(2)|\\}_{t}$ is a submartingale adapted to the filtration $\\{\\mathcal{F}_{t}\\}_{t}$ . As a result, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{1\\leq t\\leq T}\\mathbb{E}[|\\overline{{\\Delta}}_{t}(1)-\\overline{{\\Delta}}_{t}(2)|]\\leq\\mathbb{E}[|\\overline{{\\Delta}}_{T}(1)-\\overline{{\\Delta}}_{T}(2)|]\\leq\\left(\\mathbb{E}[(\\overline{{\\Delta}}_{T}(1)-\\overline{{\\Delta}}_{T}(2))^{2}]\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We use the following oservation about a martingale sequence $\\{X_{i}\\}_{i=1}^{t}$ adapted to a fltration $\\{\\mathcal{G}_{i}\\}_{i=1}^{t}$ to evaluate the above expression. We have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{t}X_{i}\\right)^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{t}X_{i}\\right)^{2}\\Bigg|\\mathcal{G}_{t-1}\\right]\\right]}\\\\ &{\\phantom{\\mathbb{E}\\left[\\mathbb{E}\\left[X_{t}^{2}+2X_{t}\\left(\\displaystyle\\sum_{i=1}^{t-1}X_{i}\\right)+\\left(\\displaystyle\\sum_{i=1}^{t-1}X_{i}\\right)^{2}\\Bigg|\\mathcal{G}_{t-1}\\right]\\right]}}\\\\ &{\\phantom{\\mathbb{E}\\left[\\mathbb{E}\\left[X_{t}^{2}\\right]+\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{i=1}^{t-1}X_{i}\\right)^{2}\\right]\\right]}}\\\\ &{\\phantom{\\mathbb{E}\\left[\\mathbb{E}\\left[\\left(\\left(\\displaystyle\\sum_{i=1}^{t-1}\\mathbb{E}_{i}\\right)^{2}\\right)^{2}\\right]\\right]}=\\sum_{i=1}^{t}\\mathbb{E}\\left[X_{i}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Wwhere the third step uses the facts that $\\left(\\sum_{i=1}^{t-1}X_{i}\\right)$ $\\mathcal{G}_{t-1}$ measure and $\\mathbb{E}[X_{t}|\\mathcal{G}_{t-1}]=0$ and fourth step is obtained by recursively applying second and third steps. Using the relation in Eqn. (113) in Eqn. (112), we obtain, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{\\leq t\\leq T}{\\operatorname*{sup}}\\mathbb{E}[|\\overline{{\\Delta}}_{t}(1)-\\overline{{\\Delta}}_{t}(2)|]\\leq\\left(\\mathbb{E}[(\\overline{{\\Delta}}_{T}(1)-\\overline{{\\Delta}}_{T}(2))^{2}]\\right)^{1/2}}\\\\ &{\\leq\\displaystyle\\left(\\sum_{k=1}^{T}\\mathbb{E}\\left[\\left(\\sum_{m=1}^{M}\\widetilde{\\eta}_{k}^{(T)}\\cdot\\frac{\\gamma}{M}\\cdot(\\widehat{P}_{k}^{m}(1|1,1)-\\widehat{P}_{k}^{m}(1|1,2))\\widehat{V}_{k-1}^{m}\\right)^{2}\\right]\\right)^{1/2}}\\\\ &{\\leq\\displaystyle\\left(\\sum_{k=1}^{T}\\left(\\widetilde{\\eta}_{k}^{(T)}\\right)^{2}\\cdot\\frac{2\\gamma^{2}p(1-p)}{B M^{2}}\\cdot\\underset{m=1}{\\overset{M}{\\prod}}\\mathbb{E}\\left[\\left(\\widehat{V}_{k-1}^{m}\\right)^{2}\\right]\\right)^{1/2}}\\\\ &{\\leq\\displaystyle\\left(\\sum_{k=1}^{T}\\left(\\widetilde{\\eta}_{k}^{(T)}\\right)^{2}\\cdot\\frac{2\\gamma^{2}p(1-p)}{B M(1-\\gamma)^{2}}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us focus on the term involving the step sizes. We separately consider the scenario for constant step sizes and linearly rescaled step sizes. For constant step sizes, we have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\left(\\widetilde{\\eta}_{k}^{(T)}\\right)^{2}=\\sum_{k=1}^{T}\\left(\\eta_{k}\\prod_{i=k+1}^{T}(1-\\eta_{i})\\right)^{2}=\\sum_{k=1}^{T}\\eta^{2}(1-\\eta)^{2(T-k)}\\le\\frac{\\eta^{2}}{1-(1-\\eta)^{2}}\\le\\eta.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Similarly, for linearly rescaled step sizes, we have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\left(\\widetilde{\\eta}_{k}^{(T)}\\right)^{2}=\\sum_{k=1}^{\\tau}\\left(\\widetilde{\\eta}_{k}^{(T)}\\right)^{2}+\\sum_{k=\\tau+1}^{T}\\left(\\eta_{k}\\prod_{i=k+1}^{T}(1-\\eta_{i})\\right)^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\sum_{k=1}^{\\tau}\\left(\\widetilde{\\eta}_{\\tau}^{(T)}\\right)^{2}+\\sum_{k=\\tau+1}^{T}\\eta_{k}^{2}(1-\\eta_{T})^{2(T-k)}}\\\\ &{\\le\\eta_{\\tau}^{2}(1-\\eta_{T})^{2(T-\\tau)}\\cdot\\tau+\\eta_{\\tau}^{2}\\cdot\\frac{1}{\\eta_{T}(2-\\eta_{T})}}\\\\ &{\\le3\\eta_{T}\\cdot\\eta_{T}\\cdot T\\cdot\\exp\\left(-\\frac{4T\\eta_{T}}{3}\\right)+3\\eta_{T}}\\\\ &{\\le\\frac{9}{4e^{\\eta_{T}}}\\eta_{T}+3\\eta_{T}}\\\\ &{\\le4\\eta_{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second sep uses $\\begin{array}{r}{c_{\\eta}\\le\\log N\\le\\frac{1}{1-\\gamma}}\\end{array}$ and the fact that $\\widetilde{\\eta}_{k}^{(T)}$ is inereasing in $k$ in this regime. (See Eqn 19)adffth stss $x e^{-4x/3}\\leq3/4e$ . On plugging results from Eqns. (115) and (116) into Eqn. (114) along with the value of $p$ , we obtain, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{1\\leq t\\leq T}\\mathbb{E}[|\\overline{{\\Delta}}_{t}(1)-\\overline{{\\Delta}}_{t}(2)|]\\leq\\sqrt{\\frac{8\\eta_{T}}{3B M(1-\\gamma)}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "as required. ", "page_idx": 37}, {"type": "text", "text": "B.5.5 Proof of Lemma 6 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "For the proof, we fix an agent $m$ . In order to obtain the required lower bound on $\\widehat{V}_{t}^{m}$ , we define an auxiliary sequence $\\overline{{Q}}_{t}^{m}$ that evolves as described in Algorithm 5. Essentially, $\\overline{{Q}}_{t}^{m}$ evolves in a manner almost identical to $\\widehat{Q}_{t}^{m}$ except for the fact that there is only one action and hence there is no maximization step in the update rule. ", "page_idx": 37}, {"type": "text", "text": "Algorithm 5: Evolution of $\\overline{{Q}}$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1: $r\\gets1$ $\\overline{{Q}}_{0}^{m}=Q^{\\star}(1,1)$ for all $m\\in\\{1,2,\\ldots,M\\}$   \n2: for $t=1,2,\\ldots,T$ do   \n3: for $m=1,2,\\ldots,M\\,{\\bf d o}$   \n4: $\\overline{{Q}}_{t-1/2}^{m}\\leftarrow(1-\\eta_{t})\\overline{{Q}}_{t-1}^{m}(a)+\\eta_{t}(1+\\widehat{P}_{t}^{m}(1|1,1)\\overline{{Q}}_{t-1}^{m})$   \n5: Compute $\\overline{{Q}}_{t}^{m}$ according to Eqn. (8)   \n6: end for ", "page_idx": 37}, {"type": "text", "text": "7: end for ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "It is straightforward to note that $\\widehat{Q}_{t}^{m}(1)\\geq\\overline{{Q}}_{t}^{m}$ , which can be shown using induction. From the initialization, it follows that $\\widehat{Q}_{0}^{m}(1)\\geq\\overline{{Q}}_{0}^{m}$ . Assuming the relation holds for $t-1$ ,we have, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{Q}_{t-1/2}^{m}(1)=(1-\\eta_{t})\\widehat{Q}_{t-1}^{m}(1)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,1)\\widehat{V}_{t-1}^{m})}\\\\ &{\\qquad\\qquad\\geq(1-\\eta_{t})\\widehat{Q}_{t-1}^{m}(1)+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,1)\\widehat{Q}_{t-1}^{m}(1))}\\\\ &{\\qquad\\qquad\\geq(1-\\eta_{t})\\overline{{Q}}_{t-1}^{m}+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,1)\\overline{{Q}}_{t-1}^{m})}\\\\ &{\\qquad\\qquad=\\overline{{Q}}_{t-1/2}^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since ${\\widehat{Q}}_{t}^{m}$ and $\\overline{{Q}}_{t}^{m}$ follow the same averaging schedule, it immediately follows from the above relation that $\\widehat{Q}_{t}^{m}(1)\\geq\\overline{{Q}}_{t}^{m}$ Since $\\widehat{V}_{t}^{m}\\geq\\tilde{Q}_{t}^{m}(\\overline{{1}})\\geq\\overline{{Q}}_{t}^{m}$ we willuse the sequence $\\overline{{Q}}_{t}^{m}$ to establish the required lower bound on $\\widehat{V}_{t}^{m}$ ", "page_idx": 37}, {"type": "text", "text": "We claim that for all time instants $t$ and all agents $m$ \uff0c ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\overline{{Q}}_{t}^{m}]=\\frac{1}{1-\\gamma p}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Assuming (118) holds, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\widehat{V}_{t}^{m})^{2}]\\geq\\left(\\mathbb{E}[\\widehat{V}_{t}^{m}]\\right)^{2}\\geq\\left(\\mathbb{E}[\\overline{{Q}}_{t}^{m}]\\right)^{2}\\geq\\left(\\frac{1}{1-\\gamma p}\\right)^{2}\\geq\\frac{1}{2(1-\\gamma)^{2}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "as required. In the above expression, the first inequality follows from Jensen's inequality, the second from therelation $\\widehat{V}_{t}^{m}\\ge\\overline{{Q}}_{t}^{\\bar{m}}\\ge0$ and the third from (118). ", "page_idx": 38}, {"type": "text", "text": "We now mve now to prove the claim (118using induction. For the base case. $\\begin{array}{r}{\\mathbb E[\\overline{{Q}}_{0}^{m}]=\\frac{1}{1-\\gamma p}}\\end{array}$ holds by choice of initialization. Assume that E[Q-1 = 1-p holds for some $t-1$ for all $m$ ", "page_idx": 38}, {"type": "text", "text": "\u00b7If $t$ is not an averaging instant, then for any client $m$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\overline{{Q}}_{t}^{m}=(1-\\eta_{t})\\overline{{Q}}_{t-1}^{m}+\\eta_{t}(1+\\gamma\\widehat{P}_{t}^{m}(1|1,1)\\overline{{Q}}_{t-1}^{m})\\quad\\quad}\\\\ &{}&{\\implies\\mathbb{E}[\\overline{{Q}}_{t}^{m}]=(1-\\eta_{t})\\mathbb{E}[\\overline{{Q}}_{t-1}^{m}]+\\eta_{t}(1+\\gamma\\mathbb{E}[\\widehat{P}_{t}^{m}(1|1,1)\\overline{{Q}}_{t-1}^{m}])}\\\\ &{}&{\\qquad=(1-\\eta_{t})\\mathbb{E}[\\overline{{Q}}_{t-1}^{m}]+\\eta_{t}(1+\\gamma p\\mathbb{E}[\\overline{{Q}}_{t-1}^{m}])\\quad\\quad}\\\\ &{}&{\\qquad=\\frac{(1-\\eta_{t})}{1-\\gamma p}+\\eta_{t}\\left(1+\\frac{\\gamma p}{1-\\gamma p}\\right)=\\frac{1}{1-\\gamma p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The third line follows from the independence of $\\widehat{P}_{t}^{m}(1|1,1)$ and $\\overline{{Q}}_{t-1}^{m}$ and the fourth line uses the inductive hypothesis. ", "page_idx": 38}, {"type": "text", "text": "\u00b7If $t$ is an averaging instant, then for all clients $m$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\overline{{Q}}_{t}^{m}=\\frac{(1-\\eta_{t})}{M}\\sum_{j=1}^{M}\\overline{{Q}}_{t-1}^{j}+\\eta_{t}\\frac{1}{M}\\sum_{j=1}^{M}(1+\\gamma\\widehat{P}_{t}^{j}(1|1,1)\\overline{{Q}}_{t-1}^{j})}\\\\ {\\displaystyle\\implies\\mathbb{E}[\\overline{{Q}}_{t}^{m}]=\\frac{(1-\\eta_{t})}{M}\\sum_{j=1}^{M}\\mathbb{E}[\\overline{{Q}}_{t-1}^{j}]+\\eta_{t}\\frac{1}{M}\\sum_{j=1}^{M}(1+\\gamma\\mathbb{E}[\\widehat{P}_{t}^{j}(1|1,1)\\overline{{Q}}_{t-1}^{j}])}\\\\ {\\displaystyle=\\frac{(1-\\eta_{t})}{M}\\sum_{j=1}^{M}\\frac{1}{1-\\gamma p}+\\eta_{t}\\frac{1}{M}\\sum_{j=1}^{M}\\bigg(1+\\frac{\\gamma p}{1-\\gamma p}\\bigg)=\\frac{1}{1-\\gamma p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we again make use of independence and the inductive hypothesis. ", "page_idx": 38}, {"type": "text", "text": "Thus, (119) and (120) taken together complete the inductive step. ", "page_idx": 38}, {"type": "text", "text": "C Analysis of Fed-DVR-Q ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we prove Theorem 2 that outlines the performance guarantees of Fed-DVR-Q. There are two main parts of the proof. The first part deals with establishing that for the given choice of parameters described in Section 4.1.3, the output of the algorithm is an $\\varepsilon$ -optimal estimateof $Q^{\\star}$ With probability $1-\\delta$ . The second part deals with deriving the bounds on the sample and communication complexity based on the choice of prescribed parameters. We begin with the second part, which is easier of the two. ", "page_idx": 38}, {"type": "text", "text": "C.1 Establishing the sample and communication complexity bounds ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Establishing the communication complexity. We begin with bounding $\\mathsf{C C}_{\\mathsf{r o u n d}}$ From the description of Fed-DVR-Q, it is straightforward to note that each epoch, i.e., each call to the REFINEEsTIMATE routine, involves $I+1$ rounds of communication, one for estimating $\\tau\\overline{{Q}}$ and the remaining ones during the iterative updates of the Q-function. Since there are a total of $K$ epochs, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathsf{C C}_{\\mathrm{round}}(\\mathsf{F e d-D V R-Q};\\varepsilon,M,\\delta)\\le(I+1)K\\le\\frac{16}{\\eta(1-\\gamma)}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the second bound follows from the prescribed choice of parameters in Sec. 4.1.3. Similarly, since the quantization step is designed to compress each coordinate into $J$ bits, each message transmitted by an agent has a size of no more than $J\\cdot|S||A|$ bits. Consequently, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{C C}_{\\mathtt{b i t}}(\\mathsf{F e d\\!-\\!D V\\!R\\!-\\!Q\\!;}\\varepsilon,M,\\delta)\\le J\\cdot|S||A|\\cdot\\mathsf{C C}_{\\mathsf{r o u n d}}(\\mathsf{F e d\\!-\\!D V\\!R\\!-\\!Q;}\\varepsilon,M,\\delta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{32|S|A|}{\\eta(1-\\gamma)}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right)\\log_{2}\\left(\\frac{70}{\\eta(1-\\gamma)}\\sqrt{\\frac{4}{M}\\log\\left(\\frac{8K I|S||\\mathcal{A}|}{\\delta}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where once again in the second step we plugged in the choice of $J$ from Sec. 4.1.3. ", "page_idx": 38}, {"type": "text", "text": "Establishing the sample complexity. In order to establish the bound on the sample complexity, note that during epoch $k$ , each agent takes a total of $\\lceil L_{k}/M\\rceil+I\\cdot B$ samples, where the first term corresponds to approximating $\\widetilde{T}_{L}(Q^{(k-1)})$ and the second term corresponds to the samples taken during the iterative update scheme. Thus, the total sample complexity is obtained by summing up over all the $K$ epochs. We have, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathtt{S C}(\\mathtt{F e d}\\mathrm{-}\\mathsf{D V R}\\mathrm{-}\\mathrm{O};\\varepsilon,M,\\delta)\\leq\\sum_{k=1}^{K}\\left(\\left\\lceil\\frac{L_{k}}{M}\\right\\rceil+I\\cdot B\\right)\\leq I\\cdot B\\cdot K+\\frac{1}{M}\\sum_{k=1}^{K}L_{k}+K.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "To continue, notice that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{M}\\sum_{k=1}^{K}L_{k}\\leq\\frac{39200}{M(1-\\gamma)^{2}}\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)\\left(\\sum_{k=1}^{K_{0}}4^{k}+\\sum_{k=K_{0}+1}^{K}4^{k-K_{0}}\\right)}}\\\\ &{\\leq\\frac{39200}{3M(1-\\gamma)^{2}}\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)\\left(4^{K_{0}}+4^{K-K_{0}}\\right)}\\\\ &{\\leq\\frac{156800}{3M(1-\\gamma)^{2}}\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)\\left(\\frac{1}{1-\\gamma}+\\frac{1}{(1-\\gamma)\\varepsilon^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the first line follows from the choice of $L_{k}$ in Sec. 4.1.3 and the last line follows from $\\begin{array}{r}{K_{0}\\,=\\,\\lceil\\frac{1}{2}\\log_{2}(\\frac{1}{1-\\gamma})\\rceil}\\end{array}$ . Plugging this relation and the choices of $I$ and $B$ (cf. Sec. 4.1.3) into the previous bound yields ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{S C}(\\mathsf{F e d}\\!\\cdot\\!\\mathsf{D V}\\mathsf{R}\\!-\\!\\Omega;\\varepsilon,M,\\delta)\\le\\displaystyle\\frac{4608}{\\eta M(1-\\gamma)^{3}}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right)\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)+K}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{156800}{3M(1-\\gamma)^{2}}\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)\\left(\\frac{1}{1-\\gamma}+\\frac{1}{(1-\\gamma)\\varepsilon^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{313600}{\\eta M(1-\\gamma)^{3}\\varepsilon^{2}}\\log_{2}\\left(\\frac{1}{(1-\\gamma)\\varepsilon}\\right)\\log\\left(\\frac{8K I|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)+K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Plugging in the choice of $K$ finishes the proof. ", "page_idx": 39}, {"type": "text", "text": "C.2  Establishing the error guarantees ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we show that the Q-function estimate returned by the Fed-DVR-Q algorithm is $\\varepsilon$ -optimal with probability at least $1-\\delta$ . We claim that the estimates of the Q-function generated by the algorithm across different epochs satisfy the following relation for all $k\\leq K$ with probability $1-\\delta$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|Q^{(k)}-Q^{\\star}\\|_{\\infty}\\leq\\frac{2^{-k}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The required bound on $\\|Q^{(K)}-Q^{\\star}\\|_{\\infty}$ immediately follows by plugging in the value of $K$ Thus, for the remainder of the section, we focus on establishing the above claim. ", "page_idx": 39}, {"type": "text", "text": "Step 1: fixed-point contraction of REFINEEsTIMATE. Firstly, note that the variance-reduced update scheme carried out during the REFINEEsTIMATE routine resembles that of the classic $\\mathrm{Q}-$ learning scheme, i.e., fixed-point iteration, with a different operator defined as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}(Q):=\\mathcal{T}(Q)-\\mathcal{T}(\\overline{{Q}})+\\widetilde{\\mathcal{T}}_{L}(\\overline{{Q}}),\\quad\\mathrm{for~some~fixed~}\\overline{{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, the update scheme at step $i\\geq1$ in (11) can then be written as ", "page_idx": 39}, {"type": "equation", "text": "$$\nQ_{i-\\frac{1}{2}}^{m}=(1-\\eta)Q_{i-1}+\\eta\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{i-1}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\widehat{\\mathcal{H}}_{i}^{(m)}(Q):=\\widehat{\\mathcal{T}}_{i}^{(m)}(Q)-\\widehat{\\mathcal{T}}_{i}^{(m)}(\\overline{{Q}})+\\widetilde{\\mathcal{T}}_{L}(\\overline{{Q}})$ is a stochastic, unbiased estimate of the operator $\\mathcal{H}$ similar to $\\widehat{T}_{i}^{(m)}(Q)$ Let $Q_{\\mathcal{H}}^{\\star}$ denote the fixed point of $\\mathcal{H}$ Then the update scheme in (123) drives the sequence $\\{Q_{i}^{m}\\}_{i\\geq0}$ t0 $\\mathcal{Q}_{\\mathcal{H}}^{\\star}$ ; further, as long as $\\|Q^{\\star}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}$ is small,the required eror $\\|Q_{i}-Q^{\\star}\\|_{\\infty}$ can also be controlled. The following lemmas formalize these ideas and pave the path to establish the claim in (121). The proofs are deferred to Appendix C.3. ", "page_idx": 39}, {"type": "text", "text": "Lemma 7. Let $\\delta\\in(0,1)$ . Consider the REFINEEsTIMATE routine described in Algorithm 3 and let $Q_{\\mathcal{H}}^{\\star}$ denote the fixed point of the operator $\\mathcal{H}$ defined in (122) for some fixed $\\overline{{Q}}$ . Then the iterates generated by REFINEESTIMATE $Q_{I}$ satisfy ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|Q_{I}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\leq\\frac{1}{6}\\left(\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}+\\|Q^{\\star}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\right)+\\frac{D}{70}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with probability $1-\\textstyle{\\frac{\\delta}{2K}}$ ", "page_idx": 40}, {"type": "text", "text": "Lemma 8. Consider the REFINEEsTIMATE routine described in Alg. 3 and let $Q_{\\mathcal{H}}^{\\star}$ denote the fixed point of the operator $\\mathcal{H}$ defined in Eqn. (122) for a fixed $\\overline{{Q}}$ . The following relation holds with probability $1-\\frac{\\delta}{2K}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|Q_{\\mathcal{H}}^{\\star}-Q^{\\star}\\|_{\\infty}\\le\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}\\cdot\\sqrt{\\frac{16\\kappa^{\\prime}}{L(1-\\gamma)^{2}}}+\\sqrt{\\frac{64\\kappa^{\\prime}}{L(1-\\gamma)^{3}}}+\\frac{2\\kappa^{\\prime}\\sqrt{2}}{3L(1-\\gamma)^{2}}+\\frac{D}{70},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "whenever $L\\ge32\\kappa^{\\prime}$ , where $\\begin{array}{r}{\\kappa^{\\prime}=\\log\\left(\\frac{12K|S||A|}{\\delta}\\right)}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Step 2: establishing the linear contraction. We now leverage the above lemmas to establish the desired contraction in (121). Instantiating the operator (122) at each $k$ -th epochbysetting ${\\overline{{Q}}}:=Q^{(k-1)}$ and $L:=L_{k}$ we define ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{k}(Q):=\\mathcal{T}(Q)-\\mathcal{T}(Q^{(k-1)})+\\widetilde{\\mathcal{T}}_{L_{k}}(Q^{(k-1)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "whose fixed point is denoted as $Q_{\\mathcal{H}_{k}}^{\\star}$ . Using the results from Lemmas 7 and 8 with $D:=D_{k}$ and $\\mathcal{H}=\\mathcal{H}_{k}$ , we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q^{(k)}-Q^{*}\\|_{\\infty}\\leq\\|Q^{(k)}-Q_{\\mathcal{H}_{k}}^{*}\\|_{\\infty}+\\|Q_{\\mathcal{H}}^{*}-Q_{\\mathcal{H}_{k}}^{*}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{6}\\left(\\|Q^{(k-1)}-Q^{*}\\|_{\\infty}+\\|Q^{*}-Q_{\\mathcal{H}_{k}}^{*}\\|_{\\infty}\\right)+\\frac{D_{k}}{70}+\\|Q_{\\mathcal{H}_{k}}^{*}-Q^{*}\\|_{\\infty}}\\\\ &{\\qquad\\qquad=\\frac{1}{6}\\left(\\|Q^{(k-1)}-Q^{*}\\|_{\\infty}+7\\|Q^{*}-Q_{\\mathcal{H}_{k}}^{*}\\|_{\\infty}\\right)+\\frac{D_{k}}{70}}\\\\ &{\\qquad\\qquad\\leq\\|Q^{(k-1)}-Q^{*}\\|_{\\infty}\\left(\\frac{1}{6}+\\frac{7}{6}\\sqrt{\\frac{16\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{2}}}\\right)+\\frac{7}{6}\\left(\\sqrt{\\frac{64\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{3}}}+\\frac{2\\sqrt{2}\\kappa^{\\prime}}{3L_{k}(1-\\gamma)^{2}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq\\|Q^{(k-1)}-Q^{*}\\|_{\\infty}\\left(\\frac{1}{6}+\\frac{7}{6}\\sqrt{\\frac{16\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{2}}}\\right)+\\frac{7}{6}\\sqrt{\\frac{100\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{3}}}+\\frac{13D_{k}}{420},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "holds withprobabity $1\\,-\\,{\\textstyle\\frac{\\delta}{K}}$ Here, we invoke Lemma7 in the second step and Lemma in the fourth step corresponding to the REFINEEsTIMATE routine during the -th epoch. In the last step, we used the fact that $\\begin{array}{r}{\\frac{\\bar{L}_{k}(1-\\bar{\\gamma})^{2}}{\\kappa^{\\prime}}\\geq1}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "We now use induction along with the recursive relation in (125) to establish the required claim (121). Let us frst considerthe ase $0\\leq k\\leq K_{0}$ .The base ase, $\\begin{array}{r}{\\|Q^{(0)}-Q^{\\star}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ , holds by defintion. Let us assume the relation holds for $k-1$ . Then, from (125) and choice of $L_{k}$ (Sec. 4.1.3), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Q^{(k)}-Q^{\\star}\\|_{\\infty}\\le\\|Q^{(k-1)}-Q^{\\star}\\|_{\\infty}\\left(\\frac{1}{6}+\\frac{7}{6}\\sqrt{\\frac{16\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{2}}}\\right)+\\frac{7}{6}\\sqrt{\\frac{100\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{3}}}+\\frac{13D_{k}}{420}}&{}\\\\ {\\le\\frac{2^{-(k-1)}}{1-\\gamma}\\left(\\frac{1}{6}+2^{-k}\\cdot\\frac{7}{6}\\sqrt{\\frac{8}{19600}}\\right)+2^{-k}\\cdot\\frac{7}{6}\\sqrt{\\frac{50}{19600(1-\\gamma)}}+\\frac{104}{420}\\cdot\\frac{2^{-(k-1)}}{1-\\gamma}}&{}\\\\ {\\le\\frac{2^{-(k-1)}}{1-\\gamma}\\left(\\frac{1}{6}+\\frac{7}{6}\\sqrt{\\frac{91}{39200}}+\\frac{1}{4}\\right)}&{}&\\\\ {\\le\\frac{2^{-k}}{1-\\gamma}.}&{}&{(126)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now we move to the second case, for $k>K_{0}$ . From (125) and choice of $L_{k}$ (Sec. 4.1.3), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{Q^{(k)}-Q^{\\star}\\|_{\\infty}\\le\\|Q^{(k-1)}-Q^{\\star}\\|_{\\infty}\\left(\\frac{1}{6}+\\frac{7}{6}\\sqrt{\\frac{16\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{2}}}\\right)+\\frac{7}{6}\\sqrt{\\frac{100\\kappa^{\\prime}}{L_{k}(1-\\gamma)^{3}}}+\\frac{13D_{k}}{420}}&\\\\ &{\\le\\frac{2^{-(k-1)}}{1-\\gamma}\\left(\\frac{1}{6}+2^{-(k-K_{0})}\\cdot\\frac{7}{6}\\sqrt{\\frac{8}{19600}}\\right)+2^{-(k-K_{0})}\\cdot\\frac{7}{6}\\sqrt{\\frac{50}{19600(1-\\gamma)}}+\\frac{104}{420}\\cdot}&\\\\ &{\\le\\frac{2^{-(k-1)}}{1-\\gamma}\\left(\\frac{1}{6}+\\frac{7}{6}\\sqrt{\\frac{1}{196}}+\\frac{1}{4}\\right)}&\\\\ &{\\le\\frac{2^{-k}}{1-\\gamma}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Byaunion bound argument wecan conclude that the relation $\\begin{array}{r}{\\|Q^{(k)}-Q^{\\star}\\|_{\\infty}\\leq\\frac{2^{-k}}{1-\\gamma}}\\end{array}$ holds for ll $k\\leq K$ with probability at least $1-\\delta$ ", "page_idx": 41}, {"type": "text", "text": "Step 3: confirm the compressor bound. The only thing left to verify is that the inputs to the compressor are always bounded by $D_{k}$ during the $k$ -th epoch, for all $1\\le k\\le K$ .Thefollowing lemma provides a bound on the input to the compressor during any run of the REFINEEsTIMATE routine. ", "page_idx": 41}, {"type": "text", "text": "Lemma 9. Consider the REFINEEsTIMATE routine described in Algorithm 3 with some for some fixed Q. For all  I and allagents m, the following bound holds with probability - 2k : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|Q_{i-\\frac{1}{2}}^{m}-Q_{i-1}\\|_{\\infty}\\leq\\eta\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\left(\\frac{7}{6}\\cdot(1+\\gamma)+2\\gamma\\right)+\\frac{\\eta D(1+\\gamma)}{70}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For the $k$ -th epoch, it follows that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\|Q^{(k-1)}-Q_{\\mathcal{H}_{k}}^{\\star}\\|_{\\infty}\\left(\\frac{7}{6}\\cdot(1+\\gamma)+2\\gamma\\right)+\\frac{\\eta D_{k}(1+\\gamma)}{70}}\\\\ &{\\le\\frac{13}{3}\\left(\\|Q^{(k-1)}-Q^{\\star}\\|_{\\infty}+\\|Q^{\\star}-Q_{\\mathcal{H}_{k}}^{\\star}\\|_{\\infty}\\right)+\\frac{D_{k}(1+\\gamma)}{70}}\\\\ &{\\le\\frac{13}{3}\\cdot\\frac{15}{14}\\cdot\\|Q^{(k-1)}-Q^{\\star}\\|_{\\infty}+\\frac{2D_{k}}{70}}\\\\ &{\\le\\left(\\frac{195}{42}+\\frac{16}{70}\\right)\\cdot\\frac{2^{-(k-1)}}{1-\\gamma}}\\\\ &{\\le8\\cdot\\frac{2^{-(k-1)}}{1-\\gamma}:=D_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In the third step, we used the same sequence of arguments as used in (126) and (127) and, in the fourth step, we used the bound on $\\|Q^{(k-1)}-Q^{\\star}\\|_{\\infty}$ from (121) and the prescribed value of $D_{k}$ ", "page_idx": 41}, {"type": "text", "text": "C.3  Proof of auxiliary lemmas ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "C.3.1 Proof of Lemma 7 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Let us begin with analyzing the evolution of the sequence $\\{Q_{i}\\}_{i=1}^{I}$ during a run of the REFINEEsTIMATE routine. The sequence $\\{Q_{i}\\}_{i=1}^{I}$ satisfies the following recursion: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle Q_{i}=Q_{i-1}+\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{C}\\left(Q_{i-\\frac{1}{2}}^{m}-Q_{i-1};D,J\\right)}\\\\ {\\displaystyle=Q_{i-1}+\\frac{1}{M}\\sum_{m=1}^{M}\\left(Q_{i-\\frac{1}{2}}^{m}-Q_{i-1}+\\zeta_{i}^{m}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n=\\frac{1}{M}\\sum_{m=1}^{M}\\left(Q_{i-\\frac{1}{2}}^{m}+\\zeta_{i}^{m}\\right)=(1-\\eta)Q_{i-1}+\\frac{\\eta}{M}\\sum_{m=1}^{M}\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{i-1})+\\underbrace{\\frac{1}{M}\\sum_{m=1}^{M}\\zeta_{i}^{m}}_{=:\\zeta_{i}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In the above expression, $\\zeta_{i}^{m}$ denotes the quantization noise introduced at agent $m$ in the $i$ -th update. Subtracting $Q_{\\mathcal{H}}^{\\star}$ from both sides of (128), we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i}-Q_{\\mathcal{\\hat{H}}}^{\\star}=(1-\\eta)(Q_{i-1}-Q_{\\mathcal{\\hat{H}}}^{\\star})+\\displaystyle\\frac{\\eta}{M}\\sum_{m=1}^{M}\\left(\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{i-1})-Q_{\\mathcal{\\hat{H}}}^{\\star}\\right)+\\zeta_{i}}\\\\ &{\\qquad=(1-\\eta)(Q_{i-1}-Q_{\\mathcal{\\hat{H}}}^{\\star})+\\displaystyle\\frac{\\eta}{M}\\sum_{m=1}^{M}\\left(\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{i-1})-\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{\\hat{H}}}^{\\star})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\eta}{M}\\sum_{m=1}^{M}\\left(\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{H}(Q_{\\mathcal{H}}^{\\star})\\right)+\\zeta_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Consequently, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|Q_{i}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\leq(1-\\eta)\\|Q_{i-1}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+\\frac{\\eta}{M}\\displaystyle\\sum_{m=1}^{M}\\left\\|\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{i-1})-\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{H}}^{\\star})\\right\\|_{\\infty}}&{{}}\\\\ {+\\left\\|\\displaystyle\\frac{\\eta}{M}\\displaystyle\\sum_{m=1}^{M}\\left(\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{H}(Q_{\\mathcal{H}}^{\\star})\\right)\\right\\|_{\\infty}+\\|\\zeta_{i}\\|_{\\infty}\\,,}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which we shall proceed to bound each term separately. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Regarding the second term, it follows that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\mathcal{H}}_{i}^{(m)}(Q)-\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{\\hbar}}^{\\star})\\right\\|_{\\infty}=\\left\\|\\widehat{\\mathcal{T}}_{i}^{(m)}(Q)-\\widehat{\\mathcal{T}}_{i}^{(m)}(Q_{\\mathcal{\\hbar}}^{\\star})\\right\\|_{\\infty}\\leq\\gamma\\left\\|Q-Q_{\\mathcal{\\hbar}}^{\\star}\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which holds for all $Q$ since $\\widehat{\\mathcal{T}}_{i}^{(m)}$ is a $\\gamma$ -contractive operator. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Regarding the third term, notice that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{H}(Q_{\\mathcal{H}}^{\\star})\\right)=\\frac{1}{M B}\\sum_{m=1}^{M}\\sum_{z\\in\\mathcal{Z}_{i}^{(m)}}\\left(\\mathcal{T}_{z}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{T}_{z}(\\overline{{Q}})-\\mathcal{T}(Q_{\\mathcal{H}}^{\\star})+\\mathcal{T}(\\overline{{Q}})\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Note that $T_{z}(Q_{\\mathcal{H}}^{\\star})-T_{z}(\\overline{{Q}})-T(Q_{\\mathcal{H}}^{\\star})+T(\\overline{{Q}})$ is a zero-mean random vector satisfying ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{T}_{z}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{T}_{z}(\\overline{{Q}})-\\mathcal{T}(Q_{\\mathcal{H}}^{\\star})+\\mathcal{T}(\\overline{{Q}})\\|_{\\infty}\\leq2\\gamma\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, each of its coordinate is a $(2\\gamma\\Vert\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\Vert_{\\infty})^{2}$ sub-Gaussian vector. Applying the tail bounds for a maximum of sub-Gaussian random variables [Vershynin, 2018], we obtain that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widehat{\\mathcal{H}}_{i}^{(m)}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{H}(Q_{\\mathcal{H}}^{\\star})\\right)\\right\\rVert_{\\infty}\\le2\\gamma\\lVert\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\rVert_{\\infty}\\cdot\\sqrt{\\frac{2}{M B}\\log\\left(\\frac{8K I|S||A|}{\\delta}\\right)}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "holds with probability at least $1-\\frac{\\delta}{4K I}$ ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Turning to the last term, by the construction of the compression routine described in Section 4.1.2, it is straightforward to note that $\\zeta_{i}^{m}$ is a zero-mean random vector whose coordinates are independent, $D^{2}\\cdot4^{-J}$ -sub-Gaussian random variables. Thus, $\\zeta_{i}$ is also a zer-meanrandmvetorwhecorinatsare iepne $\\frac{D^{2}}{M{\\cdot}4^{J}}$ -sub-Gaussian random variables. Hence, we can similarly conclude that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|\\zeta_{i}\\|_{\\infty}\\leq D\\cdot2^{-J}\\cdot{\\sqrt{{\\frac{2}{M}}\\log\\left({\\frac{8K I|S||A|}{\\delta}}\\right)}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "holds with probability at least 1 - 4K1- ", "page_idx": 42}, {"type": "text", "text": "Combining the above bounds into (130), and introducing the short-hand notation $\\kappa\\quad:=$ (8KIIlLA), we obtain with probability a least  2i ", "page_idx": 43}, {"type": "equation", "text": "$$\nQ_{i}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\leq(1-\\eta(1-\\gamma))\\|Q_{i-1}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+2\\eta\\gamma\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\cdot\\sqrt{\\frac{2\\kappa}{M B}}+D\\cdot2^{-J}\\cdot\\sqrt{\\frac{2\\kappa}{M}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Unrolling the above recursion over $i=1,\\dots,I$ yields the following relation, which holds with probability at least $1-\\textstyle{\\frac{\\delta}{2K}}$ 2K\u00b7 ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q_{I}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\leq(1-\\eta(1-\\gamma))^{I}\\,\\|Q_{0}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+\\sqrt{\\frac{2\\kappa}{M}}\\left(\\frac{2\\eta\\gamma}{\\sqrt{B}}\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+D\\cdot2^{-J}\\right)\\cdot\\overset{I}{\\underset{i=1}{\\longrightarrow}}(1-\\eta(1-\\gamma))}\\\\ &{\\qquad\\qquad\\leq(1-\\eta(1-\\gamma))^{I}\\,\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+\\frac{1}{\\eta(1-\\gamma)}\\sqrt{\\frac{2\\kappa}{M}}\\left(\\frac{2\\eta\\gamma}{\\sqrt{B}}\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+D\\cdot2^{-J}\\right)}\\\\ &{\\qquad\\qquad\\leq\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\,\\bigg((1-\\eta(1-\\gamma))^{I}+\\frac{2\\gamma}{(1-\\gamma)}\\sqrt{\\frac{2\\kappa}{M B}}\\bigg)+\\frac{D\\cdot2^{-J}}{\\eta(1-\\gamma)}\\cdot\\sqrt{\\frac{2\\kappa}{M}}\\,\\left(135\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}}{6}+\\frac{D}{70}\\leq\\frac{1}{6}\\left(\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}+\\|Q^{\\star}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\right)+\\frac{D}{70}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here, the fourth step is obtained by plugging in the prescribed values of $B,I$ and $J$ in Sec. 4.1.3. ", "page_idx": 43}, {"type": "text", "text": "C.3.2 Proof of Lemma 8 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Intuitively, the error $\\|Q_{\\mathcal{H}}^{\\star}-Q^{\\star}\\|_{\\infty}$ depends on the error term ${\\widetilde{\\cal T}}_{L}({\\overline{{{Q}}}})-{\\cal T}({\\overline{{{Q}}}})$ . If the latter is small,. then $\\mathcal{H}(Q)$ is close to $\\tau(Q)$ and consequently so are $Q_{\\mathcal{H}}^{\\star}$ and $Q^{\\star}$ .Thus, we begin with bounding the term $\\widetilde{\\cal T}_{L}(\\overline{{{Q}}})-\\cal T(\\overline{{{Q}}})$ . We have, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{\\mathcal{T}}_{L}(\\overline{{Q}})-\\mathcal{T}(\\overline{{Q}})}}\\\\ {{\\displaystyle=\\overline{{Q}}+\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{C}\\left(\\widetilde{T}_{L}^{(m)}(\\overline{{Q}})-\\overline{{Q}}\\right)-\\mathcal{T}(\\overline{{Q}})}}\\\\ {{\\displaystyle=\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widetilde{T}_{L}^{(m)}(\\overline{{Q}})+\\widetilde{\\zeta}_{L}^{(m)}\\right)-\\mathcal{T}(\\overline{{Q}})}}\\\\ {{\\displaystyle=\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widetilde{T}_{L}^{(m)}(\\overline{{Q}})-\\widetilde{T}_{L}^{(m)}(Q^{\\star})-\\mathcal{T}(\\overline{{Q}})+\\mathcal{T}(Q^{\\star})\\right)+\\frac{1}{M}\\sum_{m=1}^{M}\\widetilde{\\zeta}_{L}^{(m)}+\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widetilde{T}_{L}^{(m)}(Q^{\\star})-\\widetilde{T}_{L}^{(m)}(Q^{\\star})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where once again (m) := T(m) (Q) - Q - 6 (T(m)(Q) - Q) denotes the quantization eror at agent $m$ . Similar to the arguments of (133) and (134), we can conclude that each of the following relations hold with probability at least $\\begin{array}{r}{1-\\frac{\\delta}{6K}}\\end{array}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widetilde{\\mathcal{T}}_{L}^{(m)}(\\overline{{Q}})-\\widetilde{\\mathcal{T}}_{L}^{(m)}(Q^{\\star})-\\mathcal{T}(\\overline{{Q}})+\\mathcal{T}(Q^{\\star})\\right)\\right\\rVert_{\\infty}\\leq2\\gamma\\lVert\\overline{{Q}}-Q^{\\star}\\rVert_{\\infty}\\cdot\\sqrt{\\frac{2}{L}\\log\\left(\\frac{12K|S||\\mathcal{S}|}{\\delta}\\right)}}\\\\ {\\left\\lVert\\frac{1}{M}\\sum_{m=1}^{M}\\widetilde{\\zeta}_{L}^{(m)}\\right\\rVert_{\\infty}\\leq D\\cdot2^{-J}\\cdot\\sqrt{\\frac{2}{M}\\log\\left(\\frac{12K|S||\\mathcal{A}|}{\\delta}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For the third term, we can rewrite it as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\widetilde{\\mathcal{T}}_{L}^{(m)}(Q^{\\star})-\\mathcal{T}(Q^{\\star})\\right)=\\frac{1}{M\\lceil L/M\\rceil}\\sum_{m=1}^{M}\\sum_{l=1}^{\\lceil L/M\\rceil}\\left(\\mathcal{T}_{Z_{l}^{(m)}}(Q^{\\star})-\\mathcal{T}(Q^{\\star})\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We will use Bernstein inequality element wise to bound the above term. Let $\\pmb{\\sigma}^{\\star}\\in\\mathbb{R}^{|S|\\times|A|}$ be such that $[\\sigma^{\\star}(s,a)]^{2}=\\mathrm{Var}(\\bar{T_{Z}(Q^{\\star})(s,a))}$ , i.e., $(s,a)$ -th element of $\\pmb{\\sigma}$ denotes the standard deviation of the random variable $\\tau_{Z}(Q^{\\star})(s,a)$ .Since $\\begin{array}{r}{\\|\\dot{{\\mathcal{T}}}_{Z}(\\dot{Q}^{\\star})-{\\mathcal{T}}(Q^{\\star})\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ a.s.,Bernsein inequality gives usthat ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\bigg(\\widetilde{\\mathcal{T}}_{L}^{(m)}(Q^{\\star})(s,a)-\\mathcal{T}(Q^{\\star})(s,a)\\bigg)\\bigg|\\leq\\sigma^{\\star}(s,a)\\sqrt{\\frac{2}{L}\\log\\bigg(\\frac{6K|S||A|}{\\delta}\\bigg)}+\\frac{2}{3L(1-\\gamma)}\\log\\bigg(\\frac{6K|S||A|^{2}}{\\delta}\\bigg)\\,,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$(s,a)\\;\\in\\;{\\cal S}\\;\\times\\;{\\cal A}$ $1\\,-\\,{\\frac{\\delta}{6K}}$ . On combining ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\tilde{T}_{L}(\\overline{{Q}})(s,a)-Tilde{T}(\\overline{{Q}})(s,a)\\Big|=\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}\\cdot\\sqrt{\\frac{8\\kappa^{\\prime}}{L}}+\\sigma^{\\star}(s,a)\\sqrt{\\frac{2\\kappa^{\\prime}}{L}}+\\frac{2\\kappa^{\\prime}}{3L(1-\\gamma)}+D\\cdot2^{-J}\\cdot\\sqrt{\\frac{2\\kappa^{\\prime}}{L}}\\|\\overline{{Q}}\\|_{\\infty}\\|_{\\infty}^{2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "holds simultaneously for al $(s,a)\\ \\in\\ S\\ \\times\\ A$ wih probability at least $1\\;-\\;\\textstyle{\\frac{\\delta}{2K}}$ ,where $\\kappa^{\\prime}\\;=\\;$ $\\log\\left(\\frac{12K|S||A|}{\\delta}\\right)$ Weushis na $\\|Q_{\\mathcal{H}}^{\\star}-Q^{\\star}\\|_{\\infty}$ using the following lemma. ", "page_idx": 44}, {"type": "text", "text": "Lemma 10 (Wainwright [2019b]). Let $\\pi^{\\star}$ and $\\pi_{\\mathcal{H}}^{\\star}$ respectively denote the optimal policies w.rt. $Q^{\\star}$ and $Q_{\\mathcal{H}}^{\\star}$ .Then, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|Q_{\\mathcal{H}}^{\\star}-Q^{\\star}\\|_{\\infty}\\leq\\operatorname*{max}\\left\\lbrace(I-\\gamma P^{\\pi^{\\star}})^{-1}\\left|\\widetilde{\\mathcal{T}}_{L}(\\overline{{Q}})-\\mathcal{T}(\\overline{{Q}})\\right|,(I-\\gamma P^{\\pi_{\\mathcal{H}}^{\\star}})^{-1}\\left|\\widetilde{\\mathcal{T}}_{L}(\\overline{{Q}})-\\mathcal{T}(\\overline{{Q}})\\right|\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Here, for any deterministic policy $\\pi_{i}$ $P^{\\pi}\\;\\;\\;\\in\\;\\;\\;\\mathbb{R}^{|S||A|\\times|S||A|}$ is given by $\\begin{array}{r l}{(P^{\\pi}Q)(s,a)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)Q(s^{\\prime},\\pi(s^{\\prime}))}\\end{array}$ ", "page_idx": 44}, {"type": "text", "text": "Furthermore, it was shown in Wainwright [2019b, Proof of Lemma 4] that if the error $\\vert\\widetilde{\\cal T}_{L}(\\overline{{{Q}}})(s,a)-$ $\\tau(\\overline{{Q}})(s,a)|$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\widetilde{\\mathcal{T}}_{L}(\\overline{{Q}})(s,a)-\\mathcal{T}(\\overline{{Q}})(s,a)\\right|\\leq z_{0}\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}+z_{1}\\pmb{\\sigma}^{\\star}(s,a)+z_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for some $z_{0},z_{1},z_{2}\\ge0$ With $z_{1}<1$ , then the bound in Lemma 10 can be simplified to ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|Q_{\\mathcal{H}}^{\\star}-Q^{\\star}\\|_{\\infty}\\leq\\frac{1}{1-z_{1}}\\left(\\frac{z_{0}}{1-\\gamma}\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}+\\frac{z_{1}}{(1-\\gamma)^{3/2}}+\\frac{z_{2}}{1-\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "On comparing, (141) with (142), we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\nz_{0}\\equiv\\sqrt{\\frac{8\\kappa^{\\prime}}{L}};\\quad z_{1}\\equiv\\sqrt{\\frac{2\\kappa^{\\prime}}{L}};\\quad z_{2}\\equiv\\frac{2\\kappa^{\\prime}}{3L(1-\\gamma)}+D\\cdot2^{-J}\\cdot\\sqrt{\\frac{2\\kappa^{\\prime}}{M}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Moreover, the condition $L\\ge32\\kappa^{\\prime}$ implies that $z_{1}<1$ and ${\\frac{1}{1-z_{1}}}\\leq{\\sqrt{2}}$ . Thus, on plugging in the above values in (143), we can conclude that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q_{\\mathcal{H}}^{\\star}-Q^{\\star}\\|_{\\infty}\\leq\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}\\cdot\\sqrt{\\frac{16\\kappa^{\\prime}}{L(1-\\gamma)^{2}}}+\\sqrt{\\frac{64\\kappa^{\\prime}}{L(1-\\gamma)^{3}}}+\\frac{2\\kappa^{\\prime}\\sqrt{2}}{3L(1-\\gamma)^{2}}+\\frac{D\\cdot2^{-J}}{(1-\\gamma)}\\cdot\\sqrt{\\frac{4\\kappa^{\\prime}}{M}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\overline{{Q}}-Q^{\\star}\\|_{\\infty}\\cdot\\sqrt{\\frac{8\\kappa^{\\prime}}{L(1-\\gamma)^{2}}}+\\sqrt{\\frac{32\\kappa^{\\prime}}{L(1-\\gamma)^{3}}}+\\frac{2\\sqrt{2}\\kappa^{\\prime}}{3L(1-\\gamma)^{2}}+\\frac{D}{40},\\qquad\\quad(144)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where once again we use the value of $J$ in the last step. ", "page_idx": 44}, {"type": "text", "text": "C.3.3 Proof of Lemma 9 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "From the iterative update rule in (123), for any agent $m$ wehave, ", "page_idx": 44}, {"type": "equation", "text": "$$\nQ_{i-\\frac{1}{2}}^{m}-Q_{i-1}=\\eta(\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{i-1})-Q_{i-1})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n=\\eta(\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{i-1})-\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{\\mathcal{H}}^{\\star})+\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{H}(Q_{\\mathcal{H}}^{\\star})+Q_{\\mathcal{H}}^{\\star}-Q_{i-1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q_{i-\\frac{1}{2}}^{m}-Q_{i-1}\\|_{\\infty}\\leq\\eta\\left(\\|\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{i-1})-\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{\\mathcal{H}}^{\\star})\\|_{\\infty}+\\|\\widehat{\\mathcal{H}}_{i-1}^{(m)}(Q_{\\mathcal{H}}^{\\star})-\\mathcal{H}(Q_{\\mathcal{H}}^{\\star})\\|_{\\infty}+\\|Q_{\\mathcal{H}}^{\\star}-Q_{i-1}\\|_{\\infty}\\right)}\\\\ {\\leq\\eta\\left(\\gamma\\|Q_{i-1}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+2\\gamma\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+\\|Q_{\\mathcal{H}}^{\\star}-Q_{i-1}\\|_{\\infty}\\right)}\\\\ {=\\eta\\left((1+\\gamma)\\|Q_{i-1}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}+2\\gamma\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\right)}\\\\ {\\leq\\eta\\|\\overline{{Q}}-Q_{\\mathcal{H}}^{\\star}\\|_{\\infty}\\left(\\frac{7}{6}\\cdot(1+\\gamma)+2\\gamma\\right)+\\frac{\\eta D\\left(1+\\gamma\\right)}{70},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "holds with probability 1 - 2kI: . Here,the second inequality follows from (131) and (132), The last step in the above relation foliows from (135) evaluated at a general value of $i$ and the prescribed value of J. By a union bound argument, the above relation holds for all with probability at least - k\u00b7 ", "page_idx": 45}, {"type": "text", "text": "D  Numerical Experiments ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we corroborate our theoretical results through simulations. For the simulations, we consider an MDP with 3 states and two actions, i.e., $\\mathcal{S}=\\bar{\\{}0,}1,2\\}$ and $A=\\{0,1\\}$ . The discount parameter is set to $\\gamma\\,=\\,0.9$ . The reward and transition kernel of the MDP is based on the hard instance constructed in Appendix B. Specifically, the reward and transition kernel of state O is given by the expression in Eqn. (14a). Similarly, the reward and transition kernel corresponding to state 1 and 2 are identical and given by Eqns. (14b) and (14c) with $p=0.8$ ", "page_idx": 45}, {"type": "image", "img_path": "6YIpvnkjUK/tmp/02434a8834540331c2a02c2471deb71f654b2766082c05e276a2f048aaba6f96.jpg", "img_caption": ["(a) Sample Complexity "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "6YIpvnkjUK/tmp/bcea53a3aa7afdcb985c38b85dc5e435aacfb48eef37a065c40537df489c407e.jpg", "img_caption": ["(b) Communication Complexity "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 1: Comparison between sample and communication complexities of Fed-DVR-Q and the algorithm Fed-SynQ from Woo et al. [2023]. ", "page_idx": 45}, {"type": "text", "text": "We perform three empirical studies. In the first study, we compare the proposed algorithm Fed-DVRQ to the Fed-SynQ algorithm proposed in Woo et al. [2023]. We consider a Federated Q-learning setting with 5 agents. The parameters for both the algorithms were set to the suggested values in the respective papers._ Both the algorithms were run with $10^{7}$ samples at each agent. For the communication cost of Fed-SynQ we assume that each real number is expressed using 32 bits. In Fig 1a, we plot the error rate of the algorithm as a function of the number of samples used. In Fig. 1b we plot the corresponding communication complexities. As evident from Fig 1a, Fed-DVR-Q achieves a smaller error than Fed-SynQ under the same sample budget. Similarly, as suggested by Fig. 1b, Fed-DVR-Q also requires much less communication (measured in terms of the number of bits transmitted) than Fed-SynQ, demonstrating the effectiveness of the proposed approach and corroborating our theoretical results. ", "page_idx": 45}, {"type": "text", "text": "In the second study, we examine the effect of the number of agents on the sample and communication complexity of Fed-DVR-Q. We vary the number of agents from 5 to 25 in multiples of 5 and record the sample and communication complexity to achieve an error rate of $\\varepsilon=0.03$ .Thesample and communication complexities as a function of number of agents are plotted in Figs. 2a and 2b respectively. The sample complexity decreases as $1/M$ while the communication complexity is independent of the number of agents. This corroborates the linear speedup phenomenon suggested by our theoretical results and the independence between communication complexity and the number of agents. ", "page_idx": 45}, {"type": "image", "img_path": "6YIpvnkjUK/tmp/caaaa8819b8710e3832428cd009e39c11aab799ca44f0b497f632dbdc3d0d619.jpg", "img_caption": ["(a) Sample Complexity "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "6YIpvnkjUK/tmp/8c9ba6909842dc8068f32ab1329827e57a771dce1f92a4f01da4504ea3cc6585.jpg", "img_caption": ["Figure 2: Dependence of sample and communication complexities of Fed-DVR-Q on the number of agents. ", "(b) Communication Complexity "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "6YIpvnkjUK/tmp/e12341780f04164e0cfee4d741f7860ea48b7db0601bf59d6ce5985af154d4f8.jpg", "img_caption": ["Figure 3: Communication complexity of Fed-DVR-Q as a function of effective horizon,i. $\\textstyle{\\frac{1}{1-\\gamma}}$ "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "In the last study, we compare the communication complexity of Fed-DVR-Q as function of the discount parameter $\\gamma$ We consider the same setup as in the first study and vary the values of $\\gamma$ from 0.7 to 0.9 in steps of 0.05. We run the algorithm to achieve an accuracy of $\\varepsilon=0.1$ with parameter choices prescribed in Sec. 4.1.3. We plot the communication cost of Fed-DVR-Q against the effective horizon, i.e., 1- in Fig. 3.As evident from the figure, the communication scales linearly with the effective horizon, which matches the theoretical claim in Theorem 2. ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: In the abstract and introduction, we describe that we study the samplecommunication complexity trade-off in Federated Q-learning and derive both converse and achievability results. In Sec. 3 we derive the lower bound on communication complexity and in Sec. 4 we outline the algorithm that matches the lower bound derived earlier. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We consider an infinite horizon MDP in the tabular setting and derive the results for the class of intermittent communication algorithms. We acknowledge that these assumptions might be restrictive for a certain class of applications and extension to more general settings is discussed as a future direction in Sec. 5. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Both Theorem 1 and 2 clearly state all assumptions used in the statement of main result. The proofs for both the theorems can be found in the appendix. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We have a section with numerical experiments in Appendix D. The section contains all relevant details of our implementation to reproduce the results. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 48}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not have associated code or data. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The relevant details can be found in Appendix D Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The error bars associated with the plots are small and hence we omit them. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The empirical studies require no specific compute resources can be easily completed on a regular laptop. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We have read the NeurIPs Code of Ethics and the paper conforms to the NeurIPS Code of Ethics. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper is concerned with foundational research and is theoretical in nature with no direct societal impact. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper is theoretical is nature and does not involve release of data or code and hence poses no such risks. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not use any existing assets. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not release any new assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve any crowdsourcing. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 52}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]