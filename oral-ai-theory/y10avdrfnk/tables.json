[{"figure_path": "y10avdRFNK/tables/tables_5_1.jpg", "caption": "Table 1: Per-epoch complexity (in FLOPs) and per-particle minimum number of sequential operations (maximum parallelization) for the JKOnet and JKOnet* model families (we refer to the linear parametrization of our model with JKOnet). Here, T is the length of the population trajectory, N the number of particles in the snapshots of the population (assumed constant), d is the dimensionality of the system, n is the number of features for the linear parametrization, D, \u025b, TeacherForcing (TF) are JKOnet parameters: the number of inner operations (which may or not be constant), the accuracy required for the Sinkhorn algorithm, and a training modality (see [9] for details), respectively. MG stands for the Monge gap regularization [51].", "description": "This table compares the computational complexity of different models in terms of FLOPS per epoch and sequential operations per particle.  It breaks down the complexity based on different parameters such as trajectory length, number of particles, dimensionality, and number of features in the linear parametrization. It also notes the ability of each model to learn different energy components of the diffusion process (potential, interaction, and internal energy).", "section": "3 Learning diffusion at lightspeed"}, {"figure_path": "y10avdRFNK/tables/tables_9_1.jpg", "caption": "Table 1: Per-epoch complexity (in FLOPs) and per-particle minimum number of sequential operations (maximum parallelization) for the JKOnet and JKOnet* model families (we refer to the linear parametrization of our model with JKOnet). Here, T is the length of the population trajectory, N the number of particles in the snapshots of the population (assumed constant), d is the dimensionality of the system, n is the number of features for the linear parametrization, D, \u025b, TeacherForcing (TF) are JKOnet parameters: the number of inner operations (which may or not be constant), the accuracy required for the Sinkhorn algorithm, and a training modality (see [9] for details), respectively. MG stands for the Monge gap regularization [51].", "description": "This table compares the computational complexity of different models for learning diffusion processes.  It breaks down the FLOPS per epoch and sequential operations per particle, highlighting the impact of model features, training parameters, and algorithm choices on computational efficiency.", "section": "3 Learning diffusion at lightspeed"}]