[{"figure_path": "y10avdRFNK/figures/figures_1_1.jpg", "caption": "Figure 1: Given a sequence of snapshots (\u03bc\u03bf, ..., \u03bc\u03c4) of a population of particles undergoing diffusion, we want to find the parameters \u03b8 of the parametrized energy function J\u03b8 that best explains the particles evolution. Given \u03b8, the effects mismatch is the Wasserstein distance between the observed trajectory and the predicted trajectory obtained iteratively solving the JKO step with J\u03b8.", "description": "The figure illustrates the core idea of JKOnet*.  Given a sequence of snapshots representing the evolution of a population of particles undergoing diffusion, the goal is to learn the parameters of an energy function that best describes this evolution.  The figure shows how the model minimizes the Wasserstein distance between the observed particle trajectory and the trajectory predicted by iteratively solving the JKO (Jordan-Kinderlehrer-Otto) scheme using the learned energy function.  The mismatch, visually depicted by the different arrow lengths, represents the difference between the gradients of the true and estimated energy functions. Minimizing this mismatch is the key objective of the model.", "section": "3 Learning diffusion at lightspeed"}, {"figure_path": "y10avdRFNK/figures/figures_6_1.jpg", "caption": "Figure 2: Level curves of the true (green-colored) and estimated (blue-colored) potentials (31), (33), (36) and (37), see Appendix F. See also Figure 6 in Appendix A.", "description": "This figure displays the level curves of four different potential functions (Styblinski-Tang, Flowers, Ishigami, and Friedman) used in the experiments.  It shows both the true potentials (in green) and the potentials estimated by the JKOnet* model (in blue). This visual comparison helps to assess the accuracy of the model in learning the underlying energy functionals.", "section": "4 Experiments"}, {"figure_path": "y10avdRFNK/figures/figures_7_1.jpg", "caption": "Figure 3: Numerical results of Section 4.1. The scatter plot displays points (xi, yi) where xi indexes the potentials in Appendix F and yi are the errors (EMD, normalized so that the maximum error among all models and all potentials is 1) obtained with the different models. We mark with NaN each method that has diverged during training. The plot on the bottom-left shows the EMD error trajectory during training (normalized such that 0 and 1 are the minimum and maximum EMD), averaged over all the experiments. The shaded area represents the standard deviation. The box plot analyses the time per epoch required by each method. The statistics are across all epochs and all potential energies.", "description": "The figure presents a comparative analysis of different models (JKOnet*, JKOnet variants, and baselines) in learning diffusion processes.  The scatter plot visualizes the EMD (Earth Mover's Distance) error for each model on different potential energy functions, with missing values (NaN) indicating divergence. The line plot illustrates the EMD error's convergence behavior over epochs, highlighting the training efficiency. Finally, the boxplot provides a comparison of the time per epoch for each model.", "section": "4.1 Training at lightspeed"}, {"figure_path": "y10avdRFNK/figures/figures_8_1.jpg", "caption": "Figure 4: Numerical results of Section 4.2, reported in full in Figure 7 in Appendix A. The colors represent the EMD error, which appears to scale sublinearly with the dimension d.", "description": "This figure shows the Earth Mover's Distance (EMD) error for different numbers of particles and dimensions. The color intensity represents the EMD, with darker colors indicating higher errors. The results suggest that the EMD scales sublinearly with the dimension d, meaning the error does not increase proportionally with the dimension. This is a key finding from the scaling laws experiment in Section 4.2, demonstrating the effectiveness of JKOnet* in high-dimensional settings.", "section": "4.2 Scaling laws"}, {"figure_path": "y10avdRFNK/figures/figures_8_2.jpg", "caption": "Figure 5: Visualizations of Section 4.4. The top row shows the two principal components of the scRNA-seq data, ground truth (green, days 1-3, 6-9, 12-15, 18-21, 24-27) and interpolated (blue, days 4-5, 10-11, 16-17, 22-23). The bottom row displays the estimated potential level curves over time. The bottom left plot superimposes the same three level curves for days 1-3 (solid), 12-15 (dashed), and 24-27 (dashed with larger spaces) to highlight the time-dependency.", "description": "This figure visualizes the results from Section 4.4, which focuses on applying the JKOnet* model to single-cell RNA sequencing (scRNA-seq) data to predict cellular processes. The top row displays the first two principal components of the scRNA-seq data, showing both the ground truth (green) and the interpolated predictions (blue) for different time points.  The bottom row shows the estimated potential energy level curves over time, providing a visual representation of the energy landscape that drives the cellular processes. The bottom-left subplot highlights the time dependency of the potential energy level curves by superimposing those from three different time points.", "section": "4.4 Learning single-cell diffusion dynamics"}, {"figure_path": "y10avdRFNK/figures/figures_8_3.jpg", "caption": "Figure 3: Numerical results of Section 4.1. The scatter plot displays points (xi, yi) where xi indexes the potentials in Appendix F and yi are the errors (EMD, normalized so that the maximum error among all models and all potentials is 1) obtained with the different models. We mark with NaN each method that has diverged during training. The plot on the bottom-left shows the EMD error trajectory during training (normalized such that 0 and 1 are the minimum and maximum EMD), averaged over all the experiments. The shaded area represents the standard deviation. The box plot analyses the time per epoch required by each method. The statistics are across all epochs and all potential energies.", "description": "This figure presents a comparison of different models' performance in learning potential energy functions. The scatter plot shows the normalized EMD error for various potential functions, highlighting JKOnet*'s superior accuracy and the time per epoch for each model, demonstrating JKOnet*'s efficiency. The bottom left plot displays the EMD error trajectory over training epochs for a more detailed analysis of model convergence.", "section": "4.1 Training at lightspeed"}, {"figure_path": "y10avdRFNK/figures/figures_14_1.jpg", "caption": "Figure 6: Level curves of the true (green-colored) and estimated (blue-colored) potentials in Appendix F, from top-left to bottom-right, row-by-row.", "description": "This figure shows the level curves of the true and estimated potentials for different test functions described in Appendix F. The true potential is represented in green, and the estimated potential (obtained using JKOnet*) is in blue.  Each row shows a different test function, illustrating how the model performs in different scenarios.", "section": "A Eye candies"}, {"figure_path": "y10avdRFNK/figures/figures_15_1.jpg", "caption": "Figure 7: Additional numerical results for Section 4.2. Each heat-map corresponds to a functional in Appendix F, from top-left to bottom-right, row-by-row. The x-axis corresponds to the dimension and the y-axis corresponds to the number of particles. The colors represent the EMD error. Thus, a method that scales well to high-dimensional settings should display a relatively stable color along the rows: the error is related to the norm and, thus, is linear in the dimension d; here, the growth is sublinear.", "description": "This figure displays the results of an experiment testing the scalability of the proposed JKOnet* method.  The heatmaps show the Earth Mover's Distance (EMD) error for different numbers of particles and dimensions.  Lower EMD indicates better performance. The results suggest sublinear scaling of the EMD error with respect to the dimension, indicating good scalability of JKOnet* for high-dimensional problems.", "section": "4.2 Scaling laws"}, {"figure_path": "y10avdRFNK/figures/figures_16_1.jpg", "caption": "Figure 8: Comparisons between implicit and explicit losses and predictions for time-varying potentials for 3 different particles trajectories. From left to right: explicit loss (16) - explicit prediction (13), explicit loss (16) - explicit prediction (13) with more observations, implicit loss (17) - explicit prediction (13), implicit loss (17) - implicit prediction (15).", "description": "This figure compares the performance of implicit and explicit prediction schemes for time-varying potentials in a diffusion process.  It shows four sets of trajectory plots, each representing a different combination of loss function (implicit or explicit) and prediction method (implicit or explicit). The plots illustrate how different methods and parameters affect the accuracy of predictions.", "section": "4.4 Learning single-cell diffusion dynamics"}, {"figure_path": "y10avdRFNK/figures/figures_17_1.jpg", "caption": "Figure 9: Data pipeline for JKOnet*.", "description": "The figure shows the data pipeline for the JKOnet* model. It starts with a measurement system that provides snapshots of the population data at different time steps. These snapshots are then used to compute the optimal couplings between consecutive snapshots, and to fit the densities of each snapshot. The resulting data, which consists of the snapshots, couplings, and densities, is then used to train the JKOnet* model.", "section": "C.1 Data pre-processing"}, {"figure_path": "y10avdRFNK/figures/figures_18_1.jpg", "caption": "Figure 3: Numerical results of Section 4.1. The scatter plot displays points (xi, yi) where xi indexes the potentials in Appendix F and yi are the errors (EMD, normalized so that the maximum error among all models and all potentials is 1) obtained with the different models. We mark with NaN each method that has diverged during training. The plot on the bottom-left shows the EMD error trajectory during training (normalized such that 0 and 1 are the minimum and maximum EMD), averaged over all the experiments. The shaded area represents the standard deviation. The box plot analyses the time per epoch required by each method. The statistics are across all epochs and all potential energies.", "description": "The figure presents a comparison of different models' performance in learning diffusion processes using various potential functions. The scatter plot shows the normalized error (EMD) for each method and potential function, highlighting the superior performance of JKOnet*. The bottom-left plot displays the EMD error trajectory over training epochs for a better understanding of the convergence speed. The box plot visualizes the computation time for each method, confirming the efficiency of JKOnet*. ", "section": "4.1 Training at lightspeed"}, {"figure_path": "y10avdRFNK/figures/figures_28_1.jpg", "caption": "Figure 3: Numerical results of Section 4.1. The scatter plot displays points (xi, yi) where xi indexes the potentials in Appendix F and yi are the errors (EMD, normalized so that the maximum error among all models and all potentials is 1) obtained with the different models. We mark with NaN each method that has diverged during training. The plot on the bottom-left shows the EMD error trajectory during training (normalized such that 0 and 1 are the minimum and maximum EMD), averaged over all the experiments. The shaded area represents the standard deviation. The box plot analyses the time per epoch required by each method. The statistics are across all epochs and all potential energies.", "description": "This figure presents the numerical results of Section 4.1 of the paper.  It compares different models' performance in learning potential energy functions. The scatter plot visualizes the normalized EMD errors, indicating the accuracy of each model. The bottom-left plot shows the EMD error trajectories during training, illustrating convergence speed. Finally, a box plot compares the computational time per epoch for each method.", "section": "4.1 Training at lightspeed"}]