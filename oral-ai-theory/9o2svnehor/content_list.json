[{"type": "text", "text": "Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Raffaele Paolino\u2217,1,2 Sohir Maskey\u2217,1 Pascal Welke3 Gitta Kutyniok1,2,4,5 ", "page_idx": 0}, {"type": "text", "text": "1Department of Mathematics, LMU Munich 2Munich Center for Machine Learning (MCML) 3Faculty of Computer Science, TU Wien 4Institute for Robotics and Mechatronics, DLR-German Aerospace Center 5Department of Physics and Technology, University of Troms\u00f8 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce $r$ -loopy Weisfeiler-Leman $\\scriptstyle{r-\\ell\\mathbf{WL}})$ , a novel hierarchy of graph isomorphism tests and a corresponding GNN framework, $r$ -\u2113MPNN, that can count cycles up to length $r{+}2$ . Most notably, we show that $r$ -\u2113WL can count homomorphisms of cactus graphs. This extends 1-WL, which can only count homomorphisms of trees and, in fact, we prove that $r$ -\u2113WL is incomparable to $k$ -WL for any fixed $k$ . We empirically validate the expressive and counting power of $r$ -\u2113MPNN on several synthetic datasets and demonstrate the scalability and strong performance on various real-world datasets, particularly on sparse graphs. Our code is available on GitHub. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) (Scarselli et al., 2009; Bronstein et al., 2017) have become a prevalent architecture for processing graph-structured data, contributing significantly to various applied sciences, such as drug discovery (Stokes et al., 2020), recommender systems (Fan et al., 2019), and fake news detection (Monti et al., 2019). ", "page_idx": 0}, {"type": "text", "text": "Among various architectures, Message Passing Neural Networks (MPNNs) (Gilmer et al., 2017) are widely used in practice, as they encompass only local computation, leading to fast and scalable models. Despite their success, the representational power of MPNNs is bounded by the WeisfeilerLeman (WL) test, a classical algorithm for graph isomorphism testing (Xu et al., 2019; Morris et al., 2019). This limitation hinders MPNNs from recognizing basic substructures like cycles (Chen et al., 2020). However, specific substructures can be crucial in many applications. For example, in organic chemistry, the presence of cycles can impact various chemical properties of the underlying molecules (Deshpande et al., 2002; Koyut\u00fcrk et al., 2004). Therefore, it is crucial to investigate whether GNNs can count certain substructures and to design architectures that surpass the limited power of MPNNs. ", "page_idx": 0}, {"type": "text", "text": "Several models have been proposed to surpass the limitations of WL. Many of these models draw inspiration from higher-order WL variants (Morris et al., 2019), enabling them to count a broader range of substructures. For instance, GNNs emulating 3-WL can count cycles up to length 7. However, this increased expressivity comes at a high computational cost, as 3-WL does not respect the sparsity of real-world graphs, posing serious scalability issues. Hence, there is a critical need to design expressive GNNs that respect the inherent sparsity of real-world graphs (Morris et al., 2023). ", "page_idx": 0}, {"type": "image", "img_path": "9O2sVnEHor/tmp/0c2cac7be41849dbab9687744f2c681e901224c84530062b86e52a521a5e20b5.jpg", "img_caption": ["Figure 1: Visual depiction of $r$ -\u2113GIN: During preprocessing, we calculate the path neighborhoods $\\mathcal{N}_{r}(v)$ for each node $v$ in the graph $G$ . Paths of varying lengths are processed separately using simple GINs, and their embeddings are pooled to obtain the final graph embedding. The forward complexity scales linearly with the sizes of $\\bar{\\mathcal{N}}_{r}(v)$ , enabling efficient computation on sparse graphs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Main Contributions. We introduce a novel class of color refinement algorithms called $r$ -loopy Weisfeiler-Leman test (r-\u2113WL) and a corresponding class of GNNs named $r$ -loopy Graph Isomorphism Networks ( $r$ -\u2113GIN). The key idea is to collect messages not only from neighboring nodes but also from the paths connecting any two distinct neighboring nodes, as illustrated in Figure 1. This approach enhances the resulting GNNs\u2019 expressivity beyond 1-WL. In particular, $r$ -\u2113WL can count cycles up to length $r{+}2$ , even surpassing the $k$ -WL hierarchy. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we prove that $r$ -\u2113WL can homomorphism-count any cactus graph with cycles up to length $r{+}2$ . Cactus graphs are valuable due to their structural properties and simplicity, making them useful for modeling in areas such as electrical engineering (Nishi et al., 1986) and computational biology (Paten et al., 2011). For instance, aromatic compounds often form cactus graphs, where the molecular core, usually a cycle, is coonected to functional groups (e.g., carboxyl groups) that can significantly impact the properties of the molecule. Thus, the ability to homomorphism-count cactus graphs can enhance model performance, and it allows us to compare the expressive power of $r$ -\u2113WL with other popular GNNs in a quantitative manner (Barcel\u00f3 et al., 2021; B. Zhang et al., 2024). Specifically, we show that $r$ -\u2113WL is more expressive than GNNs that include explicit homomorphism counts of cycle graphs, known as $\\mathcal{F}$ -Hom-GNNs (Barcel\u00f3 et al., 2021). Additionally, 1-\u2113WL can already separate infinitely many graphs that Subgraph $k$ -GNNs (Frasca et al., 2022; Qian et al., 2022) cannot (see, e.g., Figure 7). The higher expressivity, paired with the local computations, highlights the enhanced potential of $r$ -\u2113GIN, showing its competitive performance and the efficiency of its forward pass on real-world datasets, see Section 7. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The notion of expressivity in standard neural networks is linked to the ability to approximate any continuous function (Cybenko, 1989; Hornik et al., 1989). In contrast, GNN expressivity is measured by the ability to distinguish non-isomorphic graphs. According to the Stone-Weierstrass theorem, these criteria are equivalent (Chen et al., 2019; Dasoulas et al., 2021): a network that can distinguish all graphs can approximate any continuous function. Therefore, research often focuses on determining which graphs a GNN can distinguish (Morris et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "Xu et al. (2019) and Morris et al. (2019) proved that the expressive power of MPNNs is bounded by 1- WL. Subsequent works (Maron et al., 2018; Morris et al., 2019, 2020) introduced higher-order GNNs that have the same expressive power as $k$ -WL or its local variants (Geerts et al., 2022). Although these networks are universal (Maron et al., 2019b; Keriven et al., 2019), their exponential time and space complexity in $k$ renders them impractical. Abboud et al. (2022) proposed $k$ -hop GNNs which aggregate information from $k$ -hop neighbors, thus, enhancing expressivity beyond 1-WL but within 3-WL (Feng et al., 2022). Michel et al. (2023) and Graziani et al. (2024) construct GNNs that process paths emanating from each node to overcome 1-WL. Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) surpass 1-WL by decomposing the initial input graph into a bag of subgraphs. However, subgraph GNNs are upper-bounded by 3-WL (Frasca et al., 2022). A different line of work leverages positional encoding through unique node identifiers (Vignac et al., 2020), random features (Abboud et al., 2021; Sato et al., 2021) or eigenvectors (Lim et al., 2022; Maskey et al., 2022) to augment the expressive power of MPNNs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "While the predominant approach for gauging the expressive power of GNNs is within the $k$ -WL hierarchy, such a measure is inherently qualitative, as it cannot shed light on substructures a particular GNN can encode. Lov\u00e1sz (1967) showed that homomorphism counts is a complete graph invariant, meaning two graphs are isomorphic if and only if their homomorphism counts are identical. Building on this result, B. Zhang et al. (2024) advocate for homomorphism-count as a quantitative measure of expressivity, as GNN architectures can homomorphism-count particular families of motifs. Tinhofer (1986, 1991) established that 1-WL is equivalent to counting homomorphisms from graphs with tree-width one, while Dell et al. (2018) proved the equivalence between $k$ -WL and the ability to count homomorphisms from graphs with tree-width $k$ . Nguyen et al. (2020), Barcel\u00f3 et al. (2021), Welke et al. (2023), and Jin et al. (2024) used homomorphism counts to develop expressive GNNs. ", "page_idx": 2}, {"type": "text", "text": "Manually augmenting node features with homomorphism counts can be disadvantageous as performance depends on the chosen substructures. This can be alleviated by designing domain-agnostic GNNs that can learn structural information suitable for the task at hand. For instance, higher-order GNNs can count a large class of substructures as homomorphisms (B. Zhang et al., 2024), but they suffer from scalability issues. We propose $r$ -\u2113WL and $r$ -\u2113GIN, which can count homomorphisms of cactus graphs without adding explicit substructure counts. Our method is scalable to large datasets, particularly when the graphs in these datasets are sparse. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{G}$ be the set of all simple and undirected graphs, and let $G\\in{\\mathcal{G}}$ . We denote the set of nodes by $V(G)$ and the set of edges by $E(G)$ . The direct neighborhood of a node $v\\in V(G)$ is defined as ${\\mathcal{N}}(v):=\\{u\\in V(G)\\mid\\,\\{v,u\\}\\in E(G)\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1. Let $F,G\\in{\\mathcal{G}}$ . A homomorphism from $F$ to $G$ is a map $h:V(F)\\to V(G)$ such that $\\{u,v\\}\\in E(F)$ implies $\\{h(u),h(v)\\}\\in E(G)$ . $A$ subgraph isomorphism is an injective homomorphism. ", "page_idx": 2}, {"type": "text", "text": "Intuitively, a homomorphism from $F$ to $G$ is an edge-preserving map. A subgraph isomorphism ensures that $F$ actually occurs as a subgraph of $G$ . Consequently, it also maps distinct edges to distinct edges. A visual explanation can be found in Figure 5. We denote by ${\\mathrm{Hom}}(F,G)$ the set of homomorphisms from $F$ to $G$ and by $\\mathrm{hom}(F,G)$ its cardinality. Similarly, we denote by $\\operatorname{Sub}(F,G)$ the set of subgraph isomorphisms from $F$ to $G$ and by and $\\operatorname{sub}(F,G)$ its cardinality. ", "page_idx": 2}, {"type": "text", "text": "3.1 Graph Invariants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In order to unify different expressivity measures, we recall the definition of graph invariants. ", "page_idx": 2}, {"type": "text", "text": "Definition 2. Let $P$ be a designated set, referred to as the palette. $A$ graph invariant is a function $\\zeta:\\mathcal{G}\\rightarrow P$ such that $\\zeta(G)\\,=\\,\\zeta(H)$ for all isomorphic pairs $G,H\\,\\in\\,{\\mathcal{G}}$ . $\\zeta$ is $a$ complete graph invariant i ${}^{\\mathfrak{c}}\\zeta(G)\\neq\\zeta(F)$ for all non-isomorphic pairs $G,F\\in{\\mathcal{G}}$ . ", "page_idx": 2}, {"type": "text", "text": "Complete graph invariants have maximal expressive power. However, no polynomial-time algorithm to compute a complete graph invariant is known. To compare the expressive power of different graph invariants, such as graph colorings and GNN architectures, we introduce the following definition. ", "page_idx": 2}, {"type": "text", "text": "Definition 3. Let $\\gamma,\\zeta$ be two graph invariants. We say that $\\gamma$ is more powerful than $\\zeta\\,(\\gamma\\subseteq\\zeta)$ if for every pair $G,H\\in{\\mathcal{G}},$ , $\\gamma(G)=\\gamma(H)$ implies $\\zeta(G)=\\zeta(H)$ . We say that $\\gamma$ is strictly more powerful than $\\zeta\\;i f\\gamma\\subseteq\\zeta$ and there exists a pair $F,G\\in{\\mathcal{G}}$ such that $\\gamma(G)\\neq\\gamma(H)$ and $\\zeta(G)=\\zeta(H)$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Message Passing Neural Networks and Weisfeiler-Leman ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Message passing is an iterative algorithm that updates the colors of each node $v\\in V(G)$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nc^{(t+1)}(v)\\leftarrow f^{(t+1)}\\left(c^{(t)}(v),g^{(t+1)}\\left(\\left\\{\\left\\{c^{(t)}(u)\\ |\\ u\\in\\mathcal{N}(v)\\right\\}\\right\\}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The graph output after $t$ iterations is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nc^{(t)}(G):=h\\left(\\left\\{\\left\\{c^{(t)}(v)\\mid v\\in V(G)\\right\\}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $g^{(t)},h$ are functions on the domain of multisets and $f^{(t)}$ is a function on the domain of tuples. For each $t$ , the colorings $\\boldsymbol{c}^{(t)}$ are graph invariants. When the subsets of nodes with the same colors cannot be further split into different color groups, the algorithm terminates; the stable coloring after convergence is denoted by $c(G)$ . ", "page_idx": 3}, {"type": "text", "text": "Choosing injective functions for all $f^{(t)}$ and setting $g^{(t)}$ and $h$ as the identity function results in 1-WL (Weisfeiler et al., 1968). If $f^{(t)},g^{(t)},h$ are chosen as suitable neural networks, one obtains a Message Passing Neural Network (MPNN). Xu et al. (2019) proved that MPNNs are as powerful as 1-WL if the functions $f^{(t)},g^{(t)}$ , and $h$ are injective on their respective domains. The $k$ -WL algorithms uplift the expressive power of 1-WL by considering interactions between $k$ -tuples of nodes. This results in a hierarchy of strictly more powerful graph invariants (see Appendix B.1 for a formal definition). ", "page_idx": 3}, {"type": "text", "text": "3.3 Homomorphism and Subgraph Counting Expressivity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A more nuanced graph invariant can be built by considering the occurrences of a motif $F$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 4. Let $F\\in{\\mathcal{G}}$ . A graph invariant $\\zeta$ can homomorphism-count $F$ if for all pairs $G,H\\in{\\mathcal{G}}$ $\\zeta(G)=\\zeta(H)$ implies $\\operatorname{hom}(F,G)=\\operatorname{hom}(F,H)$ . By analogy, $\\zeta$ can subgraph-count $F$ if for all pairs $G,H\\in{\\mathcal{G}}$ , $\\zeta(G)=\\zeta(H)$ implies $\\operatorname{sub}(F,G)=\\operatorname{sub}(F,H)$ . ", "page_idx": 3}, {"type": "text", "text": "If $\\mathcal{F}$ is a family of graphs, we say that $\\zeta$ can homomorphism-count $\\mathcal{F}$ if $\\zeta$ can homomorphism-count every $F\\in{\\mathcal{F}}$ ; we denote the vector of homomorphism-count by $\\hom({\\bar{\\mathcal{F}}},G):=(\\hom({\\bar{F}},G))_{F\\in{\\mathcal{F}}}$ . Interpreting $\\operatorname{hom}({\\mathcal{F}},\\cdot)$ as a graph invariant, given by $G\\mapsto\\operatorname{hom}(\\mathcal{F},G)$ , another graph invariant $\\zeta$ can homomorphism-count $\\mathcal{F}$ if and only if $\\zeta\\subseteq\\operatorname{hom}({\\mathcal{F}},\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "The ability of a graph invariant to count homomorphisms is highly relevant because $\\operatorname{hom}(\\mathcal{G},\\cdot)$ is a complete graph invariant. Conversely, if $\\zeta$ is a complete graph invariant, then $\\zeta$ can homomorphismcount all graphs (Lov\u00e1sz, 1967). Additionally, homomorphism-counting serves as a quantitative expressivity measure to compare different WL variants and GNNs, such as $k$ -WL, Subgraph GNNs, and other methods (Lanzinger et al., 2024; B. Zhang et al., 2024), and allows for relating them to our proposed $r$ -\u2113WL variant, as detailed in Corollary 2. ", "page_idx": 3}, {"type": "text", "text": "4 Loopy Weisfeiler-Leman Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce a new graph invariant by enhancing the direct neighborhood of nodes with simple paths between neighbors. ", "page_idx": 3}, {"type": "text", "text": "Definition 5. Let $G\\in{\\mathcal{G}}$ . $A$ simple path of length $r$ is a collection $\\mathbf{p}=\\left\\{p_{i}\\right\\}_{i=1}^{r+1}$ of $r{+}1$ nodes such that $\\left\\{p_{i},p_{i+1}\\right\\}\\in E(G)$ and $i\\neq j\\implies p_{i}\\neq p_{j}$ for every $i,j\\in\\{1,\\ldots,r\\},$ ,. ", "page_idx": 3}, {"type": "text", "text": "Simple paths are the building blocks of $r$ -neighborhoods, which in turn are the backbone of our $r$ -\u2113WL algorithm. The following definition is inspired by (Cantwell et al., 2019; Kirkley et al., 2021). ", "page_idx": 3}, {"type": "text", "text": "Definition 6. Let $G\\in{\\mathcal{G}}$ and $r\\in\\mathbb{N}\\setminus\\{0\\}$ , we define the $r$ -neighborhood $\\mathcal{N}_{r}(v)$ of $v\\in V(G)$ as $\\mathcal{N}_{r}(v):=\\{\\mathbf{p}\\mid\\mathbf{\\upnu}_{\\mathbf{F}}$ simple path of length $\\cdot,\\,p_{1},p_{r+1}\\in\\mathcal{N}(v),v\\notin\\mathbf{p}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "For consistency, we set $\\mathcal{N}_{0}(v):=\\mathcal{N}(v)$ . An example of the construction of $r$ -neighborhood is shown in Figure 2, where different $r.$ - neighborhoods of node $v$ are represented with different colors. ", "page_idx": 3}, {"type": "image", "img_path": "9O2sVnEHor/tmp/c84f0e30d7d524cd210700881d93431dd77ab12687922145890ac4e1d84bcf8c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "We generalize 1-WL in (1) as follows. ", "page_idx": 3}, {"type": "text", "text": "Figure 2: Example of $r$ -neighborhoods. ", "page_idx": 3}, {"type": "text", "text": "Definition 7. We define the $r$ -loopy Weisfeiler-Leman $r$ -\u2113WL) test by the following color update: $c_{r}^{(t+1)}(v)\\leftarrow\\mathrm{HASH}_{r}\\left(c_{r}^{(t)}(v),\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{0}(v)\\right\\}\\right\\},\\ldots,\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{r}(v)\\right\\}\\right\\}\\right),$ (2) where $c_{r}^{(t)}(\\mathbf{p}):=\\left(c_{r}^{(t)}(p_{1}),c_{r}^{(t)}(p_{2}),\\dots,c_{r}^{(t)}(p_{r+1})\\right)$ is the sequence of colors of nodes in the path. ", "page_idx": 3}, {"type": "text", "text": "We denote by $c_{r}^{(t)}(G)$ the final graph output after $t$ iterations of $r$ -\u2113WL, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{r}^{(t)}(G)=\\mathrm{HASH}_{r}\\left(\\left\\{\\left\\{c_{r}^{(t)}(v)\\ |\\ v\\in V(G)\\right\\}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and by $c_{r}(G)$ the stable coloring after convergence. The stable coloring $c_{r}$ serves as graph invariant and will be referred to as $r$ -\u2113WL. ", "page_idx": 4}, {"type": "text", "text": "5 Expressivity of $r$ -\u2113WL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We analyze the expressivity of $r$ -\u2113WL in terms of its ability to distinguish non-isomorphic graphs, subgraph-count, and homomorphism-count motifs. The proofs for all statements are in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "5.1 Isomorphism Expressivity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "It is straightforward to check that 0-\u2113WL corresponds to 1-WL, since $\\mathcal{N}_{0}(v)=\\mathcal{N}(v)$ for all nodes $v$ .   \nHowever, increasing $r$ leads to a strict increase in expressivity. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Let $0\\leq q<r$ . Then, $r$ -\u2113WL is strictly more powerful than $q$ -\u2113WL. In particular, every $r$ -\u2113WL is strictly more powerful than 1-WL. ", "page_idx": 4}, {"type": "text", "text": "This shows that the number of graphs we can distinguish monotonically increases with $r$ . We empirically verify this fact on several synthetic datasets in Section 7. ", "page_idx": 4}, {"type": "text", "text": "5.2 Subgraph Expressivity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recent studies highlight limitations in the ability of certain graph invariants to subgraph-count cycles. For instance, 1-WL cannot subgraph-count cycles (Chen et al., 2020, Theorem 3.3), while 3-WL can only subgraph-count cycles of length up to 7 (Arvind et al., 2020, Theorem 3.5). Similarly, Subgraph GNNs have limited cycle-counting ability (Huang et al., 2022, Proposition 3.1). In contrast, $r$ -\u2113WL can count cycles of arbitrary length, as shown in the following statement. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. For any $r\\geq1$ , $r$ -\u2113WL can subgraph-count all cycles with at most $r+2$ nodes. ", "page_idx": 4}, {"type": "text", "text": "Since 3-WL cannot subgraph-count any cycle with more than 7 nodes, Theorem 1 implies that 6-\u2113WL is not less powerful than 3-WL. This observation generalizes to any $k$ -WL, as shown next. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. Let $k\\,\\in\\,\\mathbb{N}$ . There exists $r\\,\\in\\,\\mathbb{N}$ , such that $r{-}\\ell W L$ is not less powerful than $k$ -WL.   \nSpecifically, $r\\in\\mathcal{O}(k^{2})$ , with k(k2+1)\u22122 for even k and r \u2264 $\\begin{array}{r}{r\\le\\frac{(k+1)^{2}}{2}-2}\\end{array}$ for odd $k$ . ", "page_idx": 4}, {"type": "text", "text": "The $r$ -\u2113WL color refinement algorithm surpasses the limits of the $k$ -WL hierarchy while only using local computation. This is particularly important since already 3-WL is computationally infeasible, whereas our method can scale efficiently to higher orders if the graphs are sparse, which is commonly the case in real-world applications. ", "page_idx": 4}, {"type": "text", "text": "5.3 Homomorphism Expressivity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The following section unveils a close connection between the expressivity of $r$ -\u2113WL and cactus graphs (Harary et al., 1953), a significant class between trees and graphs with tree-width 2. ", "page_idx": 4}, {"type": "text", "text": "Definition 8. A cactus graph is a graph where every edge lies on at most one simple cycle. For $r\\geq2$ , an $r$ -cactus graph is a cactus where every simple cycle has at most r vertices. We denote by M the set of all cactus graphs, and by ${\\mathcal{M}}^{r}$ the set of all $q$ -cactus graphs for $q\\leq r$ . ", "page_idx": 4}, {"type": "text", "text": "Figure 6 shows two examples of cactus graphs. From the expressivity perspective, the ability to homomorphism-count cactus graphs establishes a lower bound strictly between the homomorphismcounting capabilities of 1-WL and 3-WL (Neuen, 2024), as cactus graphs are a strict superset of all trees and a strict subset of all graphs of treewidth two. With this in mind, we are now ready to present our significant result on the homomorphism expressivity of our $r$ -\u2113WL algorithm. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $r\\geq0$ . Then, $r$ -\u2113WL can homomorphism-count $\\mathcal{M}^{r+2}$ . ", "page_idx": 4}, {"type": "text", "text": "We refer to Appendix G for a detailed proof of Theorem 2, which is fairly involved and requires defining canonical tree decompositions of cactus graphs and unfolding trees of $r$ -\u2113WL. Demonstrating their strong connection, we then follow the approach in (Dell et al., 2018; B. Zhang et al., 2024) to decompose homomorphism counts of cactus graphs. In fact, we prove a more general result, showing that $r$ -\u2113WL can count all fan-cactus graphs, see Appendix G for more details. ", "page_idx": 5}, {"type": "text", "text": "The class $\\mathcal{M}^{2}$ contains only forests; hence, Theorem 2 implies the standard results on the ability of 1-WL to count forests. Since forests are the only class of graphs 1-WL can count, Theorem 2 implies that $r$ -\u2113WL is always strictly more powerful than 1-WL, corroborating the claim in Proposition 1. ", "page_idx": 5}, {"type": "text", "text": "The implications of Theorem 2 are profound: it establishes that $r$ -\u2113WL can homomorphism-count a large class of graphs. Specifically, Theorem 2 provides a quantitative expressivity measure that enables comparison of $r$ -\u2113WL\u2019s expressivity with other WL variants and GNNs. This comparison is achieved by examining the range of graphs that $r$ -\u2113WL can homomorphism-count against those countable by other models, as detailed in works by Barcel\u00f3 et al. (2021) and B. Zhang et al. (2024). For instance, B. Zhang et al. (2024) showed that Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) are limited to homomorphism-count graphs with end-point shared NED. Hence, Subgraph GNNs can not homomorphism-count $F=\\{\\overleftarrow{\\cdots}\\partial\\}$ , while 1-\u2113WL can. Based on this, we can identify pairs of graphs that 1-\u2113WL can distinguish but Subgraph GNNs cannot. We summarize these and other implications of Theorem 2 in the following corollary. ", "page_idx": 5}, {"type": "text", "text": "i) $r$ -\u2113WL is more powerful than $\\mathcal{F}$ -Hom-GNNs, where $\\mathcal{F}=\\{C_{3},\\ldots,C_{r+2}\\}$ .   \nii) 1-\u2113WL is not less powerful than Subgraph GNNs. In particular, any $r{-}\\ell W L$ can separate infinitely many graphs that Subgraph GNNs fail to distinguish.   \niii) For any $k>0$ , 1-\u2113WL is not less powerful than Subgraph $k$ -GNNs. In particular, any $r$ -\u2113WL can separate infinitely many graphs that Subgraph $k$ -GNNs fail to distinguish.   \niv) $r$ -\u2113WL can subgraph-count all graphs $F$ such that $\\mathrm{spasm}(F)\\,\\,\\,\\subset\\,\\,\\,\\mathcal{M}^{r+2}$ , where $\\operatorname{spasm}(F):=\\{\\bar{H}\\in\\mathcal{G}\\mid\\exists$ surjective $h\\in{\\mathrm{Hom}}(F,H)\\}$ . In particular, $i f1\\le r\\le4$ , then $r$ -\u2113WL can subgraph-count all paths up to length $r+3$ . ", "page_idx": 5}, {"type": "text", "text": "A detailed explanation of Subgraph ( $k$ -)GNNs, $\\mathcal{F}$ -Hom-GNNs, along with the proofs of Corollary 2, can be found in Appendix H. Finally, we note that Theorem 2 states a loose lower bound on the homomorphism expressivity of $r$ -\u2113WL. This observation opens the avenue for future research to explore tight lower bounds, or upper bounds, on the homomorphism expressivity of $r$ -\u2113WL. ", "page_idx": 5}, {"type": "text", "text": "6 Loopy Message Passing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we build a GNN emulating $r$ -\u2113WL. ", "page_idx": 5}, {"type": "text", "text": "Definition 9. For $t\\in\\{0,\\ldots,T-1\\}$ and $k\\in\\{0,\\ldots,r\\}$ , $r$ -\u2113MPNN applies the following message, update and readout functions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{k}^{(t+1)}(v)=f_{k}^{(t+1)}\\left(\\left\\{\\left\\{c_{k}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{k}(v)\\right\\}\\right\\}\\right),}\\\\ &{\\ c_{r}^{(t+1)}(v)=g^{(t+1)}\\left(c_{r}^{(t)}(v),\\,m_{0}^{(t+1)}(v),\\ldots,m_{r}^{(t+1)}(v)\\right),}\\\\ &{y e r\\,c_{r}^{(T)}(G)=h\\left(\\left\\{\\left\\{c_{r}^{(T)}(v)\\mid v\\in V(G)\\right\\}\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and final readout la ", "page_idx": 5}, {"type": "text", "text": "In the following statement, we link the expressive power of $r$ -\u2113MPNN and $r$ -\u2113WL ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. For fixed $t,r\\geq0,$ , t iterations of $r{-}\\ell W L$ are more powerful than $r$ -\u2113MPNN with $t$ layers.   \nConversely, $r$ -\u2113MPNN is more powerful than $r$ -\u2113WL if the functions $\\boldsymbol{f}^{(t)},\\boldsymbol{g}^{(t)}$ in (3) are injective. ", "page_idx": 5}, {"type": "text", "text": "The previous result derives conditions under which $r$ -\u2113MPNN is as expressive as $r$ -\u2113WL. To implement $r$ -\u2113MPNN in practice, we choose suitable neural layers for f k(t ), g(t), and h in Definition 9. As a consequence of ( $\\mathrm{Xu}$ et al., 2019, Lemma 5), the aggregation function in (3) can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{k}^{(t+1)}\\left(\\left\\{\\left\\{c_{k}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{k}(v)\\right\\}\\right\\}\\right):=f\\left(\\sum_{\\mathbf{p}\\in\\mathcal{N}_{k}(v)}g(\\mathbf{p})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for suitable functions $f,g$ . Since 1-WL is injective on forests (Arvind et al., 2015), hence on paths, and since GIN can approximate 1-WL (Xu et al., 2019), we choose $f=\\mathrm{MLP}$ and $g=\\mathrm{GIN}$ . Hence, $r$ -\u2113GIN is defined as an $r$ -\u2113MPNN that updates node features via ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{r}^{(t+1)}(v):=\\mathrm{MLP}\\left(x_{r}^{(t)}(v)+(1+\\varepsilon_{0})\\sum_{u\\in\\mathcal{N}_{0}(v)}x_{r}^{(t)}(u)+\\sum_{k=1}^{r}(1+\\varepsilon_{k})\\sum_{\\mathbf{p}\\in\\mathcal{N}_{k}(v)}\\mathrm{GIN}_{k}(\\mathbf{p})\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To reduce the number of learnable parameters in (4), the $\\mathrm{GIN}_{k}$ can be shared among all $k$ . Nothing prevents from choosing a different path-processing layer; we opted for GIN because it is simple yet maximally expressive on paths. We refer to Figure 1 for a visual depiction of $r$ -\u2113GIN. ", "page_idx": 6}, {"type": "text", "text": "Computational Complexity The complexity of $r$ -\u2113GIN is $\\begin{array}{r}{\\mathcal{O}(|E|+\\sum_{v\\in V(G)}\\sum_{k=1}^{r}2k|\\mathcal{N}_{k}(v)|)}\\end{array}$ . The former addend is the standard message complexity, while the latter arises from applying GIN to paths of length $k\\le r$ . This implies that our model\u2019s complexity scales linearly with the number of edges, and with the number of paths within $\\mathcal{N}_{k}(v)$ . The number of such paths is typically less than the number of edges. For example, ZINC12K has overall 598K edges while only containing 374K paths in $\\mathcal{N}_{r}(v)$ for $1\\le r\\le5$ . Hence, the runtime overhead is small in practice. Compared to 3-WLGNN (Dwivedi et al., 2022a), which has the same cycle-counting expressivity, our model requires ca. 10 seconds/epoch while 3-WLGNN takes ca. 329.49 seconds/epoch on ZINC12K. Our runtime is comparable to that of GAT, MoNet, or GatedGCN (see Table 10 for a thorough comparison). ", "page_idx": 6}, {"type": "text", "text": "Comparison with (Michel et al., 2023) PathNN updates node features by computing all possible paths starting from each node. In contrast, our approach selects paths between distinct neighbors, potentially resulting in fewer paths. For instance, a tree\u2019s $r$ -neighborhoods $(r\\geq1)$ ) are empty, while counts of paths between nodes are quadratic. Notably, Michel et al. (2023) do not explore the impact of increasing the path length on architecture expressiveness, a consideration we address (see, e.g., Proposition 1 and Corollary 1). Another significant contribution of our work, which we assert does not hold (at least not trivially) for PathNN, is the provable ability to subgraph-count cycle graphs (see, e.g., Theorem 1) and homomorphism-count cactus graphs (see, e.g., Theorem 2). ", "page_idx": 6}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "All instructions to reproduce the experiments are available on GitHub (MIT license). Additional information on the training and test details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Expressive Power. We showcase the expressive power of $r$ -\u2113GIN on synthetic datasets: ", "page_idx": 6}, {"type": "text", "text": "\u2022 GRAPH8C (Balcilar et al., 2021) comprises 11 117 connected non-isomorphic simple graphs on 8 nodes; 312 pairs are 1-WL equivalent but none is 3-WL equivalent. \u2022 EXP_ISO (Abboud et al., 2022) comprises 600 pairs of 1-WL equivalent graphs. ", "page_idx": 6}, {"type": "image", "img_path": "9O2sVnEHor/tmp/1150e96d731ba577814b5629b06436ac80bf27cb2a9639543665ea9e0d4a3532.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Indistinguishable pairs at initialization, symlog scale. For GRAPH8C and EXP_ISO, we report the proportion of indistinguished pairs: 2 graphs are deemed indistinguishable if the ${\\mathrm{L}}^{1}$ distance of their embeddings is less than $10^{\\bar{-}3}$ . For COSPECTRAL10 and SR16622, we report the $L^{1}$ distance between graph embeddings. We report the mean and standard deviation over 100 seeds. ", "page_idx": 6}, {"type": "image", "img_path": "9O2sVnEHor/tmp/e81641eac15877bc71787aa7c7605ac538c47feae0cefed50c08339395bbb5f5.jpg", "img_caption": ["Figure 4: Test accuracy on synthetic classification task: (left) shared and (right) non-shared weights. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 COSPECTRAL10 (van Dam et al., 2003): the dataset comprises two cospectral 4-regular non-isomorphic graphs on 10 nodes which are 1-WL equivalent (see, e.g., Figure 8a). \u2022 SR16622 (Michel et al., 2023) comprises two strongly regular graphs on 16 nodes, namely the Shrikhande and the $4\\!\\times\\!4$ rook graph, which are 3-WL equivalent (see, e.g., Figure 8b). ", "page_idx": 7}, {"type": "text", "text": "The goal is to check whether the model can distinguish non-isomorphic pairs at initialization. The results are shown in Figure 3. ", "page_idx": 7}, {"type": "text", "text": "Additionally, Table 1 shows the performance on BREC (Wang et al., 2024), which includes 400 pairs of non-isomorphic graphs ranging from 1-WL to 4-WL equivalent. The baselines include PPGN, which is 3-WL equivalent and can count up to 7-cycles and homomorphism-count all graphs of tree-width 2; NestedGNN which is between 1-WL and 3-WL; GSN which is more powerful than 1-WL but whose expressive power depends on the chosen pattern. ", "page_idx": 7}, {"type": "text", "text": "Finally, Figure 4 reports the performance on synthetic classification tasks: ", "page_idx": 7}, {"type": "text", "text": "\u2022 EXP, CEXP (Abboud et al., 2021) require expressive power beyond 1-WL.   \n\u2022 CSL (Murphy et al., 2019) comprises 150 cycle graphs with skip links (see, e.g., Figure 8c). The task is to predict the length of the skip link. ", "page_idx": 7}, {"type": "text", "text": "Counting Power. Following (B. Zhang et al., 2024), we use the SUBGRAPHCOUNT dataset (Chen et al., 2020) to test the ability to homomorphism- and subgraphs-count exemplary motifs. ", "page_idx": 7}, {"type": "table", "img_path": "9O2sVnEHor/tmp/093fb69f45ff1a73d654a94dcbce0e35ef9ec08906504c4fc27566a04db58c7b.jpg", "table_caption": ["Table 1: Num. of distinguished pairs (\u2191). Results from (Wang et al., 2024). ", "Table 2: Test MAE for homomorphism- and subgraph-counts. Results from (B. Zhang et al., 2024). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "There is a strict hierarchy in the expressive power of the baselines: MPNN $\\sqsubseteq$ Subgraph GNN $\\sqsubseteq$ local 2-GNN $\\sqsubseteq$ local 2-FGNN. These variants, apart from MPNNs, are more expressive than 1-WL and can subgraph-count up to 7-cycles. ", "page_idx": 8}, {"type": "text", "text": "Real-World Datasets. We experimented with three benchmark datasets: ZINC250K (Irwin et al., 2012), ZINC12K (Dwivedi et al., 2022a), and QM9 (Wu et al., 2018) which consist of 250 000, 12 000, and 130 831 molecular graphs, respectively. We report the mean and standard deviation over 4 random seeds. ", "page_idx": 8}, {"type": "text", "text": "For ZINC250K and ZINC12K, we selected as baseline models standard MPNNs (GIN, GCN, GAT), Subgraph GNNs (NestedGNN, GNNAK $^{+}$ , SUN), domain-agnostic GNNs fed with substructure counts (GSN, CIN), a GNN processing paths (PathNN), and expressive GNNs with provable cycle counting power (HIMP, SignNet, I2-GNN, DRFWL). Following the standard procedure, we kept the number of parameters under 500K (Dwivedi et al., 2022a) for ZINC12K. The results are detailed in Table 3. ", "page_idx": 8}, {"type": "text", "text": "For the QM9 dataset, we followed the setup of (Huang et al., 2022; Zhou et al., 2023). Specifically, the test MAE is multiplied by the standard deviation of the target and divided by the corresponding conversion unit. The baseline results and models were obtained from (Zhou et al., 2023), including expressive GNNs with provable cycle counting power. We omit methods that use additional geometric features to focus on the model\u2019s expressive power. The results are presented in Table 4. ", "page_idx": 8}, {"type": "table", "img_path": "9O2sVnEHor/tmp/bf2a67de8687ddca052e470c8d2e97763ab4d7b0fa38fccb809b47923aa4ac89.jpg", "table_caption": ["Table 3: Test MAE (\u2193) on ZINC dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "9O2sVnEHor/tmp/a7f4962c4932547e548b9dbafaba516f707fd3692a0c9e19802850e1507d556b.jpg", "table_caption": ["Table 4: Normalized test MAE (\u2193) on QM9 dataset. Top three models as nd rd "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Discussion of Results The results in Figures 3 and 4 and Table 1 constitute a strong empirical validation of our theory: increasing $r$ leads to more expressive $r$ -\u2113MPNN. Albeit 6-\u2113WL is not less powerful than 3-WL (see, e.g., Section 5.2), in practice, smaller values of $r$ can already distinguish pair of graphs that are 3-WL equivalent, such as the Shrikhande and the $(4\\!\\times\\!4)$ rook graphs. In the BREC dataset, 4-\u2113GIN distinguishes all pairs of strongly regular graphs, significantly outperforming 3-WL (0/50 graphs). Notably, 4-\u2113GIN can already distinguish 257 out of 400 total pairs of graphs, surpassing other expressive GNNs like PPGN (233/400), theoretically equivalent to 3-WL, and NestedGNN (166/400). Refer to (Wang et al., 2024, Table 2) for detailed baseline results. ", "page_idx": 8}, {"type": "text", "text": "The results in Table 2 further substantiate our theory, as $r$ -\u2113WL can effectively count cycles of length $r{+}2$ (see, e.g., Theorem 1). ", "page_idx": 8}, {"type": "text", "text": "On molecular datasets, we observe that $r$ -\u2113GIN, although designed for subgraph-counting cycles and homomorphism-counting cactus graphs, is highly competitive. Notably, we outperform the baseline 0-\u2113GIN by $226\\%$ on ZINC12K and $400\\%$ on ZINC250K and surpass domain-agnostic methods such as CIN or GSN. We conjecture that this is attributed to straightforward optimization, driven by the simplicity of the architecture (see, e.g., Figure 1) and its inductive bias towards counting cycles. ", "page_idx": 9}, {"type": "text", "text": "Limitations Path calculations can become infeasible for dense graphs due to $\\mathcal{O}(N\\,d^{r})$ complexity, where $N$ is the number of nodes and $d$ is the average degree. However, for sparse graphs, the runtime remains reasonably low. For instance, preprocessing ZINC12K for $r=5$ takes just over a minute. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper,we introduce a novel hierarchy of color refinement algorithms, denoted as $r$ -\u2113WL, which incorporates an augmented neighborhood mechanism accounting for nearby paths. We establish connections between $r.$ -\u2113WL and the classical $k$ -WL. We construct a GNN ( $\\textsl{\\ddot{r}}$ -\u2113MPNN) designed to emulate and match the expressive powerof $r$ -\u2113WL. Theoretical and empirical evidence support the claim that $r$ -\u2113MPNN can effectively subgraph-count cycles and homomorphism-count cactus graphs. ", "page_idx": 9}, {"type": "text", "text": "Future research could focus on precisely characterizing the expressivity of $r$ -\u2113WL tests by identifying the maximal class of graphs that $r$ -\u2113WL can homomorphism-count. This would facilitate comparisons by constructing pairs of graphs that $r$ -\u2113WL cannot separate, but other WL variants can. Another promising direction involves exploring the generalization capabilities of GNNs with provable homomorphism-counting properties. The ability to homomorphism-count certain motifs could provide a mathematical framework to support the intuitive notion that the capacity to count relevant features may improve generalization. We observed this improved generalization experimentally in our ablation study on ZINC12K (see, e.g., Table 8). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "R.P. is funded by the Munich Center for Machine Learning (MCML). ", "page_idx": 9}, {"type": "text", "text": "S.M. is funded by the NSF-Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (MoDL) (NSF DMS 2031985) and DFG SPP 1798, KU 1446/27-2. ", "page_idx": 9}, {"type": "text", "text": "P.W. is funded by the Vienna Science and Technology Fund (WWTF) project StruDL (ICT22-059). ", "page_idx": 9}, {"type": "text", "text": "G.K. acknowledges partial support by the Konrad Zuse School of Excellence in Reliable AI (DAAD), the Munich Center for Machine Learning (BMBF) as well as the German Research Foundation under Grants DFG-SPP-2298, KU 1446/31-1 and KU 1446/32-1. Furthermore, G.K. acknowledges support from the Bavarian State Ministry for Science and the Arts as well as by the Hightech Agenda Bavaria. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abboud, R., Ceylan, I. I., Grohe, M., and Lukasiewicz, T. (2021). \u201cThe Surprising Power of Graph Neural Networks with Random Node Initialization\u201d. In: International Joint Conference on Artificial Intelligence (IJCAI), pp. 2112\u20132118.   \nAbboud, R., Dimitrov, R., and Ceylan, I. I. (2022). \u201cShortest Path Networks for Graph Property Prediction\u201d. In: Learning on Graphs Conference (LoG), 5:1\u20135:25.   \nArvind, V., Fuhlbr\u00fcck, F., K\u00f6bler, J., and Verbitsky, O. (2020). \u201cOn Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties\u201d. In: Journal of Computer and System Sciences 113, pp. 42\u201359.   \nArvind, V., K\u00f6bler, J., Rattan, G., and Verbitsky, O. (2015). \u201cOn the Power of Color Refinement\u201d. In: International Symposium Fundamentals of Computation Theory (FCT). Vol. 9210. Lecture Notes in Computer Science, pp. 339\u2013350.   \nBalcilar, M., Heroux, P., Gauzere, B., Vasseur, P., Adam, S., and Honeine, P. (2021). \u201cBreaking the Limits of Message Passing Graph Neural Networks\u201d. In: International Conference on Machine Learning (ICML), pp. 599\u2013608.   \nBarcel\u00f3, P., Geerts, F., Reutter, J., and Ryschkov, M. (2021). \u201cGraph neural networks with local graph parameters\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 25280\u201325293.   \nBevilacqua, B., Frasca, F., Lim, D., Srinivasan, B., Cai, C., Balamurugan, G., Bronstein, M. M., and Maron, H. (2021). \u201cEquivariant Subgraph Aggregation Networks\u201d. In: International Conference on Learning Representations (ICLR).   \nBodnar, C., Frasca, F., Otter, N., Wang, Y., Li\u00f2, P., Montufar, G. F., and Bronstein, M. (2021). \u201cWeisfeiler and Lehman Go Cellular: CW Networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 2625\u20132640.   \nBouritsas, G., Frasca, F., Zafeiriou, S., and Bronstein, M. M. (2023). \u201cImproving Graph Neural Network Expressivity via Subgraph Isomorphism Counting\u201d. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 45.1, pp. 657\u2013668.   \nBresson, X. and Laurent, T. (2017). \u201cResidual gated graph convnets\u201d. In: arXiv preprint arXiv:1711.07553.   \nBronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. (2017). \u201cGeometric Deep Learning: Going beyond Euclidean Data\u201d. In: IEEE Signal Processing Magazine 34.4, pp. 18\u201342.   \nCantwell, G. T. and Newman, M. E. J. (2019). \u201cMessage Passing on Networks with Loops\u201d. In: Proceedings of the National Academy of Sciences 116.47, pp. 23398\u201323403.   \nChen, Z., Chen, L., Villar, S., and Bruna, J. (2020). \u201cCan Graph Neural Networks Count Substructures?\u201d In: Advances in Neural Information Processing Systems (NeurIPS), pp. 10383\u201310395.   \nChen, Z., Villar, S., Chen, L., and Bruna, J. (2019). \u201cOn the equivalence between graph isomorphism testing and function approximation with gnns\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 15868\u201315876.   \nColbourn, C. J. and Booth, K. S. (1981). \u201cLinear Time Automorphism Algorithms for Trees, Interval Graphs, and Planar Graphs\u201d. In: SIAM Journal on Computing 10.1, pp. 203\u2013225.   \nCurticapean, R., Dell, H., and Marx, D. (2017). \u201cHomomorphisms are a good basis for counting small subgraphs\u201d. In: ACM SIGACT Symposium on Theory of Computing (STOC), pp. 210\u2013223.   \nCybenko, G. (1989). \u201cApproximation by superpositions of a sigmoidal function\u201d. In: Mathematics of control, signals and systems 2.4, pp. 303\u2013314.   \nDasoulas, G., Santos, L. D., Scaman, K., and Virmaux, A. (Jan. 7, 2021). \u201cColoring Graph Neural Networks for Node Disambiguation\u201d. In: International Joint Conference on Artificial Intelligence (IJCAI), pp. 2126\u20132132.   \nDell, H., Grohe, M., and Rattan, G. (2018). \u201cLov\u00e1sz Meets Weisfeiler and Leman\u201d. In: International Colloquium on Automata, Languages, and Programming (ICALP). Vol. 107. LIPIcs, 40:1\u201340:14.   \nDeshpande, M., Kuramochi, M., and Karypis, G. (2002). \u201cAutomated Approaches for Classifying Structures\u201d. In: ACM SIGKDD Workshop on Data Mining in Bioinformatics (BIOKDD), pp. 11\u201318.   \nDwivedi, V. P., Joshi, C. K., Luu, A. T., Laurent, T., Bengio, Y., and Bresson, X. (2022a). \u201cBenchmarking Graph Neural Networks\u201d. In: Journal of Machine Learning Research 24.43, pp. 1\u201348.   \nDwivedi, V. P., Ramp\u00e1ek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T., and Beaini, D. (2022b). \u201cLong Range Graph Benchmark\u201d. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., pp. 22326\u201322340.   \nFan, W., Ma, Y., Li, Q., He, Y., Zhao, Y. E., Tang, J., and Yin, D. (2019). \u201cGraph Neural Networks for Social Recommendation\u201d. In: The World Wide Web Conference (WWW), pp. 417\u2013426.   \nFeng, J., Chen, Y., Li, F., Sarkar, A., and Zhang, M. (2022). \u201cHow Powerful are K-hop Message Passing Graph Neural Networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS).   \nFey, M., Yuen, J. G., and Weichert, F. (2020). \u201cHierarchical Inter-Message Passing for Learning on Molecular Graphs\u201d. In: ICML Graph Representation Learning and Beyond $(G R L+)$ Workhop.   \nFey, M. and Lenssen, J. E. (2019). \u201cFast Graph Representation Learning with PyTorch Geometric\u201d. In: ICLR Workshop on Representation Learning on Graphs and Manifolds.   \nFrasca, F., Bevilacqua, B., Bronstein, M., and Maron, H. (2022). \u201cUnderstanding and Extending Subgraph GNNs by Rethinking Their Symmetries\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 31376\u201331390.   \nF\u00fcrer, M. (2001). \u201cWeisfeiler-Lehman Refinement Requires at Least a Linear Number of Iterations\u201d. In: Automata, Languages and Programming. Ed. by F. Orejas, P. G. Spirakis, and J. van Leeuwen, pp. 322\u2013333.   \nGeerts, F. and Reutter, J. L. (2022). \u201cExpressiveness and Approximation Properties of Graph Neural Networks\u201d. In: International Conference on Learning Representations (ICLR).   \nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). \u201cNeural Message Passing for Quantum Chemistry\u201d. In: International Conference on Machine Learning (ICML), pp. 1263\u20131272.   \nGraziani, C., Drucks, T., Jogl, F., Bianchini, M., scarselli, franco, and G\u00e4rtner, T. (2024). \u201cThe Expressive Power of Path-Based Graph Neural Networks\u201d. In: Forty-first International Conference on Machine Learning.   \nHagberg, A. A., Schult, D. A., Swart, P., and Hagberg, J. (2008). \u201cExploring Network Structure, Dynamics, and Function using NetworkX\u201d. In: Python in Science Conference (SciPy),   \nHarary, F. and Uhlenbeck, G. E. (1953). \u201cOn the Number of Husimi Trees, I\u201d. In: Proceedings of the National Academy of Sciences of the United States of America 39.4, pp. 315\u2013322.   \nHornik, K., Stinchcombe, M., and White, H. (1989). \u201cMultilayer feedforward networks are universal approximators\u201d. In: Neural networks 2.5, pp. 359\u2013366.   \nHu\\*, W., Liu\\*, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., and Leskovec, J. (2020a). \u201cStrategies for Pre-training Graph Neural Networks\u201d. In: International Conference on Learning Representations (ICLR).   \n\u2013 (2020b). \u201cStrategies for Pre-training Graph Neural Networks\u201d. In: International Conference on Learning Representations.   \nHuang, Y., Peng, X., Ma, J., and Zhang, M. (2022). \u201cBoosting the Cycle Counting Power of Graph Neural Networks with $\\mathrm{I^{2}}$ -GNNs\u201d. In: International Conference on Learning Representations (ICLR).   \nIoffe, S. and Szegedy, C. (2015). \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\u201d. In: International Conference on Machine Learning, (ICML). Vol. 37, pp. 448\u2013456.   \nIrwin, J. J., Sterling, T., Mysinger, M. M., Bolstad, E. S., and Coleman, R. G. (2012). \u201cZINC: A Free Tool to Discover Chemistry for Biology\u201d. In: Journal of Chemical Information and Modeling 52, pp. 1757\u20131768.   \nJin, E., Bronstein, M., Ceylan, I. I., and Lanzinger, M. (2024). \u201cHomomorphism Counts for Graph Neural Networks: All About That Basis\u201d. In: International Conference on Machine Learning (ICML).   \nKeriven, N. and Peyr\u00e9, G. (2019). \u201cUniversal invariant and equivariant graph neural networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 7090\u20137099.   \nKingma, D. P. and Ba, J. (2015). \u201cAdam: A Method for Stochastic Optimization\u201d. In: International Conference on Learning Representations (ICLR).   \nKipf, T. N. and Welling, M. (2017). \u201cSemi-Supervised Classification with Graph Convolutional Networks\u201d. In: International Conference on Learning Representations (ICLR).   \nKirkley, A., Cantwell, G. T., and Newman, M. E. J. (2021). \u201cBelief Propagation for Networks with Loops\u201d. In: Science Advances 7.17, eabf1211.   \nKorte, B. and Vygen, J. (2018). Combinatorial Optimization. Springer.   \nKoyut\u00fcrk, M., Grama, A. Y., and Szpankowski, W. (2004). \u201cAn efficient algorithm for detecting frequent subgraphs in biological networks\u201d. In: Bioinformatics 20 Suppl 1, pp. i200\u20137.   \nLanzinger, M. and Barcelo, P. (2024). \u201cOn the Power of the Weisfeiler-Leman Test for Graph Motif Parameters\u201d. In: International Conference on Learning Representations (ICLR).   \nLim, D., Robinson, J. D., Zhao, L., Smidt, T., Sra, S., Maron, H., and Jegelka, S. (2022). \u201cSign and Basis Invariant Networks for Spectral Graph Representation Learning\u201d. In: International Conference on Learning Representations (ICLR).   \nLov\u00e1sz, L. M. (1967). \u201cOperations with structures\u201d. In: Acta Mathematica Academiae Scientiarum Hungarica 18, pp. 321\u2013328.   \nMaron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y. (2019a). \u201cProvably Powerful Graph Networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 2153\u20132164.   \nMaron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y. (2018). \u201cInvariant and Equivariant Graph Networks\u201d. In: International Conference on Learning Representations (ICLR).   \nMaron, H., Fetaya, E., Segol, N., and Lipman, Y. (2019b). \u201cOn the Universality of Invariant Networks\u201d. In: International Conference on Machine Learning (ICML), pp. 4363\u20134371.   \nMaskey, S., Parviz, A., Thiessen, M., St\u00e4rk, H., Sadikaj, Y., and Maron, H. (2022). \u201cGeneralized Laplacian Positional Encoding for Graph Representation Learning\u201d. In: NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations.   \nMichel, G., Nikolentzos, G., Lutzeyer, J. F., and Vazirgiannis, M. (2023). \u201cPath Neural Networks: Expressive and Accurate Graph Neural Networks\u201d. In: International Conference on Machine Learning (ICML), pp. 24737\u201324755.   \nMonti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. (2017). \u201cGeometric deep learning on graphs and manifolds using mixture model cnns\u201d. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5115\u20135124.   \nMonti, F., Frasca, F., Eynard, D., Mannion, D., and Bronstein, M. M. (2019). Fake News Detection on Social Media using Geometric Deep Learning. arXiv: 1902.06673.   \nMorris, C., Lipman, Y., Maron, H., Rieck, B., Kriege, N. M., Grohe, M., Fey, M., and Borgwardt, K. (2023). \u201cWeisfeiler and Leman Go Machine Learning: The Story so Far\u201d. In: Journal of Machine Learning Research 24, 333:1\u2013333:59.   \nMorris, C., Rattan, G., and Mutzel, P. (2020). \u201cWeisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 21824\u201321840.   \nMorris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. (2019). \u201cWeisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks\u201d. In: AAAI Conference on Artificial Intelligence (AAAI), pp. 4602\u20134609.   \nMurphy, R., Srinivasan, B., Rao, V., and Ribeiro, B. (2019). \u201cRelational Pooling for Graph Representations\u201d. In: International Conference on Machine Learning (ICML), pp. 4663\u20134673.   \nNeuen, D. (2024). \u201cHomomorphism-Distinguishing Closedness for Graphs of Bounded Tree-Width\u201d. In: International Symposium on Theoretical Aspects of Computer Science (STACS), 53:1\u201353:12.   \nNguyen, H. and Maehara, T. (2020). \u201cGraph Homomorphism Convolution\u201d. In: International Conference on Machine Learning (ICML), pp. 7306\u20137316.   \nNishi, T. and Chua, L. (1986). \u201cUniqueness of solution for nonlinear resistive circuits containing CCCS\u2019s or VCVS\u2019s whose controlling coefficients are finite\u201d. In: IEEE Transactions on Circuits and Systems 33.4, pp. 381\u2013397.   \nPaszke, A. et al. (2019). \u201cPyTorch: An Imperative Style, High-Performance Deep Learning Library\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 8024\u20138035.   \nPaten, B., Diekhans, M., Earl, D., John, J. S., Ma, J., Suh, B., and Haussler, D. (2011). \u201cCactus graphs for genome comparisons\u201d. In: Journal of Computational Biology 18.3, pp. 469\u2013481.   \nQian, C., Rattan, G., Geerts, F., Niepert, M., and Morris, C. (2022). \u201cOrdered subgraph aggregation networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 21030\u201321045.   \nSato, R., Yamada, M., and Kashima, H. (2021). \u201cRandom Features Strengthen Graph Neural Networks\u201d. In: SIAM International Conference on Data Mining (SDM), pp. 333\u2013341.   \nScarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009). \u201cThe Graph Neural Network Model\u201d. In: IEEE Transactions on Neural Networks 20.1, pp. 61\u201380.   \nStokes, J. M. et al. (2020). \u201cA deep learning approach to antibiotic discovery\u201d. In: Cell 180.4, 688\u2013 702.e13.   \nTinhofer, G. (1986). \u201cGraph isomorphism and theorems of Birkhoff type\u201d. In: Computing 36, pp. 285\u2013 300.   \n\u2013 (1991). \u201cA note on compact graphs\u201d. In: Discrete Applied Mathematics 30, pp. 253\u2013264.   \nvan Dam, E. R. and Haemers, W. H. (2003). \u201cWhich Graphs Are Determined by Their Spectrum?\u201d In: Linear Algebra and its Applications. Vol. 373. Combinatorial Matrix Theory Conference, pp. 241\u2013272.   \nVelickovic, P., Cucurull, G., Casanova, A., Romero, A., Li\u00f2, P., and Bengio, Y. (2018). \u201cGraph Attention Networks\u201d. In: International Conference on Learning Representations (ICLR).   \nVignac, C., Loukas, A., and Frossard, P. (2020). \u201cBuilding powerful and equivariant graph neural networks with structural message-passing\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 14143\u201314155.   \nWang, Y. and Zhang, M. (2024). \u201cAn Empirical Study of Realized GNN Expressiveness\u201d. In: International Conference on Machine Learning (ICML).   \nWeisfeiler, B. and Lehman., A. (1968). \u201cThe reduction of a graph to canonical form and the algebra which appears therein\u201d. In: Nauchno-Technicheskaya Informatsia 9.   \nWelke, P., Thiessen, M., Jogl, F., and G\u00e4rtner, T. (2023). \u201cExpectation-Complete Graph Representations with Homomorphisms\u201d. In: International Conference on Machine Learning (ICML), pp. 36910\u201336925.   \nWu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. (2018). MoleculeNet: A Benchmark for Molecular Machine Learning. arXiv: 1703.00564.   \nXu, K., Hu, W., Leskovec, J., and Jegelka, S. (2019). \u201cHow Powerful Are Graph Neural Networks?\u201d In: International Conference on Learning Representations (ICLR).   \nYou, J., Gomes-Selman, J. M., Ying, R., and Leskovec, J. (2021). \u201cIdentity-Aware Graph Neural Networks\u201d. In: AAAI Conference on Artificial Intelligence (AAAI), pp. 10737\u201310745.   \nZhang, B., Gai, J., Du, Y., Ye, Q., He, D., and Wang, L. (2024). \u201cBeyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness\u201d. In: International Conference on Learning Representations (ICLR).   \nZhang, M. and Li, P. (2021). \u201cNested Graph Neural Networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 15734\u201315747.   \nZhao, L., Jin, W., Akoglu, L., and Shah, N. (2022). \u201cFrom Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness\u201d. In: International Conference on Learning Representations (ICLR).   \nZhou, J., Feng, J., Wang, X., and Zhang, M. (2023). \u201cDistance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Additional Figures 16 ", "page_idx": 14}, {"type": "text", "text": "B Additional Notions 18   \nB.1 Higher-Order Weisfeiler-Leman Tests 18   \nC.1 Synthetic Datasets 20   \nC.2 Real-World Datasets 20 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D Preparation for Proofs 23 ", "page_idx": 14}, {"type": "text", "text": "E Appendix for Section 5.1 23 ", "page_idx": 14}, {"type": "text", "text": "F Appendix for Section 5.2 24 ", "page_idx": 14}, {"type": "text", "text": "G.1 Tree Decomposition Preliminaries 26   \nG.2 Cactus Graphs and their Canonical Tree Decomposition 26   \nG.3 Alternative $r$ -\u2113WL . . 29   \nG.4 The Unfolding Tree of $r$ -\u2113WL 30   \nH Implications of Theorem 2 40   \nH.1 Appendix on $\\mathcal{F}$ -Hom-GNNs and Proof of Corollary 2 i) 40   \nH.2 Appendix on Subgraph GNNs and Proof of Corollary 2 ii) 41   \nH.3 Appendix on Subgraph $k$ -GNNs and Proof of Corollary 2 iii) . 42   \nH.4 Proof of Corollary 2 iv) . . 43 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "I Appendix for Section 6 43 ", "page_idx": 14}, {"type": "image", "img_path": "9O2sVnEHor/tmp/c497c7f5213d9670778844cf944047f623a097718bf09941c28a40f9df000351.jpg", "img_caption": ["Figure 5: Examples of non-injective homomorphism (row 1), subgraph isomorphism (row 2), bijective homomorphism with non-homomorphic inverse (row 3), and isomorphism (row 4). For better clarity, the mappings $h:V(F)\\to V(G)$ are visually represented with colors, where $F$ is consistently on the left, and $G$ is on the right in each row. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "9O2sVnEHor/tmp/62dd295c661c2b44b10e57e965546c4d2abe0cf587dd44fad7c40e57133cde15.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Example of two non-isomorphic graphs that are $r$ -\u2113WL equivalent but not $(r{+}1)$ -\u2113WL equivalent: a chordal cycle (left) and a cactus graph (right). ", "page_idx": 15}, {"type": "image", "img_path": "9O2sVnEHor/tmp/7d85646e22f8cc935896a5c1ef9a1300e9d635c95371d807364f3cc43e011cbd.jpg", "img_caption": ["(a) Input graph $F$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "9O2sVnEHor/tmp/2ab5010f854ffa639da622096480d7d2ec12df35f71f9fff32d4d6c0024f219e.jpg", "img_caption": ["(c) Twisted F\u00fcrer graph $H(F)$ : $\\hom(F,H(F))=34$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 7: Example of graphs that Subgraph GNNs cannot separate but ${1-\\ell\\mathrm{WL}}$ can: Subgraph GNNs cannot separate $G(F)$ and $H(F)$ . However, since $\\hom(F,G(F))\\neq\\hom(F,H(F))$ and $F$ is a cactus graph, 1-\u2113WL can separate $G(F)$ and $H(F)$ by Theorem 2. ", "page_idx": 16}, {"type": "image", "img_path": "9O2sVnEHor/tmp/60444b949334f1988c5eb35c530656bb96499a811a3695107a9f9b5f3be12224.jpg", "img_caption": ["(a) COSPECTRAL10. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "9O2sVnEHor/tmp/a3dcf89ce445f63ba679730b1c110c59685a60a2d326c2537ddde93974a92726.jpg", "img_caption": ["(b) SR16622. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "9O2sVnEHor/tmp/ec2ccc15557a55d802391295d3bef26592a21d56aef73831e4f7b3d6172d84dd.jpg", "img_caption": ["(c) CSL example, skip length 2 (left) and 3 (right). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 8: Some synthetic datasets. The dotted lines are the common edges. The orange edges identifies $\\mathcal{N}_{1}(v)$ . ", "page_idx": 17}, {"type": "text", "text": "B Additional Notions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Higher-Order Weisfeiler-Leman Tests ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "It is possible to uplift the expressive power of WL by considering higher-order interactions. The simplest higher-order variant of WL is the $k$ -dimensional Weisfeiler-Leman test, denoted by $k$ -WL. Given a graph $G$ with nodes $V(G)$ and edges $E(G)$ , the algorithm generates a new graph $H$ where each node is a $k$ -tuple of elements of $V(G)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nV(H)=\\left\\{\\mathbf{v}=\\{v_{i}\\}_{i=1}^{k}\\ |\\ v_{i}\\in V(G)\\right\\}=V(G)^{k},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and edges $E(H)$ are built among those $k$ -tuples that differ in one entry only ", "page_idx": 17}, {"type": "equation", "text": "$$\nE(H)=\\{\\{\\mathbf{v},\\mathbf{u}\\}\\mid d_{H}(\\mathbf{v},\\mathbf{u})=1\\,,\\ \\mathbf{u},\\mathbf{v}\\in V(H)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d_{H}$ is the Hamming distance. The algorithm assigns to each node $\\mathbf{v}\\in V(H)$ an initial color depending on the isomorphic type of the induced subgraph $G[\\mathbf{v}]$ . The color refinements scheme is ", "page_idx": 17}, {"type": "text", "text": "exactly (1) applied to $H$ . While $H$ can be generated by a simple algorithm, the approach quickly becomes impractical as the number of nodes and edges grows exponentially in $k$ . ", "page_idx": 18}, {"type": "image", "img_path": "9O2sVnEHor/tmp/5039d526689c9054a1885c62913eaa98409960242e0d9563d2f020f77f5b4259.jpg", "img_caption": ["(b) 1-WL after one iteration. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "9O2sVnEHor/tmp/d7da2366ffeaefd55980576653d260e3fa6c337ffdbc634d214e10ff5fec0a12.jpg", "img_caption": ["(c) 3-WL at initialization. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "9O2sVnEHor/tmp/2a8ee1dc0e28e2bbecce762163729a21a567d86153284c9cc24a895c9e8412a0.jpg", "img_caption": ["(d) 1-\u2113WL after one iteration. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 9: The input graphs cannot be distinguished by 1-WL, since the color distribution after convergence of the algorithm is equal. 3-WL can distinguish them at the cost of creating new dense graphs. Our proposed 1-\u2113WL can distinguish the two graphs heeding the original graph sparsity. ", "page_idx": 18}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our model is implemented in PyTorch (BSD-3 license) (Paszke et al., 2019), using PyTorch Geometric (MIT license) (Fey et al., 2019). The $r$ -neighborhoods are computed with NetworkX (Creative Commons Zero v1.0 Universal) (Hagberg et al., 2008) as preprocessing. Hyperparameters on real-world datasets were tuned using grid search; for synthetic experiments, we fixed one configuration of hyperparameters. All experiments were run on an internal cluster with Intel Xeon CPUs (28 cores, 192GB RAM) and GeForce RTX 3090 Ti GPUs (4 units, 24GB memory each), as well as Intel Xeon CPUs (32 cores, 192GB RAM) and NVIDIA RTX A6000 GPUs (3 units, 48GB memory each). All models are trained with Adam optimizer (Kingma et al., 2015). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "C.1 Synthetic Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The SR16622 dataset is retrieved from the official PATHNN repository (MIT license) (Michel et al., 2023). The GRAPH8C dataset is downloaded from Australian National University webpage (Creative Commons Attribution 4.0 International (CC BY 4.0) license). The EXP, EXP_ISO, and CEXP datasets are downloaded from GNN-RNI official repository (GPL-3.0 license) (Abboud et al., 2021), while the corresponding splits are generated via Stratified 5-fold cross-validation. The CSL dataset is provided by torch_geometric, while the corresponding splits are taken from PathNN official repository. The SUBGRAPHCOUNT dataset is taken from the official repository (MIT license) of (Zhao et al., 2022). The BREC dataset is downloaded from its official repository (MIT license) (Wang et al., 2024). The configuration of hyperparameters can be found in Table 5. For the synthetic datasets, we fixed one configuration and studied the effect of increasing $r$ on the expressive and counting power of the architecture. ", "page_idx": 19}, {"type": "text", "text": "For the SR16622, GRAPH8C, EXP_ISO, and COSPECTRAL10 datasets, we report the mean and standard deviation over 100 random seeds. For the EXP, CEXP, and CSL datasets, we report the mean and standard deviation of 5-fold cross-validation. For BREC, we follow the original setup and perform an $\\alpha$ -level Hotellings T-square test; see (Wang et al., 2024) for more details. For the SUBGRAPHCOUNT dataset, we report the mean and standard deviation over 4 random seeds, using the original splits from (Zhao et al., 2022). ", "page_idx": 19}, {"type": "table", "img_path": "9O2sVnEHor/tmp/7e46658232be83bc56501c3055da7ae908d3469271e4fb60f8676cb77c7f3346.jpg", "table_caption": ["Table 5: Hyperparameter configuration for synthetic experiments. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2 Real-World Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All real-world datasets are provided by torch_geometric. The splits for both ZINC datasets are also provided by torch_geometric. For QM9, we follow the set-up of (Zhou et al., 2023) and use random $80/10/10$ splits. Details for the datasets are provided in Table 6. ", "page_idx": 19}, {"type": "text", "text": "Hyperparameters were tuned using grid search. For ZINC12K, the grid was defined by Hidden Size $\\in\\bar{\\{64,128\\}}$ and Num. Layers $\\in[3,4,5\\}$ . For ZINC250K, the grid was defined by Hidden Size $\\in\\{128,256\\}$ and Num. Layers $\\in\\{4\\}$ . For the QM9 tasks, the grid was defined by Hidden Size $\\in\\{64,128\\}$ and Num. Layers $\\in\\{3,4,5\\}$ . For the QM9 tasks, we followed the training set-up of (Zhou et al., 2023), training for 400 epochs with a ReduceLROnPlateau scheduler, reducing the learning rate by a factor of 0.9 if the validation metric did not decrease for 10 epochs. The exact hyperparameters are given in Table 7. ", "page_idx": 19}, {"type": "text", "text": "All real-world datasets come with edge features. We use an encoder layer, followed by a linear layer to encode node, edge features, and atomic types before passing them to the $r.$ -\u2113GIN. Within the $r$ -\u2113GIN layers, we process the edge features via a 2-layered learnable MLP, and replace the GIN in (4) by GINE layers $\\mathrm{Hu^{*}}$ et al., 2020a). After $t$ rounds of $r$ -\u2113GIN layer, we apply a two-layered MLP as decoder layer. In all experiments, BatchNorm1D (Ioffe et al., 2015) is used in the MLP layers. We refer to Figure 1 for a depiction of the architecture. ", "page_idx": 20}, {"type": "table", "img_path": "9O2sVnEHor/tmp/9872239265def75ee4de67d5414ba5d1711baf8e527073b844913fc36a07c642.jpg", "table_caption": ["Table 6: Statistics of real-world datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "9O2sVnEHor/tmp/d4f5125a16b8a6038b1d7c56fda1317ddb3f1d1f38c426b1880af917993e3817.jpg", "table_caption": [], "table_footnote": ["Table 8: Ablation study on the effect of $r$ in $r$ -\u2113GIN, ZINC12K. "], "page_idx": 20}, {"type": "table", "img_path": "9O2sVnEHor/tmp/3ff077e3b94df8331f4f15d39d03f532f6e43b17378fe7d5f168f55d97336533.jpg", "table_caption": ["Table 9: Test metrics on long-range graph benchmark datasets (Dwivedi et al., 2022b). The baseline results are obtained from (Dwivedi et al., 2022b, Table 4). Our method is able to enhance performance over standard baselines. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "9O2sVnEHor/tmp/3c4a20a398332ac1c02d4f86773de9a497b144fd596f3c9c01525539e6a0595e.jpg", "table_caption": ["Table 10: Empirical time complexity for QM9 dataset; results from (Zhou et al., 2023). In parenthesis the size of the dataset after the computation of $r$ -neighborhoods. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D Preparation for Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We begin by recalling some core concepts that are relevant for Section 5 and the proofs therein. ", "page_idx": 22}, {"type": "text", "text": "Definition 10. $A$ node invariant $\\zeta_{(\\cdot)}$ is a mapping that assigns to each graph $G\\in\\mathcal G$ a function $\\zeta_{G}:V(G)\\to P$ , which satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall v\\in V(G),\\zeta_{G}(v)=\\zeta_{H}(h(v)),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $H$ is any graph isomorphic to $G$ and $h$ is the corresponding isomorphism from $H$ to $G$ . ", "page_idx": 22}, {"type": "text", "text": "The following definition enables us to compare the expressive power of different node invariants. ", "page_idx": 22}, {"type": "text", "text": "Definition 11 (Node Invariant Refinement). Given two node invariants $\\gamma$ and $\\zeta$ . We say that $\\zeta$ refines $\\gamma$ if for every fixed graph $G$ and nodes $u,v\\in V(G)$ , it holds $\\zeta_{G}(u)=\\zeta_{G}(v)\\Rightarrow\\gamma_{G}(u)=\\gamma_{G}(v)$ . We write $\\zeta\\subseteq\\gamma$ . ", "page_idx": 22}, {"type": "text", "text": "We emphasize that every node invariant $\\zeta$ induces a graph invariant $A[\\gamma]$ by collecting the multiset, i.e., $G\\,{\\overset{.}{\\mapsto}}\\,\\{\\{\\zeta_{G}(v)\\}\\}_{v\\in V(G)}$ . We denote the induced graph invariant of a node invariant $\\gamma$ as $A[\\gamma]$ . ", "page_idx": 22}, {"type": "text", "text": "The following lemma establishes a connection between the expressive power of two node invariants (see Definition 11) and that of their induced graph invariants (see Definition 3). ", "page_idx": 22}, {"type": "text", "text": "Lemma 1. Let $\\zeta,\\gamma$ be node invariant. If $\\zeta\\subseteq\\gamma,$ , then $A[\\zeta]$ is more powerful than $A[\\gamma]$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $G,H$ be two graphs, and let $P$ be the underlying palette of $\\zeta,\\gamma$ . Consider the function ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi:P\\longrightarrow P,\\ \\zeta(u)\\mapsto\\gamma(u)\\ \\forall u\\in V(G)\\cup V(H).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As a consequence of $\\zeta\\subseteq\\gamma,\\phi$ is well-defined, since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\zeta(u)=\\zeta(v)\\implies\\left(\\phi\\circ\\zeta\\right)(u)=\\gamma(u)=\\gamma(v)=(\\phi\\circ\\zeta)\\,(v).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Assume that $\\begin{array}{r}{\\mathcal{A}[\\zeta](G)=\\mathcal{A}[\\zeta](H)}\\end{array}$ , i.e, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{\\zeta(u)~|~u\\in V(G)\\right\\}\\right\\}=\\left\\{\\left\\{\\zeta(v)~|~v\\in V(H)\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As $\\phi$ is well-defined, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{\\phi\\circ\\zeta(u)\\;|\\;u\\in V(G)\\right\\}\\right\\}=\\left\\{\\left\\{\\phi\\circ\\zeta(x)\\;|\\;v\\in V(H)\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which leads to $\\pmb{\\mathscr{A}}[\\gamma](G)=\\pmb{\\mathscr{A}}[\\gamma](H)$ . ", "page_idx": 22}, {"type": "text", "text": "E Appendix for Section 5.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide the proof of Proposition 1 from the main paper. ", "page_idx": 22}, {"type": "text", "text": "Proposition 1. Let $0\\leq q<r$ . Then, $r$ -\u2113WL is strictly more powerful than $q$ -\u2113WL. In particular, every $r$ -\u2113WL is strictly more powerful than 1-WL. ", "page_idx": 22}, {"type": "text", "text": "Proof of Proposition 1. Let $r\\geq0$ . We aim to prove that $(r+1)$ -\u2113WL is strictly more powerful than $r$ -\u2113WL. We begin by demonstrating that $(r+1)$ -\u2113WL is more powerful than $r{-}\\ell\\mathbf{W}\\mathbf{L}$ . ", "page_idx": 22}, {"type": "text", "text": "To establish this, we rely on Lemma 1. Specifically, we demonstrate that the underlying $(r+1)$ -\u2113WL node invariant $c_{r+1}$ refines $c_{r}$ . Moreover, we go beyond and show that the node invariant $c_{r+1}^{(t)}$ refines c(rt)at every iteration t \u22650, which shows that t iterations of (r + 1)-\u2113WL are more powerful than t iterations of $r$ -\u2113WL. ", "page_idx": 22}, {"type": "text", "text": "sFtoarr tt hwiist hp utrhpeo ssae,m lee tl $G$ ebles . a Bgyr aipnhd uwcittiho nn, owdee  saests $V(G)$ h. aFtor $t=0$ , $c_{r+1}^{(0)}\\subseteq c_{r}^{(0)}$ since both algorithms ", "page_idx": 22}, {"type": "equation", "text": "$$\nc_{r+1}^{(t)}(u)=c_{r+1}^{(t)}(v)\\implies c_{r}^{(t)}(u)=c_{r}^{(t)}(v)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "holds; we need to prove that (5) implies ", "page_idx": 22}, {"type": "equation", "text": "$$\nc_{r+1}^{(t+1)}(u)=c_{r+1}^{(t+1)}(v)\\implies c_{r}^{(t+1)}(u)=c_{r}^{(t+1)}(v).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since HASH in Definition 7 is injective, $c_{r+1}^{(t)}(u)=c_{r+1}^{(t)}(v)$ in (5) leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{c_{r+1}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{q}(u)\\right\\}\\right\\}=\\left\\{\\left\\{c_{r+1}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{q}(v)\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $q\\in\\{0,\\ldots,r\\}$ . The assumption $c_{r}^{(t)}(u_{l,k}^{q})=c_{r}^{(t)}(v_{l,k}^{q})$ in (5) is satisfied for every path ${\\bf u}_{l}^{q}=$ $\\left\\{u_{l,k}^{q}\\right\\}\\in\\mathcal{N}_{q}(u)$ and ${\\bf v}_{l}^{q}=\\left\\{v_{l,k}^{q}\\right\\}\\in\\mathcal{N}_{q}(v)$ for $q=0,\\ldots,r,l=1,\\ldots,|\\mathcal{N}_{v}|$ and $k=1,\\ldots,q+1$ Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{k}(u)\\right\\}\\right\\}=\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\mid\\mathbf{p}\\in\\mathcal{N}_{k}(v)\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Inputting this into Definition 7, we get (6), i.e, $c_{r+1}^{(t+1)}\\subseteq c_{r}^{(t+1)}$ ", "page_idx": 23}, {"type": "text", "text": "The \u201cstrictly\u201d can be deduced as follows. The cycle graph on $(2r+6)$ nodes equipped with a chord between nodes 1 and $r+4$ is $r$ -\u2113WL equivalent to the graph consisting of two $(r+3)$ -cycles connected by one edge; however, they are not $(r+1)$ -\u2113WL equivalent (see, e.g., Figure 6). \u53e3 ", "page_idx": 23}, {"type": "text", "text": "F Appendix for Section 5.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The goal of this subsection is to provide a proof for Theorem 1 and Corollary 1. In fact, we present and prove a more general statement. Specifically, for a graph $G$ and $v\\in V(G)$ , we introduce the node invariant $\\mathrm{sub}(F^{x},G^{v})$ , defined as the count of subgraph isomorphisms from $F$ to $G$ that are rooted, meaning that $x$ is mapped to $v$ . Let us denote this node invariant as $\\operatorname{sub}(F^{x},\\cdot)$ . Our result establishes that $c_{r}^{(1)}(\\cdot)$ refines $\\operatorname{sub}(C^{x},\\cdot)$ for every cycle graph $C$ with at most $r$ nodes. In simpler terms, $c_{r}^{(1)}$ can determine how often node $v$ appears in a cycle $C$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 2. Let $r\\geq1$ . For every cycle graph $C$ with at most $r+2$ nodes and $x\\in V(C)$ , it holds c(r1 )(\u00b7) \u2291sub(Cx, \u00b7). ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 2. Let $G$ be any graph, $u,v\\,\\in\\,V(G)$ , and $q\\,=\\,1,\\ldots,r\\,+\\,2$ . Let $C$ be a cycle graph with $q$ nodes. It is important to note that for every $x_{1},x_{2}\\in C$ , we have $\\operatorname{sub}(C^{x_{1}},G^{v})\\,=$ $\\operatorname{sub}(C^{x_{2}},G^{v})$ since every node in $C$ is automorphic to each other. Therefore, we can arbitrarily choose any $x\\in V(C)$ . ", "page_idx": 23}, {"type": "text", "text": "We show that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{sub}\\,(C^{x},G^{u})\\neq\\mathrm{sub}\\,(C^{x},G^{v})\\implies c_{r}^{(1)}(u)\\neq c_{r}^{(1)}(v)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The number of injective homomorphisms from the $q$ -long cycles $C^{x}$ to $G^{v}$ , i.e., $\\operatorname{sub}(C_{q}^{a},G^{v})$ , is equal to the number of paths of length $(q-2)$ between distinct neighbors of $v$ . ", "page_idx": 23}, {"type": "text", "text": "The neighborhood $\\mathcal{N}_{(q-2)}(v)$ comprises exactly all paths of length $(q-2)$ between any two distinct neighbors of $v$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{sub}\\,(C^{x},G^{v})=\\left|\\mathcal{N}_{(q-2)}(v)\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{sub}\\,(C^{x},G^{u})\\neq\\mathrm{sub}\\,(C^{x},G^{v})\\implies\\left|{\\mathcal{N}}_{(q-2)}(u)\\right|\\neq\\left|{\\mathcal{N}}_{(q-2)}(v)\\right|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{c_{q-2}^{(0)}(\\mathbf{p}):\\mathbf{p}\\in\\mathcal{N}_{(q-2)}(u)\\right\\}\\right\\}\\neq\\left\\{\\left\\{c_{q-2}^{(0)}(\\mathbf{p}):\\mathbf{p}\\in\\mathcal{N}_{(q-2)}(v)\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, as HASH in Definition 7 is injective, we get the thesis $c_{q-2}^{(1)}(u)\\neq c_{q-2}^{(1)}(v)$ . ", "page_idx": 23}, {"type": "text", "text": "Now, Theorem 1 from the main paper is a simple corollary of Lemma 2. ", "page_idx": 23}, {"type": "text", "text": "Theorem 1. For any $r\\geq1$ , $r$ -\u2113WL can subgraph-count all cycles with at most $r+2$ nodes. ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 1. Combining Lemma 2 and Lemma 1, we get that $c_{r}^{(1)}$ (as a graph invariant) is stronger than the induced graph invariant $\\mathcal{A}\\left[\\mathrm{sub}(C^{x},\\cdot)\\right]$ . Now, consider graphs $G,H$ , and assume without loss of generality that $|V(G)|=n=|V({\\dot{H}})|$ . ", "page_idx": 23}, {"type": "text", "text": "If $c_{r}^{(1)}(G)=c_{r}^{(1)}(H)$ , we have $A\\left[\\operatorname{sub}(C^{x},\\cdot)\\right](G)=A\\left[\\operatorname{sub}(C^{x},\\cdot)\\right](H)$ . Hence, by definition of induced graph invariants, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{\\{\\operatorname{sub}(C^{x},G^{v})\\,|\\,v\\in V(G)\\}\\right\\}=\\left\\{\\left\\{\\operatorname{sub}(C^{x},H^{w})\\,|\\,w\\in V(H)\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{v\\in V(G)}\\operatorname{sub}(C^{x},G^{v})=\\frac{1}{n}\\sum_{w\\in V(H)}\\operatorname{sub}(C^{x},H^{w}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is equivalent to $\\mathrm{sub}(C,G)=\\mathrm{sub}(C,H)$ . ", "page_idx": 24}, {"type": "text", "text": "We proceed to restate Corollary 1 and provide its proof. ", "page_idx": 24}, {"type": "text", "text": "Corollary 1. Let $k\\,\\in\\,\\mathbb{N}$ . There exists $r\\,\\in\\,\\mathbb{N}$ , such that $r$ -\u2113WL is not less powerful than $k$ -WL.   \nSpecifically, $r\\in\\mathcal{O}(k^{2})$ , with k(k2+1)\u22122 for even k and r \u2264 $\\begin{array}{r}{r\\le\\frac{(k+1)^{2}}{2}-2}\\end{array}$ for odd $k$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Corollary 1. Let $k>0$ . We need to show that there exist $r_{k}\\in\\mathbb{N}$ and a pair of graphs $G,H$ , such that $k{-}\\mathrm{WL}(G)=k{-}\\mathrm{WL}(H)$ and $r_{k}{-}\\ell\\mathrm{WL}(G)\\neq r_{k}{-}\\ell\\mathrm{WL}(H)$ . ", "page_idx": 24}, {"type": "text", "text": "The hereditary treewidth $\\operatorname{hdtw}(F)$ of a graph $F$ is the maximum treewidth of $\\varphi(F)$ where $\\varphi$ is an edge surjective homomorphism. Neuen (2024) has shown that $k$ -WL can subgraph-count a graph $F$ if and only if $\\mathrm{hdtw}(F)\\leq k$ . This directly implies that for $F$ with hereditary treewidth larger than $k$ , there exist graphs $G_{F},H_{F}$ with $k\\mathrm{-WL}(\\dot{G}_{F})\\;\\overline{{{=}}}\\;k\\mathrm{-WL}(H_{F})$ and $\\operatorname{sub}(F,{\\dot{G}})\\neq\\operatorname{sub}(F,{\\dot{H}})$ . ", "page_idx": 24}, {"type": "text", "text": "Since the hereditary tree-width of cycle graphs is not uniformly bounded (Arvind et al., 2020), for every $k>0$ there exists a cycle $C_{c_{k}}$ of length $c_{k}\\in\\mathbb{N}$ with hereditary treewidth larger than $k$ . Setting $F=C_{c_{k}}$ concludes the existence proof with $r_{k}=c_{k}-2$ . ", "page_idx": 24}, {"type": "text", "text": "To see that $r_{k}\\,\\in\\,O(k^{2})$ , note that the complete graph $K_{n}$ on $n$ vertices has treewidth $n-1$ and exactly $\\binom{n}{2}$ edges. For odd $n$ , $K_{n}$ is Eulerian, i.e., there exists an edge surjective homomorphism from a cycle to $K_{n}$ which uses each edge exactly once, i.e., from $C_{\\left(n\\atop2\\right)}$ . If $n$ is odd, the minimum $T$ -join which makes $K_{n}$ Eulerian contains exactly $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ edges (see, e.g., Korte et al., 2018). As a result, there exists an edge surjective homomorphism from $C_{\\left(n\\atop2\\right)}$ to $K_{n}$ if $n$ is odd, and an edge surjective homomorphism from $C_{\\left({n\\atop2}\\right)+{\\frac{n}{2}}}$ to $K_{n}$ if $n$ is even. This implies that hd $\\mathrm{tw}(C_{c_{k}})>k$ for $\\begin{array}{r}{c_{k}={\\binom{k+1}{2}}+{\\lceil\\frac{k+1}{2}\\rceil}}\\end{array}$ . Hence, $r_{k}:=c_{k}-2\\in O(k^{2})$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "G Appendix on Homomorphism Counting and Section 5.3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide background information and all proofs related to homomorphism counts.   \nWe begin by introducing additional definitions and notation. ", "page_idx": 24}, {"type": "text", "text": "Definition 12 (Induced Subgraph). Let $G=(V(G),E(G))$ and $S\\subset V(G)$ . The induced subgraph $G[S]$ of $G$ over $S$ is defined as the graph $G[S]$ with vertices $V(G[S])=S$ and edges $E(G[S])=$ $\\{\\{\\dot{u_{}},\\dot{v}\\}\\in E(G)\\,|\\,u,v\\in S\\}$ . ", "page_idx": 24}, {"type": "text", "text": "The following definition indicates whether a pair of nodes is connected by an edge or not. ", "page_idx": 24}, {"type": "text", "text": "Definition 13 (Atomic Type). For a tuple of nodes $(u_{1},u_{2})$ , the atomic type $\\mathrm{atp}_{G}\\left(\\left(u_{1},u_{2}\\right)\\right)$ of $G$ over $(u_{1},u_{2})$ indicates where $\\{u_{1},u_{2}\\}\\in E(G),$ , i.e., $\\mathrm{atp}_{G}((u_{1},u_{2}))=1$ if $\\{u_{1},u_{2}\\}\\in E(G)$ and zero otherwise. ", "page_idx": 24}, {"type": "text", "text": "We continue by defining tree graphs, an important class of graphs closely related to the 1-WL test. ", "page_idx": 24}, {"type": "text", "text": "Definition 14 (Tree Graph). $A$ graph $T$ is called $a$ tree (graph) if it is connected and does not contain cycles. $A$ rooted tree $\\bar{T^{s}}\\,\\,\\bar{=\\,}\\,(V(T^{s}),E(T^{s}))$ is a tree in which a node $s\\ \\in\\ V(T^{s})$ is singled out. This node is called the root of the tree. For each vertex $t\\,\\in\\,V(T^{s})$ , we define its depth $\\mathrm{dep}_{T^{s}}(t)\\,:=\\,\\mathrm{dist}_{T^{s}}(t,s),$ , where dist denotes the shortest path distance between $t$ and $s$ . The depth of $T^{s}$ is then the maximum depth among all nodes $t\\in V(T)$ . We define $\\mathrm{Desc}_{T^{s}}(t)$ the set of descendants of $t,$ , i.e., $\\mathrm{)esc}_{T^{s}}(t)\\,=\\,\\{t^{\\prime}\\in T^{s}\\ |\\ \\mathrm{dep}_{T^{s}}(t^{\\prime})=\\mathrm{dep}_{T^{s}}(t)+\\mathrm{e}_{\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}$ distT s(t, t\u2032)}. For each $t\\;\\in\\;V(T^{s})\\;\\backslash\\;\\dot{\\{}s\\}$ , we define the parent node $\\operatorname{pa}_{T^{s}}(t)$ of $t$ as the unique node $t^{\\prime}\\,\\in\\,\\mathcal{N}(t)$ such that $\\mathrm{dep}_{T^{s}}(t)\\,=\\,\\mathrm{dep}_{T^{s}}(t^{\\prime})+1$ . We define the subtree of $T^{s}$ rooted at node $t$ by $T^{s}[t]$ , i.e., $T^{s}[t]:=T^{s}[\\mathrm{Desc}_{T^{s}}(t)]$ . ", "page_idx": 24}, {"type": "text", "text": "The remainder of this section is structured as follows. Appendix G.1 introduces the basics of tree decompositions. In Appendix G.2, we present the class of fan cactus graphs, encompassing all cactus graphs, and develop its canonical tree decomposition. We present an alternative formulation of $r$ -\u2113WL in Appendix G.3 for technical reasons. Subsequently, in Appendix G.4, we define the unfolding tree of $r$ -\u2113WL and illustrate its relation to the $r$ -\u2113WL colors and canonical tree decompositions of fan cactus graphs. Finally, in Appendix G.4.1, we establish the groundwork to conclude the proof of Theorem 2, a simple corollary of all the results in this section. ", "page_idx": 25}, {"type": "text", "text": "G.1 Tree Decomposition Preliminaries ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Along with its notation, this subsection closely adheres to the conventions outlined by B. Zhang et al.   \n(2024, Section C). We start with a formal definition of a tree decomposition for a graph. ", "page_idx": 25}, {"type": "text", "text": "Definition 15 (Tree Decomposition). Let $G\\,=\\,(V(G),E(G))$ . $A$ tree decomposition of $G$ is $a$ tree $\\boldsymbol{T}\\,=\\,(V(T),E(T))$ together with a function $\\beta_{T}\\,:\\,V(T)\\,\\to\\,2^{V(G)}$ satisfying the following conditions: ", "page_idx": 25}, {"type": "text", "text": "1. Each tree node $t\\in V(T)$ is mapped to a non-empty subset of vertices $\\beta_{T}(t)\\subset V(G)$ in $G$ , referred to as a bag. We say tree node t contains vertex u if $u\\in\\beta_{T}(t)$ .   \n2. For each edge $\\{u,v\\}\\,\\in\\,E(G)$ , there exists at least one tree node $t\\,\\in\\,V(T)$ such that $\\{u,v\\}\\subset\\beta_{T}(t)$ .   \n3. For each vertex $u\\in V(G)$ , all tree nodes $t$ containing $u$ form a connected subtree, i.e., the induced subgraph $T\\left[\\{t\\in V(T):u\\in\\beta_{T}(t)\\}\\right]$ is connected. ", "page_idx": 25}, {"type": "text", "text": "If $(T,\\beta_{T})$ is a tree decomposition of $G$ , we refer to the tuple $(G,T,\\beta_{T})$ as a tree-decomposed graph. The width of the tree decomposition $T$ of $G$ is defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t\\in V(T)}|\\beta_{T}(t)|-1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If $T$ has root s, we also denote it as $(G,T^{s},\\beta_{T})$ . ", "page_idx": 25}, {"type": "text", "text": "Definition 16 (Treewidth). The treewidth of a graph $G$ , denoted as $\\operatorname{tw}(G)$ , is the minimum positive integer $k$ such that there exists a tree decomposition of width $k$ . ", "page_idx": 25}, {"type": "text", "text": "G.2 Cactus Graphs and their Canonical Tree Decomposition ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Cactus graphs play a crucial role in graph theory due to their unique structural properties. Before delving into their canonical tree decomposition, we define the concept of a rooted $r$ -cactus graph. To simplify the notation, we assume that graphs in this section are connected and that $V(G)\\subseteq\\mathbb{N}$ for all graphs $G$ . Further, we assume that $r\\in\\mathbb{N}$ throughout this section. ", "page_idx": 25}, {"type": "text", "text": "Definition 17 (Rooted $r$ -Cactus Graph). $A$ cactus graph is a graph where every edge lies on at most one simple cycle. An $r$ -cactus graph is a cactus graph where every simple cycle has at most $r$ vertices. A rooted cactus (graph) $G^{s}$ is a cactus graph $G$ with a root node $s\\in V(G)$ . ", "page_idx": 25}, {"type": "text", "text": "Now, we introduce the notion of a fan cactus, which is an essential concept for our subsequent discussions on the canonical tree decomposition of these graphs. ", "page_idx": 25}, {"type": "text", "text": "Definition 18 (Fan Cactus). Let $G^{s}$ be a rooted $r$ -cactus. For every simple cycle $C$ in $G$ let $v_{C}$ be the unique vertex in $C$ that is closest to s. We obtain $a$ fan $r$ -cactus $F^{s}$ from a rooted $r$ -cactus $G^{s}$ by adding an arbitrary number of edges $\\{v_{C},w\\}$ to any cycle $C$ with $w\\in V(C)$ . Let $\\mathcal{M}^{r+2}$ be the class of graphs $F$ with $s\\in V(F)$ such that $F^{s}$ is a fan $r$ -cactus. ", "page_idx": 25}, {"type": "text", "text": "Remark 1. Every $r$ -cactus is a fan $r$ -cactus. Every fan $r$ -cactus is outerplanar. Every outerplanar graph has tree-width at most 2. ", "page_idx": 25}, {"type": "text", "text": "Figure 10 shows an example of a fan 6-cactus. As fan cacti are outerplanar, graph isomorphism can be decided in linear time. One way to do so is to use a canonicalization function, that maps graphs to a unique representative of each set of isomorphic graphs. We denote the set of all such representatives as $\\mathring{\\mathcal{M}}^{r+2}/\\cong\\subseteq\\mathcal{M}^{r+2}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 3 (Colbourn et al. (1981)). There exists a function canon : $:\\mathcal{M}^{r+2}\\rightarrow\\mathcal{M}^{r+2}/\\cong s u$ ch that ", "page_idx": 25}, {"type": "text", "text": "1. $G\\cong\\operatorname{canon}(G)$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nG\\cong H\\Longleftrightarrow V(\\operatorname{canon}(G))=V(\\operatorname{canon}(H))\\land E(\\operatorname{canon}(G))=E(\\operatorname{canon}(H)).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, given $G\\in\\mathcal{M}^{r+2}$ , $\\operatorname{canon}(G)$ can be computed in linear time. ", "page_idx": 26}, {"type": "text", "text": "For each $G\\in\\mathcal{M}^{r+2}$ we denote the isomorphism between $G$ and $\\operatorname{canon}(G)$ as $\\operatorname{canon}_{G}$ . Colbourn et al. (1981) describe a bottom-up algorithm to obtain $\\operatorname{canon}(G)$ of a fan $r$ -cactus $G$ . We will implicitly use the results of this canonicalization to define a canonical tree decomposition of fan $r$ -cacti. The crucial point in the algorithm is a simple way to decide which \u201cdirection\u201d to use when dealing with a cycle in the underlying cactus graph. Each undirected, rooted cycle allows for a choice between two directions when building a tree decomposition. We will first define a tree decomposition for a rooted cycle which depends on a choice of direction and then define a canonical direction of cycles in $G$ based on $\\operatorname{canon}_{G}$ . ", "page_idx": 26}, {"type": "text", "text": "Definition 19 (Tree Decomposition of Rooted Cycle). Let $C_{n}$ be a cycle graph on n nodes $v_{0}$ to $v_{n-1}$ . The path $T$ on nodes $w_{1},\\dots,w_{2n-3}$ with bags $\\beta(w_{1})=\\{v_{0},v_{1}\\}$ and for $i\\geq2$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\beta(w_{i})={\\left\\{\\begin{array}{l l}{\\beta(w_{i-1})\\cup\\{v_{i/2+1}\\}}&{\\ i f i\\ i s\\ e\\nu e n}\\\\ {\\beta(w_{i-1})\\setminus\\{v_{(i-1)/2}\\}}&{\\ i f i\\ i s\\ o d d}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is a tree decomposition of $C_{n}$ . We say that $v_{0}$ and $v_{1}$ correspond to $w_{1}$ and $v_{i}$ corresponds to $w_{2i-1}$ for $i\\geq2$ . ", "page_idx": 26}, {"type": "text", "text": "A depiction of the tree decomposition $T^{0}$ (right) of $C_{6}$ (left) is shown below. Note that we have to choose one of two possible orientations of the undirected cycle to construct $T^{0}$ . We address this choice in the next definition. ", "page_idx": 26}, {"type": "image", "img_path": "9O2sVnEHor/tmp/692a7d13abdf3d94da6fc6ebb44deaa43265093a7b3656d078fa6b7fe25b4c24.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Definition 20 (Canonical Tree Decomposition of Undirected Rooted Cycle). Let $F^{s}$ be a fan $r$ -cactus and $C$ be a simple cycle in the underlying cactus $G$ . Let $v_{C},v_{1},\\ldots,v_{n-1}$ and $v_{C},v_{n-1},\\ldots,v_{1}$ be the two directions of $C$ rooted at $v_{C}$ . We define the canonical tree decomposition of $C$ in $G$ as the tree decomposition of the smaller of the two orientations canon $_{F}(v_{C})$ , ${\\mathrm{canon}}_{F}(v_{1}),\\ldots,{\\mathrm{canon}}_{F}(v_{n-1})$ and canon $_{F}(v_{C})$ , ${\\mathrm{canon}}_{F}(v_{n-1}),\\ldots,{\\mathrm{canon}}_{F}(v_{1})$ . ", "page_idx": 26}, {"type": "text", "text": "The choice of \u201csmaller\u201d does not matter as long as it defines a total order. One can, for example, use a lexicographical order. Based on Definition 20, we now define a canonical tree decomposition of fan cactus graphs, in the sense that any two isomorphic fan cactus graphs will have isomorphic tree decompositions. ", "page_idx": 26}, {"type": "text", "text": "Definition 21 (Canonical Tree Decomposition of Fan $r$ -Cactus Graphs). Let $F^{s}$ be a fan $r$ -cactus and $G^{s}$ its underlying $r$ -cactus. We define the canonical tree decomposition $T^{\\tilde{s}}$ of $F$ rooted at $\\tilde{s}$ as follows ", "page_idx": 26}, {"type": "text", "text": "1. Node Gadget: For all $v\\in V(F)$ add a node $t$ to $V(T)$ and set $\\beta(t)=\\{v\\}$ . We choose s\u02dc such that $\\bar{\\beta}(\\tilde{s})=\\{s\\}$ .   \n2. Tree Edge Gadget: For all $\\{v,w\\}\\in E(G)$ that are not on a simple cycle in $F$ add a node $x_{\\{v,w\\}}$ to $V(T)$ with $\\beta(x_{\\{v,w\\}})=\\{v,w\\}$ and edges $\\left\\{v,x_{\\{v,w\\}}\\right\\}$ and $\\left\\{w,x_{\\{v,w\\}}\\right\\}$ to $E(T)$   \n3. Cycle Gadget: For each (undirected) cycle $C$ in the underlying cactus $G$ , add a copy of its canonical tree decomposition $T_{C}^{v_{C}}$ of $C$ rooted at $v_{C}$ to $T$ and connect nodes in it to the corresponding node gadgets. ", "page_idx": 26}, {"type": "text", "text": "See Figure 10 for an illustration. For the discussions in subsequent sections, we introduce the following definition. ", "page_idx": 26}, {"type": "image", "img_path": "9O2sVnEHor/tmp/66bad1d9af01dc05e445413a953baa76ec1119c5a9a6bec23a6af8a477005f12.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 10: Example of a fan 6-cactus $F^{1}$ (left) and its canonical tree decomposition $(T,1)$ . The underlying rooted 6-cactus $G^{1}$ (on colored, thick edges) of $F^{1}$ contains three simple cycles $C_{1},C_{2},C_{3}$ . Additional diagonal edges must have $v_{C_{i}}$ as one endpoint. ", "page_idx": 27}, {"type": "text", "text": "Definition 22 (Depth in the Canonical Tree Decomposition of Fan $r$ -Cactus Graphs). Let $(F,T^{s})$ be a canonical tree decomposition of a fan $r$ -cactus. We define the depth $\\mathrm{dep}(t)$ of $t\\in V(T)$ recursively as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\nI.\\ \\mathrm{dep}(s)=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The depth of $(F,T^{s})$ is then the maximum depth of any node $t\\in V(T^{s})$ . ", "page_idx": 27}, {"type": "text", "text": "Intuitively, for a given fan $r$ -cactus graph $F$ with its canonical tree decomposition $T^{s}$ , Definition 22 captures the depth (see Definition 15) of the tree $T^{s}$ , if cycles in $F$ and the corresponding bags in $T^{s}$ were replaced by single edges. ", "page_idx": 27}, {"type": "text", "text": "Lemma 4. Let $F^{s}$ be a fan $r$ -cactus. The canonical tree decomposition $(F,T^{\\tilde{s}})$ is a tree decomposition of $F^{s}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. We need to show that (1) $T$ is a tree, (2) for every edge $e\\in E(F)$ there exists some bag $\\beta(v)$ with $e\\subseteq\\beta(v)$ , and (3) $T[\\{t\\in V(T):u\\in\\beta(t)\\}]$ is connected. ", "page_idx": 27}, {"type": "text", "text": "To see that $T$ does not contain cycles, note that we replace each cycle with its cycle gadget, which is a path. It is easy to see that $T$ is connected as $G$ is connected. ", "page_idx": 27}, {"type": "text", "text": "For (2), note that tree edges $e\\in V(F)$ have their own gadget node in $x_{e}$ with $\\beta(x_{e})=e$ . Similarly, each edge $e$ on a simple cycle $C$ of the underlying cactus $F$ of $G$ is contained in some bag within the cycle gadget of $C$ . Finally, for diagonal edges $\\{v_{C},w\\}\\in E(F)\\setminus E(G)$ , $v_{C}$ is contained in any bag of the cycle gadget of $C$ . As a result, $\\{v_{C},v\\}$ is contained in the bag of the corresponding node of $v$ . ", "page_idx": 27}, {"type": "text", "text": "For (3), note that in the tree edge gadget, nodes $t$ with $v\\in\\beta(t)$ are connected to the node gadget of $v$ . In the cycle gadget, any node $t$ with $w\\in\\beta(t)$ is either directly or via its neighbor connected to the node gadget of $w$ if $w\\ne v_{C}$ . As the cycle gadget is connected and $v_{C}$ is in any bag of the gadget, a path to the node gadget of $v_{C}$ exists where every bag contains $v_{C}$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "We conclude this subsection with a formal definition of when two canonical tree decompositions are isomorphic and prove the main result of this section, i.e., that canonical tree decompositions of fan $r$ -cacti $\\bar{G}^{s},H^{t}$ are isomorphic whenever $G^{s},H^{t}$ are isomorphic. ", "page_idx": 28}, {"type": "text", "text": "Definition 23 (Isomorphism between canonical tree-decomposed graphs). Given two canonical tree-decomposed graphs $(G,T^{s})$ and $(\\Tilde{G},\\Tilde{T}^{\\Tilde{s}})$ , a pair of mappings $(\\rho,\\tau)$ is called an isomorphism between $(G,T^{s})$ and $(\\tilde{G},\\tilde{T}^{\\tilde{s}})$ , denoted by $(G,T^{s})\\cong({\\tilde{G}},{\\tilde{T}}^{\\tilde{s}})$ , if the following holds: ", "page_idx": 28}, {"type": "text", "text": "\u2022 $\\rho$ is an isomorphism between $G$ and $\\tilde{G}$ , \u2022 $\\tau$ is an isomorphism between $T^{s}$ and $\\tilde{T}^{\\tilde{s}}$ , \u2022 For any $t\\in T^{s}$ , we have $\\rho(\\beta_{T}(t))=\\beta_{\\tilde{T}}(\\tau(t))$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma 5. Let $G^{s}\\cong H^{t}$ be rooted $r$ -fan cacti. Then $(G^{s},T[G^{s}])\\cong(H^{t},T[H^{t}]).$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $\\rho$ be a root preserving isomorphism between $G^{s}$ and $H^{t}$ . According to Lemma 3 then there exist isomorphisms $\\operatorname{canon}_{G}$ and $\\mathrm{canon}_{H}$ with $\\rho=\\operatorname{canon}_{G}\\circ\\operatorname{canon}_{H}^{-1}$ . We construct from $\\rho$ as follows: It is easy to see that $\\rho$ induces a bijective mapping between the nodes of $T[G^{s}]$ and $\\dot{T}[H^{t}]$ that assigns each gadget node $v\\in V(T[G^{s}])$ to the unique gadget node $\\tau(v)\\in V(T[H^{t}])$ with $\\beta(\\bar{\\tau}(v))=\\rho(\\beta(v))$ . By the same argument, $\\tau$ maps the root of $T[G^{s}]$ to the root of $T[H^{\\bar{t}}]$ . ", "page_idx": 28}, {"type": "text", "text": "Now assume by contradiction that $\\tau$ is not an isomorphism between $T[G^{s}]$ and $T[H^{t}]$ . That means that w.l.o.g. there exists $\\{v,w\\}\\,\\in\\,E(T[G^{s}])$ with $\\bar{\\{\\tau(v),\\tau(w)\\}}\\,\\not\\in\\,\\dot{E}(T[H^{t}])$ . However, for the bags of $v,w$ it holds $\\mathrm{canon}_{G}(\\beta(v))=\\mathrm{canon}_{H}(\\beta(\\tau(v)))$ and ${\\mathrm{canon}}_{G}(\\beta(w))\\stackrel{}{=}{\\mathrm{canon}}_{H}(\\beta(\\tau(w)))$ . This cannot happen, as the addition of edges in Definition 21 depends only on the images of the bags under canon. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "G.3 Alternative $r$ -\u2113WL ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this subsection, we define slightly modified versions of 1-WL and $r$ -\u2113WL that we consider in the subsequent sections. ", "page_idx": 28}, {"type": "text", "text": "Definition 24 (Alternative 1-WL and $r$ -\u2113WL). The alternative 1-WL test refines vertices\u2019 colors as ", "page_idx": 28}, {"type": "equation", "text": "$$\nc^{(t+1)}(v)\\gets\\mathrm{HASH}\\left(c^{(t)}(v),\\left\\{\\left\\{\\left(\\operatorname{atp}(v,u),c^{(t)}(u)\\right)\\ |\\ u\\in V(G)\\right\\}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Equivalently, we define the alternative $r$ -\u2113WL via ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{r}^{(t+1)}(v)\\leftarrow\\mathrm{HASH}_{r}\\left(c_{r}^{(t)}(v),\\left\\{\\left\\{\\left(\\mathrm{atp}(v,u),c^{(t)}(u)\\right)\\ |\\ u\\in V(G)\\right\\}\\right\\},\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\ |\\ \\mathbf{p}\\in\\mathcal{N}_{1}(v)\\right\\}\\right\\},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\vdots}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\ |\\ \\mathbf{p}\\in\\mathcal{N}_{r}(v)\\right\\}\\right\\}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is well-known that both the alternative 1-WL test and the standard 1-WL test are equally powerful (in terms of their expressive power). Similarly, the alternative $k$ -WL test and the standard $k$ -WL test are equally powerful. For the sake of simplicity in the subsequent discussion, we will refer to both the alternative 1-WL and $k$ -WL tests simply as the 1-WL and $k$ -WL tests, respectively. Although this practice may seem like a slight abuse of notation, it is justified because the expressive power of these tests remains unaffected. ", "page_idx": 28}, {"type": "text", "text": "Finally, as noted in Section 6, we alter the $r$ -\u2113WL algorithm slightly by incorporating atomic types into the path representation. Hence, we update node features according to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{r}^{(t+1)}(v)\\gets\\mathrm{HASH}_{r}\\left(c_{r}^{(t)}(v),\\Big\\{\\Big\\{\\Big(\\mathrm{atp}(v,u),c^{(t)}(u)\\Big)~\\lvert~u\\in V(G)\\Big\\}\\Big\\},\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\Big\\{\\Big\\{\\Big(\\mathrm{atp}(v,\\mathbf{p}),c_{r}^{(t)}(\\mathbf{p})\\Big)~\\lvert~\\mathbf{p}\\in\\mathcal{N}_{1}(v)\\Big\\}\\right\\},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\vdots}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\Big\\{\\Big\\{\\Big(\\mathrm{atp}(v,\\mathbf{p}),c_{r}^{(t)}(\\mathbf{p})\\Big)~\\lvert~\\mathbf{p}\\in\\mathcal{N}_{r}(v)\\Big\\}\\Big\\}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\Big(\\mathrm{atp}(v,\\mathbf{p}),c_{r}^{(t)}(\\mathbf{p})\\Big):=\\Big(\\Big(\\mathrm{atp}(v,p_{1}),c_{r}^{(t)}(p_{1})\\Big)\\,,\\dots,\\Big(\\mathrm{atp}(v,p_{q+1}),c_{r}^{(t)}(p_{q+1})\\Big)\\Big)$ for $\\mathbf{p}=$ $\\{p_{i}\\}_{i=1}^{q+1}\\in\\mathcal{N}_{q}(v)$ . The definition of atomic types $\\operatorname{atp}(\\cdot,\\cdot)$ is given in Definition 13. Clearly this version of $r$ -\u2113WL is more powerful than the standard version, according to Definition 3. However, it is unclear whether $r$ -\u2113WL with atomic types is strictly more powerful than standard $r{-}\\ell\\mathbf{W}\\mathbf{L}$ . ", "page_idx": 29}, {"type": "text", "text": "G.4 The Unfolding Tree of $r$ -\u2113WL ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Given Definition 20, we assume, for the remainder of this appendix, that every fan cactus graph has a unique labeling function, allowing us to select a unique orientation for every cycle in the graph. We call this orientation the canonical orientation. If not otherwise mentioned, we consider the canonical orientation of cycle graphs. ", "page_idx": 29}, {"type": "text", "text": "We begin this section by introducing a critical concept known as bag isomorphism (Dell et al., 2018;   \nB. Zhang et al., 2024). ", "page_idx": 29}, {"type": "text", "text": "Definition 25 (Bag Isomorphism). Let $(F,T^{s})$ be a tree-decomposed graph, and $G$ be a graph. A homomorphism $f$ from $F$ to $G$ is called $a$ bag isomorphism from $(F,T^{s})$ to $G$ if, for all $t\\in V(T^{s})$ , the mapping $f$ is an isomorphism from $F[\\beta_{T^{s}}(t)]$ to $G[f(\\beta_{T^{s}}(t))]$ . We denote by $\\operatorname{BIso}((F,T^{s}),G)$ the set of all bag isomorphisms from $(F,T^{s})$ to $G$ , and set bIs $\\mathsf{o}((\\bar{F},T^{s}),G)=|\\mathrm{BIso}((\\bar{F},T^{s}),G)|$ . ", "page_idx": 29}, {"type": "text", "text": "Moving forward, we proceed to define $r$ -\u2113WL unfolding trees, which intuitively construct, for a given graph and a node in the graph, the computational graph of the $r$ -\u2113WL algorithm and its canonical tree decomposition. ", "page_idx": 29}, {"type": "text", "text": "Definition 26 (Unfolding tree of $r{-}\\ell\\mathbf{W}\\mathbf{L}$ ). Given a graph $G_{i}$ , vertex $v\\in V(G)$ and a non-negative $\\textit{D}\\in\\mathrm{~\\mathbb{Z}~}$ , the depth- $2D$ $r$ -\u2113WL unfolding tree of a graph $G\\ \\in\\ {\\mathcal{M}}^{r+2}$ at node $v$ , denoted as $\\left(F^{(D)}(v),T^{(D)}(v)\\right)$ , is a tree-decomposition $(F,T^{s})$ constructed in the following way: ", "page_idx": 29}, {"type": "text", "text": "1. Initialization: $V(F)=\\{v\\}$ without edges, and $T^{s}$ only has a root node s with $\\beta_{T^{s}}(s)=$ $\\{v\\}$ . Define a mapping ${\\dot{V}}({\\dot{F}})\\to V(G)$ as $\\pi(v)=v$ . ", "page_idx": 29}, {"type": "text", "text": "2. Introduce nodes: For each leaf node $t$ with $|\\beta_{T^{s}}(t)|=1$ in $T^{s}$ , do the following procedure: ", "page_idx": 29}, {"type": "text", "text": "Let $\\beta_{T}(t)=\\{g\\}$ . For each $w\\in V(G)$ do the following:   \na) Add a fresh child $t_{w}$ to t in $T^{s}$ .   \n$^b$ ) Add a fresh vertex $f$ to $F$ and extend $\\pi$ with $[f\\mapsto w]$ . c) Define the bag of $t_{w}$ by $\\beta_{T^{s}}(t_{w})=\\beta_{T^{s}}(t)\\cup\\{f\\}$ .   \nd) Add an edge between $f$ and g if $\\left\\{\\pi(f),\\pi(g)\\right\\}\\in E(G)$ . ", "page_idx": 29}, {"type": "text", "text": "3. Introduce paths: For each $q=1,\\hdots,r_{!}$ , do: For each length q path with canonical orientation $\\mathbf{p}=\\{p_{i}\\}_{i=1}^{q+1}\\in\\mathcal{N}_{q}(g),$ , do the following: a) Add a fresh path $\\mathbf{t_{p}}\\;=\\;\\left\\{t_{\\{p_{1}\\}},t_{\\{p_{1},p_{2}\\}},t_{\\{p_{2}\\}},\\ldots,t_{\\{p_{q}\\}},t_{\\{p_{q},p_{q+1}\\}},t_{\\{p_{q+1}\\}}\\right\\}$ to $t$ in $T^{s}$ . $^b$ ) Add $q+1$ fresh vertices $f_{1},\\dotsc,f_{q+1}$ to $F$ and extend $\\pi$ with $[f_{i}\\ \\mapsto\\ p_{i}]$ for every $i=1,\\dotsc,q+1$ . ", "page_idx": 29}, {"type": "image", "img_path": "9O2sVnEHor/tmp/2c782c436d0d86b58fc6766cba6d3692312df51dd43aa4670bba5a51a46aeeb8.jpg", "img_caption": ["Figure 11: The depth-2 unfolding tree of graph $G$ at vertex 1 for $2{-}\\ell\\mathbf{W}\\mathbf{L}$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "$c$ ) For $i=1,\\dots,q,$ , let the bag of $t_{\\{p_{i},p_{i+1}\\}}$ be defined via $\\beta_{T^{s}}(t_{\\{p_{i},p_{i+1}\\}})=\\beta_{T^{s}}(t)\\cup$ $\\{f_{i},f_{i+1}\\}$ .   \n$d$ ) For $i=1,\\ldots,q+1$ , let the bag of $t_{\\{p_{i}\\}}$ be defined via $\\beta_{T^{s}}(t_{\\{p_{i}\\}})=\\beta_{T^{s}}(t)\\cup\\{f_{i}\\}$ .   \n$e_{.}$ ) For $i=1,\\ldots,q+1,$ , add edges between $f_{i}$ and $f_{i+1}$ .   \n$f)$ Add edges between $g$ and $f_{1},\\ldots,f_{q+1}$ such that for every $i\\,=\\,1,.\\,.\\,,q$ , we have $F[\\beta_{T^{s}}(t_{\\{p_{i},p_{i+1}\\}})]\\,=\\,F[\\{f_{i},f_{i+1},g\\}]\\,\\cong\\,G[\\pi(\\beta_{T^{s}}(t_{\\{p_{i},p_{i+1}\\}}))].$ , i.e., add edges between $g$ and $f_{i}$ if and only if there is an edge between $\\{\\pi(g),\\dot{\\pi}(f_{i})\\}\\in E(G)$ . ", "page_idx": 30}, {"type": "text", "text": "4. Forget nodes: If $t$ is a leaf node of $T^{s}$ with $|\\beta_{T^{s}}(t)|=2$ and parent $t^{\\prime}$ with $|\\beta_{T^{s}}(t)|=1$ , do the following: ", "page_idx": 30}, {"type": "text", "text": "a) Add a fresh child $t_{1}$ of $t$ to $T^{s}$ .   \n$^b$ ) Let $f$ be that vertex introduced at $t$ , that is, we have $\\beta_{T^{s}}(t)\\setminus\\beta_{T^{s}}(t^{\\prime})=\\{f\\}$ .   \n$c_{.}$ ) We set $\\beta_{T^{s}}(t_{1})=\\{f\\}$ . ", "page_idx": 30}, {"type": "text", "text": "5. Forget paths: If $\\mathbf{t_{p}}=\\left\\{t_{\\{p_{1}\\}},t_{\\{p_{1},p_{2}\\}},t_{\\{p_{2}\\}},\\dots,t_{\\{p_{q}\\}},t_{\\{p_{q},p_{q+1}\\}},t_{\\{p_{q+1}\\}}\\right\\}$ is a leaf path of $T^{s}$ with parent $t^{\\prime}$ of $t_{\\{p_{1}\\}}$ , do the following: a) For $i=2,\\ldots,q+1,$ , add a fresh child $\\tilde{t}_{\\{p_{i}\\}}\\;t o\\;t_{\\{p_{i}\\}}$ . $^b$ ) Let $f_{2},\\ldots,f_{q+1}$ be the vertices introduced at $\\bf{t_{p}}$ , that is, we have $\\beta_{T^{s}}(t_{\\{p_{i}\\}})\\mathrm{~.~}$ $\\beta_{T^{s}}(t^{\\prime})=\\{f_{i}\\}$ . $c$ ) For $i=2,\\ldots,q+1$ , we set $\\beta_{T^{s}}(\\tilde{t}_{\\{p_{i}\\}})=\\{f_{i}\\}$ . ", "page_idx": 30}, {"type": "text", "text": "We refer to Figure 11 for the depth-2 2-\u2113WL unfolding tree of an example graph. ", "page_idx": 30}, {"type": "text", "text": "Theorem 4. Let $r\\,\\geq\\,1$ . For any graph $G$ , any vertex $v\\,\\in\\,V(G)$ , and any non-negative integer $D$ , let $\\left(F^{(D)}(v),T^{(D)}(v)\\right)$ be its depth- $2D$ $r$ -\u2113WL unfolding tree at node $v$ . Then, $F^{(D)}(v)$ is $a$ fan $r$ -cactus graph, and $\\dot{T}^{(D)}(v)$ is an $r$ -canonical tree decomposition of $F^{(D)}(v)$ . Moreover, the constructed mapping $\\pi$ in Definition $26$ is a bag isomorphism from $\\left(F^{(D)}(v),T^{(D)}(v)\\right)$ to the graph $G$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Clear by the definition of the depth- $2D$ unfolding tree of $r$ -\u2113WL. ", "page_idx": 30}, {"type": "text", "text": "We present the following results that fully characterize when two graphs and their respective nodes have the same $r$ -\u2113WL colors in terms of their $r$ -\u2113WL unfolding trees. ", "page_idx": 30}, {"type": "text", "text": "Theorem 5. Let $r\\in\\mathbb{N}$ . For any two connected graphs $G,H$ , any vertices $v\\in V(G)$ and $x\\in V(H)$ and any $D\\in\\mathbb{N}$ , it holds: $c_{r}^{({D})}(v)=c_{r}^{({D})}(x)$ if and only if there exists a root preserving isomorphism between $\\left(F^{(D)}(v),T^{(D)}(v)\\right)$ and $\\left(F^{(D)}(x),T^{(D)}(x)\\right)$ . ", "page_idx": 30}, {"type": "text", "text": "Proof of $\\begin{array}{r}{\\mathbf{\\Sigma}^{\\leftarrow}\\Longrightarrow\\mathbf{\\Sigma}^{\\bullet}}\\end{array}$ . The proof is based on induction over $D$ . When $D\\,=\\,0$ , the theorem obviously holds. Assume that the theorem holds for $D\\leq d$ , and consider $D\\,=\\,d+1$ . We show that if $c_{r}^{(d+1)}(v)\\,=\\,c_{r}^{(d+1)}(x)$ , then there exists an isomorphism $(\\rho,\\tau)$ from $\\left(F^{(d+1)}(v),T^{(d+1)}(v)\\right)$ to $\\left(F^{(d+1)}(x),T^{(d+1)}(x)\\right)$ such that $\\rho(v)=x$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{\\left(\\operatorname{atp}(v,u),c_{r}^{(d)}(u)\\right)\\mid u\\in V(G)\\right\\}\\right\\}=\\left\\{\\left\\{\\left(\\operatorname{atp}(x,y),c_{r}^{(d)}(y)\\right)\\mid y\\in V(H)\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "i.e., $|V(G)|\\ =\\ |V(H)|$ , and we set $n\\;=\\;|V(G)|$ . We enumerate $V(G)\\;=\\;\\{w_{1},\\ldots,w_{n}\\}$ and $V(H)=\\{z_{1},\\ldots,z_{n}\\}$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\nc_{r}^{(d)}(w_{i})=c_{r}^{(d)}(z_{i})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $i=1,\\hdots,n$ . Also, again since c(rd+1)(v) = c(rd+1)(x), we have for every q = 1, . . . , r, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\left\\{\\left(\\left(\\mathrm{atp}(v,u_{1}),c_{r}^{(d)}(u_{1})\\right),\\ldots,\\left(\\mathrm{atp}(v,u_{q+1}),c_{r}^{(d)}(u_{q+1})\\right)\\right)\\mid\\{u_{1},\\ldots,u_{q+1}\\}=\\mathbf{u}\\in\\mathcal{N}_{q}(v)\\right\\}\\right\\}}\\\\ &{=\\left\\{\\left\\{\\left(\\left(\\mathrm{atp}(x,y_{1}),c_{r}^{(d)}(y_{1})\\right),\\ldots,\\left(\\mathrm{atp}(x,y_{q+1}),c_{r}^{(d)}(y_{q+1})\\right)\\right)\\mid\\{y_{1},\\ldots,y_{q+1}\\}=\\mathbf{y}\\in\\mathcal{N}_{q}(x)\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In particular, $|\\mathcal{N}_{q}(v)|=|\\mathcal{N}_{q}(x)|$ and we can enumerate the paths in $\\mathcal{N}_{q}(v)$ and ${\\mathcal{N}}_{q}(x)$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\nc_{r}^{(d)}({\\bf u}_{l}^{q})=c_{r}^{(d)}({\\bf y}_{l}^{q})\\;\\;\\mathrm{and}\\;\\;\\mathrm{atp}(v,{\\bf u}_{l}^{q})=\\mathrm{atp}(v,{\\bf y}_{l}^{q})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for every $l=1,\\dots,|\\mathcal{N}_{q}(v)|$ . ", "page_idx": 31}, {"type": "text", "text": "Now, by definition of the $r{-}\\ell\\mathbf{W}\\mathbf{L}$ unfolding tree, the graph $F^{(d+1)}(v)$ is isomorphic to the union of: a) all graphs $F^{(d)}(w_{i})$ for $i=1,\\hdots,n$ , plus additional edges between $w_{i}$ to $v$ if $\\{w_{i},v\\}\\in E(G)$ , and b) all graphs $F^{(d)}(p_{l,k}^{q})$ for $q=1,\\ldots,r,l=1,\\ldots,|\\mathcal{N}_{q}(v)|$ for any path $\\mathbf{p}_{l}^{q}=\\left\\{p_{l,1}^{q},\\dots,p_{l,q+1}^{q}\\right\\}\\in$ $\\mathcal{N}_{q}(v)$ . And adding, for $k=1,\\dotsc\\ q$ , edges between $p_{l,k}^{q}$ and $p_{l,k+1}^{q}$ . And adding, for $k=1,\\dotsc q+1$ , edges $p_{l,k}^{q}$ and $v$ if there is one in $G$ , i.e., if $\\left\\{p_{l,k}^{q},v\\right\\}\\in E(G)$ . ", "page_idx": 31}, {"type": "text", "text": "Similarly, the tree $T^{(d+1)}(v)$ is isomorphic to the disjoint union of all trees $T^{(d)}(w_{i})$ (for $i=1,\\hdots,n)$ and $T^{(d)}(p_{l,k}^{q})$ (for $q=1,\\ldots,r,k=1,\\ldots,q+1$ and $l=1,\\dots,|\\mathcal{N}(v)|)$ . Plus adding the following fresh tree nodes and edges: a root node $s$ , nodes $t_{w_{i}}$ (for $i=1,\\dotsc,n)$ that connects to $s$ and the root of $T^{(d)}(w_{i})$ . And for $q=1,\\ldots,r,l=1,\\ldots,|\\mathcal{N}_{q}(v)|$ for any path $\\mathbf{p}_{l}^{q}\\in\\mathcal{N}_{q}(v)$ a path of length $2q$ , given by tplq = t plq,1 , t plq,1,plq,2 , . . . , t plq,q,plq,q+1 , t plq,q+1 , where s is attached to t pq  . And finally, connecting the trees T (d)(plq,k) at root node plq,k to t plq,k . ", "page_idx": 31}, {"type": "text", "text": "By (8) and induction, there exist isomorphisms $(\\rho_{i},\\tau_{i})$ from $(F^{(d)}(w_{i}),T^{(d)}(w_{i}))$ to $(F^{(d)}(z_{i}),T^{(d)}(z_{i}))$ such that $\\rho_{i}(w_{i})=z_{i}$ for $i=1,\\hdots,n$ . By (9) and induction, there exist isomorphisms $(\\rho_{l,k}^{q},\\tau_{l,k}^{q})$ from $(F^{(d)}(u_{l,k}^{q}),T^{(d)}(u_{l,k}^{q}))$ to $(F^{(d)}(y_{l,k}^{q}),T^{(d)}(y_{l,k}^{q}))$ such $\\rho_{i}\\bigl(u_{l,k}^{q}\\bigr)=y_{l,k}^{q}$ for $q=1,\\hdots,r$ , $k=1,\\ldots,q+1$ and $l=1,\\dots,|\\mathcal{N}_{q}(v)|$ . ", "page_idx": 31}, {"type": "text", "text": "We now construct $\\rho$ by merging all $\\rho_{i}$ and $\\rho_{l,k}^{q}$ , and construct $\\tau$ by merging all $\\tau_{i}$ and $\\tau_{l,k}^{q}$ . We finally specify an appropriate mapping for the tree root, its direct children and the paths attached to the tree root. Then, it is easy to see that $(\\rho,\\tau)$ is well-defined and an isomorphism between $\\left(F^{(d+1)}(v),T^{(d+1)}(v)\\right)$ and $\\left(F^{(d+1)}(x),T^{(d+\\hat{1})}(\\dot{x})\\right)$ such that $\\rho(v)=x$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Proof of $\\begin{array}{r}{\\overleftarrow{\\mathbf{\\Omega}}^{*\\epsilon}\\overleftarrow{\\mathbf{\\Omega}}\\longleftarrow{\\mathbf{\\Omega}}^{*}}\\end{array}$ . We now prove the other direction, again via induction over $D$ . When $D=0$ the assertion obviously holds. Assume that the assertion holds for $D\\leq d$ . Now, assume that there exists an isomorphism $(\\rho,\\tau)$ between $\\left(F^{(d+1)}(v),T^{(d+1)}(v)\\right)$ and $\\left(F^{(d+1)}(x),T^{(d+1)}(x)\\right)$ such that $\\rho(v)=x$ . We show that $c_{r}^{(d+1)}(v)=c_{r}^{(d+1)}(x)$ . ", "page_idx": 31}, {"type": "text", "text": "We begin our proof by establishing the equality of two multisets: $\\left\\{\\left\\{(c_{r}^{(d)}(w),\\mathrm{atp}(v,w))|w\\in V(G)\\right\\}\\right\\}$ and $\\left\\{\\left\\{(c_{r}^{\\bar{(d)}}(z),\\mathrm{atp}(v,\\bar{z}))|z\\in V(H)\\right\\}\\right\\}$ . The proof of this equivalence closely mirrors the argument presented in the proof of B. Zhang et al. (2024, Lemma C.14). Since $\\tau$ is an isomorphism it maps all tree nodes $T^{(d+1)}(v)$ of depth 2 with 1 element in their bag to the corresponding tree nodes in $T^{(d+1)}(x)$ . Let $s_{1},\\ldots,s_{n}$ and $t_{1},\\ldots,t_{n}$ be the nodes in $T^{(d+1)}(v)$ and $T^{(d+1)}(x)$ of depth 2 with 1 element in their bag, respectively. For $i=1,\\hdots,n$ , let $s_{i}^{\\prime}$ and $t_{i}^{\\prime}$ the parents of $s_{i}$ and $t_{i}$ , respectively. We then choose the order such that the following holds for all $i=1,\\hdots,n$ ", "page_idx": 31}, {"type": "text", "text": "2. $\\tau$ is an isomorphism from the subtree rooted at $s_{i}$ in $T^{(d+1)}(v)$ , i.e., $T^{(d+1)}(v)[s_{i}]$ , the subtree rooted at $t_{i}$ in $T^{(d+1)}(x)$ , i.e., $T^{(d+1)}(v)[t_{i}]$ . 3. For all $s\\in\\mathrm{Desc}_{T^{(d+1)}(v)}\\big(s_{i}\\big)$ , it holds $\\rho(\\beta_{T^{(d+1)}(v)}(s))=\\beta_{T^{(d+1)}(x)}(\\tau(s)).$ . 4. By the definition of the unfolding tree, $\\rho$ is an isomorphism from the induced subgraph $\\dot{F^{(d+1)}}(v)\\left[T^{(d+1)}(v)[s_{i}]\\right]$ and the induced subgraph $\\bar{F^{(d+1)}}(x)\\left[T^{(d+1)}(x)[t_{i}]\\right]$ . ", "page_idx": 32}, {"type": "text", "text": "By the last three items, we get that $\\left(F^{(d+1)}(v)\\left[T^{(d+1)}(v)[s_{i}]\\right],T^{(d+1)}(v)[s_{i}]\\right)$ and $\\left(F^{(d+1)}(x)\\left[T^{(d+1)}(x)[t_{i}]\\right],T^{(d+1)}(x)[t_{i}]\\right)$ are isomorphic. By definition of the $r$ -\u2113WL unfolding tree, $\\left(F^{(d+1)}(v)\\left[T^{(d+1)}(v)[s_{i}]\\right],T^{(d+1)}(v)[s_{i}]\\right)$ is isomorphic to $(F^{(d)}(w_{i}),T^{(d)}(w_{i}))$ for some $w_{i}~\\in~V(G)$ that satisfies $\\left\\{\\tilde{w}_{i},v\\right\\}\\ \\in\\ E_{F^{(d+1)}(v)}$ if and only if $\\{w_{i},v\\}\\ \\in\\ E(G)$ . And $\\left(F^{(d+1)}(x)\\left[T^{(d+1)}(x)[t_{i}]\\right],T^{(d+1)}(x)[t_{i}]\\right)$ is isomorphic to $(F^{(d)}(z_{i}),F^{(d)}(z_{i}))$ for some $z_{i}~\\in$ $V(H)$ that satisfies $\\{\\tilde{z}_{i},x\\}\\in E(F^{(d+1)}(x))$ if and only if $\\{z_{i},x\\}\\in E(G)$ . Hence, by induction, we have $\\mathrm{atp}\\left(v,w_{i}\\right)=\\mathrm{atp}\\left(x,z_{i}\\right)$ and $c_{r}^{(d)}(w_{i})=c_{r}^{(d)}(z_{i})$ for all $i=1,\\hdots,n$ . ", "page_idx": 32}, {"type": "text", "text": "It remains to show that, for every $q=1,\\hdots,r$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\left\\{\\left(\\left(\\mathrm{atp}(v,u_{1}),c_{r}^{(d)}(u_{1})\\right),\\ldots,\\left(\\mathrm{atp}(v,u_{q+1}),c_{r}^{(d)}(u_{q+1})\\right)\\right)\\mid\\{u_{1},\\ldots,u_{q+1}\\}=\\mathbf{u}\\in\\mathcal{N}_{q}(v)\\right\\}\\right\\}}\\\\ &{=\\left\\{\\left\\{\\left(\\left(\\mathrm{atp}(x,y_{1}),c_{r}^{(d)}(y_{1})\\right),\\ldots,\\left(\\mathrm{atp}(x,y_{q+1}),c_{r}^{(d)}(y_{q+1})\\right)\\right)\\mid\\{y_{1},\\ldots,y_{q+1}\\}=\\mathbf{y}\\in\\mathcal{N}_{q}(x)\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Fix $q=1,\\hdots,r$ . Since $\\tau$ is an isomorphism it maps all paths of length $q$ in $T^{(d+1)}(v)$ connected to $v$ to paths of length $q$ in $T^{(d+1)}(x)$ connected to $x$ . ", "page_idx": 32}, {"type": "text", "text": "By construction of the $r{-}\\ell\\mathbf{W}\\mathbf{L}$ unfolding tree and since $(\\rho,\\tau)$ is an isomorphism, it holds $\\left|\\mathcal{N}_{q}(v)\\right|=$ $|\\mathcal{N}_{q}(x)|$ . Denote the relevant bags at depth 2 by $s_{l,k}^{\\prime q}$ for $l=1,\\dots,|\\mathcal{N}_{q}(v)|$ and $k=1,\\ldots,q+1$ . Denote by $s_{l,k}^{q}$ and $t_{l,k}^{q}$ the parents of $s_{l,k}^{\\prime q}$ and $t_{l,k}^{\\prime q}$ , respectively. We then choose the order $l\\,=$ $1,\\ldots,|\\mathcal{N}_{q}(v)|$ and $k=1,\\ldots,q+1$ such that it holds ", "page_idx": 32}, {"type": "text", "text": "2. $\\tau$ is an isomorphism from the subtree rooted at $s_{l,k}^{q}$ in $T^{(d+1)}(v)$ , i.e., $T^{(d+1)}(v)[s_{l,k}^{q}]$ , to the subtree rooted at $t_{l,k}^{q}$ in $T^{(d+1)}(x)$ , i.e., $T^{(d+1)}(x)[t_{l,k}^{q}]$ . ", "page_idx": 32}, {"type": "text", "text": "3. For all $s\\in\\mathrm{Desc}_{T^{(d+1)}(v)}\\big(s_{l,k}^{q}\\big)$ , it holds $\\rho(\\beta_{T^{(d+1)}(v)}(s))=\\beta_{T^{(d+1)}(x)}(\\tau(s)).$ ", "page_idx": 32}, {"type": "text", "text": "4. By the definition of the unfolding tree, $\\rho$ is an isomorphism from the induced subgraph $\\dot{F^{(d+1)}}(v)\\left[T^{(d+1)}(v)[s_{l,k}^{q}]\\right]$ and the induced subgraph $\\bar{F}^{(d+1)}(x)\\left[T^{(d+1)}(x)[t_{l,k}^{q}]\\right]$ . ", "page_idx": 32}, {"type": "text", "text": "By the last three items, we get that $\\left(F^{(d+1)}(v)\\left[T^{(d+1)}(v)[s_{l,k}^{q}]\\right],T^{(d+1)}(v)[s_{l,k}^{q}]\\right)$ and $\\left(F^{(d+1)}(\\boldsymbol{x})\\left[T^{(d+1)}(\\boldsymbol{x})[t_{l,k}^{q}]\\right],T^{(d+1)}(\\boldsymbol{x})[t_{l,k}^{q}]\\right)$ are isomorphic. By definition of the $r$ -\u2113WL unfolding tree, $\\left(F^{(d+1)}(v)\\left[T^{(d+1)}(v)[s_{l,k}^{q}]\\right],T^{(d+1)}(v)[s_{l,k}^{q}]\\right)$ is isomorphic to $(F^{(d)}(w_{l,k}^{q}),T^{(d)}(w_{l,k}^{q}))$ for $w_{l,k}^{q}\\in V(G)$ that satisfies $\\left\\{\\tilde{w}_{l,k}^{q},v\\right\\}\\in E_{F^{(d+1)}(v)}$ if and only if $\\left\\{w_{l,k}^{q},v\\right\\}\\in E(G)$ . And $\\left(F^{(d+1)}(\\boldsymbol{x})\\left[T^{(d+1)}(\\boldsymbol{x})[t_{l,k}^{q}]\\right],T^{(d+1)}(\\boldsymbol{x})[t_{l,k}^{q}]\\right)$ is isomorphic to $(F^{(d)}(z_{l,k}^{q}),T^{(d)}(z_{l,k}^{q}))$ for $z_{l,k}^{q}\\in V(G)$ that satisfies $\\left\\{\\tilde{z}_{l,k}^{q},v\\right\\}\\in E(F^{(d+1)}(v))$ if and only if $\\left\\{z_{l,k}^{q},v\\right\\}\\in E(G)$ . Hence, by induction, we have $c_{r}^{(d)}(w_{l,k}^{q})\\,=\\,c_{r}^{(d)}(z_{l,k}^{q})$ for all indices. By Item 1, we then have $c_{r}^{(d)}(\\mathbf{w}_{1}^{\\mathbf{q}})~{=}$ $c_{r}^{(d)}(\\mathbf{z}_{1}^{\\mathbf{q}})$ and $\\mathrm{atp}(v,\\mathbf{w}_{l}^{q})=\\mathrm{atp}(v,\\mathbf{z}_{l}^{q})$ for every $q=1,\\hdots,r$ and $l=1,\\dots,|\\mathcal{N}_{q}(v)|$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "We introduce the following definition that provides a similarity measure between a graph and a tree-decomposed graph. ", "page_idx": 33}, {"type": "text", "text": "Definition 27. Given a graph $G$ and a tree-decomposed graph $(F,T^{s})$ , define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{cnt}\\left((F,T^{s}),G\\right)=\\left|\\left\\{v\\in V\\mid\\exists D\\in\\mathbb{N}\\,s.t.\\;\\left(F^{(D)}(v),T^{(D)}(v)\\right)\\cong\\left(F,T^{s}\\right)\\right\\}\\right|,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(F^{(D)}(v),T^{(D)}(v))$ is the depth- $2D$ $r$ -\u2113WL unfolding tree of $G$ at $v$ . ", "page_idx": 33}, {"type": "text", "text": "The counting function cnt $((F,T^{s}),G)$ serves as a key metric, allowing us to draw connections between $r$ -\u2113WL colorings of two different graphs. ", "page_idx": 33}, {"type": "text", "text": "Corollary 3. Let $r\\;\\in\\;\\mathbb{N}$ . Let $G$ and $H$ be two graphs. Then, $c_{r}(G)_{.}=\\,c_{r}(H)$ if and only $i f$ cnt $((F,T^{s}),G)=\\mathrm{cnt}\\left((F,T^{s}),H\\right)$ holds for all graphs $(F,T^{s})\\in\\mathcal{M}^{r+2}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof of $\\begin{array}{r}{\\lefteqn{\\mathrm{\\Sigma}^{\\epsilon\\epsilon}}=\\mathrm{\\Sigma}^{,\\bullet}}\\end{array}$ . Let $c_{r}(G)=c_{r}(H)$ , i.e., ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{c_{r}(v)\\;|\\;v\\in V(G)\\right\\}\\right\\}=\\left\\{\\left\\{c_{r}(x)\\;|\\;x\\in V(H)\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Assume, by contradiction, that there exists a tuple $(F,T^{s})\\in\\mathcal{M}^{r+2}$ such that cnt $((F,T^{s}),G)\\neq$ cnt $((F,T^{\\dot{s}}),H)$ . Let $c_{1},\\ldots,c_{k}$ be the final colors of nodes in $V(G)$ and $V(H)$ . Then, define for $i=1,\\ldots,k$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{at}\\left((F,T^{s}),G[c_{i}]\\right):=\\left|\\left\\{v\\in V(G)\\,|\\,c_{r}(v)=c_{i}\\mathrm{~and~}\\exists D\\in\\mathbb{N}\\mathrm{~s.t.~}(F^{(D)}(v),T^{(D)}(v))\\cong(F,T^{s})\\right\\}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{cnt}\\left((F,T^{s}),G\\right)=\\sum_{i=1}^{k}\\operatorname{cnt}\\left((F,T^{s}),G[c_{i}]\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{cnt}\\left((F,T^{s}),H\\right)=\\sum_{i=1}^{k}\\operatorname{cnt}\\left((F,T^{s}),H[c_{i}]\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $\\operatorname{cnt}\\big((F,T^{s}),G\\big)\\neq\\operatorname{cnt}\\big((F,T^{s}),H\\big)$ , there exist an index $i=1,\\ldots,k$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{cnt}\\left((F,T^{s}),G[c_{i}]\\right)\\neq\\mathrm{cnt}\\left((F,T^{s}),H[c_{i}]\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore, there exists $i_{n}\\in\\mathbb{N}$ such that there are exactly $n$ nodes $v_{1},\\ldots,v_{n}$ and $x_{1},\\ldots,x_{n}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\nc_{r}(v_{1})=.\\ldots=c_{r}(v_{i_{n}})=c_{i}{\\mathrm{~and~}}c_{r}(x_{1})=.\\ldots=c_{r}(x_{i_{n}})=c_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Hence, as $c_{r}$ refines $c_{r}^{(D)}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nc_{r}^{(D)}(v_{1})=.\\ldots=c_{r}^{(D)}(v_{i_{n}})=c_{r}^{(D)}(x_{1})=.\\ldots=c_{r}^{(D)}(x_{i_{n}}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By (10), there exists some $D\\in\\mathbb{N}$ such that (without loss of generality) $(F^{(D)}(v_{1}),T^{(D)}(v_{1}))\\cong$ $(F,T^{s})$ . Then, by Theorem 5, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(F^{(D)}(v_{1}),T^{(D)}(v_{1}))\\cong...\\cong(F^{(D)}(v_{i_{n}}),T^{(D)}(v_{i_{n}}))\\cong(F^{(D)}(x_{i_{n}}),T^{(D)}(x_{i_{n}}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cong...\\cong(F^{(D)}(x_{1}),T^{(D)}(x_{1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "There does not exist any other node $w$ with $c_{r}(w)=c_{1}$ such that the corresponding unfolding tree is isomorphic to $(F^{(D)}(v_{1}),T^{(D)}(v_{1}))$ . Hence, $\\operatorname{cnt}((F,T^{s}),G[c_{i}])=\\operatorname{cnt}((F,T^{s}),H[c_{i}])$ , which is a contradiction. ", "page_idx": 33}, {"type": "text", "text": "Proof of $\\begin{array}{r}{\\overleftarrow{\\mathbf{\\Omega}}^{*\\epsilon}\\overleftarrow{\\mathbf{\\Omega}}\\longleftarrow{\\mathbf{\\Omega}}^{*}}\\end{array}$ . Suppose that $\\operatorname{cnt}((F,T^{s}),G)\\,=\\,\\operatorname{cnt}((F,T^{s}),H)$ for all $(F,T^{s})\\,\\in\\,\\mathcal{M}^{r+2}$ . Let $c_{1},\\ldots,c_{k_{G}}$ with multiplicities $m_{1},\\ldots,m_{k_{G}}$ and $\\tilde{c}_{1},\\hdots,\\tilde{c}_{k_{H}}$ with multiplicities $\\tilde{m}_{1},\\ldots,\\tilde{m}_{k_{G}}$ be the final colors of $r{-}\\ell\\mathbf{W}\\mathbf{L}$ applied to $G$ and $H$ , respectively. Consider some $v\\ \\in\\ V(G)$ such that $c_{r}(v)\\;=\\;c_{1}$ . Let $D$ be sufficiently large (any $D$ after convergence of $r{-}\\ell\\mathbf{W}\\!\\mathrm{L})$ ), then $\\mathrm{cnt}((F^{(D)}(v),T^{(D)}(v)),G)\\,=\\,\\mathrm{cnt}((F^{(D)}(v),T^{(D)}(v)),H)$ since $(F^{(D)}(v),T^{(D)}(v))\\,\\in\\,\\mathcal{M}^{r+2}$ . Hence, without loss of generality, $c_{1}=\\tilde{c}_{1}$ and $m_{1}=\\tilde{m}_{1}$ . Repeating this argument for all colors finishes the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "G.4.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we employ techniques adapted from the works of Dell et al. (2018) and B. Zhang et al.   \n(2024) to derive a proof for Theorem 2 from the established result in Corollary 3. ", "page_idx": 34}, {"type": "text", "text": "Definition 28 (Definition 20 in (Dell et al., 2018)). Let $(F,T^{t})$ and $(\\tilde{F},\\tilde{T}^{s})$ be two tree-decomposed graphs. A pair of mappings $(\\rho,\\tau)$ is said to be a bag isomorphism homomorphism from $(F,\\Bar{T}^{t})$ to $(\\tilde{F},\\tilde{T}^{s})$ if it satisfies the following conditions ", "page_idx": 34}, {"type": "text", "text": "1. $\\rho$ is a homomorphism from $F$ to $\\tilde{F}$ .   \n2. $\\tau$ is a homomorphism from $T^{t}$ to $\\tilde{T}^{s}$ .   \n3. $\\tau$ is depth-surjective, i.e., the image of $T^{t}$ under $\\tau$ contains vertices at every depth present   \nin $\\tilde{T}^{s}$ .   \n4. For all $t^{\\prime}\\in T^{t}$ , we have $\\mathrm{dep}_{T^{t}}(t^{\\prime})=\\mathrm{dep}_{\\tilde{T}^{s}}(\\tau(t^{\\prime}))$ and $F[\\beta_{T^{t}}(t^{\\prime})]\\cong\\tilde{F}[\\beta_{\\tilde{T}^{s}}(\\tau(t^{\\prime}))].$   \n5. For all $t^{\\prime}\\in T^{t}$ , the set equality $\\rho(\\beta_{T^{t}}(t^{\\prime}))=\\beta_{\\tilde{T}^{s}}(\\tau(t^{\\prime}))$ holds.   \n6. The depth of $T^{t}$ and $\\tilde{T}^{s}$ is equal. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l l l l l}{{\\mathcal{V}}}&{{d e n o t e}}&{{t h e}}&{{s e t}}&{{o f}}&{{b a g}\\quad{i s o m o r p h i s m}}&{{h o m o m o r p h i s m s}\\quad{f r o m}\\quad(F,T^{t})}&{{t o m e}}\\\\ {{\\tilde{F}},{\\tilde{T}}^{s})}&{{b y}}&{{\\mathrm{BIsoHom}\\left((F,T^{t}),(\\tilde{F},\\tilde{T}^{s})\\right)\\quad{a n d}}}&{{s e t}}&{{\\mathrm{bIsoHom}\\left((F,T^{t}),(\\tilde{F},\\tilde{T}^{s})\\right)}}&{{=}}\\\\ {{\\mathrm{BIsoHom}\\left((F,T^{t}),(\\tilde{F},\\tilde{T}^{s})\\right)\\mid.}}&{{}}&{{}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We continue with the following lemma that shows a linear relation between the number of bag isomorphisms and the output of the counting function in Definition 27. ", "page_idx": 34}, {"type": "text", "text": "Lemma 6. Let $r\\in\\mathbb{N}$ . For any tree-decomposed graph $(F,T^{s})\\in\\mathcal{M}^{r+2}$ and any graph $G$ , it holds $\\mathrm{bIso}\\left((F,T^{s}),G\\right)=\\sum_{(\\tilde{F},\\tilde{T}^{t})\\in\\mathcal{M}^{r+2}}\\mathrm{bIsoHom}\\left((F,T^{s})\\,,\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)\\cdot\\mathrm{cnt}\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),G\\right).$ ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Proof. Let $(F,T^{s})$ be a tree-decomposed graph such that $T^{s}$ has depth $2D$ . The sum is over all isomorphism types $(\\tilde{F},\\tilde{T}^{t})$ of tree-decomposed graphs. This sum is finite and thus well-defined as $\\mathrm{bIsoHom}\\left((F,T^{s}),\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)=0$ holds if $\\tilde{T}^{t}$ has depth unequal to $2D$ or nodes with at least $(r+1)\\cdot(|V(\\dot{G})|-1)$ children. ", "page_idx": 34}, {"type": "text", "text": "Assume that for the root bag of $(F,T^{s})$ it holds $\\beta_{T^{s}}(s)\\,=\\,\\{v\\}$ . Let $x\\in V(G)$ be any vertex in $G$ , and denote by $(F^{(D)}(x),T^{(D)}(x))$ the depth- $2D$ $r$ -\u2113WL-unfolding tree at node $x$ . Define the following two sets, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}(x)=\\left\\{h\\in\\mathrm{BIso}((F,T^{s}),G)\\mid h(v)=x\\right\\},}\\\\ &{S_{2}(x)=\\left\\{(\\rho,\\tau)\\in\\mathrm{BIsoHom}\\left((F,T^{s})\\,,\\left(F^{(D)}(x),T^{(D)}(x)\\right)\\right)\\mid\\rho(v)=x\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We prove that $|S_{1}(x)|=|S_{2}(x)|$ for every $x\\in V(G)$ , which is equivalent to (11). For this, we show for any bag isomorphism $h$ from $(F,T^{s})$ to $G$ with $h(v)=x$ , there exists a unique bag isomorphism homomorphism $\\sigma$ from $(F,T^{s})$ to $(F^{({D})}(x),T^{({D})}(\\dot{x}))$ with $\\sigma(v)=x$ such that $h=\\pi\\circ\\sigma$ , where $\\pi$ is the bag isomorphism from $(F^{(D)}(x),T^{(D)}(x))$ to $G$ , defined in Definition 26 and Theorem 4, respectively. To visualize this proof idea, see Figure 12. ", "page_idx": 34}, {"type": "text", "text": "First, define $\\rho(v):=x$ . Let $v_{1},\\ldots,v_{n}\\in V(F)$ be nodes that correspond to bags in $T^{s}$ of depth 2 with one element inside the bag and their parents having two elements in their bag, i.e., $\\{v_{i}\\}$ are the corresponding bags. Similarly, set $x_{1},\\ldots,x_{m}\\in V(F^{(D)}(x))$ nodes that correspond to bags of depth 2 in $T^{(D)}(x)$ , with one element inside the bag and their parents having two elements in their bag. Since $h$ is a bag isomorphism and $\\pi$ as well, for every $i=1,\\hdots,n$ there exists a $j_{i}$ such that $h(v_{i})\\,=\\,\\tilde{x}_{j_{i}}\\,=\\,\\pi(x_{j_{i}})$ , where $\\tilde{x}_{j_{i}}\\,\\in\\,V(G)$ and $x_{j_{i}}\\,\\in\\,V\\left(F^{(D)}(x)\\right)$ . Since $\\pi$ and $h$ are bag isomorphisms, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nF[\\{\\{v,v_{i}\\}\\}]\\cong G[\\{\\{x,\\tilde{x}_{j_{i}}\\}\\}]\\cong F^{(D)}(x)[\\{\\{x,x_{j_{i}}\\}\\}].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now, set $\\rho(v_{i})\\,=\\,x_{j_{i}}$ for every $i=1,\\hdots,n$ . Based on (12), we can easily define $\\tau$ such that $\\tau$ satisfies Definition 28 with respect to bags that are of depth 1 and 2. ", "page_idx": 35}, {"type": "text", "text": "For $q=1,\\hdots,r$ and $l=1,\\dots,|\\mathcal{N}_{q}(v)|$ , let $\\mathbf{p}_{l}^{q}$ be a path of length $2q$ starting from the root node $s$ in $T^{s}$ . Every such path $\\mathbf{p}_{l}^{q}$ in $T^{s}$ corresponds to unique path $\\mathbf{v}_{l}^{q}$ , that is in $\\bar{\\mathcal{N}}_{q}(x)$ , of length $q$ in $F$ . We represent the path by vl,1, vl,2, . . . , vl,q+1 , where every consecutive node is connected to each other and for k = 1, . . . q + 1, we have v, vlq,k $\\left\\{v,v_{l,k}^{q}\\right\\}\\in E(F)$ iff $\\left\\{h(v),h(v_{l,k}^{q})\\right\\}\\in E(G)$ as $h$ is a bag isomorphism. Further for every node $k=1,\\ldots,q+1$ there exists a $j_{l,k}^{q}$ such that $h(v_{l,k}^{q})=\\tilde{x}_{j_{l,k}^{q}}=$ $\\pi(\\boldsymbol{x}_{j_{l,k}^{q}})$ , where $\\tilde{x}_{j_{l,k}^{q}}\\in V(G),\\left\\{\\tilde{x}_{j_{l,1}^{q}},\\ldots,\\tilde{x}_{j_{l,q+1}^{q}}\\right\\}\\in\\mathcal{N}_{q}(x)$ and $\\left\\{x_{j_{l,1}^{q}},\\ldots,x_{j_{l,q+1}^{q}}\\right\\}\\in\\mathcal{N}_{q}(x)$ set $\\sigma(v_{l,k}^{q})=x_{j_{l,k}^{q}}$ for every $k=1,\\ldots,q+1$ . Clearly, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nF\\left[\\left\\{\\{v,v_{i},v_{i+1}\\}\\right\\}\\right]\\cong G\\left[\\left\\{\\left\\{x,\\tilde{x}_{j_{i}},\\tilde{x}_{j_{i+1}}\\right\\}\\right\\}\\right]\\cong F^{(D)}(x)\\left[\\left\\{\\left\\{x,x_{j_{i}},x_{j_{i+1}}\\right\\}\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, based on (13), we can easily define $\\tau$ such that $\\tau$ satisfies Definition 28 with respect to bags that correspond to paths in $\\mathcal{N}_{q}(v)$ for $q=1,\\hdots,r$ . Now, following this construction recursively leads to a bag isomorphism $\\rho$ such that $h=\\pi\\circ\\rho$ . ", "page_idx": 35}, {"type": "text", "text": "It remains to show that $(\\rho,\\tau)$ is unique (up to isomorphism). For this, let $(\\rho_{1},\\tau_{1})$ be another bag isomorphism homomorphism between $(F,T^{s})$ and $(F^{(D)}(x),T^{(D)}(x))$ such that $\\rho_{1}(v)=x$ and $h=\\pi\\circ\\rho_{1}$ . We show that $\\rho=\\rho_{1}$ . ", "page_idx": 35}, {"type": "text", "text": "We begin by showing that $\\rho(v)=\\rho_{1}(v)$ for every $v$ that is not in a cycle. Adopting the previous notations, consider $v_{1},\\ldots,v_{n}\\in V(F)$ and $x_{1},\\ldots,x_{m}\\in V(F^{(D)}(x))$ . For each $i=1,\\dots,n$ , let $k_{i}$ and $l_{i}$ be the indices such that $\\rho(v_{i})=x_{k_{i}}$ and $\\rho_{1}(v_{i})=x_{l_{i}}$ . Consequently, $\\pi(x_{k_{i}})=\\pi(x_{l_{i}})$ . We note that the image of $h(v_{i})$ is not contained in a cycle in $G$ , as otherwise, $h$ would not be a bag isomorphism. Similarly, $x_{k_{i}}$ and $x_{l_{i}}$ are not contained in a cycle; otherwise, $\\rho$ and $\\rho_{1}$ would not be bag isomorphisms. Now, $\\pi$ is an injective mapping if the domain is restricted to nodes that are of depth 1 and 2, and not contained in a cycle. Hence, $x_{k_{i}}=x_{l_{i}}$ . ", "page_idx": 35}, {"type": "text", "text": "We continue by showing that for every $w\\in V(F)$ , that is contained in a cycle, we have $\\rho(w)=\\rho_{1}(w)$ . This follows a similar argument as the nodes that are not included in any cycle. We summarize the argument shortly: It must hold that $\\rho(w)$ and $\\rho_{1}(w)$ are contained in a cycle, and $\\pi(\\rho(w))$ and $\\pi(\\rho_{1}(\\bar{w}))$ as well. Now, $\\pi$ is injective if the domain is restricted to nodes that are only contained in cycles. Hence, $\\rho_{1}=\\rho$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "We continue this subsection by introducing the concept of a bag extension in the context of treedecomposed graphs. This definition formalizes the notion of one tree-decomposed graph being an extension of another. ", "page_idx": 35}, {"type": "text", "text": "Definition 29 (Definition 20 in (Dell et al., 2018)). Let $(F,T^{t})$ be a tree-decomposed graph. A bag extension of $(F,T^{t})$ is a graph $(H,T^{t})$ with $V(H)=V(F)$ such that for every $t\\,\\in\\,V\\left(T^{t}\\right)$ the induced subgraph ${\\dot{H}}[\\beta_{T^{t}}(t)]$ is an extension of $F[\\beta_{T^{t}}(T)]$ , i.e., if $e\\,\\in\\,E\\left(F[\\beta_{T^{t}}(T)]\\right)$ , then $e\\in E\\left(H[\\beta_{T^{t}}(T)]\\right)$ . We define $\\mathrm{bExt}\\left((F,T^{t}),(\\tilde{F},\\tilde{T}^{s})\\right)$ as the number of bag extensions of $(F,T^{t})$ that are isomorphic to $(\\tilde{F},\\tilde{T}^{s})$ . ", "page_idx": 35}, {"type": "text", "text": "Intuitively, a bag extension of a tree-decomposed graph $(F,T^{s})$ can be achieved by adding an arbitrary number of edges to $F$ . Each added edge must be contained within a bag that corresponds to a node in the tree $T^{s}$ . ", "page_idx": 35}, {"type": "text", "text": "Definition 30 (Definition C.28 in (B. Zhang et al., 2024)). Given a tree-decomposed graph $(F,T^{r})$ and a graph $G$ , a bag-strong homomorphism from $(F,T^{s})$ to $G$ is a homomorphism $f$ from $F$ to $G$ such that, for all $t\\in V(T^{r})$ , $f$ is a strong homomorphism from $F[\\beta_{T^{s}}(t)]$ to $G[f(\\beta_{T^{s}}(t))]$ , i.e., $\\{u,v\\}\\in E\\left(F[\\beta_{T^{s}}(t)]\\right)$ iff $\\{f(u),f(v)\\}\\in E\\left(G[f(\\beta_{T^{s}}(t))]\\right)$ . Denote $\\mathrm{3StrHom}((F,T^{s}),G)$ to be the set of all bag-strong homomorphisms from $(F,T^{s})$ to $G$ , and denote $\\mathrm{bStrHom}((F,T^{s}),G)=$ $|\\mathrm{BStrHom}((F,\\bar{T}^{s}),G)|$ . ", "page_idx": 35}, {"type": "text", "text": "We continue with decomposing the number of homomorphism from a fan cactus graph to any graph. Lemma 7. Let $r\\in\\mathbb{N}$ . For any tree-decomposed graph $(F,T^{s})\\in\\mathcal{M}^{r+2}$ and any graph $G$ , it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{hom}\\left(F,G\\right)=\\sum_{\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}}\\mathrm{bExt}\\left(\\left(F,T^{s}\\right),\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)\\cdot\\mathrm{bStrHom}\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),G\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "image", "img_path": "9O2sVnEHor/tmp/ca5dbf0472e527fd6c8c3bcc1670b7e03f55bcac56806fbf65afeebdfe5755e0.jpg", "img_caption": ["Figure 12: Visualization of proof idea of Lemma 6 "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Proof. The proof follows the lines of Lemma C.29. in (B. Zhang et al., 2024). First, (14) is well-defined as $T^{s}$ is finite, hence, there can only be finitely many bag extensions of $(F,T^{s})$ . Further, consider the set ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S=\\left\\{\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),\\left(\\rho,\\tau\\right),g\\right)\\mid\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}\\,,\\left(\\rho,\\tau\\right)\\in\\mathrm{BExt}\\left(\\left(F,T^{s}\\right),\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)\\ ,\\mathrm{~}}\\\\ {g\\in\\mathrm{BstrHom}\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),G\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We consider the mapping $\\sigma$ from $S$ to $\\mathrm{hom}(F,G)$ via $\\left(\\left(\\rho,\\tau\\right),g\\right)\\mapsto g\\circ\\rho$ . We show that for every homomorphism $h$ there exists a unique, up to automorphisms, $\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2},(\\rho,\\tau)$ and $g$ such that h = g \u25e6\u03c1. ", "page_idx": 36}, {"type": "text", "text": "We begin with the existence part. For $h\\in\\operatorname{hom}(F,G)$ , we define $\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2},(\\rho,\\tau)$ and $g$ as follows. ", "page_idx": 36}, {"type": "text", "text": "\u2022 We define $\\tilde{F}$ by adding the edges given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\{\\{u,v\\}\\mid u,v\\in V(F),\\exists t\\in T^{s}{\\mathrm{~s.t.~}}\\left\\{u,v\\right\\}\\in\\beta_{T^{s}}(t),\\left\\{h(u),h(v)\\right\\}\\in E(G)\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We define $\\tilde{T}^{t}:=T^{s}$ . Clearly, $\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}$ and it is a bag extension as only edges are added that are contained within a bag that corresponds to a node in $T^{s}$ . ", "page_idx": 37}, {"type": "text", "text": "\u2022 We define $\\rho$ and $\\tau$ as the identity mappings on their respective domain, leading to $(\\rho,\\tau)\\in$ $\\mathrm{BExt}\\left(\\left(F,T^{s}\\right),\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)$ .   \n\u2022 We define $g=h$ . For $x\\in\\tilde{T}^{t}$ , we show that $g$ is a strong homomorphism from $\\tilde{F}[\\beta_{\\tilde{T}^{t}}(x)]$ to $G[g\\left(\\beta_{\\tilde{T}^{t}}(x)\\right)]$ . Let $\\{u,v\\}\\in E\\left(\\tilde{F}[\\beta_{\\tilde{T}^{t}}(x)]\\right)$ , then $\\{g(u),g(v)\\}\\in E\\left(G[g\\left(\\beta_{\\tilde{T}^{t}}(x)\\right)]\\right)$ as $h$ is a homomorphism with respect to the edges $E(F)$ and in (15) only edge $\\{u,v\\}$ were added that satisfy $\\{h(u),h(v)\\}\\in E(G)$ . On the other hand $\\{g(u),g(v)\\}\\in E\\left(G[g\\left(\\beta_{\\tilde{T}^{t}}(x)\\right)]\\right)$ , but $\\{u,v\\}\\not\\in E\\left(\\tilde{F}[\\beta_{\\tilde{T}^{t}}(x)]\\right)$ would contradict (15) as $u,v$ are contained in the same bag $\\beta_{\\tilde{T}^{t}}(x)$ . Hence, $\\dot{\\boldsymbol{g}}\\in\\operatorname{BstrHom}\\left(\\left(\\tilde{\\boldsymbol{F}},\\tilde{T}^{t}\\right),\\boldsymbol{G}\\right)$ . ", "page_idx": 37}, {"type": "text", "text": "We finally prove the uniqueness part, i.e., that $\\sigma\\left((\\tilde{F}_{1},\\tilde{T}_{1}^{t_{1}}),(\\rho_{1},\\tau_{1}),g_{1}\\right)\\,=\\,h$ implies that there exists an isomorphism $(\\tilde{\\rho},\\tilde{\\tau})$ from $\\left(\\tilde{F}_{1},\\tilde{T}_{1}^{t_{1}}\\right)$ to $\\left(\\tilde{F},\\tilde{T}^{t}\\right)$ such that $\\tilde{\\rho}\\circ\\rho_{1}=\\rho,\\tilde{\\tau}\\circ\\tau_{1}=\\tau$ . We first prove that $\\tilde{F}_{1}\\cong\\tilde{F}$ and $\\tilde{T}_{1}^{t_{1}}\\cong\\tilde{T}^{t}$ . ", "page_idx": 37}, {"type": "text", "text": "1. For any $u,v\\in V(F)$ , we obviously have $\\rho(u)=\\rho(v)$ iff $u=v$ iff $\\rho_{1}(u)=\\rho_{1}(v)$ as $\\rho$ and $\\rho_{1}$ are injective mappings. ", "page_idx": 37}, {"type": "text", "text": "2. Let $u,v\\in V(F)$ . Consider $\\{\\rho_{1}(u),\\rho_{1}(v)\\}\\in E(\\tilde{F}_{1})$ , we show that $\\{\\rho(u),\\rho(v)\\}\\in E(\\tilde{F})$ . If $\\{u,v\\}\\,\\in\\,E(F)$ , then clearly $\\{\\rho(u),\\rho(v)\\}\\,\\in\\,E(\\tilde{F})$ as $\\rho$ is a homomorphism. Hence, assume that $\\{u,v\\}\\not\\in E(F)$ . Then, $u,v$ must be contained in the same bag of $T^{s}$ as $\\rho_{1}$ is a bag extension and only node pairs are added if they are in the same bag. Hence, $\\rho(u)$ and $\\rho(v)$ are contained in the same bag. As $g_{1}$ is a homomorphism, we have $\\{g_{1}(\\rho_{1}(u)),g_{1}(\\rho_{1}(v))\\}\\in$ $E(G)$ . But, then also $\\{g(\\rho(u)),g(\\rho(v))\\}\\in E(G)$ , and as $g$ is a strong homomorphism (with respect to the bag in which $\\rho(u)$ and $\\rho(v)$ are contained), we have $\\{\\rho(u),\\rho(v)\\}\\in E(\\tilde{F})$ . By symmetry of the argument, we have $\\{\\rho_{1}(u),\\rho_{1}(v)\\}\\in E(\\tilde{F}_{1})$ iff $\\{\\rho(u),\\rho(v)\\}\\in E(\\tilde{F})$ . 3. Since $\\rho_{1}$ and $\\rho$ are bag extension, they are bijective on their respective domain. Hence, $\\tilde{\\rho}=\\rho\\circ\\rho_{1}^{-1}$ defines an isomorphism from $\\tilde{F}_{1}$ to $\\tilde{F}$ . On the other hand, $\\tilde{T}_{1}^{t_{1}}\\cong\\tilde{T}^{t}$ trivially holds, again with $\\tilde{\\tau}=\\tau\\circ\\tau_{1}^{-1}$ ", "page_idx": 37}, {"type": "text", "text": "We have $\\tilde{\\rho}\\circ\\rho_{1}=\\rho,\\tilde{\\tau}\\circ\\tau_{1}=\\tau$ . We show that the tuple $(\\tilde{\\rho},\\tilde{\\tau})$ is an isomorphism, i.e., it remains to show that for any $b\\in\\tilde{T}_{1}^{t_{1}}$ , we have $\\tilde{\\rho}(\\beta_{\\tilde{T}_{1}^{t_{1}}}(b))=\\beta_{\\tilde{T}^{t}}(\\tilde{\\tau}(b))$ . Since $\\tau_{1}$ is surjective, we can choose $a$ such that $\\tau_{1}(a)=b$ . Then, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\rho}(\\beta_{\\tilde{T}_{1}^{t_{1}}}(\\tau_{1}(a)))=\\tilde{\\rho}(\\rho_{1}(\\beta_{T^{s}}(a)))=\\rho(\\beta_{T^{s}}(a))=\\beta_{\\tilde{T}^{t}}(\\tau(a))=\\beta_{\\tilde{T}^{t}}(\\tilde{\\tau}\\circ\\tau_{1}(a))=\\beta_{\\tilde{T}^{t}}(\\tilde{\\tau}(b)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The first and third equalities hold since $(\\rho_{1},\\tau_{1})$ and $(\\rho,\\tau)$ are bag extensions. ", "page_idx": 37}, {"type": "text", "text": "Definition 31 (Definition 30 in (B. Zhang et al., 2024)). Given two tree-decomposed graphs $(F,T^{s})$ and $(\\tilde{F},\\tilde{T}^{t})$ , a homomorphism $(\\rho,\\tau)$ from $(F,T^{s})$ to $(\\tilde{F},\\tilde{T}^{t})$ is called bag-strong surjective $i f\\,\\rho$ is a bag-strong homomorphism from $(F,T^{s})$ to $\\tilde{F}$ and is surjective on both vertices and edges, and $\\tau$ is an isomorphism from $T^{s}$ to $\\tilde{T}^{t}$ such that for all $x\\in V(T^{s}),$ , we have $\\rho(\\beta_{T^{s}}(x))=\\beta_{\\tilde{T}^{t}}(\\tau(x))$ . Denote $\\mathrm{BStrSurj}((F,T^{s}),(\\tilde{F},\\tilde{T}^{t}))$ to be the set of all bag-strong subjective homomorphisms from $(F,T^{s})$ to $(\\tilde{F},\\tilde{T}^{t})$ , and denote bStrSur $\\mathrm{i}((F,T^{s}),(\\tilde{F},\\tilde{T}^{t}))=|\\mathrm{BStrSurj}((F,T^{s}),(\\tilde{F},\\tilde{T}^{t}))|$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma 8. Let $r\\in\\mathbb{N}$ . For any tree-decomposed graph $(F,T^{s})\\in\\mathcal{M}^{r+2}$ and any graph $G$ , it holds ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{bStrHom}\\left(\\left(F,T^{s}\\right),G\\right)=\\sum_{\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}}\\mathrm{bStrSurj}\\left(\\left(F,T^{s}\\right),\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)\\frac{\\mathrm{bIso}\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),G\\right)}{\\mathrm{aut}\\left(\\tilde{F},\\tilde{T}^{t}\\right)},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where aut $(\\tilde{F},\\tilde{T}^{t})$ counts the number of automorphisms of $(\\tilde{F},\\tilde{T}^{t})$ . ", "page_idx": 37}, {"type": "text", "text": "Proof. The proof follows the lines of Lemma C.31. in (B. Zhang et al., 2024). Consider the set ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S=\\left\\{\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),\\left(\\rho,\\tau\\right),g\\right)\\mid\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}\\,,\\left(\\rho,\\tau\\right)\\in\\mathrm{BStrSurj}\\left(\\left(F,T^{s}\\right),\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\right)\\ ,\\mathrm{~}}\\\\ {g\\in\\mathrm{BIso}\\left(\\left(\\tilde{F},\\tilde{T}^{t}\\right),G\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We consider the mapping $\\sigma$ from $S$ to $\\mathrm{BStrHom}\\left(\\left(F,T^{s}\\right),G\\right)$ via $\\left(\\left(\\rho,\\tau\\right),g\\right)\\mapsto g\\circ\\rho.$ . We show that for every bag-strong homomorphism $h$ there exists a unique, up to automorphisms, $\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}$ , bag-strong surjective homomorphism $(\\rho,\\tau)$ and $g$ such that $h=g\\circ\\rho$ . ", "page_idx": 38}, {"type": "text", "text": "We begin with the existence part. For $h\\ \\in\\ \\mathrm{BStrHom}\\left(\\left(F,T^{s}\\right),G\\right)$ , we define $\\left(\\Tilde{F},\\Tilde{T}^{t}\\right)\\ \\in$ $\\mathcal{M}^{r+2},(\\rho,\\tau)$ and $g$ as follows. ", "page_idx": 38}, {"type": "text", "text": "We define $\\tilde{F}$ by defining an equivalence relation $\\sim$ on $V(F)$ : $u\\sim v$ if $h(u)=h(v)$ and there exists a path $P$ in $T^{s}$ with endpoints $t_{1},t_{2}\\in V(T^{s})$ such that $u\\in\\beta_{T^{s}}(t_{1}),v\\in\\beta_{T^{s}}(t_{2})$ , and all nodes $t$ on the path $P$ satisfies that $h(u)=h(v)\\in h(\\beta_{T^{s}}(t))$ . We then define $\\rho$ as the quotient map with respect to $\\sim$ and set $\\tilde{F}=F/\\sim$ , i.e., ", "page_idx": 38}, {"type": "equation", "text": "$$\nV(\\tilde{F})=\\{\\rho(u)\\mid u\\in V(F)\\}\\,,E(\\tilde{F})=\\{\\{\\rho(u),\\rho(v)\\}\\mid\\{u,v\\}\\in E(F)\\}\\,,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is well-defined as $\\{u,v\\}\\in E(F)$ imples $\\rho(u)\\neq\\rho(v)$ since $h$ is a homomorphism. Then, $\\rho$ is surjective per construction. ", "page_idx": 38}, {"type": "text", "text": "We define the mapping $g:V({\\tilde{F}})\\to V(G)$ such that $g(\\rho(u))=h(u)$ for all $u\\in V(F)$ . This mapping $g$ is well-defined since $\\rho(u)\\,=\\,\\rho(v)$ implies $h(u)\\,=\\,h(v)$ , and $\\rho:V(F)\\to V(\\tilde{F})$ is surjective. This leads to the equality $h=g\\circ\\rho$ . To demonstrate that $g$ is a homomorphism, consider any edge $(x,y)\\in E(\\tilde{F})$ . There exists an edge $(u,v)\\in E(F)$ such that $\\rho(u)=x$ and $\\rho(v)=y$ , which implies $(h(u),h(v))\\in E(G)$ , since $h$ is a homomorphism. Consequently, this means $({\\dot{g}}(x),g(y))\\in{\\dot{E^{}}}(G)$ . ", "page_idx": 38}, {"type": "text", "text": "We continue by defining the tree $\\tilde{T}^{t}:=(V(T),E(T),\\beta_{\\tilde{T}^{t}})$ . We set $t=s$ , and define $\\tau$ to be the identity. Furthermore, we have $\\beta_{\\tilde{T}^{t}}(x)\\,=\\,\\rho(\\beta_{T^{s}}(x))$ for all $x\\in V(T)$ . It remains to prove that $(\\tilde{F},\\tilde{T}^{t})\\,\\in\\,\\mathcal{M}^{r+2}$ is a valid tree decomposition. For this, it suffices to prove that for any vertex $x\\in V(\\tilde{F})$ the subgraph $B_{\\tilde{T}^{t}}(x)$ is connected. For this, let $x\\in V(\\tilde{F})$ and $t_{1},t_{2}\\in B_{\\tilde{T}^{t}}(x)$ . Then, there exists $u\\in\\beta_{T^{s}}(t_{1}),v^{\\ast}\\in\\beta_{T^{s}}(t_{2})$ such that $\\rho(u)=x,\\rho(v)=x$ . Therefore, $u\\sim v$ . As such, there exists a path $P\\in T^{s}$ such that all nodes $b$ on $P$ satisfy $h(u)\\in h(\\beta_{T^{s}}(b))$ . Hence, for every $b\\in P$ there exists some $w_{b}\\in\\beta_{T^{s}}(b)$ such that $h(w_{b})=h(\\dot{u})$ , and consequently $w_{b}\\sim u$ . Finally, $x=\\rho(u)=\\rho(w_{b})\\in\\rho(\\beta_{T^{s}}(b))=\\beta_{\\tilde{T}^{t}(b)}$ for all $b$ in the path $P$ . Hence, $\\left(\\tilde{F},\\tilde{T}^{t}\\right)\\in\\mathcal{M}^{r+2}$ . ", "page_idx": 38}, {"type": "text", "text": "It remains to prove that $\\rho$ is a bag-strong surjective homomorphism and $g$ is a bag isomorphism. We begin by showing that $\\rho$ is a bag-strong surjective homomorphism. For this, let $t\\,\\in\\,V(T^{s})$ and $\\bar{u,v}\\,\\in\\,\\bar{\\beta}_{T^{s}}(t)$ . If $\\{u,v\\}\\;\\notin\\;\\bar{E(F)}$ , then $\\{h(u),h(v)\\}\\ \\notin\\ \\mathbf{\\bar{\\cal{E}}}(G)$ (since $h$ is a bag-strong homomorphism). Therefore, $\\{\\rho(u),\\rho(v)\\}\\,\\not\\in\\,E(\\tilde{F})$ since $g$ is a homomorphism. Hence, $\\rho$ is a bag-strong surjective homomorphism. ", "page_idx": 38}, {"type": "text", "text": "We show that $g$ is a bag isomorphism. Let $x\\,\\in\\,V({\\tilde{T}}^{t})$ , and consider $\\tilde{u},\\tilde{v}\\,\\in\\,\\beta_{\\tilde{T}^{t}}(x)$ . Since $\\rho$ is surjective, there exist $u,v\\,\\in\\,\\beta_{T^{s}}(x)$ such that $\\rho(u)\\,=\\,\\tilde{u}$ and $\\rho(v)\\,=\\,\\tilde{v}$ . We have $\\{\\rho(u),\\rho(v)\\}\\ \\notin$ $E(\\tilde{F})$ iff $\\{h(u),h(v)\\}\\not\\in E(G)$ , since both $\\rho$ and $h$ are bag-strong homomorphisms. Therefore, $\\mathrm{g}$ is a bag isomorphism. ", "page_idx": 38}, {"type": "text", "text": "We finally prove that $\\sigma\\left((\\tilde{F}_{1},\\tilde{T}^{t_{1}}),(\\rho_{1},\\tau_{1}),g_{1}\\right)\\,=\\,\\sigma\\left((\\tilde{F},\\tilde{T}^{t}),(\\rho,\\tau),g\\right)$ implies there exists an isomorphism $(\\tilde{\\rho},\\tilde{\\tau})$ from $(\\tilde{F}_{1},\\tilde{T}_{1}^{t_{1}})$ to $(\\tilde{F},\\tilde{T}^{t})$ such that $\\tilde{\\rho}\\circ\\rho_{1}\\,=\\,\\rho,\\tilde{\\tau}\\circ\\tau_{1}\\,=\\,\\tau,g_{1}\\,=\\,g\\circ\\tilde{\\rho}$ . Let $h=g_{1}\\circ\\rho_{1}=g\\circ\\rho$ . We will only show that $\\tilde{F}_{1}\\cong\\tilde{F}$ since the remaining procedure is almost the same as in previous proofs. It suffices to prove that, for all $u,v\\in V(F)$ , $\\bar{\\rho_{1}}\\bar{(}u\\r)=\\rho_{1}(v)$ iff ", "page_idx": 38}, {"type": "text", "text": "a) $h(u)=h(v)$ , and ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "b) There exists a path $P$ in $T^{s}$ with endpoints $t_{1},t_{2}\\,\\in\\,V(T)$ such that $u\\,\\in\\,\\beta_{T^{s}}(t_{1}),v\\,\\in$ $\\beta_{T^{s}}(t_{2})$ , and all node $x$ on path $P$ satisfies that $h(u)\\in h(\\beta_{T^{s}}(x))$ . ", "page_idx": 38}, {"type": "text", "text": "We begin by showing the first direction, i.e., $\\rho_{1}(u)=\\rho_{1}(v)$ implies Items a) and b). If $\\rho_{1}(u)=\\rho_{1}(v)$ , we clearly have $h(\\bar{u})=h(v)$ as $g_{1}$ is well-defined. Also, there exists $x_{1}\\in B_{T^{s}}(u),x_{2}\\in B_{T^{s}}(v)$ , i.e., $u\\,\\in\\,\\beta_{T^{s}}(x_{1})$ and $v\\,\\in\\,\\beta_{T^{s}}(x_{2})$ . Hence, $\\rho(u)\\,\\in\\,\\rho(\\beta_{T^{s}}(x_{1}))\\,\\subset\\,\\beta_{\\tilde{T}_{1}^{t_{1}}}(\\tau_{1}(\\dot{x_{1}}))$ and $\\rho_{1}(u)\\,=$ $\\rho_{1}(v)\\in\\rho_{1}(\\beta_{T^{s}}(x_{2}))\\subset\\beta_{\\tilde{T}_{1}^{t_{1}}}(\\tau_{1}(x_{2}))$ since $(\\rho_{1},\\tau_{1})$ is a homomorphism. Hence, $\\tau_{1}(x_{1}),\\tau_{1}(x_{2})\\in$ $B_{\\tilde{T}_{1}^{t_{1}}}(\\rho_{1}(u))$ . Since $\\tilde{T}_{1}^{t_{1}}[B_{\\tilde{T}_{1}^{t_{1}}}(\\rho_{1}(u))]$ is connected, there is a path $P$ in $\\tilde{T}_{1}^{t_{1}}[B_{\\tilde{T}_{1}^{t_{1}}}(\\rho_{1}(u))]$ with endpoints $\\tau_{1}(x_{1}),\\tau_{1}(x_{2})$ such that all nodes $x$ on $P$ satisfies $\\rho_{1}(u)\\in\\beta_{\\tilde{T}_{1}^{t_{1}}}(x)=\\beta_{\\tilde{T}_{1}^{t_{1}}}(\\tau\\circ\\tau^{-1}(x))=$ $\\rho_{1}\\,\\left(\\beta_{T^{s}}(\\tau_{1}^{-1}(x))\\right)$ . We conclude $h(u)=g_{1}(\\rho_{1}(u))\\in g_{1}(\\rho_{1}(\\beta_{T^{s}}(\\tau_{1}^{-1}(x))))=h(\\beta_{T^{s}}(\\tau_{1}^{-1}(x)))$ . ", "page_idx": 39}, {"type": "text", "text": "We continue by showing the second direction, i.e., $\\rho_{1}(u)=\\rho_{1}(v)$ if Items a) and b). We prove this by contradiction, i.e., assume $\\rho_{1}(u)\\neq\\rho_{1}(v)$ but the above items (a) and (b) hold. We consider two cases.First, assume that $u$ and $v$ are in the same bag of $T^{s}$ . Then, as $(\\rho_{1},\\tau_{1})$ is a homomorphism, the nodes $\\rho_{1}(u)$ and $\\rho_{1}(v)$ are in the same bag of $\\tilde{T}_{1}^{t_{1}}$ . Since $g_{1}$ is a bag isomorphism, we have $g_{1}(\\rho_{1}(u))\\neq g_{1}(\\rho_{1}(v))$ . This contradicts Item (a) above. ", "page_idx": 39}, {"type": "text", "text": "Now, consider the second case. For this, assume that $u$ and $v$ are not in the same bag of $T^{s}$ . Then, there exist two adjacent nodes $x_{1},x_{2}$ on path $P$ such that $u\\,\\in\\,\\beta_{T^{s}}(x_{1}),u\\,\\not\\in\\,\\beta_{T^{s}}(\\bar{x}_{2})$ . We have $\\beta_{T^{s}}(x_{2})\\subset\\beta_{T^{s}}(x_{1})$ as for every pair of nodes $t_{1},t_{2}$ in a canonical tree decomposition with $\\{t_{1},t_{2}\\}\\in E(T^{s})$ we have either $\\beta_{T^{s}}(t_{1})\\subset\\beta_{T^{s}}(t_{2})$ or $\\beta_{T^{s}}(t_{2})\\subset\\beta_{T^{s}}(t_{1})$ . Now, item (b) implies that there exists $w\\in\\beta_{T^{s}}(x_{2})$ such that $w\\ne u$ and $h(w)=h(u)$ . Then, $\\bar{\\rho}_{1}(w)\\in\\rho_{1}\\,(\\beta_{T^{s}}(x_{2}\\bar{)})\\subset$ $\\rho_{1}\\left(\\beta_{T^{s}}(x_{1})\\right)\\subset\\beta_{\\tilde{T}_{1}^{t_{1}}}(\\tau_{1}(\\dot{x}_{1}))$ . Therefore, $\\rho_{1}(u)$ and $\\rho_{1}(w)$ are two different nodes in $\\beta_{\\tilde{T}_{1}^{t_{1}}}\\left(\\tau_{1}(x_{1})\\right)$ with $g_{1}(\\rho_{1}(u))\\:=\\:\\dot{h}(u)\\:=\\:h(w)\\:=\\:g_{1}(\\rho_{1}(w))$ . This contradicts the condition that $g_{1}$ is a bag isomorphism. This yields the desired result that $\\tilde{F}\\cong\\tilde{F}_{1}$ . \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Finally, we restate Theorem 2, with its proof now being a straightforward corollary of the preceding results in this section. ", "page_idx": 39}, {"type": "text", "text": "Theorem 2. Let $r\\geq0$ . Then, $r$ -\u2113WL can homomorphism-count $\\mathcal{M}^{r+2}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. According to Corollary 3, if $c_{r}(G)=c_{r}(H)$ , then $\\operatorname{cnt}(F,G)=\\operatorname{cnt}(F,H)$ for every $F\\in$ $\\mathcal{M}^{r+2}$ . Utilizing Lemma 6, we extend this result to bag isomorphism counts: ${\\mathrm{bIso}}(F,G)\\;=\\;$ $\\mathrm{b}\\mathrm{Iso}(F,H)$ holds for every $F\\in\\mathcal{M}^{r+2}$ . Finally, invoking Lemma 7 and Lemma 8, we conclude that $\\operatorname{hom}(F,G)=\\operatorname{hom}(F,H)$ for all $F\\in\\mathcal{M}^{r+2}$ . \u53e3 ", "page_idx": 39}, {"type": "text", "text": "H Implications of Theorem 2 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we discuss important implications of Theorem 2 and provide proofs for the results in Corollary 2. ", "page_idx": 39}, {"type": "text", "text": "H.1 Appendix on $\\mathcal{F}$ -Hom-GNNs and Proof of Corollary 2 i) ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Recent work in the domain of MPNNs has explored enhancing the initial node features by incorporating homomorphism counts (Barcel\u00f3 et al., 2021). We summarize this approach in this section and compare it to our $r{-}\\ell\\mathbf{W}\\mathbf{L}$ algorithm. ", "page_idx": 39}, {"type": "text", "text": "Define $\\mathcal{F}=\\{P_{1}^{s},\\ldots,P_{l}^{s}\\}$ as a collection of rooted graphs, termed as patterns. In $\\mathcal{F}$ -Hom-MPNNs, the initial feature vector of a vertex $v$ in a graph $G$ combines a one-hot encoding of the label $\\chi_{G}(v)$ with homomorphism counts corresponding to each pattern in $\\mathcal{F}$ . The feature vector for each vertex $v$ is recursively defined over rounds of message passing as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{\\mathcal{F},G,v}^{(0)}=(\\chi_{G}(v),\\mathrm{hom}(P_{1}^{s},G^{v}),\\dots,\\mathrm{hom}(P_{l}^{s},G^{v}))}\\\\ &{\\mathbf{x}_{\\mathcal{F},G,v}^{(t+1)}=g^{(t+1)}\\left(\\mathbf{x}_{\\mathcal{F},G,v}^{(t)},f^{(t+1)}\\left(\\mathbf{x}_{\\mathcal{F},G,u}^{(t)}\\mid u\\in N_{G}(v)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Here, $g^{(t)}$ and $f^{(t)}$ represent the update and aggregation functions at depth $t$ , respectively. ", "page_idx": 39}, {"type": "text", "text": "H.1.1 Expressivity of $\\mathcal{F}$ -Hom-MPNNs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we summarize known results about the expressivity of $\\mathcal{F}$ -Hom-MPNNs. The main result from Barcel\u00f3 et al., 2021 can be summarized as follows. ", "page_idx": 40}, {"type": "text", "text": "Theorem 6. For any two graphs $G$ and $H$ , it holds $\\mathcal{F}$ -Hom-MPNNs can separate $G$ and $H$ if and only if hom $:(T,G)=h o m(T,H),$ , for every $\\mathcal{F}$ -pattern tree. ", "page_idx": 40}, {"type": "text", "text": "To understand the above theorem, we need to define the concept of ${\\mathcal F}.$ -pattern trees. For this, we define the graph join operator $^*$ as follows. Given two rooted graphs $G^{v}$ and $H^{w}$ , the join graph $(G*H)^{v}$ is obtained by taking the disjoint union of $G^{v}$ and $H^{w}$ , followed by identifying $w$ with $v$ . The root of the join graph is $v$ . Further, if $G$ is a graph and $P^{r}$ is a rooted graph, then joining a vertex $v$ in $G$ with $P^{r}$ results in the disjoint union of $G$ and $P^{r}$ , where $r$ is identified with $v$ . ", "page_idx": 40}, {"type": "text", "text": "Let $\\mathcal{F}=\\{P_{1},\\ldots,P_{l}\\}$ . An $\\mathcal{F}$ -pattern tree $T^{r}$ is constructed from a standard rooted tree $S^{r}\\,=$ $(V,E,\\chi)$ , which serves as the core structure, or the \"backbone\", of $T^{r}$ . To form $T^{r}$ , each vertex $s\\in V$ of the backbone may be joined to any number of duplicates of any patterns from $\\mathcal{F}$ . Conceptually, an $\\mathcal{F}$ -pattern tree is a tree graph enhanced by attaching multiple instances of any pattern from $\\mathcal{F}$ to the nodes of the backbone tree. However, it is important to note that additional patterns may not be attached to any node that already derives from a pattern in $\\mathcal{F}$ . Our method can homomorphism-count graphs where this is allowed, see Appendix H.1.2. ", "page_idx": 40}, {"type": "text", "text": "Examples of $\\mathcal{F}$ -pattern trees for $\\mathcal{F}=\\{\\mathcal{L}_{\\omega}\\}$ are ", "page_idx": 40}, {"type": "image", "img_path": "9O2sVnEHor/tmp/c991755fee8e3eea89b61b260fb503108f8e84ec001ddf697f2e5a7c4f015c91.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "where grey vertices are part of the backbones of the ${\\mathcal F}.$ -pattern trees, black vertices are the joined node and white vertices are part of the attached patterns. We define the set of $\\mathcal{F}$ -pattern trees by ${\\mathcal{F}}^{\\mathrm{Tr}}$ . ", "page_idx": 40}, {"type": "text", "text": "H.1.2 Comparison with $r$ -\u2113WL ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We compare our proposed $r$ -\u2113GIN against ${\\mathcal{F}}^{r}$ -Hom-MPNNs, where $\\mathcal{F}^{r}=\\{C_{3},\\ldots,C_{r+2}\\}$ consists of cycle graphs up to length $r+2$ . Both MPNN variants exhibit equivalent preprocessing complexity. However, after the initial layer, the computational complexity of our method is marginally higher, yet it increases linearly with the number of cycles present in the underlying graph. ", "page_idx": 40}, {"type": "text", "text": "According to Theorem 2, our method $r$ -\u2113GIN can homomorphism-count all fan $(r+2)$ -cactus graphs. In particular, $r$ -\u2113GIN can homomorphism-count all ${\\mathcal{F}}^{r}$ -pattern trees. For example, there are infinitely many fan $r$ -cactus graphs that cannot be represented as ${\\mathcal{F}}^{r}$ -pattern trees, e.g., for $r=1$ ", "page_idx": 40}, {"type": "image", "img_path": "9O2sVnEHor/tmp/85f858543fa410db463fd838b28712211e053c789374a7bc655a14cef1d47370.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "We restate Corollary 2 ii) and give a short proof. ", "page_idx": 40}, {"type": "text", "text": "Corollary 4. Let $r\\,\\in\\,\\mathbb{N}\\setminus\\{0\\}$ . Then, $r{-}\\ell W L$ is more powerful than $\\mathcal{F}$ -Hom-MPNNs, where $\\mathcal{F}=\\{C_{3},\\ldots,C_{r+2}\\}$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. The proof of Corollary 2 ii) can be stated as a summary of all finding of the previous subsection: By Theorem 2, we have $\\dot{r^{-}}\\ell\\mathrm{WL}\\subseteq\\mathrm{~hom}(\\mathcal{M}^{r+2},\\cdot)$ . By Theorem 6, we have $\\mathcal{F}$ -Hom-MPNNs $\\bar{\\mathbb{L}}\\mathrm{~hom}(\\mathcal{F}^{\\mathrm{Tr}},\\cdot)$ and $\\operatorname{hom}(\\mathcal{F}^{\\mathrm{Tr}},\\cdot)\\ \\subseteq\\mathcal{F}$ -Hom-MPNNs. Clearly, $\\mathcal{F}^{\\mathrm{Tr}}\\,\\subset\\,\\mathcal{M}^{r+2}$ . Hence, $r.$ -\u2113WL $\\subseteq\\mathrm{hom}(\\mathcal{M}^{r+2},\\cdot)\\subseteq\\mathrm{hom}(\\mathcal{F}^{\\mathrm{Tr}},\\cdot)$ . Hence, $r$ -\u2113WL is more powerful than $\\mathcal{F}$ -Hom-MPNNs. ", "page_idx": 40}, {"type": "text", "text": "H.2 Appendix on Subgraph GNNs and Proof of Corollary 2 ii) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Subgraph GNNs treat a graph as a collection of graphs $\\{G^{u}\\mid u\\in N(v)\\}$ , where $G^{u}$ is a graph obtained by marking the corresponding node $u$ . For every graph $G^{u}$ it runs an independent WL", "page_idx": 40}, {"type": "text", "text": "algorithm, i.e., ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{\\mathrm{Sub},G^{u}}^{(0)}(v)=(\\chi_{G}(v),\\mathbb{1}_{v=u}(v))}\\\\ &{\\mathbf{x}_{\\mathrm{Sub},G^{u}}^{(t+1)}(v)=g^{(t+1)}\\left(\\mathbf{x}_{\\mathrm{Sub},G^{u}}^{(t)}(v),f^{(t+1)}\\left(\\mathbf{x}_{\\mathrm{Sub},G^{u}}^{(t)}(w)\\mid w\\in N_{G}(v)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Here, $g^{(t)}$ and $f^{(t)}$ represent the update and aggregation functions at depth $t$ , respectively. The final node representations after $t$ rounds are then calculated by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{\\mathrm{Sub},G}^{t}(u)=h\\left(\\mathbf{x}_{\\mathrm{Sub},G^{u}}^{(t)}(v)\\mid v\\in V(G)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "H.2.1 Expressivity of Subgraph GNNs and Comparison with $r$ -\u2113WL ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The expressivity of subgraph GNNs is fully characterized by the class ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}^{\\mathrm{sub}}:=\\{F\\;\\vert\\;\\exists u\\in V(F)\\;\\mathrm{s.t.}\\;F\\setminus\\{u\\}\\mathrm{~is~a~forest}\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "i.e., Subgraph GNNs can separate a pair of graphs $G,H$ if and only if $\\operatorname{hom}({\\mathcal{F}}^{\\mathrm{sub}},G)\\ \\neq$ $\\operatorname{hom}(\\mathcal{F}^{\\mathrm{sub}},H)$ . Furthermore, the set $\\mathcal{F}^{\\mathrm{sub}}$ is the maximal set that satisfies this property (B. Zhang et al., 2024, Theorem 3.4). We restate Corollary 2 ii) and provide a proof. ", "page_idx": 41}, {"type": "text", "text": "Corollary 5. 1-\u2113WL is not less powerful than Subgraph GNNs. In particular, any $r$ -\u2113WL can separate infinitely many graphs that Subgraph GNNs fail to distinguish. ", "page_idx": 41}, {"type": "text", "text": "Proof. We show that already 1-\u2113WL can separate infinitely many graphs that Subgraph GNNs fail to distinguish. The other statements then follow as a simple corollary of Proposition 1. ", "page_idx": 41}, {"type": "text", "text": "For clarity, we begin by demonstrating that there exists a pair of graphs that 1-\u2113GIN can separate, but Subgraph GNNs cannot distinguish. Consider the graph $F$ defined as follows: $F=\\{\\oint\\!\\!\\!\\!\\infty-\\infty\\!\\!\\!\\}$ . It holds that $F\\in\\mathcal{M}^{3}\\setminus\\mathcal{F}^{\\mathrm{sub}}$ , where ${\\mathcal{F}}^{\\mathrm{sub}}$ is the maximal set that Subgraph GNNs can homomorphismcount. Then, by (B. Zhang et al., 2024, Theorem 3.4), there exists a pair of graphs $G(F)$ and $H(F)$ such that $\\operatorname{hom}(F,G(F))\\neq\\operatorname{hom}(F,H(F))$ and $\\operatorname{hom}(\\mathcal{F}^{\\mathrm{sub}},G(F))\\,=\\,\\hom(\\mathcal{F}^{\\mathrm{sub}},H(F))$ . Hence, Subgraph GNNs cannot separate $G(F)$ and $H(F)$ . Since $F\\in\\dot{M}^{3}$ , by $\\operatorname{hom}(F,G(F))\\neq$ $\\mathrm{hom}(F,H(\\bar{F}))$ and Theorem 2, 1-\u2113WL can separate $G(F)$ and $H(F)$ . ", "page_idx": 41}, {"type": "text", "text": "This argument can be repeated for every $F\\in\\mathcal{M}^{3}\\setminus\\mathcal{F}^{\\mathrm{sub}}$ . Since there are infinitely many graphs in $\\mathcal{M}^{3}\\setminus\\mathcal{F}^{\\mathrm{sub}}$ , the corollary follows. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "We mention that the construction of the pair of graphs $G(F)$ and $H(F)$ in the previous proof is based on (twisted) F\u00fcrer graphs and is largely motivated by the constructions by F\u00fcrer (2001) and B. Zhang et al. (2024). More precisely, we can define $G(F)$ as the F\u00fcrer graph of $F$ and $H(F)$ as the corresponding twisted F\u00fcrer graph. See Figure $\\mathrm{7c}$ for a visualization of $F,G(F)$ , and $H(F)$ . ", "page_idx": 41}, {"type": "text", "text": "H.3 Appendix on Subgraph $k$ -GNNs and Proof of Corollary 2 iii) ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Qian et al. (2022) introduced a higher-order version of Subgraph GNNs that compute representations for subgraphs made of tuples of nodes. Specifically, Subgraph $k$ -GNNs \u2013 referred to as vertexsubgraph $k$ -OSANs in the original work (Qian et al., 2022) \u2013 treat a graph as a collection of graphs $\\{G^{\\Breve{\\mathbf{u}}}\\mid\\mathbf{\\dot{u}}\\in V(G)^{k}\\}$ , where $G^{\\mathbf{u}}$ is a graph obtained by marking the corresponding nodes $\\mathbf{u}$ . For every graph $G^{\\mathbf{u}}$ it runs an independent WL-algorithm, i.e., ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{\\mathrm{Sub}(k),G^{\\mathrm{u}}}^{(0)}(v)=(\\chi_{G}(v),\\mathrm{atp}(\\mathbf{u}),\\mathbb{1}_{v=u_{1}}(v),\\dots,\\mathbb{1}_{v=u_{k}}(v))}\\\\ &{\\mathbf{x}_{\\mathrm{Sub}(k),G^{\\mathrm{u}}}^{(t+1)}(v)=g^{(t+1)}\\left(\\mathbf{x}_{\\mathrm{Sub}(k),G^{\\mathrm{u}}}^{(t)}(v),f^{(t+1)}\\left(\\mathbf{x}_{\\mathrm{Sub}(k),G^{\\mathrm{u}}}^{(t)}(w)\\mid w\\in N_{G}(v)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Here, $g^{(t)}$ and $f^{(t)}$ represent the update and aggregation functions at depth $t$ , respectively. For every $k$ -tuple $\\mathbf{u}$ , the final representations after $t$ rounds are then calculated by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{x}_{\\mathrm{Sub}(k),G}^{(t)}(\\mathbf{u})=h\\left(\\mathbf{x}_{\\mathrm{Sub}(k),G^{\\mathbf{u}}}^{(t)}(v)\\mid v\\in V(G)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The final graph representation is then given by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{x}_{\\mathrm{Sub}(k)}^{(t)}(G)=j\\left(\\mathbf{x}_{\\mathrm{Sub}(k),G}^{(t)}(\\mathbf{u})\\mid\\mathbf{u}\\in V(G)^{k}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The homomorphism-expressivity of these GNNs is characterized as follows: ", "page_idx": 41}, {"type": "text", "text": "Theorem 7 (B. Zhang et al., 2024). The homomorphism-expressivity of Subgraph $k$ -GNN is given by $\\mathcal{F}^{\\mathrm{sub}(k)}=\\{F:\\exists U\\subset V_{F}$ s.t. $|U|\\le k$ and $F\\setminus U$ is a forest $\\}$ . ", "page_idx": 42}, {"type": "text", "text": "Since the set $\\mathcal{F}^{\\mathrm{sub}(k)}$ is the maximal set of graphs that Subgraph $k$ -GNNs can homomorphism-count, we can derive the following corollary, which restates Corollary 2 iii) and provides a proof. ", "page_idx": 42}, {"type": "text", "text": "Corollary 6. For any $k\\geq1$ , 1-\u2113WL is not less powerful than Subgraph $k$ -GNNs. In particular, any $r$ -\u2113WL can separate infinitely many graphs that Subgraph $k$ -GNNs fail to distinguish. ", "page_idx": 42}, {"type": "text", "text": "Proof. The proof parallels the proof of Corollary 2. We present it here for completeness. ", "page_idx": 42}, {"type": "text", "text": "Let $k\\geq1$ . We will show that there exists a pair of graphs that 1-\u2113GIN can distinguish, but Subgraph $k$ -GNNs cannot. Consider the graph $F$ , defined as the unique graph with $k$ triangle graphs, all connected by an edge to a single node. ", "page_idx": 42}, {"type": "text", "text": "It holds that $F\\,\\in\\,{\\mathcal{M}}^{3}\\setminus{\\mathcal{F}}^{\\mathrm{sub}(k)}$ , where $\\mathcal{F}^{\\mathrm{sub}(k)}$ is the maximal set that Subgraph $k$ -GNNs can homomorphism-count. Then, by (B. Zhang et al., 2024, Theorem 3.8), there exists a pair of graphs $G(F)$ and $H(F)$ such that $\\operatorname{hom}(F,G(F))\\neq\\operatorname{hom}(F,H(F))$ and $\\operatorname{hom}(\\mathcal{F}^{\\mathrm{sub}(k)},G\\bar{(F)})=$ $\\operatorname{hom}(\\mathcal{F}^{\\mathrm{sub}(k)},H(F))$ . Hence, Subgraph $k$ -GNNs cannot separate $G(F)$ and $H(F)$ . Since $F\\in\\mathcal{M}^{3}$ , by $\\operatorname{hom}(F,G(F))\\neq\\operatorname{hom}(F,H(F))$ and Theorem 2, 1-\u2113WL can separate $G(F)$ and $H(F)$ . ", "page_idx": 42}, {"type": "text", "text": "This argument can be repeated for every $F\\,\\in\\,{\\mathcal{M}}^{3}\\,\\backslash\\,{\\mathcal{F}}^{\\mathrm{sub}\\left(k\\right)}$ . Since there are infinitely many non-isomorphic graphs in $\\mathcal{M}^{3}\\setminus\\mathcal{F}^{\\mathrm{sub}(k)}$ , the corollary follows. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "H.4 Proof of Corollary 2 iv) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Proof of Corollary $2\\ i\\nu$ ). Given graphs $F$ and $G$ , it is well-known (see, e.g., (Neuen, 2024; Curticapean et al., 2017)) that sub $(F,G)$ can be decomposed as: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{sub}(F,G)=\\sum_{F^{\\prime}\\in\\mathrm{spasm}(F)/\\sim}\\alpha(F^{\\prime})\\mathrm{hom}(F^{\\prime},G).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here, the sum ranges over all non-isomorphic graphs in $\\operatorname{spasm}(F)$ . The sum in (20) is finite since the homomorphic image of $F$ has at most $|V(F)|$ nodes. Per assumption, we have spasm $.(F)\\subset\\mathcal{M}^{r+2}$ , i.e., by Theorem 2, $r$ -\u2113WL can homomorphism-count $\\operatorname{ipasm}(F)$ . In particular, if $r$ -\u2113WL cannot separate two graphs $G$ and $H$ , we have $\\operatorname{hom}(\\operatorname{spasm}(F),G)\\,=\\,\\operatorname{hom}(\\operatorname{spasm}(F),H)$ , and hence, $\\operatorname{sub}(F,G)=\\operatorname{sub}(F,H)$ . ", "page_idx": 42}, {"type": "text", "text": "The result on subgraph-counting paths follows directly as the homomorphic image of a path $P_{r+3}$ of length $r+3$ lies in ${\\mathcal{M}}^{r}$ . \u53e3 ", "page_idx": 42}, {"type": "text", "text": "I Appendix for Section 6 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Theorem 3. For fixed t, $r\\geq0$ , t iterations of $r$ -\u2113WL are more powerful than $r$ -\u2113MPNN with t layers.   \nConversely, $r$ -\u2113MPNN is more powerful than r-\u2113WL if the functions $\\boldsymbol{f}^{(t)},\\boldsymbol{g}^{(t)}$ in (3) are injective. ", "page_idx": 42}, {"type": "text", "text": "Proof of Theorem 3. We begin by proving that $c_{r}^{(t)}\\subseteq h_{r}^{(t)}$ . We argue by induction over $t$ for any fixed $r\\geq0$ . ", "page_idx": 42}, {"type": "text", "text": "Initially, $c_{r}^{(0)}\\;=\\;h_{r}^{(0)}$ as both labeling functions start with the same base labels. Now assume   \nc(rt+1)(u) = c(rt+1)(v) for some u, v \u2208V (G). By definition,   \n$\\mathrm{IASH}\\left(c_{r}^{(t)}(u),\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\,|\\,\\mathbf{p}\\in N_{0}(u)\\right\\}\\right\\},\\ldots\\right)=\\mathrm{HASH}\\left(c_{r}^{(t)}(v),\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\,|\\,\\mathbf{p}\\in N_{0}(v)\\right\\}\\right\\},\\ldots\\right).$   \nThis implies $c_{r}^{(t)}(u)=c_{r}^{(t)}(v)$ and $\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\,|\\,\\mathbf{p}\\in\\mathcal{N}_{k}(u)\\right\\}\\right\\}=\\left\\{\\left\\{c_{r}^{(t)}(\\mathbf{p})\\,|\\,\\mathbf{p}\\in\\mathcal{N}_{k}(v)\\right\\}\\right\\},\\quad\\forall k\\in\\{0,\\ldots,r\\}\\,,$ ", "page_idx": 42}, {"type": "text", "text": "as HASH is an injective function. ", "page_idx": 42}, {"type": "text", "text": "By induction hypothesis, we hence have $h_{r}^{(t)}(u)=h_{r}^{(t)}(v)$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left\\{\\left\\{h_{r}^{(t)}({\\bf p})\\,|{\\bf p}\\in\\mathcal{N}_{k}(u)\\right\\}\\right\\}=\\left\\{\\left\\{h_{r}^{(t)}({\\bf p})\\,|{\\bf p}\\in\\mathcal{N}_{k}(v)\\right\\}\\right\\},\\quad\\forall k\\in\\{0,\\ldots,r\\}\\,,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which implies that any function, in particular f k(t+1) and g(t+1) have to return the same result.   \nTherefore, we have $h_{r}^{(t+1)}(u)=h_{r}^{(t+1)}(v)$ . ", "page_idx": 43}, {"type": "text", "text": "We proceed to prove $h_{r}^{(t)}\\ \\subseteq\\ c_{r}^{(t)}$ if all message, update, and readout functions are injective in Definition 9. For this, we show that for each $t\\geq0$ there exists an injective function $\\phi$ such that $h_{r}^{(t)}=\\phi\\circ c_{r}^{(t)}$ . For $t=0$ , we can choose $\\phi$ to be the identity function. Assume that for $t-1$ there exists an injective function $\\phi$ such that $h_{r}^{(t-1)}(v)=\\phi\\circ c_{r}^{(t-1)}(v)$ . Then, we can write ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{r}^{(t)}(v)=g^{(t)}\\left(h^{(t-1)}(v),m_{0}^{(t)}(v),\\dots,m_{r}^{(t)}(v)\\right)}\\\\ &{\\qquad\\qquad=g^{(t)}\\left(\\phi\\circ c_{r}^{(t-1)}(v),\\phi\\circ m_{0}^{(t)}(v),\\dots,\\phi\\circ m_{r}^{(t)}(v)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where for every $q=0,\\ldots,r$ , we set $\\phi\\circ m_{q}^{(t)}(v):=\\left\\{\\left\\{\\left(\\phi\\circ c_{r}^{(t-1)}(\\mathbf{p})\\right)\\mid\\mathbf{p}\\in\\mathcal{N}_{q}(v)\\right\\}\\right\\}$ and $(\\phi\\circ$ $c_{r}^{(t-1)}(\\mathbf{p}))=(\\phi\\circ c_{r}^{(t-1)}(p_{1}),\\ldots,\\phi\\circ c_{r}^{(t-1)}(p_{q+1}))$ for $\\mathbf{p}=\\{p_{i}\\}_{i=1}^{q+1}\\in\\mathcal{N}_{q}(v)$ . By assumption, all message, update, and readout functions are injective in Definition 9. Since the concatenation of injective functions is injective, there exists an injective function $\\psi$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{h_{r}^{(t)}(v)=\\psi\\Bigg(c_{r}^{(t-1)}(v),\\Big\\{\\Big\\{c_{r}^{(t-1)}({\\bf p})\\ |\\ {\\bf p}\\in\\mathcal{N}_{0}(v)\\Big\\}\\Big\\}\\,,\\ \\ \\ }\\\\ &{}&{\\Big\\{\\Big\\{c_{r}^{(t-1)}({\\bf p})\\ |\\ {\\bf p}\\in\\mathcal{N}_{1}(v)\\Big\\}\\Big\\}\\,,\\ \\ \\ }\\\\ &{}&{\\vdots\\,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{}&{\\Big\\Big\\{\\Big\\{\\Big\\{c_{r}^{(t-1)}({\\bf p})\\ |\\ {\\bf p}\\in\\mathcal{N}_{r}(v)\\Big\\}\\Big\\}\\Big\\}\\,\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As HASH in Definition 7 is injective, the inverse $\\mathrm{HASH^{-1}}$ exists and is also injective. Hence, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{r}^{(t)}(v)=\\psi\\circ\\mathrm{HASH}^{-1}\\circ\\mathrm{HASH}\\left(c_{r}^{(t-1)}(v),\\left\\{\\left\\{c_{r}^{(t-1)}(\\mathbf{p})~|~p\\in N_{0}(v)\\right\\}\\right\\},\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left\\{\\left\\{(c_{r}^{(t-1)}(\\mathbf{p})~|~\\mathbf{p}\\in N_{1}(v)\\right\\}\\right\\},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\vdots}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left\\{\\left\\{(c_{r}^{(t-1)}(\\mathbf{p})~|~\\mathbf{p}\\in N_{r}(v)\\right\\}\\right\\}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.\\left(\\vphantom{\\frac{1}{\\sum_{i}^{i}}}\\right.\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.\\left\\{\\left\\{(c_{r}^{(t-1)}(\\mathbf{p})~|~\\mathbf{p}\\in N_{r}(v)\\right\\}\\right\\}\\right)\\right.}\\\\ &{=\\psi\\circ\\mathrm{HASH}^{-1}\\left(c_{r}^{(t)}(v)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Choosing $\\phi=\\psi\\circ{\\mathrm{HASH}}^{-1}$ finishes the proof. ", "page_idx": 43}, {"type": "text", "text": "We conclude this section with the following lemma that justifies our architectural choice in (4). Lemma 9. Let $x\\in\\mathbb{Q}^{r}$ . Then there exist $\\varepsilon\\in\\mathbb{R}^{r}$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\varphi(x)=\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "is an injective function. ", "page_idx": 43}, {"type": "text", "text": "Proof. We prove this claim by induction. For $r\\,=\\,0$ , any $x\\neq0\\in\\mathbb{R}$ fulfills the claim. Now, let $\\varepsilon\\in\\mathbb{R}^{r}$ such that $\\varphi(x):\\mathbb{Q}^{r}\\;\\dot{\\rightarrow}\\;\\mathbb{R}$ is injective. The set $\\begin{array}{r}{\\mathbb{Q}[\\varepsilon_{1},\\ldots,\\varepsilon_{r}]=\\{\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}\\,|\\,x\\in\\mathbb{Q}^{r}\\}}\\end{array}$ is ", "page_idx": 43}, {"type": "text", "text": "countable and hence a proper subset of $\\mathbb{R}$ . It follows that there exists $\\varepsilon_{r+1}\\in\\mathbb{R}$ with $\\varepsilon_{r+1}\\notin\\mathbb{Q}[\\varepsilon]$ .   \nNote that $0\\in\\mathbb{Q}$ and hence $\\varepsilon_{r+1}\\neq0$ . We now prove our claim by contradiction. ", "page_idx": 44}, {"type": "text", "text": "Assume there exist $x\\neq x^{\\prime}\\in\\mathbb{Q}^{r+1}$ with $\\begin{array}{r}{\\sum_{k=0}^{r+1}\\varepsilon_{k}x_{k}=\\sum_{k=0}^{r+1}\\varepsilon_{k}x_{k}^{\\prime}}\\end{array}$ . We distinguish two cases: ", "page_idx": 44}, {"type": "text", "text": "$x_{i}=x_{i}^{\\prime}$ for all $i\\le r$ and $x_{r+1}\\neq x_{r+1}^{\\prime}$ : Then immediately ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{r+1}\\neq x_{r+1}^{\\prime}}\\\\ {\\Rightarrow}&{\\varepsilon_{r+1}x_{r+1}\\neq\\varepsilon_{r+1}x_{r+1}^{\\prime}}\\\\ {\\Rightarrow}&{\\displaystyle\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}+\\varepsilon_{r+1}x_{r+1}\\neq\\displaystyle\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}+\\varepsilon_{r+1}x_{r+1}^{\\prime}}\\\\ {\\Rightarrow}&{\\displaystyle\\sum_{k=0}^{r+1}\\varepsilon_{k}x_{k}\\neq\\displaystyle\\sum_{k=0}^{r+1}\\varepsilon_{k}x_{k}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}\\neq\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}^{\\prime}}\\end{array}$ : But then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}+\\varepsilon_{r+1}x_{r+1}=\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}^{\\prime}+\\varepsilon_{r+1}x_{r+1}^{\\prime}}}\\\\ {\\Leftrightarrow}&{\\displaystyle\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}-\\sum_{k=0}^{r}\\varepsilon_{k}x_{k}^{\\prime}=\\varepsilon_{r+1}(x_{r+1}^{\\prime}-x_{r+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The left hand side is an element of $\\mathbb{Q}[\\varepsilon_{1},\\ldots,\\varepsilon_{r}]$ . However, $\\varepsilon_{r+1}(x_{r+1}^{\\prime}-x_{r+1})\\notin\\mathbb{Q}[\\varepsilon_{1},\\dots,\\varepsilon_{r}]$ by choice of $\\varepsilon_{r+1}$ , leading to a contradiction. ", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We mention our main results in the abstract and introduction (see Section 1), and provide proofs for all claims (see Appendix E ff.) and code for all experiments (see our GitHub repository). We clearly state the scope of our theoretical results and emphasize that our method provides an expressive architecture suitable for sparse real-world graphs. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We describe the computational complexity of our method in a separate paragraph (see Section 6). We specifically address its preprocessing complexity on dense graphs in a separate paragraph (see Section 7). We also mention important future work that would complement our theoretical contributions in the conclusion section (see Section 8). ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: All theorems are numbered, and we refer to the formal proofs in the appendix (see Appendix E ff.). ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All hyperparameters, hyperparameter ranges, and used resources are given in Appendix C. Our GitHub repository provides instructions on how to reproduce all experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 46}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: All hyperparameters, hyperparameter ranges, and used resources are given in the appendix (see Appendix C). Our GitHub repository contains instructions to reproduce all experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Data splits, hyperparameters, hyperparameter grids (if used), optimizer, etc. are detailed in the main paper (see Section 7) and appendix (see Appendix C), with additional instructions provided in the anonymous GitHub link (see our GitHub repository). ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We report error bars and confidence intervals for our experimental results (see Section 7), capturing the variability due to different random seeds and initialization. Detailed information about statistical significance tests is provided in the appendix. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: All information on the compute resources is given in the appendix (see Appendix C). For the real-world experiments, we provide run times, and for the synthetic experiments, the run times are negligible. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We respect the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: While our work has numerous potential societal consequences, none of which we feel must be explicitly emphasized here, its impact lies in providing a theoretical foundation for improved graph representation learning methodologies. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our paper does not release data or models with a high risk for misuse. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All used assets are properly cited, and the licenses are mentioned. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We introduce new assets in the form of code. All details about training, datasets, and models are given in the submitted paper and/or in our GitHub repository, including detailed instructions to run the code. The license is provided under the MIT license. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]