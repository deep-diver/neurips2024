[{"figure_path": "4DA5vaPHFb/tables/tables_6_1.jpg", "caption": "Table 1: LUV comparison of ENOT on CelebA64 tasks from the Wasserstein-2 benchmark. The attributes after the method names ('Cycle', 'Objective', 'Regression') correspond to the type of amortisation loss. Column \u2018Conjugate\u2019 indicates the selected optimizer for the internal fine-tuning of c-conjugate transform. The results of our method include the mean and the standard deviation across 3 different seeds. The best scores are highlighted.", "description": "This table compares the performance of ENOT against several state-of-the-art methods on the CelebA64 dataset from the Wasserstein-2 benchmark.  The comparison uses the LUV metric to quantify the quality of the optimal transport plan approximation. Three different optimizers (None, L-BFGS, Adam) and three different loss types ('Cycle', 'Objective', 'Regression') are used for the competing methods.  The table shows the mean and standard deviation of the LUV score across three different seeds for each method.  The best results are highlighted.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_7_1.jpg", "caption": "Table 2: LUV comparison of ENOT with baseline methods on the high-dimensional (HD) tasks from Wasserstein-2 benchmark. The suffixes (\u2018Cycle\u2019, \u2018Objective\u2019, \u2018Regression\u2019) correspond to the type of amortisation loss. Column \u2018Conjugate\u2019 indicates the selected optimizer for the internal fine-tuning of c-conjugate transform. D is the dimension of the measures domain. The mean and the standard deviations of our method are computed across 5 different seeds. The best scores are highlighted.", "description": "This table compares the performance of the proposed ENOT method against several baseline methods on high-dimensional tasks from the Wasserstein-2 benchmark.  The comparison is based on the LUV (Unexplained Variance Percentage) metric, which measures the deviation of the estimated optimal transport plan from the ground truth.  Different optimizers for computing the c-conjugate transform are also considered.  The results show the mean and standard deviation of the LUV metric across five different seeds for each method and varying dimensions (D). The best performing method for each dimension is highlighted.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_7_2.jpg", "caption": "Table 2: LUV comparison of ENOT with baseline methods on the high-dimensional (HD) tasks from Wasserstein-2 benchmark. The suffixes (\u2018Cycle\u2019, \u2018Objective\u2019, \u2018Regression\u2019) correspond to the type of amortisation loss. Column \u2018Conjugate\u2019 indicates the selected optimizer for the internal fine-tuning of c-conjugate transform. D is the dimension of the measures domain. The mean and the standard deviations of our method are computed across 5 different seeds. The best scores are highlighted.", "description": "This table compares the performance of the proposed ENOT method against several baseline methods on high-dimensional tasks from the Wasserstein-2 benchmark.  The comparison is based on the LUV (Unexplained Variance Percentage) metric, which measures the deviation from the optimal transport plan. Different optimization strategies (Cycle, Objective, Regression) and conjugate optimizers are used for the baselines, while ENOT uses no conjugate optimization.  The table shows the LUV scores for different dimensions (D) of the measure space, highlighting ENOT's superior performance and efficiency.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_8_1.jpg", "caption": "Table 3: Comparison of ENOT to baseline methods for image-to-image translation. We evaluate generation task between two different datasets: Source \u21d2 Target. And compare resulting images based on Frechet Inception Distance (FID) and Mean Squared Error (MSE). Empty cells indicate that original authors of particular method did not include results for those tasks.", "description": "This table compares the performance of ENOT against several other methods for image-to-image translation.  The comparison uses two metrics: FID (Frechet Inception Distance) and MSE (Mean Squared Error) across four different image translation tasks. Lower values are better for both metrics. Note that some baselines did not report results for all tasks.", "section": "5.3 Unpaired Image-to-Image Translation"}, {"figure_path": "4DA5vaPHFb/tables/tables_8_2.jpg", "caption": "Table 3: Comparison of ENOT to baseline methods for image-to-image translation. We evaluate generation task between two different datasets: Source \u21d2 Target. And compare resulting images based on Frechet Inception Distance (FID) and Mean Squared Error (MSE). Empty cells indicate that original authors of particular method did not include results for those tasks.", "description": "This table compares the performance of ENOT against other methods for image-to-image translation tasks.  The metrics used for comparison are FID (Frechet Inception Distance) and MSE (Mean Squared Error). The results show ENOT's performance relative to CycleGAN, StarGAN, Extremal OT, and Kernel OT across various image datasets and resolutions.", "section": "5.3 Unpaired Image-to-Image Translation"}, {"figure_path": "4DA5vaPHFb/tables/tables_15_1.jpg", "caption": "Table 1: LUV comparison of ENOT on CelebA64 tasks from the Wasserstein-2 benchmark. The attributes after the method names ('Cycle', 'Objective', 'Regression') correspond to the type of amortisation loss. Column \u2018Conjugate\u2019 indicates the selected optimizer for the internal fine-tuning of c-conjugate transform. The results of our method include the mean and the standard deviation across 3 different seeds. The best scores are highlighted.", "description": "This table compares the performance of ENOT against other state-of-the-art methods on the CelebA64 dataset from the Wasserstein-2 benchmark.  The comparison is based on the LUV metric (Unexplained Variance Percentage), which measures the deviation from the optimal transport plan.  Different optimization strategies and methods for approximating the c-conjugate transform are evaluated. The table shows mean and standard deviation of LUV across multiple runs for each method.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_15_2.jpg", "caption": "Table 5: Hyperparameters for Synthetic 2D datasets from (Rout et al. [2021])", "description": "This table shows the hyperparameter settings used for the synthetic 2D datasets experiments from the Rout et al. (2021) paper.  It details the specific configurations of the potential model, conjugate model, hidden layers, training iterations, activation function, optimizers, Adam beta parameters, initial learning rate, expectile coefficient, expectile tau, and batch size.", "section": "5.2 Different Cost Functionals"}, {"figure_path": "4DA5vaPHFb/tables/tables_15_3.jpg", "caption": "Table 6: Hyperparameters for CelebA64 Wasserstein-2 benchmark tasks.", "description": "This table lists the hyperparameters used for the CelebA64 Wasserstein-2 benchmark task.  It includes specifications for the potential and conjugate models, the number of hidden layers, training iterations, activation function, optimizers (with their beta parameters), initial learning rate, expectile coefficient (\u03bb), expectile \u03c4, and batch size.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_16_1.jpg", "caption": "Table 1: LUV comparison of ENOT on CelebA64 tasks from the Wasserstein-2 benchmark. The attributes after the method names ('Cycle', 'Objective', 'Regression') correspond to the type of amortisation loss. Column \u2018Conjugate\u2019 indicates the selected optimizer for the internal fine-tuning of c-conjugate transform. The results of our method include the mean and the standard deviation across 3 different seeds. The best scores are highlighted.", "description": "This table presents a comparison of the Expectile-Regularized Neural Optimal Transport (ENOT) method with other state-of-the-art approaches on the CelebA64 tasks from the Wasserstein-2 benchmark.  The comparison is based on the LUV (Unexplained Variance Percentage) metric, which measures the deviation from the optimal transport plan.  The table shows the LUV scores for different methods, broken down by the type of amortisation loss used and the optimizer employed for fine-tuning the c-conjugate transform.  The best-performing method is highlighted.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_16_2.jpg", "caption": "Table 8: Runtime comparison for different layers sizes between W2OT (Amos [2023]) with default hyperparameters and ENOT on synthetic 2D data on tasks from Rout et al. [2021].", "description": "This table compares the runtime of the proposed ENOT method against the W2OT method from Amos (2023) for different numbers of hidden layers in the multi-layer perceptron (MLP) architecture. It demonstrates the significant speed improvement achieved by ENOT compared to W2OT, especially as the number of layers increases.  The speed-up highlights one of the key advantages of ENOT: efficiency in training.", "section": "F Results on Synthetic 2D Datasets"}, {"figure_path": "4DA5vaPHFb/tables/tables_17_1.jpg", "caption": "Table 9: Comparison of runtimes (in minutes) against the baseline (W2OT-Objective L-BFGS) on the high-dimensional (HD) tasks from the Wasserstein-2 benchmark with same networks architecture.", "description": "This table compares the computation times of the proposed ENOT method against the baseline W2OT-Objective L-BFGS method for high-dimensional tasks from the Wasserstein-2 benchmark.  It demonstrates the significant speedup achieved by ENOT across various dimensions (D).  The same network architecture was used for a fair comparison.", "section": "5.1 Results on Wasserstein-2 Benchmark"}, {"figure_path": "4DA5vaPHFb/tables/tables_17_2.jpg", "caption": "Table 10: Performance of ENOT with varying levels of expectile hyperparameter \u03c4 on W2 benchmark (1st column), showcasing intuition on convergence as \u03c4 \u2192 1; Synthetic 2D data (2nd column); Image-to-Image translation FID (3rd column), and MSE (4th column).", "description": "This table presents the performance of the ENOT model on various tasks with different values of the expectile hyperparameter \u03c4.  It shows how the model's performance (measured by LUV, Wasserstein distance, FID, and MSE) changes as \u03c4 approaches 1, illustrating the effect of this hyperparameter on model stability and accuracy. The results are presented for four different evaluation metrics across various datasets, highlighting the impact of expectile regularization and its influence on the model's behavior.", "section": "5.4 Ablation Study: Varying hyperparameters expectile and regularization weight"}]